# ISB AMPBA Data Science Machine Learning AI DL Notes

These notes will eventually go to kaggle as a data science guide.

# K-Fold Cross Validation in Python

We can improve model's performance by changing parameters of model.


# Personal Leadership - ISB Final Classes

D.V.R. Seshadri, ISB

i am gift to the world the way I am.

you can be a leader of yourself, no need to be MD or CEO. Coder can be a leader.

thought - get a thought in mind, swatch bharat

strategic - make a strategy to execute, 11m toilets

entrepreneurial - make people build

people leadership - educate people to use app / toilet

self leadership - personal leadership - most required for above leaderships.

iq - intellectual quotient
eq - emotional
sq - spiritual - max for personal leadership, how do you face tsunami of life?

What make you happy

- self satisfaction
- surrounded by friends and family
- connecting with nature

References:

- neem karoli baba and the train story
- book - The Executive and the Elephant: A Leader's Guide for Building Inner Excellence
Book by Richard L. Daft
- Oxford time management - <https://www.ox.ac.uk/students/academic/guidance/skills/time>
- Scientist's Search for Truth Paperback ‚Äì 1 December 2005
by Swami Virajeshwara (Author)




# Story Telling ST

Emotions along with the functionality for a product. Emotions create a good start.. people listen when you say I am from ISB. ST is like swimming, you have to do and learn, can't learn by watching videos.

Story retains in mind of people, we remember thirsty crow story but not pascals law.

Take 1: business presentation - the problem, the solution, the architecture, the impact, thank you

Take 2: Story - here is the story of marc, marc is manager in a retail store, he spends 50% time just arranging items, and with our solution now he is free and can spend time with customers. Story can also be around Owner, Manager, Consumer or between two stores?

Status update can be boring when BAU update is give, like business as usual.

Add emotion in story, like Suresh ki dukaan nhi chalti thi, wo dept me gya but using Our app he has no time to sit back and has earned a car and house.

5 ways to begin a story

- start with quote and contextualize it, connect the quote with context, take a stand on it. A quote with picture makes a big impact.
- start with a set up question
- tell an anecdote - a short amusing or interesting story about a real incident or person. specially related to audience.
- share an interesting fact or data -
- imagine a world where -

Presentation to story is facts to emotions. checklist:

- does it has emotions
- does it build a common groung
- does it engage a visual brain?
- it is truth , well told?
- Does it have a contrast? - achoring effect?

Develop a niggle around it.

References:

- Obama speaks to Israel - <https://www.youtube.com/watch?v=Oxfw3ZfBx6I>
- <https://economictimes.indiatimes.com/tech/information-tech/sebi-bans-ex-infy-wipro-staffers-in-insider-trading-case/articleshow/86632276.cms?from=mdr#:~:text=Two%20former%20employees%20of%20IT,US%2Dbased%20investment%20advisor%20Vanguard>.
- hacker asks 18m - <https://www.bbc.com/news/technology-53214783>







# ISB T6 Pricing Analytics

Prof. Abhinav Uppal

**Course Objectives**

Pricing is one of the most powerful tools available with businesses to maximize their profits. Businesses spend a lot of resources to enhance the value of their product offerings through innovation, advertising, promotions, sales force effort and working with their channel partners. Implementing an effective pricing strategy is critical for businesses to capture that value back into the business. However, managers often under-attend to the pricing of their products, failing to tap into this opportunity to boost their profits.

The objective of this course is to help you develop a systematic framework for assessing and formulating effective pricing strategies for businesses across a variety of contexts. Pricing decisions require careful attention to economic, marketing, organizational and psychological factors. We will learn relevant concepts and methods, and explore new approaches that will help you make better pricing decisions while keeping these factors in mind.

The course will use a combination of lectures, in-class case discussions and assignments that will help you understand and apply the concepts that we will develop in class. The list of topics covered is detailed in the course schedule.

**Learning Goals**

Critical and Integrative Thinking: Each student shall be able to identify key issues in a business setting, and develop a perspective that is supported with relevant information and integrative thinking, to draw and assess conclusions.

Effective Oral Communication: Each student shall be able to communicate verbally in an organized, clear and persuasive manner, and be a responsive listener.

Interpersonal Awareness and Working in Teams: Each student shall demonstrate an ability to work effectively in a team, exhibiting behavior that reflects an understanding of the importance of individual roles and tasks, and the ability to manage conflict and compromise, so that team goals are achieved.

**Course Materials and Reference Books**

- Lecture notes, cases, problem sets and supplementary readings on LMS.
- Nagle, Hogan and Zale, The Strategy and Tactics of Pricing, Pearson.
- Raju and Zhang, Smart Pricing, Prentice Hall.


## Session 1




company make product, we promote to enhace value, value gets back to business for salary and all.

pricing is balancing act.

important now becuase of new ideas in market.

Top-line is revenue, bottom line is the profit.

New startups want to grow top-line, its not onlyu about profit but also about revenue.

Role of pricing, why important:

impact on profit by price is max, compared to fixed cost, vol, varianle cost.

### Role of cost in pricing decisions

when i set price i set quantity, low means more buy, high means less buy. Willingness to pay is different. quantity change brings cost. even monopolist have to consider the cost besucase of relateion with quantitu.

Marginal Revenue is the revenue gained by producing one additional unit of a product or service. Marginal Cost is the cost added by producing one additional unit of a product or service.

not always sure of marginal revenue.

costs:

- fixed w.r.t. quantity sold


what ever is the cost we have to bring that cost out from market by pricing.

when we are giving discount then also we make adv and that incurs cost.

Book publisher eg:

selling will reduce inventory cost, cost to store the books.

it can bring in opportunity cost, like sell another book which can give more profit..

consultant missing:

- consumer will wait for sale.

Linking cost structure to pricing and profit

- price elasticity (of demand): how demand changes by price.

PED = delta demand % / delta price %

PED > 1 elastic, else inelastic

elasticity of revenue EOR:

% chng in revenue  / % chng in price

EOR = 1 + EOD

in elastic demand revenue inc with proce ince.. else opposit,

break even profit elasticity, = -1, EOR = 0/

where price change will work:

Break even Profit elasticiy = -1 / (current % margin * % price change)

hence, price change = % delta Q / % delta P


### Value supremem example

sell chicken below whole sale, chicken sale is related to ther prodcuts, which have difffernet % margin,

how much more chicken to sell?

2.2 is ratio, make 2.2 times the quantity,

### price sensitivity

- customer service.

Price elasticity of demand = % delta Q/ % delta p. compute elsticity dematn

by using regression, linear. get price elasticity number.

using regression unit sales against price.

eg, quantity = b0 + b1 price

price coeff b1 is +ve, generally -ve. price up qnatity down.

+ve it can be only fo rluxury good. price high then demand high. status changes fo prodcut.

if we add week to analysis then, price coeff goes -ve

we can detrend the data to improve model further.

Let also see log log model, then find price elasticiy.

Elasticity?
 detrend data with time.

linear demad =

$$ q_i = a - bp_i + ct_i + e_i $$

=> POD = -bp/q

in linear demand, price elsticity is not constant, change with price level.

for last record, pod = -0.47 <= -38 * 12.5/1000

pod = -0.47, is less than 1 hence inelastic

since in elastic , henc to increase revenue, increase price.


by how much to inc, till you reach 1.

log-log model

log q = log a -b log q

=> POD = -b

Analysis:

- hsitorically get contect,
- but real world prob needs to be handeled , be aware.

-


## Session 2

### Effect of cost change on price

Google it we looked at brek even profit elsticity convept.

what happens when cost changes for a firm?

fuel amy go up, cost amy inc. what proportion change is passed on to customers, up or down.

demand = p = 500 - 10q

marginal revenue is kitna milega, marginal cost is kitna lagega, on chaging quantity.

if marginal revenue = marginal cost, then profit is max.

if MR > MC, then we cna dec price to sell more.

if MR < MC, then inc price.

COncave convex demand cureve had different changes on price change.

### Quanitty discounts

buy 2 get one free. big maggie price, more discount.

the more you sell, the more margin you make

sell more bring production cost down.

double marginalization prob - google it.

manufacturer gives quantity doscoutn to retailer.


## Two part pricing

above cna be solved by this., tow pricing part, pfront free, then additino fee, which is usage fee. Like cab and telecom, fixed + Usage charges.

not always to be used, the person who uses more can have more upfront fix price.

Xerox prob:

- two groups, with different usgae and pricing, find optimal price for them, to two stage pricing?

this tells why one can go after only premium segment.

 We can equate to find optimal , R and p, which is fixed cost and price of a copy.

 so by two part pricing i am bale to extract willngness to pay from two differnet segmaetns, hence mroe profit.


if we have 3 segments, then we can have more var introduced, or increase part pricing, how muc surplus you cna extract. Single pricing, flat, like netflix also has benefit and varibale multi-part fee also has benefit, it all depends on cases.


## Session 3 (2hr)

Product Line Pricing

We hvae different type, iphones, oyos, cards, dropbox plans,

Contemporaneous v s tempopral

temporal price discrimination ,here consumers are separated into different groups with different demand elasticities by charging different price at different points in time.


### Cambridge software

produt line and pricing?

Assumptions:

- student version can be bought by anyobne

for different software versions, we can find:

- price for different segment
- then weather differetn segment will buy or not,
- the quantity, variable cost, sidtribution cost, fixed cost and finally profit.
- then in each version, we get max profit segment.

max profit doesnot mean the version is profitable and should be developed.

a segment may be willing to buy but weather we should develop that segment or not is diff que.

Surplus = WTP - Cost, determines weather one will buy and will we produe that version.

other approach is, depends on decision rule, marketing team can decide the decision to weather keep only corp or student, weather to keep big version only or studen tversion only.

There may be strategic resons and competitiors to consider, so we sould do some what-if analysis and look at competitiors and then change our considerations to cover versions that our competitors could target.

Data needed is, WTP of all segments across all versions, not that studen tonly for student version.

### Max Willing to Pay, how to measure?

1 identify attributes - research, market, domain language,

the attributes we take are usally max and min, like max 5000 songs and min 50 songs.

2 create profiles,

different values for different attributes,

3 code values, high level is 1 and low level is 0

not samples are collection of ones and zeros.

4 collect data

create 16 profiles and ask for liking from people

so ask for rank ordering paired comparisions, ratings.

NOW, if we have 6 atts, we need at least  samples to say about each of these atts. so 7 are enough, beause it can cover all scenarios.

many samples make it difficult to capture the data.

eg, 16 profile and ask for rating,

then run regression,

betas are partworths.

rating as a function of other attributes.

step 7, beta_price is reduce in utility when price goes from 249 to 99.

26 ratings go down if we inc price from 99 to 249


this way we get price per utility, we can then create models/sampes

and find total utility adn then mul by price per utility to get WTP.

Step 8 now WTP across customers

so we get ratings from different customers, we do get WTP for each customer, this can vary for different cust segments, then we can find WTP for different segments and do segment analysis,

Step 9: WTP customer seg table.
























## Session 4 2hr

Temporal Pricing Strategies

strategic price changes over time

coupons, discounts

skimming - higher to lower price, like electronics, apparels.

penetration pricing - is minimum price o

- network effect is that the more number of people on a service the more value it has, eg, dropbox, uber, paytm etc.

- we can use oenetration princing when n/w effect is +ve

Short-term price discount:

- price discount can be passed on to consumer only if it is in high demand compared ot competitior.

Problem Set 3:

- marginal cost = $2

- fixed cost = $25

- w = whole seller price = 19.5

- profit_retailer = (p-w) D(p) -25

= Profit_manufacturer = (w-2) D(p)

now we have size changed to 20, 80

now we ahve to give discount.

we have retailer and manufacturer profits

we canc alcualte price by makign profit of retailer 0.

manufacture ahs to be congnizant of what retailer can  do,.

we need to set price such that both manufacturer and retailer make profit. manufacturer to make max profit and followed by retailer.

Let \$c is coupon price.

if coupon is redeemed by both segments, then it is worth it.

Hence, by price discrimination and coupon strategy we can make manufacturer's profit high and retailer's low..

Game Theory : you respond to what your competitors are doing.















# ISB T6 Financial Analytics

By: Ramana Sonti | Email: ramana_sonti@isb.edu

**Course objective:**
The objective of the course is to provide ‚Äúnew ways of thinking about risk, randomness and investments‚Äù. Though the material is always set in the context of finance, almost all the content is directly relevant to any application setting.

The course is structured also as an introduction to analytics in Finance. We will consider some of the most prominent quantitative finance problems. Each problem will be motivated and formulated along with a discussion of relevant theory.

It will help students develop basic skills in financial modeling, problem solving and quantitative analysis of risk-return tradeoÔ¨Äs. Besides, it is a good primer on using data science tools to finance problems.

We will mainly cover portfolio selection, the Capital Asset Pricing Model (CAPM), and option pricing. Time permitting, we will also look at wrangling and analyzing financial data.

Suggested books

- [BKMM] ‚ÄúInvestments‚Äù by Bodie, Kane, Marcus, and Mohanty, 8th ed, 2009, McGraw Hill
- [DL] ‚ÄúInvestment Science‚Äù by David Luenberger; Oxford University Press, 2013. (or earlier editions)

## Session 1

Old type of finance analytics. we still use linear regression. not really need bayesian DL.

### Risk and return

more risk is more return, but in structured format,

we dont spend today to save for tomorrow, that's investment.

we get return, that is mean over time. so 22% return mean the return is distributed around 22%.

we can find variance adn st dev based on the probabilities of return.

st dev is kind of risk. it tells how much our return will fluctuate.
so with same risk, more return si more happiness.

as risk increases more and more return is demanded.

casino is different, we know we will loose and go for fun.

invsestment depends on only two things (expected return and risk)

function is nothing but estimation of reality, it is mathematical model.

it is all compenstion for the risk.

when combined we find exp return and exp st dev


Risky asset weight is how much we want to invest in risky asset. 1 means all in risky.

for portfolio, st dev is linear func. exp return is linear in W. resiltant, is also linear. all possible portfolio combinations lie on the straight line.

leverage means borrowing.

que is which point on line should be picked.

risk aversion - disliking risk. (A)

now optimize this, $w^* $ is optimal weight in risky asset.

aspirations meet poortunities:

line is all possible combi of risk-free and risky investment, grey line.

we are finding optimal point in this line.

curve is indifference curve.

we find points where curve cuts line. that is the points which are feasible and makes one happy.

There is not just one risky asset, there are many.

do we even know A?

w* goes down, as A goes up, as age goes up.

Let us say, A is constant.

tomorrow, multiple risky assest. we will take n risky and boil it to one risky portfolio.


## Session 2

We cnnnot exactly model or make a fucntion of utility, hapiness etc but people are trying.


Two asset protfolio:

- if the correlation is low between two assets then the portfolio gives good return. but correlation is not in our hand.
- all possible portfolio options are on the curve.

Three asset portfolio:

- if we add z, then the possible portfolios will be on the surface.

Short selling is selling something you dont own. tht is like borrwo and sell it now. people do it to gain by using price fluctuations. Like if TV is going to be chaep in future, then sell it for 1lks now and later when it costs 50k then buy and give abck to lender.

as the number of assets increase, the std dev becomes asymtotic (converges infinitely) at a value.



## Session 3

Mental Accounts and Portfolio Optimization

$$ P(r_{ref} < 5\%) \leq 10\% $$

ask the client this question.

bequest is heritasge



## Session 4

diversification is acceptable every where. what is compensation for risk.

The Capital Asset Pricing Model (CAPM)

- gives compensation for systematic risk only.

$$ E(R_{i}) = R_{f}+\beta_{i}(E(R_{m})-R_{f}) $$

$E(R_{i})$  = capital asset expected return

$R_{f}$ = risk-free rate of interest

$\beta_i$ = sensitivity

$E(R_{m})$  = expected return of the market

this eq says if you invest in only one stock then you get compensation only of systematic invt of that stock.

Infy up down is risk. Compensation is systematic portion, i.e., pandemic, gdp etc, this goes with whole market. They will affect infy with market, this is $\beta$. now much infy is related to these overall fluctuations is $\beta$.

for eg, hotels are most affected by pandemic and economy, and gdp and recession.

insurance should be separate from investment.

Comparing portfolio std dev and beta:

- CAPM satisfies all assets, hence all points are on line.
- if P Q R have same return but diff st dev, then they have same beta.
- ONLY risk that matters is the SYSTEMATIC RISK.

Uses of CAPM



adjusted colisng price of stock is the true return, they adjust the price going backwards.

risk = sys risk + co of risk


## Session 5

options,

- risk transfer instruments,
- call/puts

option is always good but comes at a cost and they are very useful.

eg, Insurane has premium,

Option pricing:

- what price should an option have.
- what should be the premium price.

The Black-Scholes Model.

we cannot make prices normally distributed as norml dist is from - infinity to + infinty

similarly return can max go to -100% and not beyond that.

log-normal is something of which the log is a normal distribution.

Volatilty Index: <https://www.google.com/search?q=volatility+index&oq=volatility+&aqs=chrome.2.69i57j0i433l2j0l4j0i131i433j0l2.3861j0j7&sourceid=chrome&ie=UTF-8>





# ISB T6 Application of Artificial Intelligence

Instructor: Dr. Manish Gupta | Email id: manish_gupta@isb.edu | TA: Vishal Siram | Email id: aai_ampba@isb.edu

**Course Objective:**
The goal of this course is to introduce students to latest technologies which could be very relevant towards their AMPBA project as well as further work on data science in industry. Specifically, in this course, I will focus on topics like chatbots, recommendation systems, word embedding methods, and image/video analytics. While the first two topics do not involve any deep learning, word embedding methods, and image/video analytics will be further extensions of topics that were covered as part of the basic deep learning course. Specifically, word embedding methods are focused on advanced deep learning for NLP/text while image/video analytics will be focused on advanced deep learning for vision. We will discuss recently proposed methods and algorithms which have disrupted businesses significantly and have a strong potential.

The objective of the course is to enable the students to

- How to build simple chatbots very easily. How to integrate a question-answer knowledge base with bots.
- Understand advanced architectures in deep learning and their business applications.
- How to harness deep learning techniques for image/video analytics use cases.
- What are the best ways to capture semantics of words.
- How to build basic recommendation systems with hands-on code.
- Prerequisites: Python, little bit of Tensorflow/Keras/PyTorch/CNTK, knowledge of basic neural architectures like CNNs and LSTMs.

## Session-1 Recommendation Systems

optimization/ linear programming / quadratic programming is often required by logistic companies.

collaborative filtering and K-neareest neighbours are  alcmost similar algos.

idea: fake people army

read more/: neural colaboraive filering.

CF prob:

- cold start
- sparsity
- scaling is also an issue

how to do map-red in CF.

hybrid recommendation methods:

- for cold start - start with popular items, then switch to CF
- mixed: several type of recommendations
- cascade: funnel kind of, hig recall, low precesion. top is low precesion, bottom is hig preceison recommendations.

Content recommendation videos

- receent - like old new people dont want
- diverse - not really imp in search but is good in recommendations
- relevance - user session activity matters,
- explaination - why recommended for a user, increases change of clicking.

rating matrix - nobody rates youtube video for eg. If user liked shared commented etc. ANy interactions can help score a video, thus we can recommend it to users.

Tag Recommendations:

- based on one tag, we can recommend related tags to users. eg, barcelona the recommend spain.

-

user get info from ads being shown on website, ads are shown by google ad exchange.

Explaination Types:

- Nearest neigbour explaination,
- content based explaination - your read sports news, hence sports article is recommmended.
- social based - if friend reads the you will too,

### Evaluation of recommendation systems

Offline:

- imp to measure accuracy. check interaction on recommended things.

- novelty an dexploration , user searched / not but recommended.

- recommnedations is personalization but then it has bad impact as well as it limits content based on someone else's eye.

- widget should not be heavy to load.

- interaction should be quick, like quick look a post/ connect a friend with just one click,

References:

- <https://recsys.acm.org/> Recommendation Research

cat u.data|cut -f3|sort  - group by column 3 and give count.







## Session-2 Word Embeddings

"In natural language processing, **Word embedding** is a term used for the representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning." - Wiki

NN has n/w and we need to feed sentense hence, word embeddings.

1hot encoding

- one word embeddingis 1 at word place and else 0.
- sparse prob
- similarity cnanot be established.
- cosine similarity  = 0

hence we use dense representation of words.

- they are short
- they are not sparse
- they carry meaning, similarity can be established
- how word2vec is done:
  - they are pre-trained model
  - pretrained by google, 3 billion words.
  - it has phrases as well, new york
  - build on 100 billion news dataset words
  - vector size is 300 dims.
  - they are trained using shallow NN.
  - started in 2013, has just one hidden layer.
  - it is quick to learn these networks because o fshallo, 1 hidden layer.
  - CBOW, Skipgram?
  - after entire learning we get weight matrix, which is vocal by 300.
  - in skipgram we try to find related word to a word. it needs less data to learn framing.
  - windosize is a paramenter, usually 5.
  - we can enable stop words, stem words etc, these are all params.
- gensim is package for this.
- 300 dim is for one word 'King'
- cosine similarity can be found b/w words
- we can visualize this to see similar words closed to each other on x-y plane.
- it aut0 creates dictionary of similarity from any corpus.

- we can do math, like, male-female.

Word Vectors - GloVe

- improves similarity, words can be similar not only if they occur in same sentence, but other sentenses as well.
- trained on wikipedia,
- much better quality than word2vec.

- word2vec used for sentiment analysis,

- outofvocabWord - word may be in test data but not in train, so we use fastText

FastText

- use embeddings for characters, word is made of parts, prefix, suffix, word root etc.
- combines skip-gram with sub-word model.

there are many models, recent ones use transformers, deep models, \~20 hidden layers.

PYNB----mahabharat file


ELMO (Embeddings form Language models)

ELMO uses biLM (bidirectional language model)


## Transformers

good for 512 inputs only.

BERT:

- transformer is broken into encoder and decoder, encoder is BERT, decoder is GPT (genreatlised pre trained transforms).


## Session-3 Image analytics 1

Inception - groups pooling and filter, calls it inception module and puts them in b/w the POOLs.



CNNs, conv > Relu > pool

Grouped Convolutions:

Depth Wise CNNs:

- dividing imgae into groups and using groups


PNASNet-5

This was not decided but build using neural architecture search.

we can do reserch or we cannot do grid search because of husge params and layers. But we do random seacrh.

Genetics is followed by evolution. it is an iterative process. Chromosome is collection of genes.

it has depth wise separable convolution.

we only change last 4 layers of n/w to fine tune recognition.

fastAI is on top of pytorch and keras.



## Session-4 Image analytics 2

Object detection algos:

- R-CNN
- SSD
- YOLO

object detection framework:

- regional detection
  - what is a region,



## Session-5 Video analytics


Dialated CNN have 0 is between the filter. they are common for segmentation purpose.

Deeplab is atrous + multi Scale + CFRs

### CRF - conditional random fields

HMMs are CRFs.

Part of speech tagging, before DL, was done using multi class (36) classification.

So likely after adjective we have noun. They helped in sequential data.

CRFs are cheaper compared to DL and still work well.

CRFs work by bringing joint speech tag. All words get prediction jointly. similarly we don't get label for eah pixed but grouped pixels.

read more on google.

### DeepLab

in 3rd conv layer, you have a receptive field, but if we need to vary this, we change violation rate. for each filter, we change by adding 0, to make filter large, and this changing resolution we are looking in input image.

Atrus special pyramid pooling (ASPP) is build after this.

input - cnn with atrus - conv output mid level - upsmapling (bilinear interpolation or DL way of upsampling) - fine grain not taken care - use fully connected CRF to bring out final details.

atrus with rate 1 is simple conv. rate 1 = no zero added.
rate 2 = 1 zero added

rate r = r-1 zeros b/w filter values.

thus receptive field becomes larger.

DeepLab used VGG-16 with 'fc6' layer, with fully convlutional n/w. they dialated conv layer. we do dialation here, and 'fc6' give good accuracy.

multi scale because we can change the scale at which we process the image.

we do max-pooling on top of it to get the final output.

Rate = 6 maeans dialation rate. so has 5 zeros.

CRF do joint labeling so we do interation to get sharper segmentaiton.

Fully connected CRF give more crisp segmentation.

More examples to follow.

code as well to lookup.

assignment will cover the notebook.

### Part 1 Assigment

group 2-3, do on colab

submit py only from colab notebook.

use logistic regression for 5 classes.

train word2vec then classify

use test only for scoring.


### Visual question answering

interesting becuse of multi-modal aspect. it is image + text. ans cna be binary or logns asnwer.

we cna combine object detection with knowledge base like wiki to ans the quesions.

we do this in closed domain. where vocal is small.

Challenges in VQA

- labeled data is created
- combining modalities, CNN for image, LSTM for text.
- attension - what part of picture is being asked for?

mostly binary ques have ans yes. that's how people ask. So the trained ata has biad prob.

Applications are anything you imageine.

Visual DataSet Visual7W:

- had abastract image to manage bias by adding images with ans 'no'

VQA Models: LSTM Q + norm L.



.
.
.



Hard and Soft Attention:

- sum of all weight is 1.
- a block might be left out, so for this, they made the sum add up to 1.

model-decoder model was changed to have alpha ti have sum equal to 1.

using hard and soft we can have differntent caption for the images.

insome case soft would be better and in some hard.

more examples to look at.

exam, May first week.








# ISB T6 Digital Media Analytics

Instructor: Prof. Madhu Viswanathan | Email: Madhu_Viswanathan@isb.edu | TA: Shreya Singireddy  

**Course Description:**
This course will provide a foundation to explore and study the ever-evolving and fast-paced field of digital and social media marketing. We will learn about how the digital economy works, online advertising, and its relationship to basic theories of advertising. While doing so, we will also investigate how to causally measure advertising effects using randomized clinical trials (RCTs) and field experiments.

## Session 1: Introduction to Digital Marketing and Web Analytics

traditional marketing -> digital marketing

things have changed, focus has shifted form prod centric to consumer centric.

customer will tell what is annoying them but not what what the problem is. you will have to find the problem.

dont start with product, start with problem,them make s/q to fit the eco-system.

customer personas  - needs wants and so on.

audience tribes - privacy is an issue. build segmentation of customers based on behaviour. eg, netflix.

customer decision journey - how customer walks from awareness to conversion. how interacts iwth different channels, like sales. this is cust journey. the behaviour comes in.

Digital Channels:

- Search - google

- Display - advt. on apps and web.

- Social Media -

- mobile - people use a lot

digital helps measure things. makes desicaon data backed.

Challenges - gdpr, ott laws.

ar - ve have improved.

evolution is there , be prepared for changes. () voo


google changed our behaviour.

locatoion based targeting, like airtel putting tents.


STP: Segemtnatioj, targeting and positioning.


zzzzzZZZZ


Challenges today:

- lot of content
- lot of people, videos etc.

make money from the content, only then content matters.

newspapers are dying because marriage, jobs, real state have moved to their own apps.

digital marketing:

- paid - web, social, influential, lead targeting
- owned - website, mobile, blog.
- earned - shares, mentions, reposts, reviews.



## Session 2: Display Advertisements

persona behaviour journey

pyscology is how ppl think of prod and services.

who is cust?

- many details like gneder, age, ocupation etc.

who is prod for?

- individuals are different, stereotypes, caring, harsh etc. batchelors are different.
- is it married men of a kind?

what is differene about the customer?

- stereotypes, forward moving

Interact customers effectively and efficiently:

- we need to be strategic about digital.
- you need to know where to cut.

Consumer deision Journey

- framework , current process, loyalty loop.
- old way to generate need was advertising.
- people do info search by frnds whoc have bought a car.
- people evaluate by brochures of cars.
- then make a choice.
- then people evaluate, is car good or bad.

per==================================
Consideration set is a set of prod a cust focuses at, eg, brezza, eco, duster.

this is funnel model, keep decreasing the coices in the set.

Now, in digital world, things have changed, ad is not really required. need is generated on its own. and people do google serch.

80% cust are looking aroudn even after purchasing prob. bust returnt he prod.

google - mement of thruth is hapamrning,


zzzzZzzzZZZZ


prospect of a customer: we neee ots6hh o


all tribe hows a  hierarchy

peopelc can be in multiple tribes.

tribes can be like, cookig, driving, etc.

you need to have attribution srategies, what channels matter, mail, social etc.

listening is big succes.


### Search Engine Marketing

seo is all about ranking, amazon has stepped up as a prod search engine.

Google algo for SEO:

- unique content
- penalty for duplicate content
- inbound link, other websites that link to your website.





## Session 3: Role of social media

Social media analytics:

- content analysis
- influencial marketing
- how to find influencer

Group assignment will be based on this, find influencer based on data.

Sentiment of tweets for airlines;

- more number of tweets is more negative.
- people hardly post =ve tweets.
- this score/ranking was similar to ASCI (american customer satisfaction index)

sentiment is just like themrmometer for fever, it tells symptoms and then we have to dig further if required.

Sentiment Anlaysis on Warren Buffet's letter to subscribers:

- It's result showed that it was related to market events, like crash in 1987, .com y2k in 2001, 2002 9/11. This was alos simple sentiment that gave us insights.

7 steps framework is used to find influencers.

Influencers are linked to words, categories, markets. We have to find that stickiness index.

Process of measuring give, customer influence value (CIV) and customer lifetime value (CLV).

4 ideal chards:

- activeness: how engages.
- clout - reachabluity
- talkativeness - retweet, reshare
- likemindedness - similarity, like amitabh is more followed and people will buy it.

Value of facebook like:

- run the experiment, identify customers, send invite to join fb page to a few customers. then look at the sales numbers. If there are postings again ,look at the likes. now we have group of people, those who ike and buy and those who don;t like but buy.

fb page has no value of page.

design is automatically done if we know the problem.


eBay Search Ad effectiveness

- spent a lot on search ad.
- roi was 300%.
- paper: <https://faculty.haas.berkeley.edu/stadelis/Tadelis.pdf>

ebay Ad test:-

- they stopped paid search and the sales was still same.

for ebay it was a scenario but for some other brand paid search may work.

experimentation is vital in digi world to figure out the ROI on ads, pages etc. Influence was lower as could be seen from data.


## Session 4: Introduction to A/B testing


## Session 5: Analytics of Digital Attribution

Correlation vs Causation

- cops vs crime is not causal, but correlated, high crime has hgih cops.
- cusality can be used to target audience

Controlled Test

-

Experiments are good when we have specific questions.

DMA Measurements - ROI of DM:

- organize the data, solves 75% probs
- experiment, turn off ad, try A/B testing
- try to find the best combo
- Apply simple stats, do regressions.


Attribution Model:

- display first, all credit goes to display
- last click models, search will get all craadits.

these are worst assesement.

<https://sudhir-voleti.shinyapps.io/CustomerAttribution/>








# ISB T5 Deep Learning

Session 1:

- Autoencode is only unsipervised DL others are supervised.
- Transformers, Birt and BPT3 are new algos

Deep Learning mimics the human brain. Neurons take input, mul by weights, then sum it up. and based on threshhold it outputs erither 0 or 1


IRIS example, it has 4 features, hence 4 inputs suppose it outputs 1 if versicolor else 0. Them it sums all inputs multiplied by weights


## The Artificial Neuron

It has  a input vector, $\overrightarrow{x}$, it is a row of features, eg, iris row with SL PL SW PW. Input vector has x1, x2, ... , xM. All are multipied by weights w1, w2, ... , wm. These are then aggregated by function `f()` called **INtegration Function** and then this passes through another function call **Activation Function** `a()`, this give output `y`. a() has the thresholding logic.


Integration Function, f(.) :

$$ f(\overrightarrow{x}) =  \sum W_i x_i <>= \theta $$

This function aggregates signals from other neurons, boosts or supresses the inputs.

Activation Function, a(.) :

This function outputs y based on threshold. It can be

- step, it is either 0 or 1.
- ramp, for x = 0 to 1 it is slope
- sigmoid - used to have gradients in output, b/w 0and 1.
- Tanh - used wehn we need garadients and values between -1 and 1.

The sudden change in function is a problem for derivative. Hence, we use sigmoid or tanh.

Rectified Linear activation function or **ReLU** is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero.

$$ f(x)= max(0,x) $$

A smooth approximation (softplus) to the rectifier is the analytic function: $ f(x)=log(1+exp(x)) $. Derivative of softplus is the logistic function.

Perceptron algorithm:

Iterative algo to learn weight vector. Update weights in proportion to the error contributed by inputs.

- Randomly initialize the weight vector
- repeat until error is less than threshold $\gamma$ or max iterations M.

$t_i$ is expected value, $y_i$ is predicted value, $\eta$ is learning rate.

$$ \overrightarrow{w}_{n+1} = \overrightarrow{w}_n + \eta * (t_i - y_i) * \overrightarrow{x}_i $$

$(t_i - y_i)$ is error.

As we correct the weights, it rotates the hyperplane and this separates the classes better. It controls how quickly the model is adapted to the problem.

**Learning Rate** is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function.

## Learning Non-Linear Patterns

**Changing Integration Function**, it is similar to kernel trick in SVM, we use quadratic or spherical intergration function, then w is mul with $x^2$ for example. We cna take data to higher dim using polynomial integration func, then a hyperplane can separate the classes and when we reduce it back we get non-linear classification.

**Multi-Layeered Perceptrons**, MLPs, have hidden layers. Each hidden layer has its x, y line. Set of lines can form a triangle, eg, three lines. Then we hve two classes one in triangle and another outside triangle.

## Back Propagation Algorithm

We have functions at each layer, we pass back the output as input back to previous layers based on the error each node contributes.



----;

Session 2:

Tensor flow and pytorch are libraries ti implement DL. Keras is wrapper on top of these libs.

Use DL only if the problem is not solvable by ML, for eg, image classification when we have 1000 object to classify from.

Layout accuracy, 3d position of objects, positional relationships in objects, come up with a network of objects. Humans have hierarcial apporach


## CNN and ImageNet

ImageNet holds 1,281,167 images for training and 50,000 images for validation, organised in 1,000 categories.


MLP can't be used for imgae because the number of parameter to estimate grows to billions. becaus it is fully connected,

In CNN, we have convolution layers. not all nodes are cnneted to each other, the network has conv and pooling.

CNN of image with a filer:

We get convolved feature in each convolution layer,

at each layer we process part of image, and we overlap in each iteration. Filter size, eg, 3x3, is also paramenter, skipping 2px is alos paramenter. This allows us to reduce the number of weights we need to train,

weights are shared so edges may be more but weights are less.

The weights will learn in back propogation.

We have multiple convolution layers. the number of filters define that.

padding is added to the image if stride and filter cannot aprse whole imgae, also to make the edge pixels count same number of itmes as the other pixels. count as in passed by the filters.

we can do zero paddign or mirror padding.

The number of weights is eaqal to number fo filters passign thirugh image and plus 1 bias which is shared by all the filters.

if we have two filters, eg, [3x3]x2, then we have weights times 2.

Number of parameer is the filter size and bias and number of passes.

so for 5x5 filter, 10 passes, params are , (5x5x3 + 1) x 10, 1 is a bias.

FOr number of output, we count by adding padding twice, - filter size + 1.




-----;


Session 3


## Pooling Layer

has 0 weights. ha filter that moves and gives max of points. it downsamples the imgae.  pool is done on all channels. so for 224x224 image with 64 channels, if we do pooling , max pool with 2x2 filter and stride 2, we get downsampled 112x112x64 size imge.

max, l2, avg pooling

why pooling, Pooling layers are used to reduce the dimensions of the feature maps. Thus, it reduces the number of parameters to learn and the amount of computation performed in the network. The pooling layer summarises the features present in a region of the feature map generated by a convolution layer.

Convolution brings out pattern, pooling finds most imp feature by avg or max.

Overfitting can be avoided by dropout.

We can augment data by flip rotate and rescaling images. dog remains a dog if we rotate.

## RNN

Neural understand only numbers, now we have text data. 30 million unique words.

vector to show word, w1 has vector with 1 at its location others 0, but cant be done for 30m words.

each word has vector of fixed size with any number in vector, like 100 numer. similar word have similar numbers.

word2vect

RNN for text based work. it learns sequences, in context, letters in words have dependencies,

Language models store bigram, trigrma etx. what comes afger what, ngram models.

in neural we do word to vector by different algos.

eg, we have 3 words and ech is represented by vecotr of 100 numbers then we have 300 inputs and 100 output uin NN.

The output number is the predicted word and we assign that to our list of words.

## Transformers

Like LSTM, Transformer is an architecture for transforming one sequence into another one with the help of two parts (Encoder and Decoder), but it differs from the previously described/existing sequence-to-sequence models because it does not imply any Recurrent Networks (GRU, LSTM, etc.).

An architecture with only attention-mechanisms without any RNN (Recurrent Neural Networks) can improve on the results in translation task and other tasks!

Another improvement is, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.

![Transformer: From ‚ÄòAttention Is All You Need‚Äô by Vaswani et al.
](https://miro.medium.com/max/700/1*BHzGVskWGS_3jEcYYi6miQ.png)

It has no RNN but knows relative position of elements.


Reference:

- <https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04>


## Session 4

RNN offer lot of flexibility:

- one to one
- one to many
- many to many

RNN output is matched against original. Output is concatenation of final neurons. If the output does not match then it backpropogates.

paperwithcode.com - to keep updated

we hve weights availabe in public by pre-trained model.

output of a CNN is embedding for the captioning RNN.

So we have dataset, [image, captions]. we pass image through public CNN for getting embeddings. Now our dataset is [Image embeddings, Captions]. Now we train on this.

Bidirectional RNN to make use of 4th word to predict 3rd word.. not onlt firward looking but backword as well.

Named-entity recognition (NER) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc

Vanishing Gradient Problem:








# ISB T5 Marketing Analytics

Marketing analytics is done on CRM database to market more effectively. It is done to optimize the costs associated with marketing products and costs for customer acquiring and retention.

Outbound means company calling a customer, Inbound means customer reaching out to company for service or product.

## Session 3

Ad Copy - we can optimize the ad copy with text keyword that has less bid rate and can still get good click thru. this can be done using text mining and a model can be built for the same.

Since htese keywords are infrequent hence less users type to comeup with thtat we have to find thousands of unfrequent keyworkds

$$ Clicks = \alpha [1 - e^{-\beta b}] $$

$\alpha$ = Traffic x CTR for the #1 ad

$\beta$ = inversely related to competition intensity

b = bid amount

Using this we can start finding the conversion rate for different keywords.

If LTV and ConversionRate is high then we can go for max bid amount and be #1 on the ad page. Else, we will have to find an optimal point.

If the budget is lower than the optimal value for keywords, then do optimization with function to be sum of profits and constraint to be less than max expenditure.




----;

# ISB ML SL2

**linear classifier** is a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector (input matrix).

**Bias** is gap (error) between actual and predicted value whereas **Variance** is the scatterness of these values.

if we have less complexity and more error then it is HB-LV, as we increse the complexity and reduce training error, it has more validation error and LB-HV. To find an optimal point where the model fits both training and validation data with less error is called **bias variance trade off**.

**Linear Discriminant Analysis** or LDA is method to find linear combination of features that characterize or seperates two or more classes. It is dimentionality reduction technique while retaining as much info as possible. It is not a classifier. LDA and PCA are both linear transformation.

LDA vs PCA [>](https://sebastianraschka.com/faq/docs/lda-vs-pca.html)

- LDA : Supervised :: PCA : Unsupervised (ignores class labels)
- LDA finds max separability, PCA finds max variance within a class.

**Vector** is element of vector space having magnitude and direction. **Vector Space** is place/space of combination of vectors. It can be 2D ro 3D. Mul vectors together goves third vector. Mul by number to scale vector (bigger/smaller). 2D has two points (x and Y), 3D has 3 and n dimention has n. Matrix is a vector space and is used to store vectors. n x m matric can have vector in row or column. In CS, the data that we have, if we store it in matrix form it forms a vector space. [YouTube](https://www.youtube.com/watch?v=fNk_zzaMoSs)

**Perceptron** is an algo for binary classification. Input is a matrix of numbers, output is weather this belongs to a class or not. hence, it is called linear classifier. It makes a hyperplane in space to classigy the objects.

## Neural Network


- Youtube Playlist <https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi>


## Bayesian Network

A Bayesian network is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph.

## SVM Support Vector Machine

support-vector machines are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis.

the method of Lagrange multipliers is a strategy for finding the local maxima and minima of a function subject to equality constraints.

The most significant benefit from solving the dual comes when you are using the "Kernel Trick" to classify data that is not linearly separable in the original feature space.

agrangian function (Primal) , applying a lagrangian stationarity condition and substitute to get dual

we can define a separating hyperplane in a systematic way by introducing slack variables  ùúâùëñ  and minimizing the total error, Slack variables are positive (or zero), local quantities that relax the stiff condition of linear separability, where each training point is seeing the same marginal hyperplane.

Kernel Trick is to map the data in much higher space so that it becomes linearly separable. A Kernel Trick is a simple method where a Non Linear data is projected onto a higher dimension space so as to make it easier to classify the data where it could be linearly divided by a plane. This is mathematically achieved by Lagrangian formula using Lagrangian multipliers






## Ensemble Learning

ensemble methods use multiple learning algos to obtain better predictive performance than could be obtained from any of the constituent learning algos alone.

An ensemble method is a technique that combines the predictions from multiple machine learning algorithms together to make more accurate predictions than any individual model.

Bagging is composed of two parts: aggregation and bootstrapping. Bootstrapping is a sampling method, where a sample is chosen out of a set, using the replacement method.

Bagging is a way to decrease the variance in the prediction by generating additional data for training from dataset using combinations with repetitions to produce multi-sets of the original data. Boosting is an iterative technique which adjusts the weight of an observation based on the last classification


## References

- Calculus <https://www.youtube.com/watch?v=lowavG2SXsQ>
- K-Means Stat Quest <https://www.youtube.com/watch?app=desktop&v=4b5d3muPQmA>



# ISB FP

Soundfile is a package to read and write files. Every SoundFile has a specific sample rate, data format and a set number of channels.

- `sf.read('filename.wav')` returns data and samplerate.
- Sound file can also be opened as SoundFile Objects `f`.
  - `f.read()` gives data
  - `f.write()` writes the file
  - `f.samplerate` returns sample rate.

librosa - A python package for music and audio analysis. `librosa.feature` is used to extract features from a wav file. These are usally 2D arrays.

MFCC is a 2D matrix format with MFCC bands on the y-axis and time on the x-axis, representing the MFCC bands over time. To simplify things, what we're going to do is take the mean across each band over time.

MLP Classifier - MLPClassifier stands for Multi-layer Perceptron classifier which in the name itself connects to a Neural Network. Unlike other classification algorithms such as Support Vectors or Naive Bayes Classifier, MLPClassifier relies on an underlying Neural Network to perform the task of classification.

PLP or perceptual linear prediction is a feature

Links:

- [PyPi with basic steps](https://pypi.org/project/SoundFile/)
- [Read The Docs](https://pysoundfile.readthedocs.io/en/latest/)



# ML Supervised Learning

- by Sailesh Kumar

Session 1:

## Inroduction to Machine Learning SL

FORMULATION is the Key Skill in Data Science. It is the art of converting busines prob to ML.  ‚ÄúOur technology, our machines, is part of our humanity. We created them to extend ourselves, and that is what is unique about human beings!‚Äù -- Ray Kurzweil. Everything is evolving, tools, products, tech, skills etc.


Products are evolving, emergent intelligence - let the technology emerge. Then learn thru unsupervised learning. Don;t make it pre intelligent, but let it evolve. Dont define classes on its own but let ML find the patterns and clusters.

Philosophy of AI, understanding the Nature of the Mind. ML Goal is to unravell the nature of mind.  What is INTELLIGENCE? Generalization: Ability to predict or assign a label to a ‚Äúnew‚Äù observation based on the model built from past experience.

## Driving Better Decisions

ML is to learn how to make better decisions, from broadcaste to peronalized decisions. tata sky vs netflix, this will apply to health n agri. From batch sms to realtime, e.g. no sundays offers but event based poersonlized offers. From data rich to AI first. ML is continuous learning, continuous improvement. It has evolved from BI to ML to DS to AI and is still progressing.

Data - Insights - Features - Models - Predictions - Decisions - Feedback - Data

Here, same paradigm can solve many problems, it can be a classification, recommendation, outlier detection or retrival. Every ML Paradigm is an OPTIMIZATION Problem



We work by taking sample from reality, eg customer buying, rides taken.

- clustering algo, outlier detection algo/method.

- rediction paradigm, is cust about to churn, car to breakdoiwn based onbserved data.

pattern recognition - usl,

recommndatin engn -

resoning alogo - its sequence of actions, not onlu 1, chatbot, solving math ptob

reinforcement - laern to rason, explore an exploit,

deep learning - not everyhting is deep learning


Architecture of Ai

- stimulus to state, then state to action? how we do this is SL/USL.

AI for industrial IoT

- DOmain knowlege is faaaaarr more imp that stats and python

- how does on think about AI,
  - metrics to be max/minimized
  - domain knowledge
  - stimulus - sensors data,
  - state - featured vectors
  - response - actions or predictions, classes etc
  - eg, RETAIL, AGRICULTURE, EDU

Democratizing AI for ALL

- making AI available
- lot of apps/models/apis/services are developed
- AI market place for India, all upload their model, and anyone can consume via API. it coneects product thinkeers with AI

- Ariculture, now I can tell what is gown an dant what stage, so no wrong growth neither import, no wastegae

Super vises:

- regresion, prediction
- clas - time here,
- recommnedation - what are you going to do in future based on past, tell fav 4 movie and we will recommend based on that.
- retrival - entity to a query.

Supervised learning paradigm:

- numerical prediction the regression
- categorical then classification, crop is  classification cz it is not based on recommendation of past dta
- recommendation - top 4 recommendation
- retrival -

eg:

- ad position is retrival based on query u show ad
- credit card is classification
- youtube - recommendation, also retrival for your query in seatch box.
- google images, - retrival n then, gif/clipart/sketch based on classification it filters.
- connect/follow on linkedIn via recommendation, seach is retrival,
- product next - recommendation
- android prce, profit model
- medicine/tereatmment classification
- resumne for Job ID - retrival, based on JD or query we provide resume.
- where to sell new prods, like jio all in one media
- like barclays new product
- MRI cancer? classification, class has interprtation and prediction. curent data vs future
- Stock regression
- credit limit, regression, cz of number
- customer churn, classifier, give data, i'll interpret.
 -stealing/shopping clss, interpretaiton
- objects - classification, detection(interpret) and recognition(predictin).
- iPads in mumbai - regression, demand forecasting
- Tweet - classification
- rhowuse  -regression, price forecasting
- song humming - retrival,
- shaadi - retrival, also can be recommendatuib vased ib your activity. this exercise is classification as we give a class label to every prob


From data to decisions

-

Outlier detection paradigm

-


session 2:

- data nuance1 - fourier transform?
- noise vs signal, every data has it, eg, some vibrations in aeroplane are noise in xyz coordinates, while take off and landing is signal. Same for voice data, ocean heights. we need to distinguish bw noise and signal

- covida data has noise, like some tests are not trust worthy. count is not the true representation. not i=enought tests, not testing in the right places, flase postve and true nregative sis alo noise, reporting err isnoise. so we take the data and create model, then model predictions remove noise.

- ML is used to find signal in noise.

Data Nuance 2:

- nobody is buying the complete logical set of things.

- grammar data - there is mixture of projection, means different products might be purchased from different baskets, eg, keyboard mouse with hammer and screw driver

Data nuance 3:

- degree of variabliluty in images is very high e.g., due to pose, light etc.
- making system invariant to the variance is what we need. eg, making system invariant to different accent of speech

DN 3 Invariance:

- ignore the variance in all kind of data. ML is art of invariance learning, what matters and what doesn't matter

DN 4 |  Missing Data

- many reasons and we need to be logical in filling missing data. like mean, averange, median or model based, predict the missing values.

DN 5 | Not so normal dist

- to use stats methods and math priciple like aucledian distnace, we need normal dist assumption and or uniform distn.
- else, we can take log, or other transformations.

DN 6 | heterogenious mixture :

- once column may have different unit values, eg, blood test
- min max may not work, because of outliers. it may work some time and not other time.

DN 7 | Multi-modality features:

- 2D, 3D,  TIme-series,
- need to normalize, reduce dimentionality,

Importance of labelled data-

- scaling can be done using cloud
- AIML can be done using tensor, SparkML etc libs
- big data can be done using spark, hadoop, kafka etc.
- these doesn;t make u special, or super.

- to make a super powerful company you need labelled data, then you can make more accurate predictions, eg, google has more labelled data than bing. google images, maps, capcha.

- it has to be kept sage, no leakage

what are you trying to predict?

### Machine Learning Feature Engineering

What variables are important for you?

Insights + Domain = Feature

- Fitbit to cardiac
- Bank tranactions to CIBL score

Two ways to do DS:

- simple features, complex model
- complex features, simple model

Here, complex model with neurons have latency and if you need fast predictions then use complex features, that;s what Google does.

While modellling we need to consider the limitations of models like we cannot do multiplication or division in some models then we might need to take log or other such tranasformations.

Model the variables that matter:

- **NOTE:** Try to never use raw number in ML, it may have noise and other things, e.g., dont use balance for creadit score, normalize it with other facts like expense, frequency of expense, (total debts)/(total income) etc.
- remove exp nature by taking log, remove senstivity of varibales

Model Deviations from Expected:

- Model to predict total_sales, demand forecasting model, then modelling just for a number like sales is not important, you need to model it;s deviation from mean, that tells weather we have met expectation or not.

"Bugs" in Feature Engineering

- Its like when telling a lie, then explaination goes complex, like Johnny Jhonny, yes papa...
- So if model is getting complicated and then too not acheiving accuracy then there is possibilty of a feature bug that the model is trying to compensate, e.g., 0 for not found and first value in field or variable having garbage values.





## Session 3

- Feature engg is exploring all features and trasnforming them
- impoertance of dimentions, eg, 23675, here 2 is most imp because of units we have. it is 10s thousands, similarly we need to find imp eg color, size etc. if we remove the thing, eg, remove 2 and make 0, then max loss of information is most imp feature.
- now, if we check, is the number divisible by 2, then, last digit becomes most important.
- many features will be there, then many feature will be engineered. task is to determine most imp feature.
- which is better? - same feature, but different class, A B are class, dengu non dengu, if sigma is high it is worse. Seperation of mean should be high and variance within the class should overlapp minimum, variance within class should be low


study, fisher distribution, selection of feature, projection of features

We reduce the dimentions into 1D, we compute w, the weigths which can be multiplie with dimensions to maximise a values:

- PCA maximizes variation in data
- Fisher maximizes separation in classes

We use fishers formula to compute the separation of classes nin numeric way. Which dimension separates the classes maximum out of 1000s columns/dimntions.

We use another method to find which projection separates the classes the max, diff in the means.

clustering is done, then we get casses and it becomes classification problem from clustering problem. In real world new classes keep emerging.

There are both clssifiers, discriminative and descriptive. once defines withing and another differentiates the classes.





now, classifier world, model:

- classifiers describe the class , such that it does not overlap each other
- discriminative classifiers dicriminate and we know the classes.



Partitioning into pure region

- cannot be 100% boundary, because of noise
- complexity of classifier can only be linear for now.then which is the best fir line to distinguish the classes is the optimization. which linear classifier maximises the purity.
- medium decision boundary is having a polynimial, all curves are degree of polynomial
- 100% accuracy on training data, is all pure region, is very complex.
- out of these, something in b/w is good, region should be as pure as possible.

Toy dataset

- art, start with feature engg, color, shape, lights, sounds
- collect the data, get sales data, then see number of times it was sold/not
- art, create labelled data, check video of toys, take interview of sample kids
- we have categorical data, shape and color. We make matrix to find probabilies/region. This is classifying a toy, as in to which cell a toy belongs to.
- Now if we increase the features then the matrix increases hence we will use decision tree.





## Session 4

Recap:

We create contingency table from categorical variables nd then try to find the pure region which the goal of ML Classifier.

- each row, column or cell is a region.

- purity is the probability of the majority class in a region

- when the data is noisy, 55-45 then we don not use majority class, we use GINI measure, we find the expected accuracy.

### 3rd measure of purity

requires entropy of distribution, used by communication sattelites.

Information Theory 101:

- rare letter vs frequent letter and its importance. rare ahve more importance.

- entropy means chaos and uncertainity.  

purity is high then entorpy is low.

uniform ditn has workst entorpy

Accuracy can be mesured by

- purity
- entorpy, or
- ginin index.

Decision tree works by dividing data by class then findinf the purity of region.

Size X Purity for a region is score, it is basis of decision tree. As we get more and more deeper in a tree shape emerges and purity incereases.

### Greedy decision tree

- if A turne dout ot be the best feature then we keep it on top of tree, then further look down to partition, all rows ahving 'a1' and remaining 25 fetures.

- we stop growing when we reach a pure partiiton, need not partition futher, they are pure above a parameter threshold., eg, c1 and e1.

- in case of numeric attributes, sort and do all possible partitions. then we get n-1 partitions.

- for numeric, we can only go horiontally or diagonally, then for diagnol we do feature engineering.

- logistic regression is good for numerical variable.

- so we cna combine ML techniques an not get stuck with one, this is wehere we can do programming in it and cnannot use library. for eg, embed logistic regression at a leaf of decision tree.

- facbook, for eg, ignore numerical var and use only cat var to make decision tree, then partitions on numberical var.


Recap:

- fisher discriminant SV tech, for feature selectiion, importance, projection
- rule based classifier, pre set of classifier, based on few features,
- learn rules based on data, birth of decision tree, hence birth of ML. DT is first algo of ML.


### K-Nearest Neighbors

- carc - classification and regression cheat
- for 2 classes we use odd k
- as we inc k, the complexity decreases
- parameter is nothing, this is non-parametric classifier
- given the hyper param what are the params
- hyper is used to control the complexity
- param is the tree lenght or the partitions
- no training time
- scoring complexity depends on k

### Kernel density function

- points are like magnets and thy have influence on the validation data. They influence distn is called 'Density Estimate'
- magnets can be +ve or -ve, then each influence is calculated, then the data belongs to the class whcoh has high influence.
- density function is influence on x by all other points.
- Thsi is Parsean Window.

### Bayesian Classifiers

- Based on Bayes Rule.
- P(Data|Class) is based on old data where you know the class or result of diagnosis.
- P(Class) is probability of such dicease occuring
- P(Data) is probability of such patients coming
- RHS is all labelled data,m that is history
- LHS is for new data that has yet to come.
- P(Class|Data) is for new data that has not been seen previously.
- Learinig on RHS , inference on LHS.
- **Comes in interview think like a Bayesian**

Multivariate data:

- we can start simple by using similar covariance matrix for all dimentions
- then inc complexity by allowing different covariance
- then by different degree of freedom
- then by increasing dimesion freedom as well.













;


# FA - Forecasting Analysis

Ninary process are only yes or no

point process tells us only occurance.

forecasting is based on past data.

predicting might not be associated with time, but forecasting is,

in timeseries, the value is correlated with the previous observations.

deterministic vs stochastic

- there are a lot of fluctuations in stochastic series

- temporal correlation is relation to the previous year, scatter pliot of values this year vs last year

- notation, T + h|T is how many periods into the future you are going to forecast.

- frequency of seasonality is important, it depends on data granularity and how often the seasonality repeats.

- cyclicality is not that it will occur at regular interval but it is random, like pandemic or recession.

- seasonality occurs at regular interval while cyclicality is random in time interval.

## Time Series Modelling

There are different methods of forecasting, but three terms need to be taken care of:

1. Seasonality - Some months have peak output value compared to other months. eg, more travel during christmas time. we need to capture this seasonality while forecasting.
2. Trend - Inc/Dec in behaviour over time,
3. Unexpected Events or Irregularity or Noise - Dynamic changes that occur in market or organization that cannot be captured. eg, pendemic, recession.

We can use algo and other methods to capture seasonality and trend but unexpected events are dynamic and difficult to capture.

**Stationary timeseries** means that the mean and variance are same between two timestamps. We need to make timeseries stationay if it is not (maybe it is like standardizing the values). We can use **rolling statistics** to find moving average, on a window size, to make a TS stationary. Moving avg plot is smoother than the actual values. This makes TS stationary, we can also do it by using Exponential smoothing.

Simple exponential smoothing $ yT = Œ± *XT + Œ±(1‚àíŒ±)* yT‚àí1  $.

Double exponential smoothing for capturing trend and seasonality $ Yt =  Œ± *Xt + (1-Œ±) (yt-1 + bt-1) $ where, $ bt = beta* (Yt ‚Äì Yt-1) +  (1-beta) * bt-1 $.





In **descriptrive modelling** of timeseries data we model to determine its **components** like seasonal patterns, trends, relation to external factors etc. This is **analysis** of timeseries data, it tells the underlying causes and the why behind the data. We decompose the timeseries data into **constitution components**. The quality is determined by how well the model describes the data. The primary objective is to develop the **mathematical model** from the data that can provide **plausible description** from the sample data.

In **Predective modelling or forecasting** we use this information to find future values. The future values can only be estimated from what has already happened.

**Components of Time Series:**

- **Level** - baseline value of series if it was a stright line
- **Trend** - optimal linear inc/dec behaviour over time
- **Seasonality** - optional repeating patterns or cyclic behaviour over time.
- **Noise** - optional varability that cannot be explained by the model.

These constituent components can be thought to combine in some way to provide the observed time series.

TS can be additive or multiplicative. if these components add then additive, else multiplied then multiplicative.

$$ y = level + trend + seasonality + noise $$

this can be modelled with assumptions using traditional statistical methods.

these components can help make forecast but not always.

**Data Prepration** needs to be done to check NULLs or non-continuous data, to make date granularity on the level we are interested in. Then:

- Group metrics by granularity of date, like, roll up to daily/monthly etc, use group by with sum/mean.
- Make date column as **index**.

**Visualizing** the data helps us see seasonality, trend.

We can also visualize our data using a method called **time-series decomposition** that allows us to decompose our time series into three distinct components: trend, seasonality, and noise.

```python
import statsmodels.api as sm
decomposition = sm.tsa.seasonal_decompose(df, model='additive')
fig = decomposition.plot()
plt.show()
```

This gives:
observed - timeseries as is
trend - how metric has gone up or down
seasonal - cyclic up and down in series, and
residual - left over after fitting a model, its actual - trend - seasonality.

## Using ARIMA - Autoregressive Integrated Moving Average

ARIMA(p, d, q), these account for seasonality, trend, and noise in data

We can find values of pdq by using machine learning algos and can improve it by using grid search. We can check for lowest AIC value, and can consider those parameters.

Summary Table - `print(results.summary().tables[1])` to print the results. This gives stats output table with coefficient value, p value and t scores.

Diagnostics - we can run model diagnostic to see how are the variations and can usnderstand if it is feasible to use this model. if the residuals are normally distributed it indicates that we can make use of model.

Validations - we can validate to understand accuracy, the predicted values are compared with real values. `pred = results.get_prediction(start=pd.to_datetime('2017-01-01'), dynamic=False)`. We can plot this to have a better unserstanding of the results. The trend should align and make sense. Next, we can find MSE and RMSE using the predicted and actual values.

Once we are happy with the model we can **Forecast** the values for the future dates.



## Using FB Prophet

Developed by Facebook in 2017, it also has advanced capabilities for modeling the **effects of holidays** on a time-series and implementing custom changepoints.

Install:

- `sudo conda install -c conda-forge fbprophet`
- in case of error, `sudo pip install pystan==2.18.0.0`, then above step.

- `import fbprophet`

- in case of error,
  - in file `/Users/vaibhavyadav/anaconda3/lib/python3.7/site-packages/fbprophet/hdays.py`
  - replace `from holidays import WEEKEND, HolidayBase, easter, rd`,
  - with

```py
from holidays import WEEKEND, HolidayBase
from dateutil.easter import easter
from dateutil.relativedelta import relativedelta as rd
```

References:

- <https://machinelearningmastery.com/time-series-forecasting-with-prophet-in-python/>
- <https://towardsdatascience.com/an-end-to-end-project-on-time-series-analysis-and-forecasting-with-python-4835e6bf050b>
- <https://www.analyticsvidhya.com/blog/2021/07/time-series-forecasting-complete-tutorial-part-1/>




;

# Unsupervised Learning

- No labels in data but we try to find patterns within, eg:
  - Clustering ‚Äì grouping data, inferring labels from the data
  - Rule-mining ‚Äì interesting relation b/w varibales, finding patterns based on purchase information
  - Recommendation-system ‚Äì suggesting new items based on usage
  - Dimension reduction ‚Äì Inferring structure from the data

USL is first arrow on data but underlyting assumptions shuld be validated in EDA, then followed by Epicyclic procss.

Association Rule Mining

- how to combine things, market basket ananlysis
- txns could be a few days of purchase as well.

Terminology:

- I is all items in store
- X is one basket/txn, called item
- D is dataset, table having each row as txn.

- Association rule is, X => Y, if X then Y, X and Y have nothing in common
X is called antecedent, LHS or body
Y is called consequent, RHS or head

How to evalueate:

- Support: how often they occur tohether in store,
  - sigma is count, txns in store
  - s is pct, count/total
- Confidence, once you buy X how often you buy Y
  - given X, X => Y, c = S(X => Y)/S(X)
- Lift, they occur vs they randomly occur.,
  - s(X => Y) / s(X)\*s(Y)
  - how much X lifts prob of buying Y.
  - lift = 0.4/0.6\*0.6 = 1.11, thsi says that you expect X adn Y 30% of time but they occur 40% of the time, hence has more lift.
- leverage is diff of occured and random, lift is ratio, ùêøùëíùë£ùëíùëüùëéùëîùëí ùëã‚Üíùëå =ùë† ùëã‚Üíùëå ‚àíùë† ùëã ‚àóùë†(ùëå)
- conviction is ratio with diff from 1. ùëêùëúùëõùë£ùëñùëêùë°ùëñùëúùëõ ùëã ‚Üí ùëå = (1‚àíùë†(ùëå))/ ( 1‚àíùëê(ùëã‚Üíùëå) )

Rules are discovered using Apriori algo. It tries to improve the efficiency by managing the frequently occuring items.
Phase 1 - Join, support, exhaustive search, 2^n itemsets, if an itemset is frequent, then all its subsets are also frequent
Phase 2 - Prune, confidence

Improving, Hash-based technique, When scanning for 1-itemsets, create a hash for 2- itemsets, and remove hash buckets that do not have enough support.
Reduce the transactions, Prune the transactions that do not contain any k-itemset, when scanning for (k+1)-itemset. Partitioning, Divide the transactions into non-overlapping datasets, Find ‚Äúlocal‚Äù frequent patterns, Verify if these local frequent patterns have enough support in the entire data.

Adv FP-Growth, Improves the efficiency of not scanning the database multiple times for the generation of candidate itemsets, Minimizing the number of frequent items considered, Multi-level or multidimensional pattern mining, Sequential and time-series patterns.




## Session 2: Recommendations

Collaborative filtering:

- filter itmes/users based on similarity from collaboration

- user-based - find similar users to focal user based posts liked, both like car and pizza hence similar
- item-based - find similar items based on users liking them, u1, u2 like a post then other posts liked by u1 are similar and can be recommended to u2

ùëà is the set of users: |U| = n.  ùë¢1,ùë¢2,...,ùë¢ùëõ
ùêº is the set of items: |I| = ùëù. ùëñ1,ùëñ2,...ùëñùëù

- Ratings data is matrix ùëü, with ùëõ rows (users) and ùëù (columns), r_jk is the rating of the user ùë¢ for the item ùëñ

User-based CF: Step 1

- We need to define a distance metric between the focal user and other users.
- Two popular metrics
  - Pearson correlation between ratings
  - Cosine similarity between ratings
- Once we decide on a metric, we can find the neighbors using k-nearest-neighbors (knn) approach.

- Correlation proximity
- Ratings by user ùë¢1 on items ùëñ1,...,ùëñùëù
- r11....r1p avf r1_
- r21....r2p avg r2_
  ùê∂ùëúùëüùëü ùë¢1,ùë¢2 =
‚àë (ùëü1i ‚àí ùëü1_) ‚àó (ùëü2i ‚àí ùëü2_)
/
sqrt( ‚àë(ùëü1i ‚àíùëü1_)^2 ) ‚àó sqrt( ‚àë(ùëü2i ‚àíùëü2_)^2 )

Cùëúùë†ùëÜùëñùëö ùë¢1,ùë¢2 = ‚àëùëü1i ‚àó ùëü2i / (  sqrt(‚àëùëü1i^2) * sqrt(‚àëùëü2i^2) )


Itembased CF use same metrics

Disadvantages of Collaborative Filtering

- Coldstart - When a new user who has not rated any item - When a new item is introduced into the market
- Serendipity- There are no surprises in the recommendations.- This can be modeled into the collaborative filtering framework
- Sparsity - Most of the ratings matrix is empty!
- Biased data (rating stickiness) - Popularity bias - Unique tastes of users is lost. - Over time the data can be biased - Is the rating the actual rating or the recommendation has biased it?
- Privacy related issues

Association rule can bundle items, Recommendation is often a single or a set of items with no apparent relationship.


The idea in matrix factorization is discover these latent features that the users and items can be aligned with,    Latent factors It is a general concept that can describe a user and an item.

  Singular Value Decomposition (SVD)  Fill (impute) the empty cells with average rating
 Non-negative Matrix Factorization (NMF)NMF can be applied when ùëÖ has only non- negative values

## Dimensionality Reduction

Dangers of Dimensionality Reduction
Reducing dimensionality can distort data (and, hence, data mining results) in misleading ways

## Outlier and Anomaly Detection

The set of data points that are considerably different than the remainder of the data

Detection schemes - Graphical , Statistical/model-based, Distance/proximity - based.

 Graphical Approaches

- Simple examples
- Boxplot (1D)
- Scatter plot (2D)
- Spinning scatterplot (3D)
- Benefits
- Intuitive, use powers of human cognition
- Limitations
- Time-consuming
- Subjective
- Difficult to use with higher dimensionality

  Statistical Approaches
- Statistical methods (also known as model-based methods) assume that the regular data follow some statistical model (a stochastic model)
- The data not following the model are outliers
- Lots of different models are available
- Effectiveness of statistical methods highly depends on whether the assumption of statistical model holds in the real data
- Many statistical techniques have been developed
- E.g., parametric vs. non-parametric

### Data is represented as a vector of features

- Three major approaches - Nearest-neighbor-based
- Density-based
- Clustering-based

 Nearest-Neighbor-Based Approach

- Simple idea:
- Compute the distance between every pair of data points and use the information about k nearest neighbors of each point

 Density-Based Approach

- Finds local outliers, i.e., by comparing data points to their local neighborhoods, instead of looking at the global data distribution
- Intuition: The density around an outlier object is significantly different from the density around its neighbors
- Method: Use the relative density of an object against its neighbors as the indicator of the degree of the object being outliers

Density-Based Approach: Local Outlier Factor (LOF)

- Basic idea:
- For each object (data point), compute the density of its local neighborhood (defined by the k nearest neighbors)
- Compute local outlier factor (LOF) of a given object as the ratio between its local density and the local densities of its nearest neighbors
- Outliers are objects with largest LOF value
- A number of further variations and refinements have been proposed

 Clustering-Based Approaches

- Regular data belongs to large and dense clusters, whereas outliers belong to small or sparse clusters, or do not belong to any clusters

Strengths

- Many clustering techniques available
- Work for many types of data
- Clusters can be regarded as summaries of the data
- Once the cluster are obtained, need only compare any object against the clusters to determine whether it is an outlier (fast)
- Weaknesses
- Effectiveness depends highly on the clustering method used ‚Äì it
may not be optimized for outlier detection
- High computational cost
- I.e., need to first find clusters
- There are some techniques that try to mitigate this cost

## Challenges of Outlier Detection

- Modeling regular objects and outliers properly
- Hard to enumerate all possible regular behaviors in an application - The border between regular and outlier objects is often a gray area - More complex outlier behavior: collective outliers, contextual outliers
- Application-specificity in outlier detection
- Choice of distance measure among objects and the model of relationship
among objects are often application-dependent
- Handling noise in outlier detection
- Noise may distort normal objects, blurring the difference between regular objects and outliers
- Understandability
- Why these are outliers? (I.e., justification of the detection)
- E.g., poor data quality, measurement malfunctions, manual entry errors, correct but abnormal data
- Specify the degree of an outlier
- The unlikelihood of the object being generated by a normal mechanism
- Supervised outlier detection methods
- Can be used, if descriptions (labels) of anomalous data are available - Otherwise, it is typically an unsupervised (exploratory) learning task

## Visualizing Data using t-SNE1

- t-SNE is mainly a non-linear dimensionality reduction technique for visualization
- t-SNE stands for t-distributed Stochastic Neighbor Embedding
- The main idea is to project high-dimensional objects to a low-dimensional objects such that there is high probability that
- t-SNE minimizes the divergence between - a distribution of pairwise similarities in high-
dimensional space
- a distribution of pairwise similarities in low- dimensional space

## Spatial and temporal anomaly detection

Report any observed value that is significantly above or below its expected value (e.g., using GESD).
Time series data Spatially distributed data
One simple case of model-based anomaly detection is when we are monitoring a single real-valued quantity over time and/or space


### Anomalous pattern detection

Main goal of pattern detection: to identify and characterize relevant subsets of a massive dataset, i.e. groups of records that differ from the rest of the data in an interesting way.



### Subset scanning

We can scan over subsets of the dataset in order to find those groups of records that correspond to a pattern.

Step 2: Consider the highest scoring potential patterns (S, P) and decide whether each actually represents a pattern.

Step 1: Compute score F(S, P) for each subset S = {xi} and for each pattern type P, where higher score means more likely to be a pattern.

### Linear-time subset scanning

Given a score function F(S) which satisfies the linear-time subset scanning property, we can optimize F(S) over the exponentially many subsets of data records, while evaluating only O(N) regions instead of O(2N).
Just sort the locations from highest to lowest priority according to some function, then search over groups consisting of the top-k highest priority locations (k = 1..N). The highest scoring subset will be one of these!

## Why Bayesian networks?

- An easily interpretable graphical representation of the relationships between a set of variables.
- Bayes Nets can be specified manually or learned automatically from data, and enable computationally efficient probabilistic inferences.
- Many practical and successful applications in medicine, manufacturing, failure diagnosis:
- Diagnosis: infer Pr(problem type | symptoms)
- Prediction: infer probability distributions for values that are
expensive or impossible to measure.
- Anomaly Detection: detect observations that are very unlikely (i.e. have low probabilities given the model).
- Active Learning: choose the most informative diagnostic test to perform given these observations.

‚ÄúX and Y are conditionally independent given Z‚Äù: Pr(X, Y | Z) = Pr(X | Z)\*Pr(Y | Z)
or
Pr(X | Z) = Pr (X | Y, Z) and Pr(Y | Z) = Pr(Y | X, Z)


Make a truth table listing all combinations of values of your variables. If there are M binary variables, the table has 2M rows.



## The many uses of Bayes Nets

Bayes Nets provide a useful graphical representation of the probabilistic relationships between many variables.
Automatic learning of Bayes net structure can be used for exploratory analysis of datasets with many attributes.
We can often improve the performance of model-based classification by moving from Na√Øve Bayes to Bayes Nets.
We can also use Bayes Nets to detect anomalies, by finding points with low probabilities given the Bayes Net.
Bayes Nets provide a compact structure which enables us to efficiently compute probability distributions for any unobserved variables given observations of other variables.




















# CT3: Fraud Analytics

Frauds: Fake insurance, wrong medical claims and invalid transactions.

Money Laundering: Hide the origin of money by passing from complex transactions.

Why ML here: It can detect fraud using behaviours in data.

[C5.0](https://cran.r-project.org/web/packages/C50/vignettes/C5.0.html) is a classification algorithm. This was used to create a model which detected Money Laundering. It reduced number of transactions reported from 30% to 1%.

Based on behavioural data ML model can detect **anomalies**. It shows a row of data that stands out, like, too many txns on online shopping.

We use supervised learning, where based on each columns (attributes) we assign a class to the data. Data has to be labbeled.

- **Deep Neural Network (DNN)** can be used for classification. **Neural Networks** has hidden layers where we use transform functions, DNN has 100s of hidden layers.

- **Perceptron** is the simplest neural network possible: a computational model of a single neuron. A perceptron consists of one or more inputs, a processor, and a single output. It defines decision boundary between classes by defining the **hyper planes** in higher dimensions.

- **SVM or Support Vector Machines** can be used for classification. It uses a technique called the kernel trick to transform your data and then based on these transformations it finds an optimal boundary between the possible outputs. It finds several hyperplanes to minimize the classification error.

- **K-Nearest Neighbours or KNN** uses simplicity metrics to group samples into classed in featured space. New data is assigned to the class which is nearest to the class.

- **Random Forests** breaks data in many samples and makes decision tree from each. New data is passed from trees and then picks the majority vote to assign data to class. This outperforms others but has issues of overfitting.

Financials fraud data is generally skewed.

**Exploratory Data Visualization** is used to view how our classes differentiate. Find patterns of differentiation in data.

- **Andrews Plot** shows curves and usually classes are well seperated in the curves.

- **Parallel Lines** each attribute is vertical line, each row is ||el line, classes can be seen separated.

In summary:

- Class labels are needed for supervised learning
- Perceptron learning lays the foundation for many of the neural network architectures
- Financial fraud data is singularly characterized by the skewed nature of the data
- Overfitting in neural networks is a generic issue
- Very powerful algorithms exist for supervised learning

t-SNE t-Distributed Stochastic Neighbor Embedding (t-SNE) is an unsupervised, non-linear technique primarily used for data exploration and visualizing high-dimensional data. It gives you a feel or intuition of how the data is arranged in a high-dimensional space. t-SNE is a powerful visualization tool that preserves topological structures in the higher dimensional space

Chernoff Faces, each attribute is specific feature on face, can have upto 17 attributes.

Plot Histo, box plot, andrew, parallel, t-SNE, chernoff, pair plot, pareto of eigen values, scatter of pca and visualize. t-SNE, reduces 6D to 2D.









ML

Dimentionality Reduction

Examples Datasets




Machine learning for Fraud Analytics

Fraud Stats:

- $26b fraud
- CNP 82% reason

Examples of Fraud:


Potential for ML use:

- banks legacy systems use about 300 rules to approve a txn
- ML algos can use other attributes of user like IP, device, day of time etc to detect fraud.
- We will use supervised learning.
- *80% time in EDA and DC.*

Anomaly Detection

-








# SA4 - Term 4

Kaggle

**Linear Regression** is relation in **observations** ($x_i$ : set of variables) and **outcome** ($y$ : variable of interest) which tells us how x variables explain y. It is a line which has shortest distance to all possible outcomes from observation.

## Covariance and Correlation

**Covariance** is variation of data from the mean. The value depends on unit and is not standardized.

To calculate, find variation of each point from mean and then the product of variations. Then sum these variations and divide by n-1:

$$Cov(Y,X) = \frac{1}{n-1}\sum_{i=1}^{n} (y_i - \bar{y})(x_i - \bar{x})$$

Here, +/- tells us relation but value does not tell the strenght as it varies with the unit of measure. We standardise values to find strength, in correlation.

**Correlation** is how variables are linearly related to each other. It is standardized and does not change with the unit of X. The sign tells us +/- relation and the value tells us the strength. $cor(Y,X) = 0$ does *not* mean that they are not related, it only means that they are *not linearly* related as they may have non-linear relation, like, $ Y = 50 - X^2 $.

To calculate divide the diff from mean by standard deviation:

$$ Cor(Y,X) = \frac{Cov(Y,X)}{S_xS_y} = \frac{1}{n-1}\sum_{i=1}^{n}\left (\frac{y_i - \bar{y}}{S_y}  \right )\left (\frac{x_i - \bar{x}}{S_x}  \right ) = \frac{\sum(y_i - \bar{y})(x_i - \bar{x})}{\sqrt{\sum(y_i - \bar{y})^2\sum(x_i - \bar{x})^2}}  $$

## Use of Linear Regression

- **Relation**: To understand relation between X and Y, e.g. is sales related to advertisement, location, color etc? If so how, +/-, less/more.

- **Prediction**: To make predictions about Y variable, once we train the model (equation) from test data we can then predict Y from new X varables.

## Steps in Linear Regression

We often do, modelling, estimation, inference, prediction and evaluation. And them may be repeat the cycle with corrections.

### Modelling (define): Develop a regression model

The initial step in doing linear regression is to model the data. In this, we **identify** the variables of interest that we think are of importance in expaining y. This is usually based on past experienece. The variables are **not exhaustive**. Simple linear regression is formulated by following equation.

$$ Y = \beta_0 + \beta_1 X + \varepsilon  $$

$\beta_k$ are called **regression coefficients**. We estimate these using the sample data.

$\epsilon$ is the **random error**. This occurs because we might be missing some important x variable or x and y variable might not be linearly related. This is difference of actual point from the fitted point. This is also measure of **goodness of fit** of the regression model.

Modelling should take 95% of time, thinking about variables, x variables. The remaining steps below, estimation, inference and prediction, should take 5% of the time.

For example,
price/advertising cost/promotion costs

- e.g. $UnitsSold = \beta_0 + \beta_1 Price + \beta_2 AdExp + \beta_3 PromExp + \epsilon$


### Estimation (fit): Using software to estimate the model

Prior data is required for each X, it could be collected over time. Use excel/r/python to fit the model. Linear regression gives **estimates** of regression coefficients, or **predicted weights**, denoted by $b_0, b_1, ..., b_r$. We use **least squares method**, min sum of vertical distances,

$$ \hat\beta_1 = \frac{Cov(Y,X)}{Var(X)} = Cor(Y,X)\frac{S_y}{S_x} $$

Here, we have an **assumption** that Y and X are *linearly related* hence the estimated coefficient we got needs to be checked against this assumption before drawing any conclusion..

It gives us **estimated regression function**

$$ y = f(X) = b_0 + b_1x_1 + ... + b_kx_k $$

Now for each i'th observations, or i'th row, we can put in the function to get **estimated** or **predicted response**, $f(X_i)$. This should be as close to actual reponse $y_i$. The difference, $y_i - f(X_i)$ is called **residuals**. Now our aim is always to predict the beta estimates such that it minimizes the residuals, or deviation from actual y.

To get best predicted weights we minimize the **sum of squared residuals (SSR)**. for all i observations.

$$ SSR = \sum_i(y_i - f(X_i))^2 $$

For one x it is called simple linear regression, for more than one x, it is called **multiple linear regression**. In multiple linear regression with **two** x, it represents a **regression plane in a three-dimensional space**. The goal of regression is to determine the values of the weights $b_0, b_1 and b_k$ such that this plane is as close as possible to the actual responses and yield the minimal SSR.

We also get **regression output**.Also here, we should get expected sign of estimators, else there might be some **problem**, like, multi-colinearity.


### Inference: Interpreting the estimated regression model

Inference in unerstanding the estimators and other stats from linear regression summary output. $\beta_1$ is, when the $X_1$ variable **increases by one unit** then the Y variable **increases by $\beta_1$ units**, all other variables in the model being kept at constant. So on for other betas.

For eg, $SalesUnits = -25096.83 - 5055.27Price + 648.61AdExp + 1802.61PromExp$. Here, $\beta_0$ is fixed cost of production, without making any unit.

$\beta_0$ is the value of Y when all X are 0. **All $x_i$ are 0**  needs to be relevant. It can be relevant when we talk about fixed cost of production. So units produced can be 0 but still b0 would be fixed cost. But in our sales model, we cannot infer that when price of product is 0, then b0 is number of untis sold. We need to make **managerally relevant interpretation** of $beta_0$. To **improve $\beta_0$ estimation** we need to adjust data. We can change data by, say, x*= x - mean of x. This will keep all coefficients same but will change $b_0$. In this case all x* will be 0 only if x is equal to mean of x. Hence, we are saying that, when our ad expediture is equal to average of ad expenditure in past two years then units sold is $b_0$. Rather than saying that when ad expenditre is zero in the original case. When we cannot make x equal to 0, for eg, $Income = b_0 + b_1Age$, here $b_0$ is **only to fit the beta** to the model but we **cannot make inference** that it is the minimum income, no!

**Regression line through Origin** means that model has **no intercept** or $b_0$. The residuals do not necessarily add up to zero. SST != SSE + SSR and $R^2$ identities are also not true.


### Prediction: Making predictions about Y variable

We can make predictions of Y value based on Xs, put value of Xs and beta estimators to get predicted Y value. For eg, create 3rd scenarios, how much to set price, how much to spend on ad and promotions, based on these scenarios we get Y that can help us decide on how to increase sales/profit. The farther the X is from the X-bar mean, the larger is the Std Err. So we should take precaution if predicting value farther from our observations.

### Evaluate: Determine how accurate the model's predictions are

**Quality of Fit** or goodness of fit tells us how good our model predicts. **Larger t, or smaller p**, means **strong** linear relationship b/w x and y, use it to keep/remove observations in model. The scatter plot of Y and Ycap can also tell us the strength. The closer the stronger, this is same as Cor(Y,Ycap).

Needs update.. We can also measure by computing,

- SST, total sum of squared deviations
- SSR, sum of squares due to regression
- SSE, smu of squared errors (residuals)
- They are related: SST = SSR + SSE

$$R^2 = SSR/SST = 1 - SSE/SST = Cor(Y,X)^2 = Cor(Y,Yhat)^2 = R^2$$

0 < R^2 < 1, SSE<SST. Higher R^2 means strong linear relationship.

Repeat steps, modelling, estimation, inference & prediction, again to improve the model.


#### Standardizing

We might need to standardize the variables when we want to compare them for their effect on expaining Y. Standardizing makes them unit free, hence we can compare them. they can be standardized by doing $(x-\mu)/\sigma$ for all X and Y. Then we can run the regression model and interpret the coefficients. Here, **one std dev change in X changes Y by b1 std dev**.

#### Missing Values

We need to handle missing missing values in the data. THere are many techniques to do this. **Delete** if you have lots of data.


## Assumptions

There are a few assumptions that we have considered when we have estimated the coefficients. One of them is that all $\sum \hat{y} - y$ or $\epsilon$ is normally distributed around 0. The more we deviate from this assumption, the bad would our model fit.

$$\epsilon \sim N(0,\sigma)$$

Since we have this assumption and we have estimated betas, we can do a hypothesis test around this.

Now, CLT says that sample mean has Normal distribution with mean is population mean, and std is pop sd / root n, $ \bar{x} \sim N(\mu,\sigma/\sqrt{n}) $ . We actually never know the population mean. We estimate it from sample. Similarly we never know actual betas, we only estimate the bees from sample training data. Hence, all bees follow a norml distn with mean beta and std is std of beta.

$$b_k \sim N(\beta_k, std_{b_k})$$

Skipping lot of derivations, leads us to below equation that we can use for hypothesis testing.

$$\frac{b_0 - \beta_0}{S_{b_0}} \sim t_{n-k-1}$$

Here, $S_{b_0}$ is std err of $b_0$, n is observations, k is number of x variables.


### Hypothesis Tests

Hypothesis test is done to handle the uncertainities of the estimates from sample. $b_0$ is estimate of beta, hence it is not a true value and we need to  have a confidence interval around it.

For eg, Toy example, for ad_expense impact on units sold, the coefficient estimate is 600 but sales people say it should be around 300, then we have to check. The estimate is from sample, hence estimate and is uncertain, it is a belief, and it can be accepted or rejected. Hence, Hypothesis test. We need to **formulate a hypothesis test** for this. If SME says 1000 dollar increase in ad increses units sold by 300 units. Then,

Ho: beta2 = 300, this is claim made, hence mu
Ha: beta2 != 300

Method 1: $x = b_2 = 648.6 ; \mu = \beta_2 = 300 ; \sigma = S_{b_2} = 209$ from regression output. Now it get back to std value and t-distribution.
Our estimated coeff, b2, follows a t-dist, its std value = (x-mu)/sigma, this is **t-statistic**. It is 1.67, or 1.67 std to the right of mu. Now on the graph, check if this lies in **rejection region** for a certain confidence level. Here, conf level is 95%, so alpha = 0.05, and it is two tail test, so we will use $\alpha/2=0.025$, deg of freedom is n-k-1, residuals. Use T.INV to get std value from probablity and deg of freedom, using `T.INV(0.025,20)` it gives cutoff value as -2.71. So if t-statistic is beyond +-2.71 then we are in rejection region, but as t-statistic does not lie in either region hence we fail to reject the NULL hypothesis. Thus, our data does not has enough evidence to not reject the NULL hypothesis. Hence, the claim may be true. Out data has much noise.

Method 2: **p-value** is probability that the sample mean would be >= sample mean, given NULL hypothesis (mean is somevalue) is true. Here, p-value is twice the value cutoff by t-statistic in two tail test. Use the t-statistic found above to calculate the p-value. if p-value is lower than alpha then we can reject the NULL hypothesis, p-value = 2X(1-T.DIST(1.67,20,TRUE)) = 0.11, since .11 > 0.05 alpha. Hence, we fail to reject the NULL hypothesis. Remember: **When p is low the NULL has to go**.

Method 3: Use **confidence interval** in regression output. If the claim in in CI, then we fail to reject the claim.


## Regression Output

Every statistical software nowadays produces regression summary or output. This can answer many questions related to variables.

### R-Square

It tells how much percentage of Y is explained by the observations. Rsq may be low, then also model may have value as coefficients tell us the marginal impact. Rsq is imp but we should also consider the algebric signs of coefficients, their magniture should be plausible, they should be precisely estimated, and also check if you have missed any imp right-hand-side variable.

### P-Values

The p-values in regression output are the values for a hypothesis test in which we say that the true value of the **coefficient is 0**, i.e., weather one unit change in x has 0 impact on y. By looking at them, we can tell which observation is **significant in explaining** my Y variable. Usually, if p-value if **higher than 0.05** then the variable becomes **insignificant** in explaining Y.

### Coefficients

They are unit dependent, their magnitude cannot be used for comparision. They tell us the change in Y variable for one unit change in them. If you need to compare then stanrdize them.

### Confidence Intervals

They tell us CI at a certain confidence percentage. They are CI for estimates of betas.

## Anomalies of Regression

There are many problems, like

### Multi-Colinearity

We might get high Rsq which means model is good fit, but also high p-value which makes observations insignificant or unexpected signs of coefficients, this might be **problem of multicollinearity**. It occurs when there is a nearly linear relationship among a set of explanatory variables, i.e., they are highly correlated, usually more than 90%. To solve it do a **pair-wise correlation** and just drop one of the correlated variable.

It might not be a problem when we only have to predict but not understand individual observation effect.


# Focused Session on Big Data Analytics

Find a polar bear in images from cameras in forest.

- use azure iot to get images from camera
- `devices.json` has camera lat, long, key and id.
- create cloud-shell, this gives bash shell on azure.
- create a resource group (project) on azure., 'streaminglab-rg'
- create storage account 'mysamg1'
- create storage account keys to allow camera to store files to your account.
- create disk space(container) named photos(dir)
- create 'azure iot hub'AIH to recieve photos in realtime. AIH can listen to multiple IOT devices at the same time. 'myhubmg1'
- get connection string to the hub, to access the cameras.
- deploy 10 camera array to AIH, they will send camera snap in some interval.
- On laptop, use node.js for sending photos (simulated cam, else cam kernel will talk to AIH)
- npm install AIH
- create devices.json has {lat: "", long: "", key: "", device_id: "").
- create deploy.json
- node deploy.js, read devices.json, the register to AIH
- key is populated for each devices once it is registered on AIH.
- to store photos, you need `azure-storage` pkg, install using node.
- create `test.js` to upload an image to AIH with camera device id, lat, long, timestamp details.
- Create 'stream analytics job'SAJ under IOT, 'polar_bear_analytics', this will take input from AIH and send the image to 'Vision Analytics'
- stream, clean, not all photo to ml
- In SAJ we can query based on window, timestamps, camera or other dimentions of data.
- SAJ -> Computer Vision APi (CVA), for this use cloud-functions
- go to function app, make func, (streaminglab-rg)
- create 'http trigger func'
- now cretae 'run.js' that simulates real time, it uploads in random time interval.
- start SAJ
- SAJ sends filtered data to clout function which at the moment logs to console.

## Custom Vision

- Create resource 'polar bear r1'
- kind: CognitiveServices
- the upload training images with tags.
- do quick training of the model.
- now publish this model to get a web-api.
- we can use this web-api in our cloud ml function, then save the perdiction result to sql server.
- create sql server, myservermg1,
- configure the server to allow zure services, create db and table.
- update the function with js code to read preds from web-api and store results in db.

For counting, use regression model.




# ML Classes

ML in practise:

- most of the time goes in ML model building
- Auto ML does not works always
- data -> model -> predict
- Learning -
- p-value tell if the model is correct. it is confidence
- missing values and outliers k liye seek domain guy/customer
- touch more on inference side.
- number has no value, inference has value in corp.

K-means algorithm, hands on:

1. decide number of cluster, k=2
2. initialize random data points as c1(x1, y1), c2(x2, y2).
3. now find eucladian distance (ED) between between all the points and all the centroids.
4. ED = [x1-x2)^2 + (y1-y2)^2]^0.5
5. now the point belongs to centroid to which they have minimum distance.
6. now calculate new c1, c2 by finding average of all new points belonging to the cluster.
7. repeat step 3 to 6, until you get same c1,c2 as in last iteration.

K-Means Variations:

- logic and flow remains same, but we can change the way we calculate the distance, hamming, manhattan etc. This changes the centroid and the point belonging to it.

Business Scenario üíº :

- Client may say that no, point A belongs to other cluster
- Change clustering method/k value/etc
- so no clustering is best, it can only be acceptable, by client.

Hierarchical Clustering:

- Find dist of each point with everyother point. 5C2.
- create a 5X5 matrix with values as distance bw points
- in dendogram, y-axis is ED between points and x-axis is points.
- This is agglomerative clustering.
- Now any cut on Y axis gives us the clusters.
![Hierarchical Clustering](/assets/images/hierarchical_clustering.png "Hierarchical Clustering")

Uber Case Study:

- Kmeans clustering to find centroid as place from where all rides can be catered.

## Knowledge Graphs

Patters:

- How different subjects are related, find the connections, eg, Trump Boeing, Binny Bansal and Infosys.
- if business prob requires connections then build graphs
- Subject is node, edge is relationship.
- we can analyse customer care text data, and get insights

-





only ISB above ^



# AI image generator Notes

- GAN is used to genrete original like images
- Deep Convolutional Generative Adverserial Networks (or DCGAN) are a deep learning architecture that generate outputs similar to the data in the training set.
- This person does not exist.com
- nVidea research lab, start with low resolution and keep training to make a full resolution image.
- two ai neural network work against each other to generate and eveluate the images.
- fake images, fake media
- computing power is still issue
- lip syncing image to audio
- dataGrid, deepFake AI models




----;




Advanced Management Programme in Business Analytics (AMPBA)


# Foundation Term

## Probability and Statistics using R

In the VY notebook

## Data Analysis using Python and Introduction to Databases

In Kaggle notebook, all python numpy basics and eda examples.

# Miscellaneous

## Additional Sessions

# Term 1

## Business Communication

Mihir, notes in md in folder

## Big Data Management-1

## Data Visualization

## Statistical Analysis-1

## Data Collection

# Term 2

## Foundational Project-1

## Big Data Management-2

## Statistical Analysis-2

## Text Analytics

## Optimization

# Term 3

## Statistical Analysis-3

## Machine Learning (Unsupervised Learning

## Contemporary Topics-1

## Big Data Applications

## Advanced Optimization and Simulation

# Term 4

## Foundational Project 2

## Statistical Analysis 4

## Machine Learning - Unsupervised Learning 2

## Machine Learning - Supervised Learning 1

## Forecasting Analytics

# Term 5

## Contemporary Topics 2

## Deep Learning

## Supply Chain Analytics

## Marketing Analytics

## Machine Learning (Supervised Learning 2)

# Term 6

## Contemporary Topics 3
