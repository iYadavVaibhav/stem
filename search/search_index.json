{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Vaibhav Yadav's Wiki","text":"<p>Built with Material for MkDocs, served with GitHub Actions. More about me here.</p>"},{"location":"#articles-and-content","title":"Articles and Content","text":""},{"location":"0-Information-Technology/aws-amazon-web-services/","title":"Amazon Web Services","text":""},{"location":"0-Information-Technology/aws-amazon-web-services/#aws-overview-infra-and-access","title":"AWS Overview, Infra, and Access","text":"<p>AWS Global Infrastructure - AWS provides various services on the cloud which can be used to build these systems. It is PaaS. AWS Global Infrastructure has multiple servers, which are in multiple data centers, which are in multiple availability zones, and they are in multiple regions. This is how AWS makes data available and disaster-proof. Regions are geo bound. Pick a region based on</p> <ul> <li>Compliance - the app might be restricted to region or global</li> <li>Latency - server should be close to the end-user or business</li> <li>Pricing - due to tax price varying in different regions.</li> <li>Availability - not all services are in all regions.</li> </ul> <p></p> <p>Interacting with AWS</p> <ul> <li>as it is Virtual, on the cloud, hence you need API to manage services. API is available as:</li> </ul> <ul> <li>AWS Console<ul> <li>a GUI, web-based to login and manage the services. Click based. Region-based. Interactive forms to use services. Login with <code>username</code>, <code>password</code> and <code>MFA</code></li> </ul> </li> </ul> <ul> <li>AWS CLI<ul> <li>a Command Line Interface that is scriptable.</li> </ul> </li> </ul> <ul> <li>AWS SDKs<ul> <li>Software Development Kits, eg Python, Java, etc. Useful if you want to stay in dev language env. Eg, if your app is using Python and Flask, then you can use Python to interact with AWS services and host the app.</li> </ul> </li> </ul> <p>Security and the AWS Shared Responsibility Model</p> <ul> <li>AWS secures the cloud, and you secure things in the cloud. You secure your data, firewalls, access by users, encryption, etc. Each AWS service has its own security model.</li> </ul> <ul> <li> <p>As a best practice</p> <ul> <li>do not use the root user (AWS email ID) for the day-to-day task. Protect the AWS Root User as it has unrestricted access to everything in the account. Also add Multi-Factor Authentication MFA, which enables added security like RSA.</li> </ul> <ul> <li>Use IMA Account to do actions as it has restricted access.</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/aws-amazon-web-services/#aws-compute","title":"AWS Compute","text":"<p>Compute as a Service</p> <ul> <li>Web server, batch job, ML; all need compute. Compute maintenance needs time.</li> </ul> <ul> <li>AWS offers compute as:<ul> <li>Virtual Machines Instances - <code>EC2</code>.</li> <li>Containers - <code>ECS</code> Elastic Container Service, <code>EKS</code> Elastic Kubernetes Service</li> <li>Serverless - <code>AWS Fargate</code> is serverless compute engine for containers</li> <li>Function as a Service - <code>AWS Lambda</code></li> </ul> </li> </ul> <p>What AWS Compute service to choose?</p> <ul> <li>Prototype on premise app - EC2</li> <li>Once a quarter or month file data wrangling - Lambda</li> <li>Microservices that need regular updates - ECS or EKS</li> </ul>"},{"location":"0-Information-Technology/aws-amazon-web-services/#aws-networking","title":"AWS Networking","text":"<p>Networking within the cloud infra.</p>"},{"location":"0-Information-Technology/aws-amazon-web-services/#amazon-vpc-virtual-private-cloud","title":"Amazon VPC - Virtual Private Cloud","text":"<ul> <li><code>Amazon VPC - Virtual Private Cloud</code> is a network configuration, the same as a modem and router in the physical world.</li> </ul> <ul> <li>Example, in Amazon EC2, instead of the default config, we can use a custom VPC configuration<ul> <li>to add <code>more security</code>, like only allowing HTTP on a certain port. Hence, no SSH on 22.</li> <li>to give different <code>access control</code> to different resources, like public/intranet/private.</li> <li>to achieve <code>high availability</code> and fault tolerance by associating different AZs. Compute is replicated. We will have more than one EC2 hence more than one VM.</li> </ul> </li> </ul> <p>Amazon VPC Configuration</p> <ul> <li>CIDR notation is used to provide the variable IP address or range of IP addresses. Eg, <code>192.168.1.0/24</code>. /16 is more, /24 is less.</li> <li><code>VPC</code> gives you a range of IPs. <code>10.1.0.0/16</code>. You need to create VPC in the AWS management console - GUI.</li> <li>Then you can use these IPs to create <code>subnets</code>, which use some IPs of VPC-IPs to make a private or public network. Subnets are associated with AZs - Availability Zones. Say in Zone-A<ul> <li>Public resources, or internet-facing resources, are added to the <code>public subnet</code> with a sub-range of the VPC IP range, eg, <code>10.1.1.0/24</code>. like web-app</li> <li>Private resources are kept in a <code>private subnet</code> with a sub-range of the VPC IP range, eg <code>10.1.3.0/24</code>. like database.</li> </ul> </li> </ul> <ul> <li>To expose the public subnet to the internet, we need <code>Internet Gateway</code>, this is just like a modem. Create an internet gateway and attach it to your VPC.</li> </ul> <ul> <li>To only expose the subnet to the corporate intranet or VPN, create <code>VGW - Virtual Private Gateway</code>. This will expose AWS to an on-premise data center.</li> </ul> <ul> <li>To make it always available, duplicate the subnets and add to another AZ.<ul> <li></li> </ul> </li> </ul> <p>Amazon VPC Routing</p> <ul> <li>When a user reached Internet Gateway, it needs to be routed to the correct subnet. For this, we need a routing table.</li> <li>AWS creates default <code>main route table</code>. This provides local traffic only.</li> <li>GUI - each VPC has routes.</li> <li>We have called subnet public/private, however, that is implemented by routes, which controls the exposure of the subnets.</li> <li>Edit route table, add a new route, and add destination <code>0.0.0.0/0</code> that takes and servers all IPs. Then add an internet gateway to it. Finally, associate it with public subnets.</li> <li>Later add a firewall for extra security.</li> <li></li> </ul> <p>Amazon VPC Security</p> <ul> <li>Subnets can be made more secure, like only allowing HTTPs to inbound and outbound traffic on port 443.</li> <li>To do this create rules in <code>Network ACLs - access control lists</code>. Like firewalls.</li> <li>Secondly, <code>security groups</code> can provide more security to EC2 instances.</li> <li></li> </ul>"},{"location":"0-Information-Technology/aws-amazon-web-services/#aws-storage","title":"AWS STORAGE","text":"<p>Storage Types</p> <ul> <li>we need to store<ul> <li>OS - application files, ubuntu</li> <li>Static Data- files - employee photo - write once, read many WORM</li> <li>Structured Data - database tables</li> </ul> </li> </ul> <ul> <li>Block Storage - data is split into chunks - one char change in 1 GB file is simple - System File, Log File.</li> </ul> <ul> <li>Object Storage - single unit - one char change in 1GB file, whole file is rewritten - WORM, Video</li> </ul> <p>Block Storage</p> <ul> <li>It is offered with Amazon <code>EC2 Instance Storage</code> and <code>Amazon Elastic Block Store</code> EBS</li> <li>EC2 has <code>instance</code> storage like internal HDD. This is block-level storage. attached physically, hence fast but tied to the lifecycle of EC2.</li> <li>EBS is like an external HDD</li> <li>EBS can be linked to one EC2 or multiple EC2s can read-write.</li> <li>EBS can be HHD or SSD</li> <li>EBS can be snap shotted to keep backup.</li> </ul> <p>Object Storage with <code>Amazon Simple Storage Service S3</code></p> <ul> <li>EBS is not fit for all as<ul> <li>they are mostly 1-1 with EC2</li> <li>have a size limit.</li> </ul> </li> <li>S3 is scalable, standalone, not tied to EC2, not mounted</li> </ul> <p>Choose the Right Storage Service</p> <ul> <li>EC2 Instance store is generally well-suited for the temporary storage of information that is constantly changing, such as buffers, caches, and scratch data</li> <li>Amazon EBS, block storage, is meant for data that changes frequently and needs to persist through instance stops, terminations, or hardware failures.</li> <li>If your data doesn\u2019t change that often, Amazon S3 might be a cost-effective and scalable storage solution for you. Amazon S3 is ideal for storing static web content and media, backups and archiving, and data for analytics. It can also host entire static websites with custom domain names.</li> </ul>"},{"location":"0-Information-Technology/aws-amazon-web-services/#aws-databases","title":"AWS DATABASES","text":"<p>Databases setup Options</p> <ul> <li>The app can connect to DB on your on-premise server. you manage everything. Or</li> <li>Database on EC2, then have to install DB s/w, manage updates, and backups and replicate it on instances for high availability. Or</li> <li>Managed AWS database service like RDS, you only code. AWS manages everything else.</li> </ul> <p>Purpose-Built Databases</p> <ul> <li>Amazon RDS offers Relational databases, which are good to manage complex schemas, which have joins and complex queries and stored procedures. However, this adds overhead to the engine. Also, RDS is charged per hour, whether you query or not. Weekends are charged too.</li> <li>Amazon DynamoDB, is key-value db, docs, non-relating, millisecond latency, usage charge, and amount.</li> <li>Amazon DocumentDB - fully managed JSON Document database. Good for content.</li> <li>Amazon Neptune is graph database. Good for social n/w. Use cases: recommendation, fraud detection.</li> <li>Amazon Quantum Ledger Database or QLDB is used in finance for ledger management. It is immutable. Good for audit and compliance.</li> <li>these are purpose-built.</li> </ul> <p>Choose the Correct Database Service</p> Database Type Use Cases AWS Service Relational Traditional applications, ERP, CRM, e-commerce Amazon RDS, Amazon Aurora, Amazon Redshift Key-value High-traffic web apps, e-commerce systems, gaming applications Amazon DynamoDB In-memory Caching, session management, gaming leader boards, geospatial applications Amazon ElastiCache for Memcached, Amazon ElastiCache for Redis Document Content management, catalogs, user profiles Amazon DocumentDB (with MongoDB compatibility) Wide column High-scale industrial apps for equipment maintenance, fleet management, and route optimization Amazon Keyspaces (for Apache Cassandra) Graph Fraud detection, social networking, recommendation engines Amazon Neptune Time series IoT applications, DevOps, industrial telemetry <code>Amazon Timestream</code> Ledger Systems of record, supply chain, registrations, banking transactions Amazon QLDB <p>Links</p> <ul> <li>NoSQL on AWS</li> </ul>"},{"location":"0-Information-Technology/aws-amazon-web-services/#aws-monitoring-optimization-scaling","title":"AWS MONITORING, OPTIMIZATION, SCALING","text":"<p>Application Management</p> <ul> <li>It is required to know how is the app performing, how is it utilizing resources.</li> <li>To measure scalability - if the demand is not constant, it should add and reduce resources.</li> <li>It should balance traffic on all resources.</li> </ul> <p>Monitoring</p> <ul> <li>it is important to see how services are being used. Eg, Monday morning latency, it is not good to take bug tickets</li> <li>Monitoring should be pro-active, it should notify before end-user raises issue.</li> <li>Monitoring should tell where is the problem? recent code change, database or ec2?</li> <li>metrics, logs, traffic, database connections, CPU usage; need to be monitored</li> <li>monitoring tool helps in this by analysing, metrics over time, called statistics. Based on stats there can be triggers.</li> <li>all info needs to be on the central console, hence CloudWatch</li> <li>All logs, metrics are in one place in AWS and can be seen using CloudWatch.</li> </ul> <p>Solution Optimization</p> <ul> <li>we need to optimize infra<ul> <li>capacity - storage - s3 - mostly auto-scaled</li> <li>performance - ec2 and  database, database mostly auto-scales, but ec2 has capacity</li> <li>availability - manage traffic and make all services available.</li> </ul> </li> <li>prevent or respond to avoid bottlenecks.</li> <li>to increase availability increase redundancy or replication of services</li> <li><code>AutoScaling</code> lets you automatically add and remove instances.</li> <li>in horizontal scaling, the problem is that each EC2 has its public IP address and the traffic needs to be sent to one available. To solve this issue, we use a <code>load balancer</code>. It removes the need for public IP addresses for each EC2.</li> </ul> <p>Traffic Routing with Amazon Elastic Load Balancing</p> <ul> <li>now we have multiple ec2 in public subnets.</li> <li>request from browser - goes to load balancer - sends to ec2.</li> <li>ELB, <code>Elastic Load Balancer</code> does the traffic management. Highly available in each zone.</li> <li>ALB - <code>application load balancer</code>, handles Http or Https.</li> <li>ALB needs, <code>listener</code>, port, and protocol<ul> <li><code>target</code> group - which backend resource to send to, ec2 s3 dynamo<ul> <li>it sends the traffic based on the health of the backend</li> </ul> </li> <li><code>rules</code> - how to divert traffic. it is default as well as we can define rules for pages, apps, etc.</li> </ul> </li> <li>Another is <code>Network Load Balancer</code> supports TCP, UDP, and TLS protocols.</li> <li></li> </ul> <p>Amazon EC2 Auto Scaling</p> <ul> <li>we have 2 instances, but demand may inc.</li> <li>instead of adding instances manually, use EC2 autoscaling, more capacity based on a threshold in CloudWatch.</li> <li>ec2 CPU goes up, CloudWatch triggers the alarm, and auto scale is asked to give more ec2 instances. each is added to ALB with health checks and thus high horizontal scalability. hence CPU down across the fleet.</li> <li>Three main components of EC2 Auto Scaling are as follows:<ul> <li>Launch template or configuration: What resource should be automatically scaled?</li> <li>EC2 Auto Scaling Group: Where should the resources be deployed?</li> <li>'Scaling policies: When should the resources be added or removed?</li> </ul> </li> </ul> <ul> <li></li> </ul>"},{"location":"0-Information-Technology/aws-amazon-web-services/#serverless-technology","title":"Serverless Technology","text":"<p>Serverless builds and runs application without maintaining the underlying infra. Server is but is managed by AWS. It is scalable, cost efficient, simplify development, quick iteration.</p> <p>Drawbacks - not for long running, always connected.</p> <ul> <li>there is server, but you pay for operations. not paying for vm. no maintenance of VM. No 24hrs or 7 dy a week payment.</li> <li>cost saving, pay for demand only, not readiness for demand.</li> <li>separation - executions are secure as all lambda functions are permission controlled in their own sandbox.</li> <li>Scalability: - functions will scale on demand, each in their sandbox.</li> <li>You will not manage the updates and patches to the server os. Instead, the server is completely abstracted from you. You need not care about scalability, updates, or other server management.</li> </ul> <ul> <li>Risks of Serverless<ul> <li>heavy workload on serverless may be more expensive than vm.</li> <li>debugging is possible but may be difficult. chained lambda function are even more difficult. logging may be effective.</li> <li>testing can be difficult.</li> <li>Sprawl - as you write more factions, it grows and may get unmanageable. Now it is different way of doing things. So think of proper use case.</li> </ul> </li> </ul> <ul> <li>Use Cases<ul> <li>Autoscaling web apps. If request increases, serverless does. If your app becomes popular enough that it is used through out the day move to vm.</li> <li>event streaming - async actions are good to handle in lambda.</li> <li>file manipulation - async actions can be done when file is uploaded or something.</li> <li>connectors - your primary server may throw something to serverless.</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/aws-amazon-web-services/#services-","title":"Services -----","text":""},{"location":"0-Information-Technology/aws-amazon-web-services/#elastic-beanstalk","title":"Elastic Beanstalk","text":"<p>Deploy and scale web applications. It is PaaS same as Heroku, Google App Engine.</p> <p>Upload and deploy web applications in a simplified, fast way.</p> <p>Use docker image on docker.io to create web app. You need to use following json to tell AWS EB what your app is:</p> <pre><code>{\n  \"AWSEBDockerrunVersion\": \"1\",\n  \"Image\": {\n    \"Name\": \"dockerusername/catnip\",\n    \"Update\": \"true\"\n  },\n  \"Ports\": [\n    {\n      \"ContainerPort\": 5000,\n      \"HostPort\": 8000\n    }\n  ],\n  \"Logging\": \"/var/log/nginx\"\n}\n</code></pre>"},{"location":"0-Information-Technology/aws-amazon-web-services/#aws-cli","title":"AWS CLI","text":"<p>You need to install AWS CLI on client machine.</p> <p>Installation</p> <p>To install, follow AWS CLI Installation.</p> <p>Authentication</p> <p>Once cli is installed, you need to add your aws credentials, these can be obtained by signing in as a non-root user and click <code>Command line or programmatic access</code> then copy <code>Short-term credentials</code> and paste into cmd. Something like:</p> <pre><code>export AWS_ACCESS_KEY_ID=\"...\"\nexport AWS_SECRET_ACCESS_KEY=\"...\"\nexport AWS_SESSION_TOKEN=\"...\n</code></pre> <p>Now you should be able to run commands using aws-cli.</p> <pre><code># check\nwhich aws\n\naws --version\n</code></pre>"},{"location":"0-Information-Technology/aws-amazon-web-services/#aws-identity-and-access-management-iam","title":"AWS Identity and Access Management - IAM","text":"<p>Each developer can have an IAM account, this authenticates (access). Then each user needs authorization (permission) to different services.</p> <p>On sign up to AWS account a root user is created. It is recommended to create IAM User other than root (default) with less permission. Billing is disable by default for IAM Users.</p> <p>[ ] To login as IAM-User you need to get a URL. Login as root &gt; IAM Identity Center &gt; Users &gt; Reset Password. Save that URL, it has account ID in it.</p> <p>AWS Resources are different services in AWS like S3, EC2.</p> <p>IAM Identities are Users, Roles. Users can be grouped to form Group to share common permission.</p> <p>Use IAM to give identities access to resources.</p> <p>Access is given by permissions. Permissions can be grouped.</p> <p>Permission is defined by Policy.</p> <p>IAM Users</p> <ul> <li>It is identity with specific permission for a single person or application</li> <li>They have \"Sign-in URL for IAM users in this account\", which can be found on dashboard. Eg https://123456789876.signin.aws.amazon.com/console</li> <li>Permission can be given to them using Roles</li> <li>the example, <code>EmergencyAccess</code> IAM user you create is specifically for use only when your user in IAM Identity Center credentials are unavailable. more</li> </ul> <p>IAM Roles</p> <ul> <li>It is similar to an IAM user, but is not associated with a specific person. It is identity with specific permission.</li> <li>Roles are used by services to talk to each other. Eg, you can give a role to the EC2 machine to read and write to the S3 bucket and RDS and DynamoDB. Similarly all services need authentication and signed API calls in each request.</li> <li>Instead of being uniquely associated with one person, a role is intended to be assumable by anyone who needs it.</li> <li>It doesn't have long-term credentials, instead it provides you with temporary security credentials.</li> <li>IAM Identity Center and other AWS services automatically create roles for their services.</li> <li>For assuming a role, others need role info. It can be shared by:<ul> <li>Role link: Send users a link that takes them to the Switch Role page with all the details already filled in.</li> <li>Account ID or alias: Provide each user with the role name along with the account ID number or account alias. The user then goes to the Switch Role page and adds the details manually.</li> <li>Saving the role link information along with the EmergencyAccess user credentials.</li> </ul> </li> <li>more</li> </ul> <p>IAM Policy</p> <ul> <li>It can be used to grant or deny permissions to IAM entity (IAM user or IAM role) to take actions. Actions are API calls, everything in AWS is API calls.</li> <li>It is a JSON document that lists the actions that the entity can perform and the resources those actions can affect.</li> <li>Any actions or resources that are not explicitly allowed are denied by default.</li> <li>Policies can be created and attached to IAM users, IAM groups of users, IAM roles, and resources.</li> <li>The policy is a rule. The policy must have an <code>effect</code>, <code>action</code>, and <code>resource</code>.</li> <li>You can attach a policy to a role to provide users who assume that role the permissions associated with this policy.</li> <li>The <code>PowerUserAccess</code> policy is commonly used to provide access to developers.</li> <li>more</li> <li>Policy JSON Example:<pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [{\n\"Effect\": \"Allow\",\n\"Action\": \"*\",\n\"Resource\": \"*\"\n}]\n}\n</code></pre> </li> </ul> <p>AWS IAM Identity Center</p> <ul> <li>Portal to create human users (just like employees in company). It also lets users do SSO and map corporate workforce to AWS.</li> <li>user can sign in using AWS access portal which is URL unique to each AWS Account and can be found by Going to Identity Center &gt; Dashboard &gt; Settings Summary Sidebar &gt; AWS access portal URL. Eg https://d-12345abcd1.awsapps.com/start</li> <li>Permission can be given by creating permission set</li> <li>Adding users, Admin</li> <li>You can add users, these are human users with real email and passwords</li> <li>You can create user groups.</li> <li>You can create permission sets, attach policy to them</li> <li>Then add user/group to this permission set.</li> </ul> <p>Best Practice</p> <ul> <li>Access AWS using temporary credentials of human users, instead of using IAM users with long-term credentials. [ ] difference in IAM user and Human User?</li> <li>provide access to your resources through identity federation ([ ]roles?) instead of creating IAM users.</li> <li>provide access to your resources with AWS IAM Identity Center, not IAM User.</li> <li>Using least-privilege permissions. The concept of least-privilege permissions is to grant users the permissions required to perform a task and no additional permissions.</li> </ul>"},{"location":"0-Information-Technology/aws-amazon-web-services/#billing-and-cost-management-budgets","title":"Billing and Cost Management - Budgets","text":"<p>You can create budgets to email you if you cross the limits in budget. It can be crated from template.</p> <p>Zero spend budget: A budget that notifies you after your spending exceeds AWS Free Tier limits.</p>"},{"location":"0-Information-Technology/aws-amazon-web-services/#amazon-cloudwatch","title":"Amazon CloudWatch","text":"<ul> <li>It is a monitoring service which has central console to see all metrics and logs.</li> <li>users can create a <code>dashboard</code> from various granular <code>metrics</code> available</li> <li>can create alarms to be raised on crossing a base value over a time<ul> <li>this can trigger <code>SNS</code>, Simple Notification Service, which creates a topic. Anyone subscribed to the <code>topic</code> will get notified.</li> <li>can trigger an <code>EC2</code> boot action,</li> <li>can scale up resources</li> </ul> </li> <li>Users can also send custom app metrics, like 'page load time' which is not AWS metric but app-specific, but can be seen on CloudWatch and can trigger alarms.</li> </ul>"},{"location":"0-Information-Technology/aws-amazon-web-services/#lightsail","title":"Lightsail","text":"<p>Easy VPS Virtual Private Server hosting. Quickly launch and manage OS with configured Dev Stack (like Ubuntu with LAMP). Add load balance, firewall, and DNS. Once requirements increase, easily move to EC2 or Lambda. Lightsail provides low-cost, pre-configured cloud resources for simple workloads just starting on AWS. Amazon EC2 is a compute web service that provides secure, resizable compute in the cloud. It has far greater scale and optimization capabilities than Lightsail.</p>"},{"location":"0-Information-Technology/aws-amazon-web-services/#batch","title":"Batch","text":"<p>Lets you do batch jobs, by giving the right CPU GPU, and memory.</p>"},{"location":"0-Information-Technology/aws-amazon-web-services/#codestar","title":"CodeStar","text":"<p>It is a development tool to develop, build and deploy the app on AWS.</p>"},{"location":"0-Information-Technology/aws-amazon-web-services/#amplify","title":"Amplify","text":"<p>It provides continuous deployment and hosting of static web resources including HTML, CSS, JavaScript, and image files which are loaded in the user's browser.</p>"},{"location":"0-Information-Technology/aws-amazon-web-services/#amazon-dynamodb","title":"Amazon DynamoDB","text":"<ul> <li>fully managed NoSQL database, serverless</li> <li>key-value database service that provides single-digit-millisecond performance with limitless scalability</li> <li>no relations</li> <li>scalable to 10 trillion requests per day</li> <li><code>tables</code>, <code>items</code>, and <code>attributes</code> are the core component.  A table is a collection of items, and each item is a collection of attributes.</li> <li>access via dynamo database API</li> <li>Pay as you go model for billing.</li> </ul>"},{"location":"0-Information-Technology/aws-amazon-web-services/#aws-glue","title":"AWS Glue","text":"<p>AWS Glue is a fully managed ETL service that makes it easy for customers to prepare and load their data for analytics.</p>"},{"location":"0-Information-Technology/aws-amazon-web-services/#amazon-s3-simple-storage-service","title":"Amazon S3 - Simple Storage Service","text":"<ul> <li>Object storage built to retrieve any amount of data from anywhere</li> <li>access by URL</li> <li>max 5TB file</li> <li>has a flat structure</li> <li>unique bucket name</li> <li>it is region constrained</li> <li>it is durable, available as it is auto-distributed in a region</li> <li>objects are: <code>buckets/folders/files</code></li> <li>bucket/folder/files are private by default and not accessible to the world.</li> <li>can be made public using actions, but only in a public bucket</li> <li>Uses: static websites, data lakes, media, backup, and storage.</li> </ul>"},{"location":"0-Information-Technology/aws-amazon-web-services/#amazon-ec2-elastic-compute-cloud","title":"Amazon EC2 - Elastic Compute Cloud","text":"<ul> <li><code>Amazon EC2</code> are virtual server instances in the cloud. Amazon EC2 gives you complete control over the instance, down to the root level. You can manage the instance as you would manage a physical server.</li> <li>You can use instances for long-running applications, especially those with state information and long-running computation cycles.</li> </ul> <ul> <li>EC2 provides various types of instances which are for different purposes, like web servers, and graphics servers.</li> <li>Amazon Machine Image - AMIs can be installed on EC2 machines.</li> <li>EC2 machines can be scaled up in no time.</li> </ul> <p>Amazon EC2 Instance Lifecycle</p> <ul> <li>EC2 is charged when running or rebooting. Once terminated, they are deleted forever.</li> <li>To update the VM, duplicate the VMv1.0, make updates to clone, the switch the app to VMv2.0, terminate v1.0</li> </ul>"},{"location":"0-Information-Technology/aws-amazon-web-services/#aws-lambda","title":"AWS Lambda","text":"<ul> <li><code>AWS Lambda</code> is a Serverless compute for running stateless code in response to triggers.</li> <li>Using AWS Lambda, you can run code without managing servers.</li> <li>You pay only for the compute time you consume. There is no charge when your code is not running.</li> <li>With Lambda, you can run code for virtually any type of application or backend.Lambda takes care of everything required to run and scale your code with high availability.</li> <li>You can set up your code to be automatically invoked from other AWS services or call it directly from any web or mobile app.</li> <li>Lambda is a suitable choice for any short-lived application that can finish running in within 15 minutes.</li> </ul> <p>When to use AWS Lambda</p> <ul> <li>Something that can build fast and let you hit the market so you can analyze and see if it works.</li> <li>put the time into - business logic and data processing logic.</li> <li>do not waste time on infra concerns like load balancing, scaling, networking; or plumbing code like logging, authentication, caching exceptions so on.</li> <li>use serverless architecture using lambda, s3, CloudFront, step functions, Cognito, AppSync, and DynamoDB. They are all scaled, available, and charged per-request basis.</li> <li>just define logic as a lambda function, and invoke it via a response to an API call or an event.</li> </ul> <ul> <li>Package and upload code as a lambda function</li> <li>Not running all time. runs when triggered.</li> <li>Lots of triggers exist, like HTTP, upload of a file, events from other AWS services, or inbuilt activity.</li> <li>Runs on managed service, it is scalable.</li> <li>You can choose, env, os, size memory, etc.</li> <li>all lambda runs in their own env.</li> <li>Not for WordPress sites, but for smaller web services or tasks, eg, resizing photos to thumbnails. Lambda is billed only when the function runs, up to 100ms intervals. So you don't need the image-resize service to always run, but only when a photo is uploaded.</li> <li>Create a lambda function, and add a role to it, if it needs to access other AWS services.</li> <li>Add a trigger to invoke the lambda function.</li> <li>Upload code to the lambda function.</li> <li>See it invoked in CloudWatch</li> </ul> <ul> <li>What is lambda?<ul> <li>Compute technology with multiple language support.</li> <li>Hooks to other lambda functions.</li> <li>Changes is other AWS Services, like S3 or Dynamo can trigger lambda.</li> <li>Lambda can be triggered with requests using API Gateway, CloudFront.</li> <li>Data Stream, SNS, messaging service can be linked to lambda</li> <li>IoT events can integrate to lambda.</li> <li>Function can be monitored, tested (without deployment) and dashboards can be built to see this. Deployment with different ways.</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/aws-amazon-web-services/#aws-fargate","title":"AWS Fargate","text":"<ul> <li>It is serverless compute engine for containers. Work with both ECS and EKS.</li> </ul>"},{"location":"0-Information-Technology/aws-amazon-web-services/#amazon-ecs-elastic-container-service","title":"Amazon ECS - Elastic Container Service","text":"<ul> <li>It is Container management services</li> <li>It runs containers on either customer-managed Amazon EC2 instances OR as an AWS-managed serverless offering running containers on AWS Fargate.</li> </ul>"},{"location":"0-Information-Technology/aws-amazon-web-services/#amazon-eks-elastic-kubernetes-service","title":"Amazon EKS - Elastic Kubernetes Service","text":"<ul> <li>run on top of EC2, hence use EC2 as a service.</li> </ul>"},{"location":"0-Information-Technology/aws-amazon-web-services/#amazon-ecr-elastic-container-registry","title":"Amazon ECR - Elastic Container Registry","text":"<ul> <li>Amazon Elastic Container Registry lets you to store, share, and deploy container images.</li> <li>It is a fully managed Docker container registry like docker hub.</li> </ul>"},{"location":"0-Information-Technology/aws-amazon-web-services/#aws-eventbridge","title":"AWS EventBridge","text":"<ul> <li>Event driven applications can be build using this serverless service from AWS.</li> </ul>"},{"location":"0-Information-Technology/aws-amazon-web-services/#aws-step-functions","title":"AWS Step Functions","text":"<p>Step Functions is based on state machines and tasks. In Step Functions, state machines are called <code>workflows</code>, which are a series of event-driven steps. Each step in a workflow is called a <code>state</code>. For example, a Task state represents a unit of work that another AWS service performs, such as calling another AWS service or API. Instances of running workflows performing tasks are called <code>executions</code> in Step Functions.</p> <pre><code>graph LR\n\nWorkflow1 --step--&gt; t1[Task1 \\n has a state]\nWorkflow1 --step--&gt; t2[Task2 \\n has a state]\nWorkflow1 --step--&gt; t3[Task3 \\n has a state]\nWorkflow1 --step--&gt; t4[Task4 \\n has a state]</code></pre> <p>You can create a step function and define the state machine in design or code.</p> <p>State machine can have <code>state</code> as</p> <ul> <li>Action on other AWS service, or</li> <li>Flows which are logics.</li> </ul> <p>Pass is a flow and can be used to pass the JSON data.</p> <p>Map is a flow and can be used to map a function to set of input, eg Map a lambda function to list of numbers. It can map in batches. So for 10 inputs in batch of 5, it will do 2 executions. It takes in list as input and returns list batches, and each batch having list of mapped result. This output needs to be reduced. Eg</p> <pre><code>#input\n{\n  \"MyMultiplicationFactor\": 7,\n  \"MyItems\":[1,2,3,4,5,6,7,8,9,10]\n}\n\n# output\n[\n  {'multiplied': [7,14,21,28,35]},\n  {'multiplied': [42,49,56,63,70]},\n]\n</code></pre> <p>To combine the output, in Output of Map you can use, \"Transform result with <code>ResultSelector</code>\" and define following JSON to combine multiplied keys into an array.</p> <pre><code>{\n  \"multiplied.$\": \"$..multiplied[*]\"\n}\n</code></pre> <p>New Result</p> <pre><code># output\n{'multiplied': [7,14,21,28,35,42,49,56,63,70]}\n</code></pre>"},{"location":"0-Information-Technology/aws-amazon-web-services/#aws-cloudformation","title":"AWS CloudFormation","text":"<p>AWS CloudFormation - lets you create and manage a collection of Amazon Web Services (AWS) resources by provisioning and updating them in a predictable way.</p> <ul> <li>it is a service provided by Amazon Web Services that enables users to model and manage infrastructure resources in an automated and secure manner.</li> <li>Using CloudFormation, developers can define and provision AWS infrastructure resources using a JSON or YAML formatted Infrastructure as Code template.</li> </ul>"},{"location":"0-Information-Technology/aws-amazon-web-services/#aws-cdk-cloud-development-kit","title":"AWS CDK (Cloud Development Kit)","text":"<p>It is AWS service for IaC, Infra as code. Very similar to AWS CloudFormation</p>"},{"location":"0-Information-Technology/aws-amazon-web-services/#sns","title":"SNS","text":"<p>Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications.</p>"},{"location":"0-Information-Technology/aws-amazon-web-services/#amazon-athena","title":"Amazon Athena","text":"<p>Amazon Athena is an interactive query service that makes it simple to analyze data directly in Amazon S3 using standard SQL. It is serverless.</p> <p>Athena can be used to query data from AWS resources, eg from Data Catalogue in Glue and tables in S3.</p>"},{"location":"0-Information-Technology/aws-amazon-web-services/#aws-api-gateway","title":"AWS API Gateway","text":"<p>Usage Plan and Message Throttling</p> <p>Usage plan defines how many call can you do in a time period, eg, 5000 req per month</p> <p>Message Throttling is limiting the rate at which you can hit the api, eg, max 100 req per second</p> <p>lets easily create, publish and maintain apis. it is traffic controller, it is gatekeeper for dynamo db. it is scalable. pay as you go.</p>"},{"location":"0-Information-Technology/aws-amazon-web-services/#amazon-datazone","title":"Amazon DataZone","text":"<p>Build catalog and make data discoverable. Read more?</p>"},{"location":"0-Information-Technology/aws-amazon-web-services/#opensearch-managed-service","title":"OpenSearch Managed Service","text":"<p>It is open source space.</p> <p>Image, doc, audio is converted to vector embeddings. Done using ML models. vector embedding is series of numbers and dots.</p> <p>Neural Search, the search term gets embedded using ML model in AWS Bedrock, then matched with data using KNN.</p> <p>Neural Search with OpenSearch Service</p> <ul> <li>Embedding model can be in sagemaker/bedrock.</li> <li>Hybrid search<ul> <li>it will have lexical and vector combined</li> <li>normalize score apply arithmetic mean, gm, hm. then do knn.</li> </ul> </li> <li>Vector Search<ul> <li>let understand semantic meaning of query</li> </ul> </li> </ul> <ul> <li>lexical search is exact text match for query in doc. token matching</li> <li>Demo</li> </ul> <p>RAG Application Retrieval Augmented Generation</p>"},{"location":"0-Information-Technology/aws-amazon-web-services/#gen-ai","title":"Gen AI","text":"<p>use pre train llm, foundation model FM</p> <p>gen ai needs data quality</p> <p>Value</p> <ul> <li>enhanced personalisation</li> </ul> <p>social media demo app</p> <p>Use case: use social media data and build gen ai app to understand that data. Social media data is pulled in real time using managed, kinesis, flink, kafka. Convert to vector using bedrock, query using <code>opensearch</code>. Q for app.</p>"},{"location":"0-Information-Technology/aws-amazon-web-services/#gen-bi","title":"Gen BI","text":"<p>help marketing team by giving generative BI capability to build generated doc, dashboards. Help them automatically generate dashboards.</p> <p>Add synonyms to data fields so that amazon q helps answer the questions. like region/platform are same.</p> <p>You can build quicksite dashboard, but customize it using amazon-q. like convert datetime to weekly, add color by dim1. All this can be said in natural language.</p> <p>Build calculations using amazon-q natural lang questions.</p>"},{"location":"0-Information-Technology/aws-amazon-web-services/#boto3","title":"Boto3","text":"<p>Boto3 is the Amazon Web Services (AWS) Software Development Kit (SDK) for Python, which allows Python developers to write software that makes use of services like Amazon S3 and Amazon EC2.</p>"},{"location":"0-Information-Technology/aws-amazon-web-services/#amazon-rds-relational-database-service","title":"Amazon RDS - Relational Database Service","text":"<ul> <li>RDS is in one private subnet. to make it high-availability, create a secondary RDS in another AZ subnet, using <code>RDS Multi-AZ deployment</code>. RDS will manage replication. One is primary another secondary. failovers handled by RDS.</li> <li>RDS is charged per hour, whether you query or not. Weekends are charged too.</li> </ul>"},{"location":"0-Information-Technology/aws-amazon-web-services/#amazon-redshift","title":"Amazon Redshift","text":"<p>Amazon redshift is managed Data Warehouse. It hyper tuned Postgres for faster reads to build OLAP data warehouse.</p>"},{"location":"0-Information-Technology/aws-amazon-web-services/#demo-hands-on-implementation-build-employee-directory-app","title":"DEMO - Hands on Implementation - Build Employee Directory App","text":"<p>We will create a basic CRUD app, an employee directory app on AWS.</p> <p>Architecture:</p> <ul> <li>App is hosted on Private Network using, <code>VPC - Virtual Private Cloud</code>.</li> <li>Backend on <code>EC2 - Elastic Compute</code>, which is VM on AWS. The app code is here without the data or files. hence this can be replicated to achieve high availability.</li> <li>Data is stored in the database in the same NW, either <code>RDS - relational data service</code> or <code>DynamoDB - key value</code>.</li> <li>Images in <code>S3 - simple storage service</code>.</li> <li>Monitoring using <code>CloudWatch</code> for health monitoring.</li> <li>Scalability using <code>ELB - Elastic Load Balancer</code>. This will distribute the load on the available servers (instances). Also <code>EC2 AutoScaling</code> will help scale out or in based on demand.</li> <li><code>IAM - Identity and Access Management</code> for security and access management.</li> <li><code>AWS Management Console</code> - to build all this.</li> </ul> <p>Demonstration: Implement security with AWS Identity and Access Management (IAM)</p> <ul> <li>Create Groups, then add Policies to them.</li> <li>Add Users to the Groups.</li> <li>For Eg, create a group to read-only ec2 state, or read-only s3.</li> </ul> <p>Employee Directory Application Hosting</p> <ul> <li>Create an EC2 machine<ul> <li>Use default VPC, this is network. each ec2 has to be in a network</li> <li>Choose a Role for this machine, eg, EC2 to have full access to S3 and Dynamo DB.</li> <li>User Data: Add script to run when this machine boots. This is Linux 'profile/env' info, like exporting variables and setting paths. Additionally, it can have, download code, unzip, install requirements, set the flask app path, and run the app.</li> <li>Configure Security Group - this is to allow HTTP requests to your machine, by adding security groups.</li> <li>Finally, launch. Once launched, you will get a public IP address, which will let you access the app.</li> </ul> </li> </ul> <p>EC2</p> <ul> <li>Use <code>Amazon Linux 2 AMI</code>, select <code>t2.micro</code> free tier instance.</li> <li>Add script, that will:<ul> <li>update all</li> <li>install node</li> <li>create app dir</li> <li>download and unzip the app code</li> <li>install dependencies</li> <li>run the app, <code>npm start</code></li> </ul> </li> <li>add storage, 8 GB is enough.</li> <li>security group, acts as a virtual firewall that controls access. we need web traffic for the app and ssh for management. HTTP allows inbound traffic on port 80.</li> <li>Connect to instance, select the instance and click connect, and then again connect. you can connect to the EC2 machine from the browser shell ssh.</li> </ul> <p>Networking</p> <ul> <li>Doing this on the AWS Console<ul> <li>Login to Console &gt; EC2 &gt; <code>Elastic IPs</code><ul> <li>Allocate Elastic IP for EC2.</li> </ul> </li> <li>Search VPC &gt; Wizard &gt; Public &amp; Private <code>subnets</code><ul> <li>Add VPC Name &gt; Then add AZs to subnets. Here one zone, AZ-a is selected.</li> </ul> </li> <li>VPC &gt; Subnets - you can add <code>more subnets</code> by<ul> <li>Create Subnet</li> <li>Select VPC ID &gt; Subnet Name &gt; AZ-b &gt; CIDR in range &gt; create.</li> <li>Similarly, create private too.</li> </ul> </li> <li>Now <code>Routing-Tables</code><ul> <li>Click Route table</li> <li>select route-table for VPC ID</li> <li>click routes - see that internet traffic is going to NAT Gateway.<ul> <li>click <code>subnet-associations</code> tab</li> </ul> </li> <li>Similarly, do for the public.</li> </ul> </li> <li>Create a <code>Security Group</code> for more security<ul> <li>add name</li> <li>add to vpc</li> <li>inbound: HTTP, source: anywhere. Outbound added auto.</li> </ul> </li> <li>Create <code>EC2</code> &gt; configuration<ul> <li>Network: Lab VPC</li> <li>Subnet: public 1</li> <li>auto-assign public IP: enable</li> <li>Config security group, pick the one created</li> </ul> </li> <li>Create a second EC2 similarly and add a second public subnet.</li> </ul> </li> <li>Now the app should work with added security, high availability, and resource restriction.</li> </ul> <p>Storage Demonstration: Create an Amazon S3 Bucket</p> <ul> <li>S3 bucket is created in the region, not tied to the subnet</li> <li>Open Console &gt; search S3 &gt; Create <code>Bucket</code></li> <li>Bucket Name: emp-photo-bucket-012</li> <li>Region: place in the same region as ec2</li> <li>create bucket</li> <li>Open Bucket</li> <li>Click the <code>Permission</code> tab</li> <li>Bucket Policy &gt; edit &gt; enter new Policy JSON, <code>IAM</code> role to allow the app access to the bucket.</li> <li>Add files as objects - click <code>upload</code></li> <li>Bucket should be accessible via the app.</li> </ul> <p>Scaling</p> <ul> <li>demo<ul> <li>make a <code>launch template</code>, what to launch when scaling</li> <li>console - ec2 - sidebar launch template - create - give a name and desc - check autoscaling</li> <li>AMI - create a mirror image of the web server, and select AMI AND t2 MICRO.</li> <li>SELECT SAME KEY-PAIR, same security group, expand advance</li> <li>IAM same role</li> <li>pass user data.</li> <li>This completes 'what to launch'</li> <li>Now, 'when to launch'<ul> <li>sidebar - 'autoscaling group' - create - select template - same vpc, select public subnets</li> <li>attach to the load balancer.</li> <li>define group size.</li> </ul> </li> </ul> </li> </ul> <ul> <li>demo: ec2 - sidebar - load balancers - create - ALB - give a name<ul> <li>scheme<ul> <li>Internet-facing - to manage client req</li> <li>internal load balancer (private IP to private IP) - for 3-tier apps</li> </ul> </li> <li>listeners - default + HTTPs</li> <li>availability zones - choose vpc, check both availability zone and public subnets</li> <li>security group - all port 80 from anywhere</li> <li>routing - give name - next - choose instances - next - create.</li> <li>find the DNS name in detail, and open it.</li> <li>this is the app being served from two availability zones and the traffic is managed by the load balancer.</li> </ul> </li> </ul> <p>Demonstration: Configure High Availability for Your Application</p> <ul> <li>Database Dynamo and file server S3 are both highly available and scalable within a region. Only EC2 will limit. So we make it available by using a load balancer and scalable using autoscaling, which adds and removes instances based on load.</li> </ul> <p>Employee Directory Application Redesign</p> <ul> <li>The Architecture<ul> <li>the app is hosted across ec2 instances inside VPC, in private subnets.</li> <li>ec2 part of autoscaling, traffic is managed by the app load balancer</li> <li>database on dynamo</li> <li>file in s3</li> </ul> </li> <li>to ensure this all works, analyze that the auto-scaling policy is working as expected. may need some tweaking to work over time. Also, install security patches and updates for EC2 as they come out.</li> <li>Now, we can redesign the app to make it completely serverless using Cloud Native services like AWS Lambda. and explore other architectures possible.</li> <li>As of now, the app is a 3-tier app<ul> <li>presentation Layer - UI - HTML, CSS, JS, or Mobile</li> <li>application Layer - Business/Application Logic</li> <li>data layer - database</li> </ul> </li> <li>EC2 is being used for both, the presentation layer and application. To improve this we can separate, these layers.</li> <li>Presentation on S3, as static website. Not all sites are static, but JS can help with this, React sites are static and make the content dynamic by using JS to make HTTP requests.</li> <li>Application Layer to be hosted as Lambda, each of CRUD as separate Lambda function.</li> <li>Amazon API Gateway, can be used to make the front end talk to lambda functions serving different events in the backend.<ul> <li>API on API Gateway acts as a front door to trigger the backend code on lambda. We can have one lambda function for all events or separate for each.</li> </ul> </li> <li>Dynamo for database, s3 for files.</li> <li>All the accesses are handled by RBAC via IAM.</li> <li>This way we can make the app modular. this can help make changes quickly without making whole infra to change and test. untouched DB, but can update code.</li> <li>Further, Amazon RT53 can be used to manage domains name calls</li> <li>CloudFront can be used to catch static content and make it available close to end users using the global infra of AWS</li> <li>With this serverless architecture, compared to the EC2 solution, we have made the app scalable, and available and thus can reduce cost. VPC and networking are managed.</li> <li>Another option is using container services. All service in AWS is API based, thus we can use any of them</li> </ul>"},{"location":"0-Information-Technology/aws-amazon-web-services/#demo-build-a-serverless-web-application","title":"DEMO - Build a Serverless Web Application","text":"<p>with AWS Lambda, Amazon API Gateway, AWS Amplify, Amazon DynamoDB, and Amazon Cognito</p> <p>Amplify Console provides continuous deployment and hosting of static web resources including HTML, CSS, JavaScript, and image files which are loaded in the user's browser. JavaScript executed in the browser sends and receives data from a public backend API built using Lambda and API Gateway. Amazon Cognito provides user management and authentication functions to secure the backend API. Finally, DynamoDB provides a persistence layer where data can be stored by the API's Lambda function.</p> <p></p> <p>Static Web Hosting - AWS Amplify hosts static web resources including HTML, CSS, JavaScript, and image files which are loaded in the user's browser.</p> <p>User Management - Amazon Cognito provides user management and authentication functions to secure the backend API.</p> <p>Serverless Backend - Amazon DynamoDB provides a persistence layer where data can be stored by the API's Lambda function.</p> <p>RESTful API - JavaScript executed in the browser sends and receives data from a public backend API built using Lambda and API Gateway.</p>"},{"location":"0-Information-Technology/aws-amazon-web-services/#demo-aws-rds-db-standalone","title":"DEMO - AWS RDS DB Standalone","text":"<p>You can create a  standalone RDS DB Instance that can be accessed from any other machine, it can be public machine or another service on AWS within VPC (like EC2 or Lambda).</p> <p>The database you create, needs following additional configurations:</p> <ul> <li>Network it will be on - This will be only network (not security), that is, the VPC it will be part of. Also we tell subnets and Availability Zones. For this you need <code>VPC security groups</code>.</li> <li>Security in Network - Now that DB is on network, you need <code>Security Group</code> so that you can define <code>Rules</code> that will control inbound and outbound traffic, that is, you can allow certain, or specific IP to access DB.</li> <li>Roles and Permission - This is next layer, where from the allowed machines, only specific user can do certain things, like, user1 can only read data but cannot write.</li> </ul> <p>Once done, you can use Endpoint (also called host), port, username and password to connect from any client/app.</p> <p>Password can be managed using <code>AWS Secret Manager</code>, it has a secret associated with instance, that secret is obscured and can be reviled on console or using code.</p>"},{"location":"0-Information-Technology/aws-amazon-web-services/#aws-stack-for-api-development","title":"AWS Stack for API Development","text":"<p>AWS offers API Gateway and other serverless techniques to build API</p> <p>Amazon API Gateway</p> <p>It lets build the API by providing front-end to:</p> <ul> <li>create a resource, like <code>order</code></li> <li>create methods on resource, like <code>GET</code></li> <li>define integration (how to do get), like, Lambda, HTTP, AWS Service etc.</li> </ul> <p>So, in it basically you define everything in \"AWS API Gateway front end\", including each resource and it's method. You can handle all request method like GET PUT POST.. in one lambda function or have multiple defined, one to handle each request.</p> <p>Finally, you need to deploy the API.</p> <p>Next steps, would be to implement security measures like API Key.</p> <p>To summarize AWS services are:</p> <ul> <li>API Gateway \u2014 web server / deploy This service is responsible for deploying and serving HTTP RESTful endpoints. Thus you can trigger actions, when HTTP calls arrives to the generated endpoints. It enables you to create, publish, maintain, monitor, and secure your own REST and Websocket APIs at any scale. It acts as router from HTTP request to lambda function.</li> <li>Lambda \u2014 business logic / compute This let you run code without provisioning or managing servers.</li> <li>DynamoDB \u2014 storage The NoSQL amazon database, where you can insert the information of your application on tables (Collections).</li> </ul> <p>Using Flask in Lambda You can add flask app to lambda function and create routes to handle all CRUD requests. More on Flask on Lambda with CORS and WSGI</p> <p>Links</p> <ul> <li>Building a REST API with AWS Gateway and Python</li> <li>Serverless API with AWS and Python Tutorial</li> <li>Deploying Python Flask microservices to AWS using open source tools</li> </ul>"},{"location":"0-Information-Technology/aws-amazon-web-services/#links","title":"Links","text":"<ul> <li>https://explore.skillbuilder.aws/learn/course/1851/play/45289/aws-technical-essentials-104</li> <li>More about compute, how to select between ec2, lambda and container - 40 mins - https://explore.skillbuilder.aws/learn/course/internal/view/elearning/199/aws-compute-services-overview?dt=tile&amp;tile=fdt</li> <li>Data Analytics, volume, variety, velocity, veracity ETL, value VIZ - 4hr - https://explore.skillbuilder.aws/learn/course/internal/view/elearning/44/data-analytics-fundamentals?dt=tile&amp;tile=fdt</li> <li><code>Heroku</code> is an alternative PaaS for deploying container-based apps on the cloud.</li> <li>Build a Serverless Web Application with AWS Lambda, Amazon API Gateway, AWS Amplify, Amazon DynamoDB, and Amazon Cognito. Links:</li> <li>https://aws.amazon.com/getting-started/hands-on/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/</li> <li>https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-dynamo-db.html</li> <li>https://medium.com/rahasak/build-serverless-application-with-aws-amplify-aws-api-gateway-aws-lambda-and-cognito-auth-a8606b9cb025</li> <li>https://trackit.io/aws-api-gateway-create-api-python-cognito-serverless/</li> <li>AWS Big Data Blog</li> <li>Kinesis dta generator</li> </ul> <p>You don't necessarily need a static server to run a Create React App project in production. The static web app allows you to host static pages written on frameworks such as Angular, react, vue-js, etc.</p>"},{"location":"0-Information-Technology/bokeh/","title":"Bokeh","text":"<p>all about bokeh visualization library</p> <p>How it is approached?</p> <ul> <li>Create building blocks of viz</li> <li>Customize these to fit your needs</li> </ul> <p>How it is implemented?</p> <ul> <li><code>pip install bokeh</code> installs py lib that defines content and functionalities of viz</li> <li>the bokeh-javascript lib, uses the py-output and renders charts.</li> <li>py-output can be html, json, components, or images.</li> </ul>"},{"location":"0-Information-Technology/bokeh/#basics-of-bokeh","title":"Basics of Bokeh","text":"<p>You need a blank chart, add marks on that chart using data and finally show the chart. Use <code>bokeh.plotting</code> module.</p> <ul> <li><code>figure()</code> creates a plot</li> <li><code>line()</code> draws line</li> <li><code>show()</code> build complete html, css, js, json as a standalone doc and saves it.</li> </ul> <pre><code>from bokeh.plotting import figure, show\n\n# prepare some data\nx = [1, 2, 3, 4, 5]\ny = [6, 7, 2, 4, 5]\n\n# create a new Figure object\np = figure()\n\n# add a line renderer, with optional thickness\np.line(x, y, line_width=2)\n\n# show the results\nshow(p)\n</code></pre>"},{"location":"0-Information-Technology/bokeh/#html-file","title":"HTML File","text":"<ul> <li>The file build using <code>show()</code> command has<ul> <li>js-libs in <code>head</code></li> <li><code>div</code> with id to hold chart canvas</li> <li><code>js-script</code> tag having<ul> <li>chart-items as <code>json</code> and<ul> <li><code>js-code</code> to build chart and render in the div.</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"0-Information-Technology/bokeh/#plot-figure-chart","title":"Plot / Figure / Chart","text":"<ul> <li>figure() function accpets args to modify the figure.</li> <li>It has sub-classes and attributes that can be used to modify figure.</li> <li>you can set background</li> </ul>"},{"location":"0-Information-Technology/bokeh/#marks-gylphs-renderers","title":"Marks / Gylphs / Renderers","text":"<ul> <li>You can add as many marks you want and combine different types. Eg, line, bars, circles, hex-tiles.</li> <li>Each type has a function like <code>line()</code>, <code>circle()</code>. All are under <code>Figure</code> class</li> <li>each function accepts args to define data (x, y) and modify visuals (color, size).</li> <li>Eg, <code>p.circle(x, y, legend_label=\"Objects\", color=\"#bb5566\", size=16)</code></li> </ul> <ul> <li>Existing properties can be modified using the object\u2019s <code>glyph</code> attribute and change its properties.<pre><code># change color of previously created object's glyph\nglyph = circle.glyph\nglyph.fill_color = \"blue\"\n</code></pre> </li> </ul>"},{"location":"0-Information-Technology/bokeh/#plot-legends","title":"Plot Legends","text":"<ul> <li>Use <code>legend_label=</code> attribute of mark, to set label name. Eg, <code>p.line(x, y, legend_label=\"Temprature\"</code>.</li> </ul> <ul> <li>This adds <code>Legend</code> object to plot. You can then use its properties to modify it. You can edit location, font, color, border, background etc. Eg, <code>p.legend.location = \"top_left\"</code></li> </ul> <ul> <li>Interactive Legends<ul> <li>click to show/hide/fade marks.</li> <li><code>p.legend.click_policy=\"mute\"</code> or hide. <code>click_policy</code> property allows to do this.</li> <li>Bokeh - Interaction Legends</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/bokeh/#plot-titles","title":"Plot Titles","text":"<ul> <li>Adding <code>title=</code> to <code>figure()</code> adds <code>Title</code> class.</li> <li>Its properties can modify the title. <code>p.title.align = \"right\"</code></li> </ul>"},{"location":"0-Information-Technology/bokeh/#plot-labels-annotations","title":"Plot Labels &amp; Annotations","text":"<ul> <li>what - they are visual elements that help chart to read. Eg: like bands, lines, labels to marks, boxes, arrows etc.</li> <li>add <code>BoxAnnotation</code> objects to figure using <code>add_layout()</code>.</li> <li>you can add as many boxes, lines as you want.</li> </ul> <pre><code># Labels\nfrom bokeh.models import LabelSet, BoxAnnotation\n\nsource = ColumnDataSource(data=dict(\n    height=[66, 71, 72, 68, 58, 62],\n    weight=[165, 189, 220, 141, 260, 174],\n    names=['Mark', 'Amir', 'Matt', 'Greg', 'Owen', 'Juan']\n))\n\np = figure()\np.scatter(x='weight', y='height', size=8, source=source)\n\nlabels = LabelSet(x='weight', y='height', text='names',\n                  x_offset=5, y_offset=5, source=source)\n\nmid_box = BoxAnnotation(bottom=60, top=70, fill_alpha=0.2, fill_color=\"#009E73\")\n\np.add_layout(labels)\np.add_layout(mid_box)\n</code></pre> <ul> <li>more Bokeh - Basic Annotations</li> </ul>"},{"location":"0-Information-Technology/bokeh/#plot-tooltips","title":"Plot Tooltips","text":"<ul> <li>they appear on mouse-hover or tap on marks</li> <li>use <code>HoverTool</code> class</li> <li>Include <code>HoverTool()</code> in the list passed to the <code>tools=[]</code> argument when calling the <code>figure()</code> function</li> <li>other than <code>x</code>, <code>y</code> there are many more properties available that can be shown.</li> <li>images can be added too.</li> </ul> <pre><code>from bokeh.models import HoverTool\n\np = figure(\n    ...\n    tools=[HoverTool()],\n    tooltips=\"Data point @x has the value @y\",\n)\n</code></pre>"},{"location":"0-Information-Technology/bokeh/#plot-theme","title":"Plot Theme","text":"<ul> <li>what - they are set of predefined parameters for color, font, line styles.</li> <li>You can use pre-defined themes or add yours by defining properties in yaml file.</li> <li>more Bokeh - Styling Themes</li> </ul>"},{"location":"0-Information-Technology/bokeh/#plot-axis","title":"Plot Axis","text":"<ul> <li>You can modify axis-label, axis-numbers its format, axis-colors, ticks, width, orientation, range, log-scales etc.</li> </ul> <pre><code>from bokeh.models import NumeralTickFormatter, DatetimeTickFormatter\n\n# change some things about the y-axis\np.y_range=(0, 25)\np.yaxis.major_label_orientation = \"vertical\"\n\n# change things on all axes\np.axis.minor_tick_in = -3\n\n# format axes ticks\np.yaxis[0].formatter = NumeralTickFormatter(format=\"$0.00\")\np.xaxis[0].formatter = DatetimeTickFormatter(months=\"%b %Y\")\n</code></pre>"},{"location":"0-Information-Technology/bokeh/#plot-grids-banding","title":"Plot Grids / Banding","text":"<ul> <li>what - grids are lines on graph that connect to axis for easier reads.</li> <li><code>Plot</code> object has methods <code>xgrid()</code>, <code>ygrid()</code>, and <code>grid()</code></li> </ul> <pre><code># change things only on the y-grid\np.ygrid.grid_line_alpha = 0.8\np.ygrid.grid_line_dash = [6, 4]\n\n# add bands to the y-grid\np.ygrid.band_fill_color = \"olive\"\np.ygrid.band_fill_alpha = 0.1\n# define vertical bonds\np.xgrid.bounds = (2, 4)\n</code></pre>"},{"location":"0-Information-Technology/bokeh/#plot-toolbar","title":"Plot Toolbar","text":"<ul> <li>Toolbar can be hidden, show few options, change location etc.</li> <li>You can set attributes of figure class, or</li> <li>Use <code>Figure.Toolbar</code> class properties to modify things.</li> </ul> <ul> <li><code>p = figure(toolbar_location=\"below\")</code> shows below</li> <li><code>p.toolbar_location = None</code> hide</li> <li><code>p.toolbar.autohide = True</code> auto hide</li> </ul>"},{"location":"0-Information-Technology/bokeh/#plot-sizes-multi-plots","title":"Plot Sizes &amp; Multi-plots","text":"<ul> <li>You can define size in pixels the attributes <code>width=</code> and <code>height=</code> when calling the <code>figure()</code> function. Eg: <code>figure(width=350,height=250)</code> or <code>p.height = 150</code></li> </ul> <ul> <li>Responsive<ul> <li>plot can be made using attribute <code>sizing_mode=\"stretch_width\"</code>.</li> <li>you can strech or scale in height/width/both</li> </ul> </li> </ul> <ul> <li>Multi-Plots<ul> <li> <p>Use <code>bokeh.layouts</code> package</p> <pre><code>from bokeh.layouts import row\n\n# show in one row horizontally\nshow(row(p1, p2, p3))\n\n# put the results in a row that automatically adjusts\n# to the browser window's width\nshow(row(children=[s1, s2, s3], sizing_mode=\"scale_width\"))\n</code></pre> </li> </ul> </li> </ul>"},{"location":"0-Information-Technology/bokeh/#dom-widgets","title":"DOM Widgets","text":"<ul> <li>what - You can add DOM elements like <code>DatePicker</code>, <code>Select</code>, <code>RangeSlider</code>, <code>Div</code> etc.</li> <li>structure - They are available as class in <code>bokeh.models</code> package, each class has attributes to define them.</li> <li>create - you can create object of class and define it.</li> <li>linking - you can use <code>js_link()</code> function of the object to link value of widget with renderer's glyph propertly, eg, link <code>RangeSlider</code> <code>value</code> to <code>size</code> of <code>circle</code>.</li> <li>rendering - create layout of all widgets and plots using <code>layout()</code> function of <code>bokeh.layouts</code> package.</li> </ul> <pre><code>from bokeh.layouts import layout\nfrom bokeh.models import Div, Spinner\n\n# prepare some data\nx = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\ny = [4, 5, 5, 7, 2, 6, 4, 9, 1, 3]\n\n# create plot with circle glyphs\np = figure(x_range=(1, 9) )\npoints = p.circle(x=x, y=y, size=30)\n\n# set up textarea (div)\ndiv = Div(text=\"&lt;p&gt;Usage...&lt;/p&gt;\", width=200, height=30)\n\n# set up spinner\nspinner = Spinner(\n    title=\"Circle size\", low=0, high=60, step=5, width=200,\n    value=points.glyph.size, # default value\n)\nspinner.js_link(\"value\", points.glyph, \"size\")\n\n# create layout\nlayout = layout(\n    [\n        [div, spinner],\n        [p],\n    ]\n)\n\n# show result\nshow(layout)\n</code></pre> <ul> <li>Use Panel, a wrapper on Bokeh, that allows advanced dashboarding.</li> </ul>"},{"location":"0-Information-Technology/bokeh/#data-sources","title":"Data Sources","text":"<ul> <li>data can be list, numpy array, pandas dataframe.</li> <li>they are all translated to <code>ColumnDataSource</code> (CDS)</li> <li>you can create your own CDS for more options</li> <li>You can pass Pandas <code>DataFrame</code> to <code>ColumnDataSource</code></li> </ul> <pre><code>from bokeh.models import ColumnDataSource\n\n# create dict as basis for ColumnDataSource\ndata_dict = {'x_values': [1, 2, 3, 4, 5],\n        'y_values': [6, 7, 2, 3, 6]}\n\n# create ColumnDataSource based on dict\nsource = ColumnDataSource(data=data_dict)\n\n# create a plot and renderer with ColumnDataSource data\np = figure()\np.circle(x='x_values', y='y_values', source=source)\n\n# Using Pandas\nsource = ColumnDataSource(df)\n</code></pre>"},{"location":"0-Information-Technology/bokeh/#data-transformation","title":"Data Transformation","text":"<ul> <li>Vectorizing - Create dynamically calculated colors and sizes based on data. You can generate sequence color and size based on data (instead of categorical).</li> <li><code>fill_color=[]</code> attribute of mark like <code>p.circle()</code> can take in one color or list of colors. List can be built with anylogic, for eg, generate color list based on y-data. Same can be given to <code>radius=[]</code> attribute.</li> <li>Or you can use <code>palettes</code> and <code>transform</code> from bokeh module. To make this more systematic. Use the <code>linear_cmap()</code> function to build color map. The required attributes for this function are:<ul> <li><code>field_name=</code>: the data sequence to map colors to</li> <li><code>palette=</code>: the palette to use</li> <li><code>low=</code>: the lowest value to map a color to</li> <li><code>high=</code>: the highest value to map a color to</li> </ul> </li> </ul> <pre><code># generate radii and colors based on data\nradii = y / 100 * 2\ncolors = [f\"#{255:02x}{int((value * 255) / 100):02x}{255:02x}\" for value in y]\n\np.circle(x, y, radius=radii, fill_color=colors)\n\n# Use Palette and Transformations\nfrom bokeh.transform import linear_cmap\nfrom bokeh.palettes import Turbo256\n\n# build list of color from palette based on y data\nmapper = linear_cmap(field_name=\"y\", palette=Turbo256, low=min(y), high=max(y))\n</code></pre>"},{"location":"0-Information-Technology/bokeh/#data-streaming","title":"Data Streaming","text":"<ul> <li>Data Streaming - AjaxDataSource</li> </ul>"},{"location":"0-Information-Technology/bokeh/#bokeh-server","title":"Bokeh Server","text":"<ul> <li>Bokeh server lets you connect events and tools to real-time Python callbacks that run on the server</li> </ul>"},{"location":"0-Information-Technology/bokeh/#render-serve-output","title":"Render / Serve / Output","text":"<ul> <li>Standalone Documents have HTML, CSS, and JavaScript but don't require Bokeh Server.</li> </ul> <ul> <li>HTML - You can generate complete HTML</li> </ul> <ul> <li>JSON - You can get plot as JSON, that can be loaded via AJAX</li> </ul> <ul> <li>Components - Get HTML, JS separately.</li> </ul> <ul> <li>PNG / SVG export - requires selenium and web drivers.</li> </ul> <ul> <li>HTML save - use <code>save()</code> to save built HTML complete file.</li> </ul> <pre><code># 1. Build the figure\nfrom bokeh.plotting import figure\np=figure()\np.circle(x=[1,2], y=[3,4])\n\n\n# 2a: Get HTML complete\nfrom bokeh.embed import file_html\nfrom bokeh.resources import CDN\nscript_tags_str = CDN.render()\n# script tags that have link to bokeh-js files, can be included in header\n\nhtml = file_html(plot, CDN, \"my plot\")\n# Complete HTML file with head, body, script, json-date, div. All built.\n# You can save this file and open in browser\n\n# 2b: Get JSON dumps. AJAX this, other JS code goes in template\nimport json\nfrom bokeh.embed import json_item\nitems_json_py_obj = json_item(p) \n# get JSON items as python dictionary\n\nitems_std_json_str = json.dumps(json_item(p)) \n# get JSON-standard string that can be sent / used in JavaScript\n\n\n# 2c: Get html, JS-code separately\nfrom bokeh.embed import components\nscript, div = components(p)\n# script has JSON-items and JS-code\n# div - has HTML code. JS fills div when executed\n</code></pre>"},{"location":"0-Information-Technology/bokeh/#links","title":"Links","text":"<ul> <li>Bokeh - Showcase - Users add their work.</li> <li>Bokeh First Steps - Basics to get started.</li> <li>Bokeh User Guide - Detailed user guide</li> <li>Bokeh - Web Page Embed</li> <li>Bokeh - Sunburst Chart Example</li> <li>Medium - Streaming Data Animation With Bokeh</li> <li>Flourish - Animated Charts, scrolly, story, race</li> </ul>"},{"location":"0-Information-Technology/bootstrap-ui/","title":"Bootstrap UI","text":"<p>twitter boortrap UI library</p>"},{"location":"0-Information-Technology/bootstrap-ui/#responsive-breakpoints","title":"Responsive breakpoints","text":"<p>Lets you display same content differently on different devices. more</p> <p>Use, <code>{-sm|-md|-lg|-xl}</code>.</p> <pre><code>// Extra small devices (portrait phones, less than 576px)\n// No media query since this is the default in Bootstrap\n\n// Small devices (landscape phones, 576px and up)\n@media (min-width: 576px) { ... }\n\n// Medium devices (tablets, 768px and up)\n@media (min-width: 768px) { ... }\n\n// Large devices (desktops, 992px and up)\n@media (min-width: 992px) { ... }\n\n// Extra large devices (large desktops, 1200px and up)\n@media (min-width: 1200px) { ... }\n</code></pre>"},{"location":"0-Information-Technology/bootstrap-ui/#navbar","title":"Navbar","text":"<p>Responsive behaviour - <code>navbar-expand-lg</code> will make the navbar expanded on devices 992 and beyond. On devices &lt;991 the navbar comes as hambureger icon.</p>"},{"location":"0-Information-Technology/bootstrap-ui/#table","title":"Table","text":"<p>Responsive table are good on small device, they make only table scroll horizontally but body width remains of device width. However, on larger device, this makes table not 100%. So user <code>table-responsive-sm</code> so that table is responsive only on sm and lower device, else it stays 100%, change <code>-sm</code> according to width of table.</p>"},{"location":"0-Information-Technology/bootstrap-ui/#layouts","title":"Layouts","text":"<p>Show divs of different width on different devices</p> <pre><code>&lt;div class=\"col-md-6 col-xl-4\"&gt;\n    Shows 2-col on medium and beyond, 3-col on x-large and beyond\n&lt;/div&gt;\n</code></pre>"},{"location":"0-Information-Technology/bootstrap-ui/#flex","title":"Flex","text":"<ul> <li>More on Bootstrap Docs Flex</li> <li>And on https://css-tricks.com/snippets/css/a-guide-to-flexbox/</li> <li>and https://css-tricks.com/almanac/properties/a/align-items/</li> </ul>"},{"location":"0-Information-Technology/bootstrap-ui/#css-variables","title":"CSS Variables","text":"<p>BS uses CSS variables that are defined once and used in whole CSS document. Eg,</p> <pre><code>/* declare */\n:root {\n  --bs-blue: #0d6efd;\n}\n\n/* Usage */\na {\n  color: var(--bs-blue);\n}\n  ```\n\n\n\n## Snippets\n\n**Hiding Elements**\n\nHide only on Small Devices `.d-none .d-sm-block`, [more](https://getbootstrap.com/docs/4.0/utilities/display/#hiding-elements).\n\n**View More**\n\n```html\n&lt;div&gt;\n    &lt;a data-toggle=\"collapse\" href=\"#emp_details\" role=\"button\"\n    aria-expanded=\"false\" aria-controls=\"emp_details\" title=\"click to toggle\"&gt;Extra Details &gt;&lt;/a&gt;\n&lt;/div&gt;\n\n&lt;br/&gt;\n\n&lt;div class=\"collapse multi-collapse\" id=\"emp_details\"&gt;\n    &lt;div class=\"card card-body\"&gt;\n        {% for attr, value in emp.__dict__.items() %}\n        &lt;div class=\"d-flex justify-content-between\"&gt;\n            &lt;div&gt;{{ attr }}&lt;/div&gt;\n            &lt;div&gt;{{ value }}&lt;/div&gt;\n        &lt;/div&gt;\n        {% endfor %}\n    &lt;/div&gt;\n&lt;/div&gt;\n</code></pre>"},{"location":"0-Information-Technology/bootstrap-ui/#links","title":"Links","text":"<ul> <li>CM - BS5 Starter Template</li> </ul>"},{"location":"0-Information-Technology/bootstrap-ui/#primer-design-system-github-ui","title":"Primer Design System - GitHub UI","text":"<p>Primer is a set of guidelines, principles, and patterns for designing and building UI at GitHub.</p> <p>Link:</p> <ul> <li>Primer Design System</li> </ul>"},{"location":"0-Information-Technology/bootstrap-ui/#branding","title":"Branding","text":"<p>Color of brand defines it and grabs attention of user. It is important to have a color palette for your brand.</p> <p>Brand can have:</p> <ul> <li>One color - Nike</li> <li>2 color pallete - primary and secondary - PayTm FedEx Tesco</li> <li>Multi color - like google</li> </ul>"},{"location":"0-Information-Technology/bootstrap-ui/#two-color-palette","title":"Two Color Palette","text":"<p>This has a primary which is mostly shown in branding, secondary is usually used less than primary. They can be adjacent or complementary on color wheel.</p> <p>Color and emotions</p> <ul> <li>blue is corporate tech finance</li> <li>gym goes with maroon, army green, yellow</li> <li>4E6E58 green, 544B3D brown</li> <li>FE9500 orange, #f58c1d</li> <li> </li> </ul> <p>Examples</p> <ul> <li>Fed Ex - #fe5900 orange. #2b007c purple - view</li> <li>PayTm - Dark Blue #012A72, Light Blue #00B8F5 - view</li> </ul> <p>Tried Pallets</p> <ul> <li>bright orange #ff532f #ec3e02 - button, second #89190d</li> <li>from https://coolors.co/visualizer/993b00-ff5833</li> <li> </li> <li>https://mycolor.space/?hex=%23FF5833&amp;sub=1</li> <li>https://coolors.co/visualizer/ff5833-3a001e - Orange Brown</li> <li>https://coolors.co/visualizer/23025f-ff5833 - Orange Purple</li> <li>dark second, #660a00</li> </ul> <ul> <li>Recommends contrast color</li> </ul>"},{"location":"0-Information-Technology/bootstrap-ui/#6a3599-purple","title":"6a3599 purple","text":""},{"location":"0-Information-Technology/bootstrap-ui/#ff5833-993b00","title":"FF5833, #993B00","text":""},{"location":"0-Information-Technology/docker/","title":"Dockerize","text":"<p>all about docker, docker-compose, images and containers</p>"},{"location":"0-Information-Technology/docker/#physical-world-to-docker","title":"Physical World to Docker","text":"<p>An app, needs files (eg, html) to run with other OS files (eg Ubuntu). Docker lets do this virtually. Docker is virtual engine that lets do all this using things like image, container etc explained below. App may be working on multiple server through network, eg, web-server connects to database-server. This can be done using multiple containers in docker.</p>"},{"location":"0-Information-Technology/docker/#overview","title":"Overview","text":"<p>Docker Image is the files required for an app, including system files and binaries.</p> <p>Containers is running instance of the image, that is, when the files run.</p> <p>Now when a service runs it may produce some other files, these land into area called scratch space.</p> <p>Scratch Space is non-persistent, not shared in between containers. Each container has its own space and is destroyed when container ends.</p> <p>Named Volume is volume having name. Volume is a file-system. Volume can be created using docker. Imagine you have created your own hard-disk. Now you can attach it to docker container. Attaching happens by mounting it to a path on container. Volume content can be opened on host machine (<code>orbstack</code> volumes can be found under orb mount).</p> <p>Above is good to store some data from container. However, you may need to sync data between container and host (specially source code when developing). For this use bind mounts.</p> <p>Bind Mounts is volume having exact mount-point on host. When mounting, you may skip copy command in Dockerfile. This is how Dev-Mode container or Dev-Container is built.</p> <p>Links: Docker - Get Started Guide</p>"},{"location":"0-Information-Technology/docker/#containers","title":"Containers","text":"<ul> <li>Before software is released, it must be tested, packaged, and installed. Containers provide a standard way to package your application's code, configurations, and dependencies into a single object.</li> <li>Containers run on top of the host OS and share the host's kernel. Each container running on a host runs its own isolated root file system in a separate namespace that may include its own OS.</li> <li>They are designed to help ensure quick, reliable, and consistent deployments, regardless of the environment.</li> <li>Containers are useful when taking a large traditional application and breaking it down into small parts, or microservices, to make the application more scalable and resilient.</li> </ul> <p>Container Attributes</p> <ul> <li>container name</li> <li>container id</li> <li>image-tag (name:version) and image-id</li> <li>ports: port of container -&gt; port on host</li> <li>Volumes: name-on-host -&gt; path mounted on container</li> <li>Networks</li> </ul> <p>Multi-Container Apps</p> <p>You may run database and web-server in same container but doing one thing in a container and doing it well matters, it lets scale easily. Since container are isolated they cannot talk to other container, there has to be a network setup. To keep things simple If two containers are on the same network, they can talk to each other. If they aren't, they can't.</p>"},{"location":"0-Information-Technology/docker/#docker-architecture","title":"Docker Architecture","text":"<p>docker inner working</p> <p>What is Docker</p> <ul> <li>Docker is a platform that helps separate application from infrastructure by using isolated environments called containers.</li> <li>it  is a set of platform as a service products that use OS-level virtualization</li> <li>It lets you put everything you need to run your application into a box that can be stored and opened when and where it is required.</li> <li>Docker-Image is template that defines layout of container, container is runtime instance of image, and runs on docker-engine, which is software that runs containers.</li> <li>Docker containers are defined by docker-image (template) and are run on docker-engine (runtime on host).</li> </ul> <p>How Docker works</p> <ul> <li>It is built in Go language and uses Linux kernel features for isolation, functionality like <code>namespaces</code> lets it possible.</li> </ul> <p>Docker Architecture &amp; Overview</p> <ul> <li>Docker Objects are items you create and use in docker. They are images, containers, networks, and volumes.</li> </ul> <ul> <li>Docker Image is read-only immutable template that defines the layout of container. They are based on other images (kinda inheritance), like your <code>app-image</code> can be build on <code>ubuntu-image</code> with added details as installation of python, mssql and configurations to run the application. To define image you create docker-file. Each instruction in file is like a layer in image. Each change in statement, update only that layer not the whole image. This makes it fast and lightweight.</li> </ul> <ul> <li>Docker Container is runtime instance of Docker Image. Created using <code>docker run</code> command. It runs on Docker Engine. You can create, start, stop, move, or delete a container using the Docker API or CLI. Container can connect to network, storage and can be saved as new image in its current state. Containers are mostly isolate from each other but you can control isolation of network/storage/subsystem on host machine. Eg, to run a container with image name ubuntu and then run command /bin/bash use: <code>$ docker run -i -t ubuntu /bin/bash</code>. It pulls image from if not available locally, crates container, allocates storage resources, adds network interface, starts container and executes the command /bin/bash. <code>-i</code> is interactively and <code>-t</code> attached to terminal, this lets you interact and see output of container in your terminal.</li> </ul> <ul> <li>Docker Engine is the software that hosts (runs) the containers. it is container runtime.</li> </ul> <ul> <li>Docker Daemon it is background process, <code>dockerd</code> that listens to Docker-API requests and manages Docker Objects (image, container, network, volume).</li> </ul> <ul> <li>Docker Client a process, <code>docker</code> that lets users interactions. It sends users commands to daemon. So <code>docker run</code> is send from client to daemon <code>dockerd</code> which does the job.</li> <li> <p>Docker Desktop is GUI that is easy and has client and daemon, and other helpers.</p> </li> </ul> <ul> <li>Docker Registries a registry that stores docker-images. It can be yours or a public register Docker Hub that anyonce can use (like GitHub). Commands like <code>docker pull</code> or <code>docker run</code> read, and <code>docker push</code> write docker-image to configured registry.</li> </ul> <pre><code>graph LR;\n\nUser &lt;--&gt; Client &lt;--&gt; Docker_API &lt;--&gt; Daemon\n\nsubgraph Registry\n    Image-1\n    Image-2\n\nend\n\nDaemon &lt;--push / pull--&gt; Image-2\nDaemon &lt;--run--&gt; Image-1\n</code></pre>"},{"location":"0-Information-Technology/docker/#docker-installation","title":"Docker Installation","text":"<p>Install using https://docs.docker.com/engine/install/ubuntu/</p>"},{"location":"0-Information-Technology/docker/#dockerfile","title":"Dockerfile","text":"<p>A docker file tells how to make an image, which base image to use, what files to copy to that image and what commands to run on it and finally what to execute on it. Here is example of docker-file located at, <code>./Dockerfile</code></p> <pre><code>FROM python:3.8\n\n# set a directory for the app\nWORKDIR /usr/src/app\n\n# copy all the files to the container\nCOPY . .\n\n# install dependencies\nRUN pip install --no-cache-dir -r requirements.txt\n\n# define the port number the container should expose\nEXPOSE 5000\n\n# run the command\nCMD [\"python\", \"./app.py\"]\n</code></pre> <p>Here, <code>FROM</code> instruction specifies the Parent Image from which you are building. <code>ENV</code> defines variable with value. <code>RUN</code> to execute container build commands. <code>WORKDIR</code> instruction sets the current working directory for subsequent instructions in the Dockerfile. <code>CMD</code> runs command on container after build. There can be only one CMD per docker file.</p> <p>ARG vs ENV: <code>ARG</code> is set at building, <code>ENV</code> while running. <code>ARG</code> can't change, <code>ENV</code> can.</p> <p>RUN vs CMD: <code>RUN</code> is an image build step, the state of the container after a <code>RUN</code> command will be committed to the container image. A Dockerfile can have many <code>RUN</code> steps that layer on top of one another to build the image. <code>CMD</code> is the command the container executes by default when you launch the built image.</p>"},{"location":"0-Information-Technology/docker/#build-image","title":"Build Image","text":"<pre><code># builds image from a Dockerfile\ndocker build -t snakeeyes:latest .\ndocker build -t &lt;img-name&gt;:&lt;tag&gt; &lt;path-dockerfile&gt;\n</code></pre> <p>Here, <code>.</code> is location of Dockerfile, an optional tag name with <code>-t</code></p>"},{"location":"0-Information-Technology/docker/#run-container-from-image","title":"Run Container from Image","text":"<p>You can run a container using 'pre build image' from repo or from a image that is built using your dockerfile. Examples:</p> <pre><code>docker run &lt;image_name&gt;\n\n# runs a container on image named busybox\ndocker run busybox\n\n# runs and passes the command to it\ndocker run busybox echo \"hello from busybox\"\n\n# get container shell in \"interactive mode\"\ndocker run -it busybox sh\n\n# detached mode\ndocker run -d --rm \\\n  --name mysite_container \\\n  -p 5000:8000 \\\n  mysite_img:latest\n</code></pre> <p>Here, <code>-it</code> swithc to run gives interactive shell in which we can pass as many commands as we want. <code>--rm</code> removes container once it exists. <code>-d</code> will detach our terminal, runs in background as daemon. <code>-P</code> will publish all exposed ports to random ports. <code>-p</code> 8888:80 assign custom ports, 80 on inside container, and exposed externally to 8888. <code>--name</code> corresponds to a name we want to give.</p>"},{"location":"0-Information-Technology/docker/#view-images","title":"View Images","text":"<p>To view <code>images</code> on your machine</p> <pre><code>$ docker images -a\n\nREPOSITORY      TAG       IMAGE ID       CREATED          SIZE\nmysite_img      latest    852440f469ff   34 minutes ago   162MB\nhello-world     latest    9c7a54a9a43c   3 months ago     13.3kB\n</code></pre>"},{"location":"0-Information-Technology/docker/#view-containers","title":"View Containers","text":"<p>To view <code>containers</code> on your machine</p> <pre><code># view all containers\n$ docker ps -a\n\nCONTAINER ID   IMAGE           COMMAND                  CREATED         STATUS         PORTS                                       NAMES\n9ea9727388df   mysite_img      \"gunicorn -b 0.0.0.0\u2026\"   5 seconds ago   Up 4 seconds   0.0.0.0:8000-&gt;8000/tcp, :::8000-&gt;8000/tcp   mysite_container\n\n# view currently running containers\n$ docker ps\n$ docker container ls\n</code></pre>"},{"location":"0-Information-Technology/docker/#stop-containers","title":"Stop Containers","text":"<p>To stop a container</p> <pre><code>docker stop mysite_container\ndocker stop &lt;container-id&gt;\n</code></pre>"},{"location":"0-Information-Technology/docker/#logs-of-container","title":"Logs of Container","text":"<pre><code># view logs\n$ docker container logs &lt;container-name&gt;\n\n# watch logs\n$ docker logs -f &lt;container-id&gt;\n</code></pre>"},{"location":"0-Information-Technology/docker/#remove-containers","title":"Remove Containers","text":"<pre><code>docker rm 305297d7a235 ff0a5c3750b9\n# deletes containers\n</code></pre>"},{"location":"0-Information-Technology/docker/#remove-image","title":"Remove Image","text":"<pre><code>docker image rm 39b5025d8e15\ndocker rmi\n</code></pre> <p>Here, <code>.</code> is location of Dockerfile, an optional tag name with <code>-t</code></p>"},{"location":"0-Information-Technology/docker/#system-prune","title":"System Prune","text":"<pre><code>docker system prune\n# delete all unused / stopped containers &amp; unlined images\n</code></pre>"},{"location":"0-Information-Technology/docker/#copying-files-from-container-to-host","title":"Copying Files from container to host","text":"<p>You can move / copy files between host and container machines using:</p> <pre><code>sudo docker cp &lt;container id&gt;:/path/to/package/in/container/package.egg-info /path/to/mounted/package/on/host/package.egg-info\n</code></pre>"},{"location":"0-Information-Technology/docker/#docker-volumes","title":"Docker Volumes","text":"<pre><code># delete volume\ndocker volume rm &lt;volume_id&gt;\n</code></pre> <p>Named Volumes - Add volume to the docker container, for persistence. This keeps data after container stops or is removed.</p> <pre><code>docker run -dp 127.0.0.1:3000:3000 --mount type=volume,src=todo-db,target=/etc/todos getting-started\n\ndocker run -dp 127.0.0.1:3000:3000 --mount type=volume,src=&lt;vol-name&gt;,target=&lt;path in container&gt; &lt;img&gt;\n</code></pre> <p>Bind Volumes - Add bind mount to sync code to docker container from host.</p> <pre><code>docker run -it --mount type=bind,src=&lt;host path&gt;,target=&lt;path in container&gt; &lt;img&gt; &lt;cmd&gt;\n\ndocker run -it --mount type=bind,src=\"$(pwd)\",target=/src ubuntu bash\n</code></pre>"},{"location":"0-Information-Technology/docker/#docker-network","title":"Docker Network","text":"<p>Every container runs in default bridge network and has an IP. This default Bridge network is shared by all the containers so two different containers can talk to each other If we provide the IP address that is assigned by the default bridge network, but this may not be secure. To make this secure we can create our own Bridge network, which is different from default Bridge network. Our Bridge network can have a name and containers can use that new Bridge network for networking.</p> <pre><code># view networks\ndocker network ls\n\n# create network\ndocker network create todo-app\ndocker network create &lt;nw-name&gt;\n\n# to see ports exposed\n$ docker port  &lt;container-name&gt;\n\n# inspect a network by name\n$ docker network inspect bridge\n$ docker network inspect &lt;nw-name&gt;\n</code></pre> <p>Here, <code>inspect bridge</code> shows details of a network. Bridge network is used by multiple container and lets one container talk to another one. But you can create another newtwork that isolates everything.</p>"},{"location":"0-Information-Technology/docker/#dockerignore","title":"Dockerignore","text":"<p><code>.dockerignore</code> file lets you define what you want docker to ignore when copying, eg, you do not want <code>.git</code> folder to be copied.</p>"},{"location":"0-Information-Technology/docker/#docker-hub","title":"Docker Hub","text":"<p>This is docker repo like GitHub where you will find pre-build images that you can use directly in your dockerfile or, build on top of pre built images.</p> <p>View more at: Docker Hub</p> <p>To push an image to docker hub do following:</p> <pre><code># build image on local\ndocker build -t yourusername/catnip .\n\ndocker login\n\ndocker push yourusername/catnip\n</code></pre> <p>If you build image name with <code>/</code> then it will create it with docker.io prefix as <code>docker.io/yourusername/catnip</code></p> <p>Note: This makes all your files in docker image public, so if you copied code of project it will be available to public.</p> <p>Alternative registry host is AWS ECR - Elastic Container Register.</p>"},{"location":"0-Information-Technology/docker/#docker-compose","title":"Docker-Compose","text":"<p>All what you do in terminal for images, container, volume, network, like build, pull, run, and their dependencies on each other, and adding env var, commands etc. can all be written in a yaml file so that it is not in terminal but in file as a code. This makes it version controlled and shareable.</p> <p>It is a utility for multi-container docker environments management. It is built in Python. It is a tool that was developed to help define and share multi-container applications. With Compose, we can create a YAML file to define the services and with a single command, can spin everything up or tear it all down. While this can be done separately by running containers separately and linking with network but this is a better way to build a cluster of containers.</p>"},{"location":"0-Information-Technology/docker/#why-and-how-to-use-docker-compose","title":"Why and How to use Docker Compose","text":"<p>The Development Workflow to make use of docker compose is that while you can start a service from image, the other way is to build an image in docker-compose using Dockerfile and set the volume to map you machine folder to folder in container. This way any change you do in code on your machine, is moved to container and the image is built using compose up or down. This way any change is quickly reflected in the container.</p> <p>Note: You can do docker up to run the container but also add <code>--build</code> whenever there is change is packages of flask so that image is rebuilt.</p> <p>You define services and within service a name of Service, that name of the Service is name of network alias for that machine.</p> <p>In docker compose, under service, if you map folders in volume it is bind mount (to sync code changes), if you give volume name and define that under volumes section it is named-volume mount (to persist the data).</p> <p>You define named volumes in <code>volumes:</code> section, this named volume can be mapped in any service. The named volume does not get removed when you tear-down or do <code>docker compose down</code>.</p>"},{"location":"0-Information-Technology/docker/#installing-docker-compose","title":"Installing docker-compose","text":"<p>It is separate to docker and can be installed using:</p> <pre><code>pip install docker-compose\n</code></pre> <p>or to install in systen as binary:</p> <pre><code>sudo curl -L \"https://github.com/docker/compose/releases/download/2.20.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\n\nsudo chmod +x /usr/local/bin/docker-compose\n</code></pre>"},{"location":"0-Information-Technology/docker/#building-docker-compose-yaml-file","title":"Building docker-compose YAML file","text":"<p>The docker compose file is a list of instructions in yaml that define multi containers as service. Here is an exmple of such file. <code>./docker-compose.yaml</code></p> <pre><code>version: '2'\n\nservices:\n\n  website:\n    build: .\n    command: &gt;\n      gunicorn -b 0.0.0.0:8000\n        --access-logfile -\n        --reload\n        \"snakeeyes.app:create_app()\"\n    environment:\n      PYTHONUNBUFFERED: 'true'\n    volumes:\n      - './the_project:/home/snakeeyes'\n      - '/home/snakeeyes/SnakeEyes_CLI.egg-info'\n    ports:\n      - '8000:8000'\n</code></pre> <p>Here you define version, services and volumes. <code>version:</code> is version of compose. <code>services:</code> defines containers, with a name as <code>website</code> and then image and properties. <code>build: .</code> tells to build the image for this container using the Dockerfile located at <code>.</code>.</p> <p><code>volumes:</code> lets mount host-folder to container-folder. It is useful when doing development, as you make changes to host-folder the changes are made on container-folder without re-building the image. This may sometime override some folders that are generated by commands and may not exist on host-folder, in that case you can whitelist, some files. just write path without colon to whitelist <code>- '/home/snakeeyes/SnakeEyes_CLI.egg-info'</code></p>"},{"location":"0-Information-Technology/docker/#commands-of-docker-compose","title":"Commands of Docker-Compose","text":"<p>Some common commands to build, start, pause and stop containers</p> <pre><code># builds image and runs\n$ docker-compose up --build\n\n# to run in background\n$ docker-compose up -d\n\n# pause the environment execution without changing the current state of your containers\n$ docker-compose pause\n\n# resume the execution\n$ docker-compose unpause\n\n# terminate the container execution, but it won\u2019t destroy any data associated with your containers\n$ docker-compose stop\n\n# remove the containers, networks, and volumes associated with this containerized environment\n$ docker-compose down\n</code></pre>"},{"location":"0-Information-Technology/docker/#execute-commands-inside-container-machine","title":"Execute commands inside Container machine","text":"<p>You can execute commands in container by passing them after term <code>exec</code> then <code>&lt;service&gt;</code> which is the service in which you want to run the command, here it is <code>website</code>. Then the command.</p> <p>Note: The relative path you give here are w.r.t the path in container not where you are running these commands. This is usually defined in <code>Dockerfile</code> as <code>WORKDIR /home/snakeeyes</code>.</p> <p>Below are some examples</p> <pre><code># execute pytest\nsudo docker-compose exec website py.test sakeeeyes/tests\n\n# execute pytest with coverage\nsudo docker-compose exec website py.test --cov-report term-missing --cov snakeeyes\n\n# excute flake8 linter\nsudo docker-compose exec website flake8 . --exclude __init__.py\n</code></pre>"},{"location":"0-Information-Technology/docker/#dockerized-project-commands","title":"Dockerized Project Commands","text":"<p>Once you have \"dockerized\" your development, you usually will use following commands once everything is set up.</p> <pre><code># run the containters\nsudo docker-compose up\n\n# stop the containers\nsudo docker-compose stop\n\n# run a command in docker-container, eg running pytest.\nsudo docker-compose exec &lt;service&gt; &lt;cmd&gt;\n\n# rebuild images and run containers\nsudo docker-compose up --build\n\n\n# --------- Docker only cmds ----------- #\n\n# Once a week, to clean up\nsudo docker system prune\n\n# show images\nsudo docker images -a\n\n# delete images\nsudo docker rmi bf4d41ae6382\n\n# show containers\nsudo docker container  ls -a\n</code></pre> <p>This lets do basic development work.</p>"},{"location":"0-Information-Technology/docker/#multi-stage-build","title":"Multi Stage Build","text":"<p>Multi stage build lets build the image in different stages using <code>AS</code> and <code>FROM</code>. There can be build stage which need env and other tools, and a prod stage which has just the final built part or the compiled code. This reduces image size and lowers build time.</p>"},{"location":"0-Information-Technology/docker/#dev-containers","title":"Dev Containers","text":"<p>\"Dev Containers\" is term used to let developers do development inside a container. The container can run in docker installed locally or on remote (like github, calls it codespace). Dev Containers provide isolated, production ready infra to work on.</p> <p>Dev Containers are made available in VS Code. It basically runs a docker container in docker enginer. Then VS Code connects to it using remote connection. Anything you run on the container, is exposed via a port. This can then be accessed via host, if you add <code>\"remote.localPortHost\": \"allInterfaces\",</code> to vs code settings, you can access this host and port on you lan via any other device.</p> <p>Caution...! Anything that is saved outside <code>workspaces</code> folder will be deleted if the container is removed. This is because only <code>workspaces</code> folder persists as volume.</p> <ul> <li>'Epic Swartz' was a ubuntu container, that has mounted volumne.<ul> <li>Ubnutu (epic swartz) is name and alias, its same thing.</li> <li><code>/workspaces/ubuntu</code> has repos. This is volume on host.</li> <li><code>/home/vscode/code/repo</code> has different repos. This is on host at <code>/Users/vab/OrbStack/docker/containers/epic_swartz/home/vscode/code/repo/microblog-api/microblog-api</code>. This got deleted on removing container.</li> </ul> </li> </ul> <p>Ways to use dev container</p> <ul> <li>New dev container - opens new container based on config selected. Maps a new volume.</li> <li>Open a folder in container - opens new container for first use. Binds folder to volume. Useful for continued development.</li> <li>Attach to running Container - open the home dir of container user, <code>/home/vscode</code></li> </ul>"},{"location":"0-Information-Technology/docker/#new-dev-container","title":"New Dev Container","text":"<p>Opening a repo in its own vscode window</p> <ul> <li>on the termial, cd to the repo and do code .</li> </ul> <p>Useful when you have to test something new once. Anything you do will get deleted if container is removed, only workspaces folder is left as volume</p>"},{"location":"0-Information-Technology/docker/#open-folder-in-container","title":"Open Folder in Container","text":"<p>Opens a host folder in container. Container has only this folder under <code>/workspaces/</code></p> <p>For first time, it asks container config and takes a while. For next runs, its starts in seconds.</p> <p>Steps</p> <ul> <li>Click remote explorer</li> <li>Open Folder in Container...</li> <li>Select folder on host. <code>/path-to/folder-in-cntnr-1</code></li> <li>Add user config file.</li> <li>Select tools/frameworks.</li> <li>Done</li> </ul> <p>What happends in background by VS Code:</p> <ul> <li>Downloads the image based on tools you selectd</li> <li>Build the image</li> <li>Attaches volume, bind mount with folder on host. Attaches volume mount <code>vscode</code> fro vscode-server stuff.</li> <li>Starts the container</li> <li>Connects VS code to container</li> <li>Opens host folder in container</li> <li>Click 'Remote Explorer' extension to see all these details in vscode.</li> </ul> <p>User Owner and Permissions in Container</p> <ul> <li>On terminal, user is <code>vscode</code>.</li> <li>Folder is located in <code>/workspaces/folder-in-cntnr-1</code></li> </ul> <p>Git Configs</p> <ul> <li>By default config from host are copied to container.</li> <li>You will need to setup the SSH configurations if you want to push from inside container. Else you can do from host.</li> </ul> <p>Extensions</p> <ul> <li>can be configured in <code>.devcontainer/devcontainer.json</code></li> </ul> <p>Ports</p> <ul> <li>can be configured in <code>.devcontainer/devcontainer.json</code></li> </ul> <p>Files location</p> <ul> <li>File saved on container is saved on host as well.</li> <li>File saved on host is reflected on container.</li> </ul> <p>Opening other folders</p> <ul> <li>Opening other folder with same framework selected was fast. That is repeated the same thing for another folder on host, selected python as framework for container, then it opened in seconds.</li> </ul> <p>Closing Tear Dow</p> <ul> <li>Closing the vscode window, stops the running container. Does not delete it.</li> </ul>"},{"location":"0-Information-Technology/docker/#opening-source-tree-in-container","title":"Opening Source Tree in Container","text":"<p>Same as opening a folder in container. Instead of one project repo, you can open a parent folder that has all your repos. This will have all repos in container. Now you can simply open any folder under parent folder (which will be a repo) in a new vscode window. You can also see this on the remote expolorer extension sidebar.</p> <p>More on this can be found on Configure separate containers</p> <p>Binded volume can be slow compared to mounted volume. You can do the same thing above (open source tree) by mounting a volume to container, and Clone your entire source tree inside of a named volume rather than locally (on host). More on Use a named volume for your entire source tree</p> <p>Links:</p> <ul> <li>https://containers.dev/</li> </ul>"},{"location":"0-Information-Technology/docker/#orbstack","title":"ORBStack","text":"<p>a tool to manage docker and virtual machine on Mac</p> <p>After installation, run local docker container with all documentation using below command</p> <pre><code>docker run -it -p 80:80 docker/getting-started\n</code></pre> <p>Links:</p> <ul> <li>docs</li> <li>discussion</li> </ul>"},{"location":"0-Information-Technology/docker/#issues","title":"Issues","text":"<p>Fix ARM and AMD issue because of Mac</p> <p><code>WARNING: The requested image's platform (linux/arm64/v8) does not match the detected host platform (linux/amd64/v3) and no specific platform was requested</code></p> <p>ARM - is for Mac AMD - is for Linux</p> <p>You need to build image for a specified platform it you need to run the image on it. So build image for AMD platform if you want to run it on a linux box. More on (stackoverflow - The requested image's platform (linux/arm64/v8) does not match the detected host platform (linux/amd64))[https://stackoverflow.com/questions/71000707/docker-get-started-warning-the-requested-images-platform-linux-arm64-v8-doe]</p>"},{"location":"0-Information-Technology/docker/#eg-food-truck","title":"Eg - Food Truck","text":"<pre><code># creates network\ndocker network create foodtrucks-net\n\n# run container with network\ndocker run -d \\\n  --name es \\\n  --net foodtrucks-net \\\n  -p 9200:9200 -p 9300:9300 \\\n  -e \"discovery.type=single-node\" \\\n  docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n\n# run web_container with network\ndocker run -it --rm --net foodtrucks-net yourusername/foodtrucks-web\n\n# delete network\ndocker network rm foodtrucks-net\n</code></pre> <p>More easy and auto managed way of adding network in between containers is to use  docker-compose. It will auto create a network for containers in compose script.</p>"},{"location":"0-Information-Technology/docker/#eg-snake-eyes","title":"Eg - Snake Eyes","text":"<pre><code>FROM python:3.7-slim\n\n# set a directory for the app\nENV INSTALL_PATH /home/snakeeyes\nRUN mkdir -p $INSTALL_PATH\nWORKDIR $INSTALL_PATH\n\n# copy all the files to the container\nCOPY the_project .\n\n# install dependencies\nRUN pip install --no-cache-dir -r requirements.txt\nRUN pip install --editable .\n\nCMD gunicorn -b 0.0.0.0:8000 --access-logfile - \"snakeeyes.app:create_app()\"\n</code></pre>"},{"location":"0-Information-Technology/docker/#links-dockerize","title":"Links - Dockerize","text":"<ul> <li>Docker Tutorial by Prateek - with AWS</li> <li>SO - docker compose overwrite fix</li> </ul>"},{"location":"0-Information-Technology/flask/","title":"Flask","text":"<p>Flask is a microframework in Python. It is used to create a webapp. It can start a python web server. It can handle HTTP requests. It can also be used to make a webapp API.</p>"},{"location":"0-Information-Technology/flask/#flask-hello-world","title":"Flask Hello World","text":"<ul> <li> <p>Create a python file <code>main.py</code>:</p> <pre><code>#!venv_app/bin/python3\nfrom flask import Flask\napp = Flask(__name__)\n\n@app.route('/')\ndef index():\n    return \"Hello from Flask App!\"\n\nif __name__ == '__main__':\n    app.run(debug=True)\n</code></pre> </li> </ul> <ul> <li>run using <code>python main.py</code></li> </ul> <ul> <li>access at <code>http://localhost:5000</code> in browser.</li> </ul>"},{"location":"0-Information-Technology/flask/#run-a-flask-app-ways","title":"Run a Flask App - Ways","text":"<ul> <li>python<ul> <li><code>python main.py</code></li> </ul> </li> </ul> <ul> <li>executable<ul> <li><code>chmod a+x main.py</code> to make app executable.</li> <li><code>./main.py</code> runs app.</li> </ul> </li> </ul> <ul> <li>Flask CLI<ul> <li><code>flask --app main.py run</code>   or</li> <li><code>export FLASK_APP=main.py</code> will make an variable that tells python which app to run.</li> <li><code>flask run</code> executes the app or if flask is not in path then do <code>python -m flask run</code></li> <li>flask --app has app command</li> <li>flask run has --host or -h, --port or -p and --no-debug</li> </ul> </li> </ul> <pre><code>set FLASK_ENV=production\nset FLASK_DEBUG=0\ncd repo\\prj1\nvenv\\Scripts\\activate\nflask --app app:create_app('uat') run --no-debug -h 0.0.0.0 -p 5002\n</code></pre>"},{"location":"0-Information-Technology/flask/#flask-native-modules","title":"Flask native modules","text":"<p>flask basics, request-response handling, contexts</p> <ul> <li>Application Instance<ul> <li><code>Flask()</code> - is a class.</li> <li>All Flask applications must create an application instance (object of class <code>Flask</code>). The web server passes all requests it receives from clients to this object for handling, using a protocol called Web Server Gateway Interface (WSGI).</li> <li><code>app = Flask(__name__)</code> name is passed to <code>Flask</code> class constructor so it knows the location of app and hence can locate static and template.</li> </ul> </li> </ul> <ul> <li>Requests<ul> <li>Request from client has lot of information in it, like header, user-agent, data etc. This information is available in <code>request object</code> and is made available to a <code>view-route function</code> to handle it. This object is not passed as an argument to function, rather it is made available using <code>contexts</code>.</li> <li><code>request</code> Object has methods and attributes having info on method, form, args, cookies, files, remote_addr, get_data().</li> </ul> </li> </ul> <ul> <li>Contexts<ul> <li>Code is logic with data. Data is variables or constants or objects. This data can be configurations, input data or data from file/database. In flask, \"Context\" is used to keep track of this data.</li> <li>It let certain objects to be globally accessible, but are not global variable. They are globally accessible to only one thread. There can be multiple threads serving multiple requests from multiple client.</li> <li>Context is simply data that is specific to something. Eg<ul> <li>App-context is specific to app, like its mail server, its database location, or other configurations. Keeps track of application-level data. Objects: <code>current_app</code>, <code>g</code>.</li> <li>Request-context is specific to request, like its browser, its client, its form data, its headers, all that is request-level. Objects: <code>request</code>, <code>session</code>.</li> </ul> </li> <li>this data is stored in object, in attribute such as <code>config</code></li> <li>this data is used by extensions in flask, hence they do not run if context is not available.</li> <li>context is automatically made available once app is initialized.</li> <li>context can be made explicitly available by calling <code>with app.app_context():</code></li> </ul> </li> </ul> <ul> <li>Request Handling - How flask handles a request?<ul> <li> <p>when there is request, web server activates a thread that initializes app and this app context is pushed with data that is available globally, similarly request context is also pushed.</p> <pre><code>graph LR;\nWeb_Browser --&gt; request --&gt; web_server --&gt; Flask_app_instance --&gt; route --&gt; function_to_execute</code></pre> </li> </ul> </li> </ul> <ul> <li>Flask variables for Request Handling<ul> <li><code>current_app</code> variable in Application context, has info of active application.</li> <li>Imp: <code>current_app</code> is app-context, but is only available when serving a request, that is, in a route function only. It can be used in any module but the function should be called when serving a request.</li> <li><code>g</code> variable in Application context, it is object that is unique for each request, temp access during handling of a request. It resets once request is served. Holds app info hence app context. Can be used to load user on each request. show logged in user on templates.</li> <li><code>request</code>, in request context, obj having client req data.</li> <li><code>session</code>, in request context, stores data across requests, i.e., a dictionary to store values that can be accessed in different requests from same session.</li> <li>Flask, in backend, makes these available to thread before dispatching a request and removes after request is handled. Explicitly, <code>current_app</code> can be made available by invoking <code>app.app_context()</code></li> <li> How does flask differentiate requests and clients?</li> </ul> </li> </ul> <ul> <li>Request Hooks<ul> <li>They are deocrators that register functions that can execute code before or after each request is processed. They are implemented as decorators  (functions that execute on event). These are the four hooks supported by Flask:</li> <li><code>before_request</code> - like authenticate</li> <li><code>before_first_request</code> - only before the first request is handled. Eg, to add server initialization tasks.</li> <li><code>after_request</code> - after each request, but only if no unhandled exceptions occurred.</li> <li><code>teardown_request</code> - after each request, even if unhandled exceptions occurred.</li> <li><code>g</code> context global storage can be used to share data between hook and view functions.</li> </ul> </li> </ul> <ul> <li>Routes or View Functions<ul> <li>They handle application URLs.</li> <li>URL-maps can be seen using <code>app.url_map</code></li> <li>redirect to url<ul> <li><code>redirect</code> - takes URL to redirect to.</li> <li><code>redirect(url_for(\"profile\"))</code></li> <li><code>url_for()</code> utility builds URL for view-function giving route from app-url-map. takes function name as str and gives its URL. Eg:<ul> <li><code>url_for('user', name='john', page=2, version=1)</code> would return <code>/user/john?page=2&amp;version=1</code>, they are good to build dynamic URLs that can be used in templates.</li> <li><code>url_for('user', name='john', _external=True)</code> would return <code>http://localhost:5000/user/john</code>.</li> <li><code>url_for('static', filename='css/styles.css', _external=True)</code> would return <code>http://localhost:5000/static/css/styles.css</code>.</li> <li><code>/static/&lt;filename&gt;</code> is special route added by Flask to serve static files.</li> </ul> </li> </ul> </li> </ul> </li> </ul> <ul> <li> <p>Response Object</p> <ul> <li> <p>Response is returned by view-function as a string (usually HTML) along with status code but can also contain headers. So rather than sending comma separated tuple values, flask lets create response object using <code>make_response()</code>.</p> <pre><code>from flask import make_response\n\n@app.route('/')\ndef index():\n    response = make_response('&lt;h1&gt;Some response with a cookie!&lt;/h1&gt;')\n    response.set_cookie('message', '51')\nreturn response\n</code></pre> </li> </ul> <ul> <li><code>return redirect('http://www.example.com')</code> is a response with URL and status code 302, however Flask lets it do easily using <code>redirect()</code> method. Another such is <code>abort(404)</code> which is treated as exception.</li> </ul> <ul> <li>session - can be used to store values, specific to current session, it is server side. Helps to pass values from one function to another.<ul> <li><code>session[\"username\"] = username</code></li> <li>permanent sessions store session data for a time period</li> </ul> </li> </ul> <ul> <li>flash - lets send extra messages to frontend<ul> <li><code>flash(\"The message\", \"info\")</code> message and level.</li> <li><code>get_flashed_messages()</code> to get messages</li> <li>it lets record a message at the end of a request and access it next request and only next request.<pre><code>{% for message in get_flashed_messages() %}\n    &lt;div class=\"flash\"&gt;{{ message }}&lt;/div&gt;\n{% endfor %}\n</code></pre> </li> </ul> </li> </ul> </li> </ul>"},{"location":"0-Information-Technology/flask/#jinja-templates-in-flask","title":"Jinja Templates in Flask","text":"<p>Templates can be used to build responses.</p> <ul> <li>render_template()<ul> <li>it makes use of template</li> <li><code>return render_template('user.html', name=name)</code></li> </ul> </li> </ul> <ul> <li> <p>Jinja Templates</p> <ul> <li>Templates are HTML file having additional Python like code in placeholders.</li> <li>Placeholders can have variables and expressions.</li> <li>They get replaced with value when template is rendered by JinJa2, the template engine.</li> <li>This lets build dynamic content on execution.</li> <li>It lets inherit, extend and import templates.</li> <li>More documentation on template design and tips and tricks</li> <li> <p>Example template is below.</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;title&gt;My Webpage&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;ul id=\"navigation\"&gt;\n    {% for item in navigation %}\n        &lt;li&gt;&lt;a href=\"{{ item.href }}\"&gt;{{ item.caption }}&lt;/a&gt;&lt;/li&gt;\n    {% endfor %}\n    &lt;/ul&gt;\n\n    {% if kenny.sick %}\n      Kenny is sick.\n    {% elif kenny.dead %}\n      You killed Kenny!  You bastard!!!\n    {% else %}\n      Kenny looks okay --- so far\n    {% endif %}\n\n    &lt;h1&gt;My Webpage&lt;/h1&gt;\n    {{ a_variable }}\n\n    {# a comment #}\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> </li> </ul> <ul> <li>Filters<ul> <li>pass value over pipe to filter functions like upper, lower, title, trim, safe.</li> <li>eg, <code>Hello, {{ name|capitalize }}</code></li> <li>Full list of filters here</li> </ul> </li> </ul> <ul> <li>Delimiters<ul> <li>{% ... %} for Statements</li> <li>{{ ... }} for Expressions to print</li> <li>{# ... #} for Comments, not included in the template output.</li> </ul> </li> </ul> <ul> <li>Assignments<ul> <li> <p>lets set a value to var and use it for logic building. We have to use namespace.</p> <pre><code>{% set ns = namespace(index=0) %}\n{% for nav_item in nav %} \n  {% if ns.index !=0 %}\n    --- some stuff ---\n  {% endif %}\n  {% set ns.index = ns.index + 1 %}\n{% endfor %}\n</code></pre> </li> </ul> </li> </ul> <ul> <li>{{ super() }}, includes code from parent block, if overriding a block.</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/flask/#macro-and-includes","title":"Macro and Includes","text":"<p>When you have one code to include at few place use <code>include</code>, eg, sidebar, nav, footer.</p> <p>When you have a code to use at many places use <code>macro</code>, eg, form elements, nav items, alerts.</p> <p>\"When you have a chunk of code that you think should be present in a different template just for better organization and it won't need to take any parameters, for example the header, footer, complex navigation menu, etc. then include is good for this case. But when you have something that will be repeated multiple times and might need some dynamic parameters, for e.g. form fields, then you should use macro for it.\"</p> <p><code>+-</code> in <code>{%</code> are only for HTML whitespace, they do not have logical meaning.</p> <p>Define Macro</p> <p>A <code>some.html</code> file can have multiple macro blocks. Eg, in <code>macros/items.html</code>:</p> <pre><code>{% macro role_icon_for(user) -%}\n  {% if user.role == 'admin' %}\n    &lt;i class=\"fa fa-3x fa-fw fa-shield\" title=\"Admin\"&gt;&lt;/i&gt;\n  {% else %}\n    &lt;i class=\"fa fa-3x fa-fw fa-user text-muted\" title=\"Member\"&gt;&lt;/i&gt;\n  {% endif %}\n{% endmacro %}\n\n{# Render a checkbox field. #}\n{%- macro checkbox_field(f) -%}\n  {{ f(type='checkbox', **kwargs) }} {{ f.label }}\n{%- endmacro -%}\n</code></pre> <p>Using Macro</p> <pre><code>{% import 'macros/items.html' as items %}\n...\n{{ items.role_icon_for(user) }}\n...\n</code></pre> <p>Call Macro from another Macro</p> <p>It is like blocks in jinja. A caller-macro can call another macro. They both can include each other code.</p> <pre><code>{# being called #}\n{%- macro form_tag(endpoint, fid='', css_class='', method='post') -%}\n  &lt;form action=\"{{ url_for(endpoint, **kwargs) }}\" method=\"{{ method }}\"\n        id=\"{{ fid }}\" class=\"{{ css_class }}\" role=\"form\"&gt;\n    {{ form.hidden_tag() }}\n    {{ caller () }}         {# code of caller will be in here #}\n  &lt;/form&gt;\n{%- endmacro -%}\n\n\n{# the caller #}\n{%- macro search_form(endpoint) -%}\n  {% call form_tag(endpoint, method='get') %}\n    &lt;label for=\"q\"&gt;&lt;/label&gt;\n    &lt;div class=\"input-group\"&gt;\n      &lt;input type=\"text\" id=\"q\" name=\"q\"&gt;\n    &lt;/div&gt;\n  {% endcall %}\n{%- endmacro -%}\n</code></pre> <p>Here, Caller-macro can also push code to called-macro which shows using <code>{{ caller () }}</code>.</p> <p>Include</p> <pre><code>{% include 'admin/_menu.html' %}\n</code></pre> <p>Link: SO - macro vs includes</p>"},{"location":"0-Information-Technology/flask/#snippets","title":"Snippets","text":"<p>Show All ORM data as Table</p> <pre><code>  {% if (flows is defined) and flows %}\n  &lt;table id=\"flows_table\" class=\"table table-striped\"&gt;\n\n    &lt;thead&gt;\n      {% set flow_h = flows[0] %}\n\n      {% for attr, value in flow_h.__dict__.items() %}\n        {% if attr not in ('_sa_instance_state') %}\n          &lt;th&gt;{{attr | replace(\"_\",\" \") | title }}&lt;/th&gt;\n        {% endif%}\n      {% endfor %}\n    &lt;/thead&gt;\n\n    &lt;tbody&gt;\n      {% for flow in flows %}\n        &lt;tr&gt;\n          {% for attr, value in flow.__dict__.items() %}\n            {% if attr not in ('_sa_instance_state') %}\n              &lt;td&gt;{{ value }}&lt;/td&gt;\n            {% endif %}\n          {% endfor %}\n        &lt;/tr&gt;\n      {% endfor %}\n    &lt;/tbody&gt;\n\n  &lt;/table&gt;\n  {% endif %}\n</code></pre> <p>Show All ORM Data in Card as Key: Value pair with skipping a few</p> <pre><code>&lt;h4&gt;{{user.user_name | title}}&lt;/h4&gt;\n\n&lt;h5&gt;User Posts&lt;/h5&gt;\n\n&lt;div class=\"row row-cols-2 g-2\"&gt;\n\n{% if (posts is defined) and posts %}\n\n  {% for post in user.posts %}\n\n    {# each post #}\n    &lt;div class=\"col\"&gt;\n      &lt;div class=\"card\"&gt;\n        &lt;div class=\"card-body\"&gt;\n\n          {% for attr, value in post.__dict__.items() %}\n            {% if attr not in ('_sa_instance_state', 'user_id') and (value) %}\n\n              &lt;p class=\"mb-1\"&gt;\n                &lt;span class=\"text-muted\"&gt;{{attr | replace(\"_\",\" \") | title }}: &lt;/span&gt;\n                &lt;b&gt;\n                  {% if value is float %}\n                    {{\"%.2f\"|format(value)}}\n                  {% else %}\n                    {{ value }}\n                  {% endif %}\n                &lt;/b&gt;\n              &lt;/p&gt;\n\n            {% endif %}\n          {% endfor %}\n\n        &lt;/div&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n\n  {% endfor %}\n\n{% else %}\n&lt;p class=\"lead\"&gt;No records found!&lt;/p&gt;\n{% endif %}\n\n\n&lt;/div&gt;\n</code></pre> <p>In above snippet, <code>and (value)</code> will only show those values that are not <code>None</code>.</p> <p>Pandas to HTML using JSON</p> <p>You can convert a df to json, more on doc, eg:</p> <pre><code>&gt;&gt;&gt; result = df.to_json(orient=\"records\")\n&gt;&gt;&gt; parsed = loads(result)\n&gt;&gt;&gt; dumps(parsed, indent=4)\n[\n    {\n        \"col 1\": \"a\",\n        \"col 2\": \"b\"\n    },\n    {\n        \"col 1\": \"c\",\n        \"col 2\": \"d\"\n    }\n]\n</code></pre> <p>In <code>view.py</code> file</p> <pre><code>from json import loads\n\ndf = # your code\ndf_json = df.to_json(orient=\"records\", index=False)\ndf_1 = loads(df_json)\n\nreturn render_template(\"show.html\", df_1=df_1)\n</code></pre> <p>In <code>show.html</code> template, generate table with this, then you may copy and hardcode column and modify values with actions, etc.</p> <pre><code>&lt;table&gt;\n\n  &lt;thead&gt;\n    &lt;tr&gt;\n      {% for col in df_1[1].keys() %}\n        &lt;th&gt;{{col}}&lt;/th&gt;\n      {% endfor %}\n    &lt;/tr&gt;\n  &lt;/thead&gt;\n\n  &lt;tbody&gt;\n    {% for row in df_1 %}\n    &lt;tr&gt;\n      {% for k,v in row.items() %}\n        &lt;td&gt;{{v}}&lt;/td&gt;\n      {% endfor %}\n    &lt;/tr&gt;\n    {% endfor %}\n  &lt;/tbody&gt;\n\n&lt;/table&gt;\n</code></pre>"},{"location":"0-Information-Technology/flask/#generators","title":"Generators","text":"<pre><code># required json format\ndf_json = df.to_json(orient=\"records\", index=False)\n\n# Generates jinja code to show json df as table\ndef generate_jijna_json2table(parsed, var_name=\"df\"):\n    ths = \"\" # table heads\n    tds = \"\" # table rows\n\n    for k,v in parsed[0].items():\n        ths += f\"\\n&lt;th&gt;{k}&lt;/th&gt;\"\n        tds += \"\\n&lt;td&gt;{{row.get('\"+k+\"')}}&lt;/td&gt;\"\n\n    html = \"\"\"\n    &lt;table class=\"table table-hover mb-4\" id=\"a_datatable\"&gt;\n\n    &lt;thead&gt;\n        &lt;tr&gt;\n            \"\"\"+ths+\"\"\"\n        &lt;/tr&gt;\n    &lt;/thead&gt;\n\n    &lt;tbody&gt;\n        {% for row in \"\"\"+var_name+\"\"\" %}\n        &lt;tr&gt;\n        \"\"\"+tds+\"\"\"\n        &lt;/tr&gt;\n        {% endfor %}\n    &lt;/tbody&gt;\n\n    &lt;/table&gt;\n    \"\"\"\n    return(html)\n</code></pre> <p>You can print what is returned from above function and paste it in jinja for further modification.</p>"},{"location":"0-Information-Technology/flask/#error-handlers","title":"Error Handlers","text":"<p><code>@app.errorhandler</code> is decorator that lets return a view from template for error responses like 404 and 500.</p> <pre><code>@app.errorhandler(404)\ndef page_not_found(e):\n    return render_template('404.html'), 404\n\n@app.errorhandler(500)\ndef internal_server_error(e):\n    return render_template('500.html'), 500\n</code></pre>"},{"location":"0-Information-Technology/flask/#flask-extensions","title":"Flask Extensions","text":"<p>Most extensions use app context to initialize themselves, eg:</p> <pre><code>from flask_bootstrap import Bootstrap\napp = Flask(__name__)\nbootstrap = Bootstrap(app)\n</code></pre>"},{"location":"0-Information-Technology/flask/#bootstrap-in-flask","title":"Bootstrap in Flask","text":"<ul> <li> <p>Bootstrap - provides templates and blocks that can be used in Jinja2 Templates</p> <ul> <li>installation <code>pip install flask-bootstrap</code></li> <li>eg, <code>{% extends \"bootstrap/base.html\" %}</code> - base.html does not exist but is available via extension. Others are <code>navbar</code>, <code>content</code>, <code>script</code></li> </ul> <pre><code>{% block scripts %}\n{{ super() }} &lt;!--includes base scripts too, else overrides--&gt;\n&lt;script type=\"text/javascript\" src=\"my-script.js\"&gt;&lt;/script&gt;\n{% endblock %}\n</code></pre> </li> </ul>"},{"location":"0-Information-Technology/flask/#moment-js-in-flask","title":"Moment JS in Flask","text":"<ul> <li>Moment.js Flask-Moment - Localization of Dates and Times<ul> <li>server should send UTC, client should present in local time and formatted to region using JavaScript.</li> <li><code>Moment.js</code> is perfect for this and is available as flask extension. It can be used in Jinja2 template.</li> <li><code>pip install flask-moment</code></li> <li> <p>include the script, <code>jQuery</code> is already attached as part of bootstrap</p> <pre><code>{% block scripts %}\n  {{ super() }}\n  {{ moment.include_moment() }}\n{% endblock %}\n</code></pre> <pre><code>from flask_moment import Moment\n\nmoment = Moment(app)\n\nreturn moment(datetime.utcnow()).format('LLL') # local time\nreturn moment(datetime.utcnow()).fromNow(refresh=True) # a few seconds ago..\n</code></pre> </li> </ul> </li> </ul>"},{"location":"0-Information-Technology/flask/#forms-in-flask","title":"Forms in Flask","text":"<p>WTForms - Object Oriented Form building. It supports forms validation, CSRF protection, internationalization (I18N), showing errors, extending forms, rendering form, file upload, reCAPTCHA and more for any Python framework, its generic. WTForms.</p> <p>It also works well with other extensions like Flask-Bootstrap and Flask-SQLAlchemy to do common tasks in one line.</p> <p>Installation</p> <pre><code>pip install -U Flask-WTF\n</code></pre> <p>Instantiation</p> <p>Global CSRF protection</p> <pre><code>from flask_wtf.csrf import CSRFProtect\n\ncsrf = CSRFProtect()\n\ndef create_app():\n    ...\n    csrf.init_app(app)\n    ...\n</code></pre>"},{"location":"0-Information-Technology/flask/#build-a-form","title":"Build a Form","text":"<p>Make a class to build form, members are form fields. See quick-start flask-wtf. Form Class can be build in main <code>app.py</code> or in module <code>forms.py</code> with fields and validate functions.</p> <pre><code>from flask_wtf import FlaskForm\n\nfrom wtforms import StringField, SubmitField, SelectField, DateField, BooleanField\n\nfrom wtforms.validators import DataRequired, Length, Optional, Regexp\n\nfrom wtforms.widgets import DateTimeLocalInput\n\nclass RegistrationForm(FlaskForm):\n    username = StringField('Username',\n                           validators=[DataRequired(), Length(min=4, max=25)])\n\n    username = StringField('Username', [validators.Length(max=40)])\n\n    level    = IntegerField('User Level', [validators.NumberRange(min=0, max=10)])\n\n    birthday  = DateTimeField('Your Birthday', format='%m/%d/%y')\n\n    signature = TextAreaField('Forum Signature')\n\n    accept_rules = BooleanField('I accept the site rules', [validators.InputRequired()])\n\n    registered_number = StringField(\n        \"Registered Number\",\n        validators=[\n            Optional(),\n            Length(min=1, max=16),\n            Regexp(\n                regex=r\"^[\\d]{1,16}$\", message=\"Registered Number must contain numbers only\"\n            )\n        ],\n        render_kw={\"placeholder\": \"Registered Number\"}\n    )\n\n    # Select\n    period = SelectField('Period', [DataRequired()],\n                        choices=[('a','Apple'), ('b','Ball')],\n                        prepend_blank=False))\n\n\n    # Select from Database\n    gym_choices = [(gym.id, gym.name) for gym in Gym.get_gyms()]\n\n    gym_id = SelectField('Select Gym', choices=gym_choices)\n\n\n    # Date\n    start_date = DateField('Start Date', format='%Y-%m-%d')\n\n    in_at = DateTimeField('In Date-Time',\n                      validators=[\n                          DataRequired()\n                          ],\n                      format=\"%Y-%m-%dT%H:%M\",\n                      default=datetime.now,\n                      widget=DateTimeLocalInput()\n                      )\n\n    # Submit\n    submit = SubmitField('Submit')\n\n    def validate_in_at(form, field):\n    \"\"\"max 10 minutes from now can be in time\"\"\"\n      if field.data &gt; datetime.now() + timedelta(minutes=10):\n          raise ValidationError('Your in time cannot be in future!')\n</code></pre> <p>Data Types Fields in <code>wtforms</code> that can be used to build form fields. more on wtforms fields</p> DataType Details StringField One line string BooleanField check box DateField date only DateTimeField date and time DecimalField decimal numbers IntegerField whole numbers SelectField dropdown picklist RadioField radio buttons SubmitField submit button HiddenField not visible EmailField Email type PasswordField Password type dots TextAreaField multi-line <p>Validators that can be used for each field. Each field accepts list of validators in <code>validators=</code> argument. More on wtform validations</p> <pre><code>from wtforms.validators import DataRequired\n</code></pre> Validator Details DataRequired() Required Field NumberRange(min=0, max=10) For IntegerField Optional() Lets continue form submission, used with DateField <p>Custom Validation lets you define your own validation method.  In the form class one can define a method <code>validate_{fieldname}</code> that validates the corresponding field. This method takes as arguments <code>field</code> and <code>form</code> so I can refer to the startdate field as <code>form.startdate_field</code>.</p> <pre><code>class SignupForm(Form):\n    age = IntegerField('Age')\n    startdate_field = DateField('Start Date', format='%Y-%m-%d')\n    enddate_field = DateField('End Date', format='%Y-%m-%d')\n\n    def validate_age(form, field):\n        if field.data &lt; 13:\n            raise ValidationError(\"We're sorry, you must be 13 or older to register\")\n\n    def validate_enddate_field(form, field):\n        if field.data &lt; form.startdate_field.data:\n            raise ValidationError(\"End date must not be earlier than start date.\")\n</code></pre> <p>Widgets</p> <p>Widgets are Classes that bring a specific selector UI for input field. Eg, Date-Picker, Color-Picker etc. WTForm Widgets and HTML5 Input Types for more.</p> <pre><code>from wtforms.widgets import DateTimeLocalInput\n</code></pre> Widget Details DateTimeLocalInput Show datetime-local input type. Lets user input date and time. <p>For extra <code>input</code> params, you can use render_kw argument to send.</p> <pre><code>in_at = DateTimeField('In Date-Time',\n                      format=\"%Y-%m-%dT%H:%M\",\n                      widget=DateTimeLocalInput(),\n                      render_kw={\"step\": \"300\"} \n                     )\n</code></pre>"},{"location":"0-Information-Technology/flask/#get-save-form-data","title":"Get Save Form Data","text":"<p>Once a user submits a form it is automatically validated, and then the data in form can be used in flask to create edit delete or do other stuff.</p> <p>Validation happens when <code>validate_on_submit()</code> is called, and it is called on <code>POST</code> method only. You can explicitly call <code>form.validate()</code> which returns boolean and then handle processing.</p> <pre><code># ADD\n@app.route('task/new', methods=['GET', 'POST'])\ndef new():\n    form = TaskForm()\n\n    if form.validate_on_submit():\n      task = Task()\n      form.populate_obj(task)         # one line fill object, OR\n\n      Task.in_at = form.in_at.data\n      Task.note = form.note.data\n      ...                             # one line and one-by-one can be combined\n\n      task.user_id = current_user.id\n      task.added_via = 'form'\n\n      db.session.add(task)\n      db.session.commit()\n\n      flash('Task added successfully!', 'success')\n      return redirect(url_for('task.all'))\n\n# EDIT\n@app.route('task/&lt;int:id&gt;/edit', methods=['GET', 'POST'])\ndef edit(id):\n    task = Task.query.get(id)\n    form = TaskForm(obj=task)     # one line form fill\n\n    if form.validate_on_submit():\n        form.populate_obj(task)   # one line obj  fill\n        db.session.commit()\n        return redirect(url_for('task.all'))\n\n    return render_template('task/form.html', form=form)\n\n# DELETE\n@checkin.route('task/&lt;int:id&gt;/delete', methods=['POST'])\ndef delete(id):\n    task = db.session.get_or_404(Task, id)\n\n    db.session.delete(task)\n    db.session.commit()\n\n    return redirect(url_for('task.all'))\n</code></pre> <p>Rendering View</p> <p>You can use same view (html template) to add or edit resource. This template recieves <code>form</code> as object and it has all the members to show label, button, inputs etc.</p> <p>Here is code to show form fileds one ny one</p> <pre><code>&lt;form method=\"POST\"&gt;\n{{ form.hidden_tag() }}\n{{ form.name.label }} {{ form.name() }}\n{{ form.submit() }}\n&lt;/form&gt;\n</code></pre> <p>Or, use <code>flask-bootstrap</code> with <code>flask-wtf</code> to magically display form with one-line</p> <pre><code>{% import \"bootstrap/wtf.html\" as wtf %}\n{{ wtf.quick_form(form) }} &lt;!-- one line --&gt;\n</code></pre> <p>Form POST Button</p> <p>Add delete button where you list resource, the code below will only add a button with a confirm check:</p> <pre><code>&lt;form action=\"{{ url_for('task.delete', id=task.id) }}\" method=\"POST\"&gt;\n    &lt;input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\"/&gt;\n    &lt;button type=\"submit\" class=\"btn btn-outline-danger\" \n      onclick=\"return confirm('Are you sure?')\" \n    &gt;\n      Delete\n    &lt;/button&gt;\n&lt;/form&gt;\n</code></pre> <p>Adding CSRF to whole app</p> <p>You may also need to add CSRF in app, in <code>app.py</code> or <code>__init__.py</code>, add</p> <pre><code>from flask_wtf.csrf import CSRFProtect\n...\ncsrf = CSRFProtect()\n...\ncsrf.init_app(app)\n</code></pre> <ul> <li> <p>Flask-WTF integration of Flask and WTForms</p> <ul> <li>Includes CSRF, file upload, and reCAPTCHA. You mostly have to use formats of WTForms but write less as few things are done automatically that are related to Flask patter.</li> <li>Form fields are Class variables with different field type</li> <li>validator functions can help validate, like <code>Email()</code>.</li> <li>Link to Flask-WTF</li> </ul> <ul> <li> <p>Validation controller</p> <pre><code>from flask import Flask, render_template, session, redirect, url_for\n\n@app.route('/', methods=['GET', 'POST'])\ndef index():\n  form = NameForm() # defined as OOP model\n\n  if form.validate_on_submit(): # cheks POST and validates\n    session['name'] = form.name.data\n    return redirect(url_for('index')) # POST -&gt; back to this function as GET\n\n  return render_template('index.html', form=form, name=session.get('name')) # When GET\n</code></pre> <ul> <li>Route - define new route, import form class and use. On submit, create object of model class to save/query data.</li> </ul> </li> </ul> </li> </ul>"},{"location":"0-Information-Technology/flask/#databases-in-flask","title":"Databases in Flask","text":"<p>DB_package or ORM - Python has packages for most database engines like MySQL, Postgres, SQLite, MongoDb etc. If not, you can use ORM that lets you use Python objects to do SQL operations, SQLAlchemy or MongoEngine are such packages.</p> <p>Flask-SQLAlchemy is wrapper on SQLAlchemy. You have to use SQLAlchemy pattern but it helps by making things tied to Flask way like session of SQLAlchemy is tied to web-request of flask.</p> <ul> <li>It is designed for Flask and adds support for SQLAlchemy to your application. So basically you use all knowledge and concept of SQLAlchemy but tied up with flask. Remember, SQL Alchemy can be used withour flask from command line or any other python program.</li> <li> <p>You can define table as a class, called model, with member variables as column names. SQLAlchemy documentation is to be reffered, just add <code>db</code> before commands. so</p> <pre><code>session.add(user)       # SQLAlchemy\ndb.session.add(user)    # Flask-SQLAlchemy\n</code></pre> </li> </ul> <p>Installation</p> <pre><code>python -m pip install flask-sqlalchemy\n</code></pre> <p>Initiation</p> <p>create <code>SQLAlchemy()</code> class object and pass <code>app</code> for context. In <code>app.py</code></p> <pre><code>from flask_sqlalchemy import SQLAlchemy\n\napp.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///' + os.path.join(basedir, 'data.sqlite')\napp.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\n\ndb = SQLAlchemy(app) # Object for all ops\n</code></pre>"},{"location":"0-Information-Technology/flask/#define-tablesmodels","title":"Define Tables/Models","text":"<p>Conventions:</p> <ul> <li>in database, table name is plural - users, books etc</li> <li>in python, model class name is singular - User, Book etc</li> </ul> <p>Tables can be defined in OOP pattern as a class called \"model\". Model is a Class which represents application entities, like, User, Task, Author, Book etc. You can define, table, its columns, data types, keys and relationships. The class has attributes that represent column name, eg <code>name = db.Column(db.String(64)</code>.</p> <p>In <code>model.py</code></p> <pre><code>from datetime import datetime\nfrom app import db\n\nclass User(db.Model):\n    __tablename__ = 'users'\n    id = db.Column(db.Integer, primary_key = True)\n    username = db.Column(db.String(50), unique=True)\n    admin = db.Column(db.Boolean)\n    created_on = db.Column(db.DateTime, default=datetime.utcnow())\n    updated_at = db.Column(db.DateTime, default=datetime.utcnow(),\n                            onupdate=datetime.utcnow())\n</code></pre> <p>You can define column as following Data Types:</p> DataType Detail Integer an integer String(size) a string with a maximum length (optional in some databases, e.g. PostgreSQL) Text some longer unicode text DateTime date and time expressed as Python datetime object. Float stores floating point values Boolean stores a boolean value <p>You can define properties like:</p> Prop Value Detail primary_key True makes primary key unique True ensures unique nullable True/False allows NULLs or not default any value of same data-type provides default value that is inserted if value provided is NONE. The default value is provided in INSERT query. onupdate any value of same data-type changes on row update db.ForeignKey('some.id') pass table_name.column_name Adds relationship server_default any value of same data-type It adds to DDL, create table statement, more"},{"location":"0-Information-Technology/flask/#create-tables","title":"Create Tables","text":"<p>Once you have created a db model in flask app, you can create db and tables using following steps, open python shell:</p> <pre><code>from app import db\ndb.create_all() # creates all tables from model class, if they don't exist\n</code></pre> <p>Advanced, if you are using application factory, then you need app_context to work with database object:</p> <pre><code>from app import db, create_app\n\napp = create_app('default')\napp_context = app.app_context()\napp_context.push()\n\ndb.create_all()\n</code></pre> <p>Now you can check SQL for tables created. You can do:</p> <pre><code>sqlite3 filename.db\n.tables\n</code></pre> <p>If you have done any changes to the model, like adding a column, you need to again recreate tables, but command above doesn't recreate existing tabke, you need to drop them and recreate, in python shell</p> <pre><code>db.drop_all()\ndb.create_all()\n</code></pre> <p>Migrations is a better way to do this without dropping created data and keeping version control to go back is, more on this later.</p>"},{"location":"0-Information-Technology/flask/#relationships-in-database","title":"Relationships in Database","text":"<p>You can define relationship in OOPs way as attribute of class. Beauty is that the tables are linked both ways. In <code>model.py</code>.</p> <p>Example of One to Many relationship. User has multiple posts but post has only one author. <code>User 1-m Post</code>:</p> <pre><code>class Post(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    title = db.Column(db.String(80), nullable=False)\n    body = db.Column(db.Text, nullable=False)\n\n    user_id = db.Column(db.Integer, db.ForeignKey('user.id'), nullable=False)\n\nclass User(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    name = db.Column(db.String(50), nullable=False)\n\n    # relationships\n    posts = db.relationship('Post',\n                backref=db.backref('author'),\n                lazy=False,\n                order_by=\"desc(Post.created_at)\")\n</code></pre> <p>Here you note, the <code>ForeignKey()</code> relation has to be defined in the table that has many records but <code>db.relationship(.. ,backref=)</code> can be in either of the two tables, so following statements are same:</p> <pre><code># in Post table, this tells Post has 1 Author and that has MANY posts\nauthor = db.relationship('User', backref=db.backref('posts'))\n\n# OR in User table, this tells User has MANY Posts, and that has 1 author\n    posts = db.relationship('Post', backref=db.backref('author'))\n</code></pre> <p>Here, <code>backref</code> adds a back-reference to other model. <code>lazy=False</code> tells SQLAlchemy to load the relationship in the same query as the parent using a JOIN statement.</p> <p>Here, <code>order_by</code> lets you specify and order and returns and ordered <code>InstrumentList</code>.</p> <p>Example of multiple One to Many relation between tables. Lets say, User has many memberships and has many membership approvals. But Membership has only one User and one Approver. Also, both member and approver are user, so two relationships. Here we have to include, <code>foreign_keys=[]</code> argument to relationship to define which foreign-key it is reffering to and avoid ambiguity.</p> <pre><code>class User(db.Model):\n    ...\n    # One to Many relationships\n    memberships = db.relationship(Membership, backref='user', foreign_keys=[Membership.user_id])\n    memberships_approved = db.relationship(Membership, backref='approver', foreign_keys=[Membership.approved_by])\n\nclass Membership(db.Model):\n    ...\n    # Many to One relationships\n    user_id = db.Column(db.Integer, db.ForeignKey('users.id'))\n    approved_by = db.Column(db.Integer, db.ForeignKey('users.id'))\n</code></pre> <p>Example, One to One relationship, like User has a Profile but only one which can have extra user details like height, weight etc. You can pass <code>uselist=False</code> to <code>relationship()</code>.</p> <pre><code>class Profile(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    height = db.Column(db.Integer)\n    user_id = db.Column(db.Integer, db.ForeignKey('user.id'), nullable=False)\n\nclass User(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    name = db.Column(db.String(50), nullable=False)\n\n    # relationships\n    profile = db.relationship('Profile', backref=db.backref('user'), uselist=False)\n</code></pre> <p>Example, Many-to-Many Relationships you will need to define a helper table. Say, Post has multiple Tags and Tags have multiple Posts</p> <pre><code># this is table\npost_tag_m2m = db.Table('tags',\n    db.Column('tag_id', db.Integer, db.ForeignKey('tag.id'), primary_key=True),\n    db.Column('post_id', db.Integer, db.ForeignKey('post.id'), primary_key=True)\n)\n\nclass Post(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    tags = db.relationship('Tag', secondary=post_tag_m2m, lazy='subquery',\n        backref=db.backref('posts', lazy=True))\n\nclass Tag(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n</code></pre> <p>Link: Models - FlaskSqlAlchemy</p>"},{"location":"0-Information-Technology/flask/#insert-or-create","title":"Insert or Create","text":"<p>Create an Object of Class to build a new row. Usually in a route in <code>views.py</code> or in python shell:</p> <pre><code>user = User()\n\nuser.username = 'john'\nuser.role = 'admin'\n\ndb.session.add(user)\ndb.session.commit()\n</code></pre> <p>Here, we create a new object. Initialize it's attributes. Finally add it to be saved. Lastly, commit it to datebase, this where <code>INSERT</code> is performed.</p> <p>To use INSERTed object for another operation, <code>id</code> is added to object after <code>commit()</code> and is made available for use.</p>"},{"location":"0-Information-Technology/flask/#read-or-select-or-query","title":"Read or Select or Query","text":"<p>Each model has <code>query</code> object is available. It has to be chained with filter-options and/or executors that build a SQL Query statement.</p> <p>Filter-Options - They are added to choose records. Eg, <code>filter()</code>, <code>filter_by()</code>, <code>limit()</code>, <code>offset()</code>, <code>order_by()</code>, <code>group_by()</code></p> <p>Executors - they are at end of chained methods and finally execute the query to get result set. Eg, <code>all()</code>, <code>first()</code>, <code>first_or_404()</code>, <code>get()</code>, <code>get_or_404()</code>, <code>count()</code>, <code>paginate()</code>, <code>fetch()</code>, <code>fetchall()</code>, <code>scalar()</code>.</p> <p>Imp, when refering column of a Models, <code>.c</code> is not required. When refering a col from table, like many2many join table, helper table, then use <code>.c</code> collection of columns, eg, <code>Post.query.join(followers, (followers.c.followed_id == Post.user_id))</code>.</p> <p>Links:</p> <ul> <li>operators, is_()</li> <li>select ORM, join where</li> <li>models - flask-sqlalchemy</li> <li>pagination - flask-sqlalchemy</li> <li>func calculations</li> <li>extact function</li> </ul> <p>scalars vs all</p> <ul> <li>use scalar when you have whole and only one ORM object to select. That is only User and whole User (all fields of User). This will give list of User object, hence easy to use. <code>items[0].name</code> is name of first user.</li> <li>use all when you have multiple ORM model to select or partial columns to select from orm model. It will return list of tuple having objects or columns. You have to unpack the tuple. <code>items[0].User.name</code> is name of first user.</li> </ul> <pre><code>items = db.session.execute(db.select(Book, Author).join(Author, Author.author_id == Book.author_id)).all() # correct\n# [(&lt;Book 1&gt;, &lt;Author: [&lt;Book 1&gt;]&gt;), ... , (&lt;Book 10&gt;, &lt;Author: [&lt;Book 9&gt;, &lt;Book 10&gt;]&gt;)]\n\nitems = db.session.execute(db.select(Book, Author).join(Author, Author.author_id == Book.author_id)).scalars().all()\n# [&lt;Book 1&gt;, &lt;Book 2&gt;, ... , &lt;Book 10&gt;]\n</code></pre> <p>More on SO - scalars or all</p> <p>Examples:</p> <pre><code># Get list of objects\n\nfrom sqlalchemy import func\n\ndb.session.scalars(db.select(User).order_by(User.id)).all()\n\n\n# Select columns with calculations including Date Operations\n\ndate_in = dt.strptime('2023-10-22', \"%Y-%m-%d\")\n\nq = db.select(\n    func.date(Checkin.in_at).label('date_in'),\n    func.count(Checkin.id).label('total'),\n    Checkin.membership_id\n).where(\n        Checkin.deleted_on.is_(None),\n        Checkin.membership_id == 10\n        (func.date(Checkin.created_at) == func.date(date_in)),\n        (func.date(Checkin.in_at) == func.date(form.in_at.data)),\n\n).group_by(\n    func.date(Checkin.in_at),\n    Checkin.membership_id\n)\n\nstr(q)\ndb.session.execute(q).all()\n\n\n# Date Comparisions --------------------------\n\nbegin_at = dt.strptime('2024-01-23 17:30', \"%Y-%m-%d %H:%M\")\nend_at = dt.strptime('2024-01-23 18:40', \"%Y-%m-%d %H:%M\")\n\nq = db.select(Task.id).where(\n    (Task.user_id == 11) &amp;\n    (Task.deleted_on.is_(None)) &amp;\n    ((Task.begin_at &lt;= begin_at) &amp; (begin_at &lt;= Task.end_at)) |\n    ((Task.begin_at &lt;= end_at) &amp; (end_at &lt;= Task.end_at)) |\n    ((begin_at &lt;= Task.begin_at) &amp; (Task.end_at &lt;= end_at))\n)\nstr(q)\ndb.session.scalars(q).all()     # list of id or blank id\n\n\n# Complex Where with Join   --------------------------\n\ndb.session.scalars(\n    db.select(Gym)\n    .join(Membership)\n    .where(\n        (Membership.user_id == 3) &amp;\n        (Gym.active) &amp;                          # Boolean\n        (~Gym.closed) &amp;                         # Boolean Reverse\n        (Membership.approved_by.is_not(None))   # NOT NULL\n    )\n).all()\n\n\n# Build Query as Modular in Part  ---------------------\n\nselected = db.select(Response)\nclauses = Response.flow_type.ilike(sql_q)\n# add an or clause\nclauses = clauses | Response.error_reason.ilike(sql_q)\nflow_id = request.args.get('flow_id', None)\nif (flow_id):\n    # add an and clause\n    clauses = (clauses) &amp;  (Response.flow_id == int(flow_id) )\nfiltered = selected.where(clauses)\nordered = filtered.order_by(Response.responded_at.desc())\ndb.session.execute(filtered).scalars().all()\n\n\n\n# Pagination\n\npaginated_responses = db.paginate(\n    ordered.limit(50),\n    page=page, per_page=10, error_out=True)\n# above will give 5 pages, with 10 on each due to limit.\n\nusers = db.paginate(db.select(User).order_by(User.join_date))\n\n\n# Get ScalarResult, not scriptable, but works with for\n\nuser = db.session.execute(db.select(User).filter_by(username=username)).scalar_one()\n\nusers = db.session.execute(db.select(User).order_by(User.username)).scalars()\n\nusers = db.session.execute(db.select(User).order_by(User.username)).scalars()\n\nu = db.session.get(User, 4)\n\nu = db.session.execute(db.select(User).filter_by(name=\"sandy\")).scalar_one()\n\nu = session.execute(db.select(User.fullname).where(User.id == 2)).scalar_one()\n\n\n# view queries\n\nuser = db.get_or_404(User, id)\n\nuser = db.one_or_404(db.select(User).filter_by(username=username))\n\n\n# 404 with message for abort\n\nuser = db.one_or_404(\n    db.select(User).filter_by(username=username),\n    description=f\"No user named '{username}'.\"\n)\n\n\n# ORM Queries\ndb.select(user_table).where(user_table.c.name == \"spongebob\")\n\n\n# JOINs\ndb.select(user_table.c.name, address_table.c.email_address).join(address_table)\n\ndb.select(address_table.c.email_address)\n  .select_from(user_table)\n  .join(address_table, user_table.c.id == address_table.c.user_id)\n\n\n# outer join\nprint(select(user_table).join(address_table, isouter=True))\nprint(select(user_table).join(address_table, full=True))\n\n\n# order group having\ndb.select(User.name, func.count(Address.id).label(\"count\"))\n        .join(Address)\n        .group_by(User.name)\n        .having(func.count(Address.id) &gt; 1)\n\ndb.select(Address.user_id, func.count(Address.id).label(\"num_addresses\"))\n    .group_by(\"user_id\")\n    .order_by(\"user_id\", desc(\"num_addresses\")\n</code></pre> <p>DEBUG</p> <pre><code># print query\nprint(q.compile(compile_kwargs={\"literal_binds\": True}))\n</code></pre> <p>More on https://stackoverflow.com/a/45551136/1055028</p> <p>Examples, OLD and Legacy, uses <code>Model.query</code>, Prefer using <code>db.session.execute(db.select(...))</code> instead.:</p> <pre><code># select by primary key, ID\nu = User.query.get(1)   # 1 is id in table\nu.name                  # prints name\nposts = u.posts.all()   # if user has 1-m relationship with Post table\n\n# SELECT\nusers = User.query.all()\n\n# select WHERE\nadmins = User.query.filter_by(role='admin').all()\n\n# select TOP 1 where\nu = User.query.filter_by(username='johndoe').first()\n# u is None if username does not exist\n\n# Select TOP n or LIMIT\nUser.query.limit(10).all()\n\n# WHERE column ENDSWITH\nusers = User.query.filter(User.email.endswith('@example.com')).all()\n\n# ORDER BY\nusers = User.query.order_by(User.username).all()\n\n# Get SQL query Statement, see there is no executor\nsql_stmt = str(User.query.filter_by(role='admin'))\n\n# 404 errors, this will raise 404 errors instead of returning None\n@app.route('/user/&lt;username&gt;')\ndef show_user(username):\n    user = User.query.filter_by(username=username).first_or_404()\n    p = Post.query.get_or_404(1)\n    return render_template('show_user.html', user=user)\n\n# Pagination\npage = User.query.order_by(User.join_date).paginate()\n\n# Joins\n\nPost.query.join(...).filter(...).order_by(...)\n\nfollowed = Post.query.join(\n            followers, (followers.c.followed_id == Post.user_id)).filter(\n                followers.c.follower_id == self.id)\n</code></pre> <p>For pagination, during a request, this will take page and per_page arguments from the query string request.args. Pass max_per_page to prevent users from requesting too many results on a single page. If not given, the default values will be page 1 with 20 items per page.</p> <p>Link:</p> <ul> <li>Quickstart - flask-sqlalchemy</li> <li>Date Filters</li> </ul>"},{"location":"0-Information-Technology/flask/#update","title":"Update","text":"<p>Load the object, modify its attributes, then do <code>add</code> and <code>commit</code>.</p> <pre><code>u = User.query.get(1)   # 1 is id in table\nu.role = 'staff'        # modified attribute\n\ndb.session.add(user)    # ready to save\ndb.session.commit()     # UPDATE is performed\n</code></pre>"},{"location":"0-Information-Technology/flask/#delete","title":"Delete","text":"<pre><code>u = User.query.get(1)   # 1 is id in table\ndb.session.delete(u)\ndb.session.commit()     # DELETE is performed\n</code></pre>"},{"location":"0-Information-Technology/flask/#raw-sql","title":"RAW SQL","text":"<p>Give your SQL statements</p> <ul> <li><code>db.session.execute(SQL)</code> returns cursor</li> <li><code>db.session.execute(SQL).all()</code> - returns List result set</li> </ul> <p>Engine object</p> <p>You can get the <code>Engine</code> object from the current session connection using, property engine / engines or method <code>get_engine()</code>. more on: engine docs</p> <p>Shell Operations</p> <p>CRUD from Flask Shell</p> <ul> <li><code>flask --app hello.py shell</code> start shell with app_context, python shell will not have that.</li> <li><code>db.create_all()</code> creates SQLite file.</li> </ul> <p>Extras - Database Schema</p> <p>For MS-SQL you may need to use schema name along with table and database name. It can be defined in configuration and then used in models. db.metadatas - You can add schema of table in model using:</p> <pre><code>class User:\n    __table_args__ = {'schema': db.metadatas['SCHEMA']}\n</code></pre> <p>and define schema in metadata when initializing <code>db</code> object in <code>__init__.py</code> of app using:</p> <pre><code>db.metadatas['SCHEMA'] = app.config.get(\"SCHEMA\") or \"[dbo]\"\n</code></pre> <p>You can also add more metadata here, like database name.</p> <p>Extras - Py ORM Model from SQL</p> <p>If you have a existing tables in database and want o Generate SQLAlchemy class model from database table - <code>sqlacodegen mssql+pyodbc://&lt;servername&gt;\\&lt;schema&gt;/&lt;database&gt;/&lt;table_name&gt;?driver=SQL+Server --outfile db.py</code></p>"},{"location":"0-Information-Technology/flask/#multiple-database-connections","title":"Multiple Database Connections","text":"<p>You can connect to multiple database with Flask-SQLAlchemy. It connects to different databases and creates different engines. All you have to do is pass the <code>bind_key</code> in any call to db.</p> <p>More on https://flask-sqlalchemy.palletsprojects.com/en/latest/binds/</p>"},{"location":"0-Information-Technology/flask/#migrations-in-database","title":"Migrations in Database","text":"<ul> <li>Why? - When DB is handled using ORM, all changes to DB is done via ORM. If you have to add a column it is added by ORM so it will delete the table and create new but to prevent data loss in table it will create a migration script to create and populate again.</li> </ul> <ul> <li>What? - <code>Flask-Migrate</code> is wrapper on <code>Alembic</code> a SQLAlchemy migration framework. It generates Py script to keep the database schema updated with the models defined. It help upgrade and roll back the schemas.</li> </ul> <ul> <li>Installation - <code>pip install flask-migrate</code></li> </ul> <p>Initiation</p> <pre><code>from flask_migrate import Migrate\nmigrate = Migrate(app, db)\nflask --app hello.py db init\n</code></pre> <p>This generates migration directory, script is generated</p> <ul> <li>Execution<ul> <li><code>upgrade()</code> has data changes to be done in this migration</li> <li><code>downgrade()</code> rolls back to previous state</li> <li>Example: Steps to make a migration<ul> <li>Make changes to model classes</li> <li><code>flask --app hello.py db migrate -m \"initial migration\"</code> to generate script</li> <li>review for accurate changes. add to source control</li> <li><code>flask--app hello.py db upgrade</code> to do migration in database</li> </ul> </li> </ul> </li> </ul> <ul> <li>Reference<ul> <li>How To Make a Web Application Using Flask in Python 3</li> <li>SQLite explorer</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/flask/#login-in-flask-flask-login","title":"Login in Flask - Flask Login","text":"<p>Flask-Login is minimal and powerful tool to manage logins in flask. It does the job and gives the flexibility as well to handle things</p>"},{"location":"0-Information-Technology/flask/#role-based-control","title":"Role based control","text":"<p>You may also need to check role of user when logged in. For this you can build your own decorator function and use with login_required. Eg:</p> <pre><code>from functools import wraps\n\nfrom flask import redirect, flash, url_for\nfrom flask_login import current_user\n\n# define below somewhere (utils or lib) and import in views\ndef role_required(*roles):\n    def decorator(f):\n        @wraps(f)\n        def decorated_function(*args, **kwargs):\n\n            if current_user.role not in roles:\n                flash('You do not have permission to do that.', 'danger')\n                return redirect('/')\n\n            return f(*args, **kwargs)\n        return decorated_function\n    return decorator\n\n\n\n# For all requests in a blueprint\n@bp.before_request\n@login_required\n@role_required('member', 'staff')\ndef before_request():\n    \"\"\" Protect all of the checkin endpoints. \"\"\"\n    pass\n\n# For individual routes\n@app.route('/&lt;int:id&gt;/delete', methods=['POST'])\n@role_required('admin', 'staff', 'owner')\ndef delete(id):\n    pass\n</code></pre>"},{"location":"0-Information-Technology/flask/#emails-in-flask","title":"Emails in Flask","text":"<ul> <li>Why password reset, confirmations</li> </ul> <ul> <li>How?<ul> <li>Emails can be sent using <code>smtplib</code> package from Python standard library.</li> <li>Email is sent by connecting to SMTP Server which takes request to send email to recipient.</li> <li>Localhost on port 25 is local server that can send email.</li> <li>External SMTP server like <code>mail.googlemail.com</code> on <code>587</code> port can be used to send emails through Google Gmail account.</li> </ul> </li> </ul> <ul> <li>Flask-Mail is a extension that wraps <code>smtplib</code><ul> <li>Installation <code>pip install flask-mail</code></li> <li>import <code>from flask_mail import Mail, Message</code></li> <li>instantiate and initialize <code>mail = Mail(app)</code></li> <li>build obj <code>msg_obj = Message('sub','sender','to')</code></li> <li>add body and html to obj, may use template for it <code>msg.body = render_template(template + '.txt', **kwargs)</code></li> <li>send <code>mail.send(msg_obj)</code></li> </ul> </li> </ul> <ul> <li>Sending Asynchronous Email<ul> <li>Message() object can be build in mail python file but Mail() object, which sends the email using msg_obj, should run in separate thread to avoid lags.</li> <li>use python Thread() class from threading package to make new thread that runs the send_async_email(app,msg) functions. this functions has<ul> <li>app object of FLask() for context</li> <li>msg object of Message() for content</li> <li>uses mail object of Mail() to send.</li> </ul> </li> <li><code>from threading import Thread</code></li> <li>The function which build Message(), add line<ul> <li><code>thr = Thread(target=send_async_email, args=[app, msg])</code> - build thread obj</li> <li><code>thr.start()</code> execute thread separately</li> <li><code>return thr</code> [ ] why this is added</li> </ul> </li> </ul> </li> </ul> <ul> <li>Local Email Server<ul> <li><code>(venv) $ python -m smtpd -n -c DebuggingServer localhost:8025</code> this command starts emulated email server.</li> </ul> </li> </ul> <ul> <li> <p>Variables that we might need to export:</p> <pre><code>export MAIL_SERVER=smtp.googlemail.com\nexport MAIL_PORT=587\nexport MAIL_USE_TLS=1\nexport MAIL_USERNAME=email_id@domain.com\nexport MAIL_PASSWORD=\"password\"\n</code></pre> </li> </ul> <ul> <li>Email is only sent when FLASK_ENV = production</li> </ul> <ul> <li>Sending errors via Email<ul> <li>Errors can be sent via email using Logs.</li> </ul> </li> </ul> <ul> <li> <p>Dev - send emails to console</p> <ul> <li><code>MAIL_SERVER = 'localhost'</code></li> <li><code>MAIL_PORT = 8025</code></li> <li><code>python -m smtpd -n -c DebuggingServer localhost:8025</code></li> </ul> <ul> <li>Links<ul> <li>Flask docs - Email errors to admin</li> <li>MG's microblog - email errors</li> <li>Pythonbasics - Flask Mail</li> </ul> </li> </ul> </li> </ul>"},{"location":"0-Information-Technology/flask/#blueprint-large-app-structure-in-flask","title":"Blueprint - Large App Structure in Flask","text":"<p>needs improvements after hands-on</p> <ul> <li>simply, module is file, package is folder. Blueprint can be implemented in both.</li> </ul> <ul> <li>single module - link<ul> <li> <p>here, one file has everything defined</p> <pre><code>/prj\n    my-app.py # imports, app, db, orm_class, form_class, route_view_functions, run\n    /static\n    /templates\n</code></pre> </li> </ul> </li> </ul> <ul> <li> no package, Blueprints as modules?</li> </ul> <ul> <li>single package, Blueprints as modules - link<ul> <li>here all ORM &amp; FORM classes are in one module</li> <li> <p>more complex, app-factory, no ORM, installable example with same structure, here</p> <pre><code>/prj\n    run.py          # import app, db. then run. What parts to run?\n    config.py       # module. config vars. With which configurations?\n\n    /my-app         # package\n        __init__.py # app, register BP\n        forms.py    # module. has form classes\n        models.py   # module. DB setup. has ORM classes.\n\n        auth.py     # module BP. import db, form, model. route-view-functions. login, register, logout. @login_required.\n        blog.py     # module BP. import db, form, model. route-view-functions. CRUD.\n\n        /template\n        /static\n</code></pre> </li> </ul> </li> </ul> <ul> <li>multi-packages, Blueprints as sub-packages - link<ul> <li>here ORM &amp; FORM classes are in separate module for each Blueprint.</li> <li>DB is top-level as it is shared by all sub-packages</li> <li> <p>auth is top-level to avoid circular dependencies.</p> <pre><code>/prj\n    run.py              # same. import app, db. then run\n    config.py           # same. module. config vars\n    /my-app             # package\n        __init__.py     # app, register BP\n        auth.py         # login init. login_manager\n        data.py         # db setup, init, open, close. crud helpers\n\n        /users\n            __init__.py # blank\n            forms.py    # module. has form classes. LoginForm, RegisterForm\n            models.py   # module. has ORM classes. User.\n            views.py    # BP Module. imports form, model. defines route-view-functions\n\n        /blog\n            __init__.py # blank\n            forms.py    # module. has form classes. CreateBlogForm, EditBlogForm\n            models.py   # module. has ORM classes. Post, Follower.\n            views.py    # BP Module. imports form, model. defines route-view-functions\n            geodata.py  # module. helper functions.\n\n        /template\n        /static\n</code></pre> </li> </ul> </li> </ul> <ul> <li>Example - Miguel Grinbers's Flasky<ul> <li>app-factory using <code>create_app()</code> in init.</li> <li>models are all in one module, not in blueprint package. [ ] why? probably all BP use models</li> <li>blueprint as sub-package's <code>__init__.py</code>, not views. [ ] why?</li> <li> <p>api package has multiple modules, all have route-view-functions.</p> <pre><code>prj/\n    run.py      # imports create_app, db, ORM-models. app init. flasky.py\n    config.py   # module. config class with vars. EnvDict\n    /app\n        __init__.py # import config, extensions. add extensions. def create_app.\n        models.py   # import db, login_manager. def all ORM and Mixin classes\n\n        /auth\n            __init.py # define BP. import views\n            forms.py  # import ORM. Form classes\n            views.py  # import db, Forms, ORMs. def route-view-functions login, logout, register, reset\n        /main\n            __init.py # define BP. import routes\n            forms.py  # import ORM. Form classes\n            views.py  # import db, Forms, ORMs. def route-view-functions\n        /api\n            __init.py         # define BP. import each routes\n            authentication.py # import ORM, api. def route-view-functions tokens\n            comments.py       # import db, ORM, api. def route-view-functions tokens\n            posts.py          # import db, ORM, api. def route-view-functions tokens\n            users.py          # import ORMs, api. def route-view-functions users, follower\n        /static\n        /templates\n</code></pre> </li> </ul> </li> </ul> <ul> <li>Example - Miguel Grinbers's microblog<ul> <li>same as above, flasky</li> <li> <p>in sub-packages, views.py is routes.py</p> <pre><code>prj/\n    run.py      # imports create_app, db, ORM-models. app init.\n    config.py   # module. config class with vars\n    /app\n        __init__.py # import config. add extensions. def create_app. import models.\n        models.py   # all ORM and Mixin classes\n\n        /auth\n            __init.py # define BP. import routes\n            forms.py  # import ORM. Form classes\n            routes.py # import db, bp, Forms, ORMs. def route-view-functions login, logout, register, reset\n        /main\n            __init.py # define BP. import routes\n            forms.py  # import ORM. Form classes\n            routes.py # import db, bp, Forms, ORMs. def route-view-functions\n        /api\n            __init.py # define BP. import routes\n            tokens.py # import db, bp. def route-view-functions tokens\n            users.py  # import db, bp, ORMs. def route-view-functions users, follower\n        /static\n        /templates\n</code></pre> </li> </ul> </li> </ul> <ul> <li>Example - Miguel Grinbers's microblog-api<ul> <li>app-factory using create_app in app.py</li> <li>Blueprints as modules</li> <li> <p>single package</p> <pre><code>/prj\n    run.py\n    config.py\n    api/\n        __init__.py # imports create_app, db\n        app.py      # create_app, db init, registers BP\n        auth.py     # import db, model. login functions\n        models.py   # import db. ORM classes\n\n        posts.py    # BP. imports db, ORM. route-view-functions posts, feed\n        tokens.py   # BP. imports db, ORM. route-view-functions tokens, reset_token\n        users.py    # BP. imports db, ORM. route-view-functions users, me, followers\n        templates/  # only email reset html\n</code></pre> </li> </ul> </li> </ul> <ul> <li>Example - Miguel Grinbers's microblog-2012<ul> <li>separate module for model, form, email and view.</li> <li>no blueprint, no app-factory, no sub-package</li> </ul> </li> </ul> <ul> <li>what looks good - why?<ul> <li>app-factory - gives flexibility</li> <li>config - for env separation<ul> <li>use config class for defaults and different envs</li> <li>use YAML file to read secrets and keep it out of git</li> <li>more on config best practice</li> </ul> </li> <li>blueprints as modules.</li> <li>links<ul> <li>Another good tutorial</li> </ul> </li> </ul> </li> </ul> <ul> <li>Why? - App needs to be structured into modules as it starts growing. It also helps reuse modules.</li> </ul> <ul> <li> <p>Without Blueprint</p> <ul> <li>Single py file app structure<ul> <li>import flask modules and extensions</li> <li>instantiate flask app <code>app = Flask(__name__)</code></li> <li>configure app with all configs, eg, <code>app.config['MAIL_PORT'] = 587</code></li> <li>initialize extensions, eg, <code>mail = Mail(app)</code>. Not all extensions are initialized, eg, FlaskForm</li> <li>DB ORM Classes</li> <li>Email functions - may use templates</li> <li>Form Classes</li> <li>error handlers functions - may use templates</li> <li>routes, they may use use<ul> <li>above extensions, eg - checks Form, sends email, writes to db, or returns an error</li> <li>native - session, flash, g</li> <li>templates.</li> </ul> </li> </ul> </li> </ul> <ul> <li>templates and static files structure<ul> <li>base template is HTML, it has blocks. Block-content can be replaced or appended</li> <li>base-template is used to build different pages which put dynamic content in blocks.</li> <li>static files can be used from static folder.</li> <li>example flow<ul> <li><code>base.html</code> has blocks, title, nav, page_content</li> <li>index or profile have <code>{% extends \"base.html\" %}</code>, it tells Jinja to use base.</li> <li>block-content can be replaced or appended using <code>{{ super() }}</code></li> <li>files from <code>static</code> folder using <code>{{ url_for('static', 'favicon.ico') }}</code></li> <li>external packages can be imported as py_var to build content as py_var and use in content. eg - wtf template can be imported from bootstrap to build content from form_object using <code>{{ wtf.quick_form(form) }}</code>.</li> </ul> </li> </ul> </li> </ul> </li> </ul> <ul> <li>Blueprint lets us divide app into mini apps. It is a collection of views, templates, static files that can be applied to an application. Blueprints are a great way to organize your application.</li> </ul> <ul> <li>Application Factory is way of initializing app<ul> <li>to serve a request, when single file app in invoked, app gets initialized with configs to serve the request. You do not have flexibility to make changes to config dynamically</li> <li>app initialization can be delayed (or controlled) by making a function to do it, called <code>factory function</code>. This can be explicitly controlled.</li> </ul> </li> </ul> <ul> <li> <p>How?</p> <ul> <li>choice - you can simply keep templates and static in one folder or can split them too and keep in blueprint sub-folders.</li> </ul> </li> </ul> <ul> <li> <p>single file split</p> <ul> <li>in single file, you can move view-routes-functions.</li> <li> <p>in <code>second.py</code></p> <pre><code>from flask import Blueprint\n\nsecond = Blueprint(\"second\", __name__)\n\n@second.route(\"/home\")\ndef home():\n    return (\"from second\")\n</code></pre> </li> </ul> <ul> <li> <p>and in <code>app.py</code></p> <pre><code>from flask import Flask\nfrom second import second\n\napp = Flask(__name__)\n\napp.register_blueprint(second, url_prefix=\"\")\n\n@app.route(\"/\")\ndef home():\n    return \"Hi\"\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n</code></pre> </li> </ul> </li> </ul> <ul> <li> <p>multiple files split</p> <ul> <li>make a sub-folder and add<ul> <li>constructor</li> <li>error-route functions</li> <li>form classes</li> <li>views-route functions</li> </ul> </li> <li> <p>db models and other functions still remain in main file.</p> <pre><code>|-app_name      # 0 top level dir - any name\n  |-app/          # 2 package having flask application\n    |-templates/\n    |-static/\n    |-__init__.py   # 2.1 app pkg constructor, factory\n    |-models.py     # 2.2 db models\n    |-email.py      # 2.3 email \n    |-main/         # 5 BP sub pkg\n      |-__init__.py   # 5.1 pkg const defines BP\n      |-errors.py     # 5.2 err handlers\n      |-forms.py      # 5.3 form classes\n      |-views.py      # 5.4 routes functions\n  |-config.py     # 3 configuration variables as OOPs\n  |-flasky.py     # 4 factory is invoked\n</code></pre> </li> </ul> <ul> <li>3 - <code>config.py</code> config as OOPs<ul> <li>the config variables like secret-key and mail-server, are now attributes of <code>Config</code> class.</li> <li><code>Config</code> class has <code>@staticmethod</code> as <code>init_app(app)</code> which can be used to do things once app is available, i.e. initialize app and more.</li> <li>This Config base class has common vars but can be extended to build different environment classes like dev, test, prod. that can have env specific vars like dev db-location.</li> <li>add a dictionary <code>conf_env</code> to pick the correct env class.</li> </ul> </li> <li>2 - <code>app/</code> App Package<ul> <li>dir having code, template and static files.</li> <li>2.1 - <code>app/__init__.py</code> App Pkg Constructor<ul> <li>this is where we build the <code>factory function</code> to initialize app explicitly and controlled.</li> <li>import Flask modules (only Flask)</li> <li>import Flask-Extensions (only those that need app init)</li> <li>instantiate extensions without <code>app</code></li> <li>factory function <code>def create_app(conf_env):</code> function to have<ul> <li>arg <code>conf_env</code> is dictionary key name (str) to pick required Env_Config_Class from <code>config.py</code> so that we have correct config vars.</li> <li>instantiate app</li> <li>add configs from object <code>app.config.from_object()</code></li> <li>add configs to extensions using <code>ext_obj.init_app(app)</code></li> <li>return app</li> </ul> </li> <li>while this makes config available in controlled way, however, it missing <code>@app.routes()</code> and other decorators associated to <code>@app</code> like error handles. This is handled using <code>Blueprint</code>.</li> <li> <p>import BP file and register it with app using <code>register_blueprint()</code> method. When a blueprint is registered, any view functions, templates, static files, error handlers, etc. are connected to the application.</p> <pre><code>from flask import Flask\nfrom flask_bootstrap import Bootstrap\nfrom flask_sqlalchemy import SQLAlchemy\nfrom config import config\n\nbootstrap = Bootstrap()\ndb = SQLAlchemy()\n\ndef create_app(config_name):\n    app = Flask(__name__)\n    app.config.from_object(config[config_name])\n    config[config_name].init_app(app)\n\n    bootstrap.init_app(app)\n    db.init_app(app)\n\n    # Routes or blueprints\n    from .main import main as main_blueprint\n    app.register_blueprint(main_blueprint)\n\n    return app\n</code></pre> </li> </ul> </li> </ul> </li> </ul> <ul> <li> <p>5 Blueprint - sub pkg</p> <ul> <li>Blueprint is like app having routes but in dormant state until registered with an application which gives it a context.</li> <li>Blueprint can be a single file, or structured as a sub-package having multiple modules and the package constructor creates blueprint.</li> <li>5.1 <code>app/main/__init__.py</code> main bp creation<ul> <li>Blueprint is native flask module</li> <li>create object of <code>Blueprint()</code> class and pass it a name and location.</li> <li> <p>import associated modules</p> <pre><code>from flask import Blueprint\nmain = Blueprint('main', __name__)\n\nfrom . import views, errors \n# last line to avoid circular dependency\n</code></pre> </li> </ul> </li> </ul> <ul> <li>5.4 <code>app/main/views.py</code> view routes<ul> <li>route function name now has namespace with BP name as prefix, so <code>url_for('main.index')</code> should be used so that 'index' of any other BP is not picked.</li> </ul> </li> <li>5.2 <code>app/main/errors.py</code> error handlers  <ul> <li>they respond to only BP route error, for app wide use <code>app_errorhandler</code> decorator instead of <code>errorhandler</code>.</li> </ul> </li> <li>5.3 <code>app/main/forms.py</code> has form objects.</li> </ul> </li> </ul> <ul> <li>4 <code>flasky.py</code> module where app instance is denied<ul> <li><code>create_app()</code> function is called.</li> </ul> </li> </ul> </li> </ul> <ul> <li>Link - http://exploreflask.com/en/latest/blueprints.html</li> </ul>"},{"location":"0-Information-Technology/flask/#testing-in-flask","title":"Testing in Flask","text":"<ul> <li>Why<ul> <li>function code only runs when it is called.</li> <li>if else code is only called when condition is met.</li> <li>ensure code for all branch and function is run by changing scenarios.</li> <li>100% coverage is when you run all functions and code in all if else try catch is tested.</li> <li>do test as you develop.</li> <li><code>pytest</code> or <code>unittest</code> to test</li> <li><code>coverage</code> to measure</li> </ul> </li> </ul> <ul> <li> <p>PyTest</p> <ul> <li>modules and functions both start with <code>test_</code></li> <li>Fixtures are setup functions, that setup how app should behave<ul> <li>You can build different fixtures to have different app instances or to test different interactions like client requests or CLI commands.</li> <li>fixtures call app-factory with test configs to make app separate from dev config.</li> <li><code>conftest.py</code> - sample below.<ul> <li>here fixure creates app, which is then passed to other fixture for specific testing.</li> <li><code>app.test_client()</code> lets make request to app without server. Available in <code>client</code> fixture.</li> <li><code>app.test_cli_runner()</code> lets test CLI commands registered with app. Available in <code>runner</code> fixture.</li> <li>these fixture names (client or runner) are passed in test_functions to use them.</li> </ul> </li> <li> <p>You can keep building fixture on top of other fixture to add predefined functionalities. Eg, on top of client add another class that can help login and logout.</p> <pre><code>@pytest.fixture\ndef app():\n    app = create_app(...)\n    with app.app_context():\n        init_db()\n    yield app\n\n@pytest.fixture\ndef client(app):\n    return app.test_client()\n\n@pytest.fixture\ndef runner(app):\n    return app.test_cli_runner()\n</code></pre> </li> </ul> </li> </ul> <ul> <li>Test Cases<ul> <li>start with <code>test_</code> in both module and function name.</li> <li> <p>use <code>assert</code></p> <pre><code>from app import create_app\n\ndef test_hello(client):\n    response = client.get('/hello')   # sends this URL request\n    assert response.data == b'Hello, World!'\n</code></pre> </li> </ul> </li> </ul> <ul> <li><code>pytest.mark.parametrize</code> lets run the test with different params</li> <li>to test context variables like <code>session</code> or <code>g</code> use <code>with client:</code> Otherwise it raises an error.</li> <li><code>setup.cfg</code> can have extra configs (not mandatory).</li> </ul> <ul> <li>Run - Pytest<ul> <li><code>pytest</code> runs test</li> <li><code>pytest -v</code> runs and shows all files</li> </ul> </li> </ul> </li> </ul> <ul> <li>UnitTest - test small units<ul> <li>use py native <code>import unittest</code></li> <li>in <code>tests/test_basics.py</code><ul> <li>import modules you need for test, <code>create_app</code>, <code>db</code></li> <li>import modules you want to test, <code>User</code>, <code>current_app</code></li> <li>define class <code>class BasicsTestCase(unittest.TestCase):</code><ul> <li>build functions<ul> <li><code>setUp()</code> runs before each test function, builds env for testing</li> <li><code>tearDown()</code> runs after each test function, removes things from env</li> <li><code>test_somecase()</code> these functions run as test.<ul> <li><code>assertTrue</code> Ok if True</li> <li><code>assertFalse</code> Ok if False</li> <li><code>with self.assertRaises(AttributeError):</code> statement that raise error.</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li>tests can be written in separate py files (modules) and the folder <code>tests</code> can have <code>__init__.py</code> as blank to make it a pkg</li> <li><code>python -m unittest</code> discovers and runs all tests.</li> <li>to run specific test class <code>unittest mypkg.tests.test_module.TestClass</code></li> <li>to run specific method <code>unittest mypkg.tests.test_module.TestClass -k test_method</code></li> </ul> </li> </ul> <ul> <li>Unittest vs PyTest<ul> <li>Unittest is universally accepted and is built in Python standard library</li> <li>PyTest has lot of features and we need to write less</li> <li>Unitest needs classes &amp; methods. Pytest only needs methods.d</li> <li>Pytest runner has full support for test cases written in UnitTest clasees.</li> <li>Use both, OOPs of Unittest and better assert of Pytest with and its better error reporting.</li> </ul> </li> </ul> <ul> <li> <p>Unittest and PyTest</p> <ul> <li>You can use Unittest and Pytest togehter to make use of best of both.</li> </ul> <ul> <li>Test Parametrization<ul> <li>when you have same test-code but have to run with different input parameters.</li> <li>Pytest uses non OOPs params, to make Unittest OOPs model work with PyTest add <code>pip install parameterized</code>.</li> <li>then use its decorator and pass params as list of tuples to argument.</li> <li>list is input scenarios</li> <li>Note it runs <code>setUp</code> and <code>tearDown</code> for each param.</li> <li>tuple is variables in each scenario. One value tuple is <code>('name1',)</code></li> <li> <p>Eg, <code>[('name1',32), ('name2',24)]</code>, <code>@parameterized.expand([(n,) for n in range(9)])</code>, <code>@parameterized.expand(itertools.product([True, False], range(9)))</code></p> <pre><code>from parameterized import parameterized\n\n...\n\nclass TestLife(unittest.TestCase):\n    # ...\n\n    @parameterized.expand([('name1',32), ('name2',24)])\n    def test_load(self, name, age): # this method runs the number of items in list.\n        u = User(name, age)\n        assert u.name = name\n</code></pre> </li> </ul> </li> </ul> <ul> <li>Test Exceptions<ul> <li> <p><code>pytest.raises()</code> can be used to test if a certain error is raised on run time.</p> <pre><code>with pytest.raises(RuntimeError):\n    data.load('corrupt_data.txt')\n</code></pre> </li> </ul> </li> </ul> <ul> <li>Mocking<ul> <li>When you have to change return value of a pre defined function. You can mock a function to return a specific value irrespective of what is passed to it without modifying its code.</li> <li>More here on MG's Unit Testing - Mocking</li> </ul> </li> </ul> </li> </ul> <ul> <li> <p>Report - Coverage</p> <ul> <li><code>coverage run -m pytest</code> runs tests and measures coverage</li> <li><code>coverage run -m unittest</code> runs tests using unittest and measures coverage</li> <li><code>coverage report</code> shows coverage report on CLI</li> <li><code>coverage html</code> builds dir for detailed report<ul> <li><code>htmlcov/index.html</code> has detailed report.</li> <li>shows code covered and not covered.</li> </ul> </li> <li>To exempt a code block from coverage, add <code># pragma: no cover</code> after code block. Make this a tough decision to skip code from testing.</li> </ul> <ul> <li>more here</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/flask/#manual-testing","title":"Manual Testing","text":"<ul> <li>Basic testing can be done using flask shell and executing functions <code>flask --app flasky.py shell</code></li> <li>do things similar to as you do in wrinting code, like import module, create objects, call functions etc.</li> <li>use <code>current_app</code> to use <code>app_context</code>, or</li> <li><code>with app.app_context():</code> when using factory</li> <li>What you test in shell should be automated by making test cases.</li> </ul> <p>A practical way of doing manual tests is using interactive coding and running it on python shell. You can write code in editor or shell and keep executing it line by line to see outputs as you go. Once you are happy with the code, you can put them in test cases. Below is an example that shows how to get started.</p> <pre><code>from flask import current_app\nfrom app import create_app, db\n\napp = create_app('default')  # testing\napp_context = app.app_context()\napp_context.push()\n\n# optional\napp.config['WTF_CSRF_ENABLED'] = False  # no CSRF during tests\nclient = app.test_client()\n\n# now you can do your work\nimport app.db_conn as db_conn\nimport app.sql_snippets as sql_snippets\n\n\nbook = db.session.get(Book, 5)\nfor attr, value in book.__dict__.items():\n    print(f\"{attr}: {value}\")\n</code></pre> <p>The code above builds basic app working with the configs and imports. It makes <code>client</code> available which can be used to interact with the flask routes. It also makes modules available which can be used for functionalities. More on how to use it below.</p> <p>For non factory app, use below code to do database operations:</p> <pre><code>from app import app\napp_context = app.app_context()\napp_context.push()\n\nfrom app import db\nfrom app.models import Book, Author\ndb.session.execute(db.select(Book, Author))\n</code></pre>"},{"location":"0-Information-Technology/flask/#writing-tests","title":"Writing Tests","text":"<p>Folder Structure The below tree shows how to organise test package and modules.</p> <pre><code>\u251c\u2500\u2500 requirements.txt          # on this level, project requirements\n\u251c\u2500\u2500 tests                     # root folder for tests, this is outside app\n\u2502   \u251c\u2500\u2500 conftest.py           # define fixtures (only in pytest)\n\u2502   \u251c\u2500\u2500 assets                # keep files here to check file upload\n\u2502   \u251c\u2500\u2500 functional            # tests functionality, usually routes\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 test_books.py\n\u2502   \u2502   \u2514\u2500\u2500 test_users.py\n\u2502   \u2514\u2500\u2500 unit                  # tests units, usually models\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500 test_models.py\n\u2514\u2500\u2500 venv\n</code></pre> <p>Test Case Execution Sequence</p> <p>Test cases are executed in alphabetical order. But they can be lying as function in different files (modules) and in different folders (packages). So the unittest module does a scan of the repo to find all functions based on search specification.</p> <p>Following are the steps of execution:</p> <ol> <li>find all files having <code>test_*</code></li> <li>Execute <code>base_test_case.py</code></li> <li>Find all function signatures having <code>test_somefunc():</code></li> <li>Start with the the function signature that is alphabetically first in alphabetically first Test Class. Executes <code>setUp</code> then <code>test_func</code> then <code>tearDown</code></li> <li>Repeat for all functions.</li> </ol> <p>Note: Any code in any module before <code>Class</code> or in Class outside function is executed before any test case.</p> <p>Philosophy - Use GIVEN.. WHEN.. THEN.. ideology when writing test cases. So your function for test can start as, in <code>tests/unit/test_models.py</code>:</p> <pre><code>from project.models import User\n\ndef test_new_user():\n    \"\"\"\n    GIVEN a User model\n    WHEN a new User is created\n    THEN check the email, hashed_password, and role fields are defined correctly\n    \"\"\"\n    user = User('johndoe@gmail.com', 'j0hnD0e')\n    assert user.email == 'johndoe@gmail.com'\n    assert user.hashed_password != 'j0hnD0e'\n    assert user.role == 'user'\n</code></pre> <p>A common practice is to use the GIVEN-WHEN-THEN structure:</p> <ul> <li>GIVEN - what are the initial conditions for the test?</li> <li>WHEN - what is occurring that needs to be tested?</li> <li>THEN - what is the expected response?</li> </ul> <p>Testing Web App</p> <p>Web app testing needs <code>client</code> and client needs <code>app_context</code>. They are used in test cases. So as a minimum, you need three function in a <code>Class</code> to get started. So in <code>tests/functional/test_basic.py</code> add:</p> <pre><code>import unittest\nfrom flask import current_app\nfrom app import create_app, db\nimport app.db_conn as db_conn\n\nclass BasicTestCase(unittest.TestCase):\n\n    def setUp(self):\n        self.app = create_app('testing')              # testing\n        self.app.config['WTF_CSRF_ENABLED'] = False   # no CSRF during tests\n        self.app_context = self.app.app_context()\n        self.app_context.push()\n        db_conn.init_db()                             # destroys and builds database\n        self.client = self.app.test_client()\n\n    def tearDown(self):\n        self.app_context.pop()\n        self.app = None\n        self.app_context = None\n        self.client = None\n\n    def test_app_exists(self):\n        \"\"\"Tests the hello from __init__.py\"\"\"\n        assert self.app is not None\n        assert current_app == self.app\n        response = self.client.get('/hello')\n        html = response.get_data(as_text=True)\n        assert 'Hello, World!' in html\n</code></pre> <p>You can add more functions to this calls. Each of theses functions will be new test-cases and the <code>setUp</code> and <code>tearDown</code> methods will run before and after each of them.</p> <p>Flow: You can write a SQL query, get the results in variable. Use the variables to send request to client. Very response in HTML with other variables from SQL query. Eg, query user details and get <code>{id: 1, name: 'John'}</code>. The request <code>user/1</code> and <code>assert f'{name}' in html</code>. Basically, values in DB should match in HTML and values you POST as HTTP request should appear in DB. Access restrictions on routes should work as expected. Redirects should work as expected. User is able to do all actions required.</p> <p>Imp: Please note, <code>db_conn.init_db()</code> this will run for all <code>test_*()</code> methods in the class. Do this only if required.</p> <p>Tip:: Write what you expect users to do and don't. Then code test cases. You can use <code>print</code> in test cases to debug steps. You can use manual-interactive-testing to help write test cases.</p> <p>Note: Tests are executed in alphabetical order within a class. So ensure you do, <code>test_a_upload()</code> then <code>test_b_check()</code></p> <p>Test File Upload</p> <p>You can test a page where user upload a file, and then match that result back with database update. In <code>tests/functional/test_admin.py</code></p> <pre><code># ... prev imports\n\nfrom werkzeug.datastructures import FileStorage\n\nclass AdminTestCase(unittest.TestCase):\n\n    # ... set up and teardown\n\n    def test_admin_upload(self):\n        \"\"\"admin can upload csv\"\"\"\n        self.login()\n\n        # upload file\n        csv_file = FileStorage(\n            stream=open(r\"tests/assets/sample.csv\", \"rb\"),\n            filename=\"sample.csv\",\n            content_type=\"text/csv\"\n        )\n        response = self.client.post(\n            \"admin/upload\",\n            data={\n                \"csv_file\": csv_file,\n            },\n            content_type=\"multipart/form-data\",\n            follow_redirects = True\n        )\n        assert response.status_code == 200\n        html = response.get_data(as_text=True)\n        assert 'Load successful!' in html\n\n        # match flow count\n        sql = f\"\"\"\n            select count(*) as flows\n            from {self.app.config.get(\"DATABASE\")}.{self.app.config.get(\"SCHEMA\")}.[my_table]\n            \"\"\"\n        res = app.db_conn.get_db().execute(sql)\n        rows = res.fetchone()\n        rows_count = rows[0]\n        assert f'{rows_count} records loaded' in html\n</code></pre> <p>Test Code Structure</p> <ul> <li>create a test module (file) of same name as test subject with test_ prefixed. Eg, <code>test_foo.py</code></li> <li>import <code>unittest</code> and other required packages</li> <li>create classes and methods for testing.</li> <li> <p>in test_method</p> <ul> <li>call code from your app, set varibles or directly put code in assert</li> <li><code>assert some-code</code> some-code can be anything that evaluates to True.</li> </ul> <pre><code>import unittest\nfrom app import User, Engine\n\nclass TestUserAdd(unittest.TestCase):\n    def test_works(self):\n        u = User()\n        assert u.exists() # anything that evalueates to True\n\nclass TestEngineWork(unittest.TestCase):\n    pass\n</code></pre> </li> </ul> <ul> <li> <p>For a Flask App</p> <ul> <li><code>setUp</code> and <code>tearDown</code> methods are special that automatically invoked before and after each test case. This makes every test case run on clean slate. You can have different one in each class, or make a base class and import it in other classes.</li> <li>request functions<ul> <li><code>response = app.client.get('/', follow_redirects=True)</code> - use response same as you do in flask app</li> <li><code>response = self.client.post('/auth/register', data={some_json}, follow_redirects=True)</code> - submit a form this way</li> </ul> </li> </ul> <ul> <li>response methods<ul> <li><code>html = response.get_data(as_text=True)</code></li> <li><code>assert response.status_code == 200</code></li> <li><code>assert response.request.path == '/auth/login' # redirected to login</code></li> <li><code>response.json['token']</code></li> </ul> </li> </ul> <pre><code># tests/test_base.py\nimport unittest\nfrom flask import current_app\nfrom app import create_app, db\n\nclass TestWebApp(unittest.TestCase):\n    def setUp(self):\n        self.app = create_app()\n        self.appctx = self.app.app_context()\n        self.appctx.push()\n        db.do_something() # you can call any method as well here\n        self.do_something() # method in this class that has code to be executed before each test, like register\n        self.client = self.app.test_client()\n\n    def tearDown(self):\n        db.drop_something() # again execute anything at end of test case\n        self.appctx.pop()\n        self.app = None\n        self.appctx = None\n        self.client = None\n\n    def test_app(self):\n        assert self.app is not None\n        assert current_app == self.app\n\n    def test_home_page_redirect(self):\n        response = self.client.get('/', follow_redirects=True)\n        slef.do_login() # funciton in this class that logs in to the app can be reused\n        assert response.status_code == 200\n        assert response.request.path == '/auth/login'\n</code></pre> </li> </ul> <ul> <li>Do s<ul> <li> <p>If a piece of code is difficult to test with test case, consider refactoring it. Eg, some code that is not in funciton and just prints as part of execution can't be called from test case, so make a function for it. Alos, you can wrap any global code in a function and call it. This also makes code only direct executable when file is ran, import doesn't execute it.</p> <pre><code>def main():\n    all_your_global_code()\n\nif __name__ == '__main__':\n    main()\n</code></pre> </li> </ul> </li> </ul> <ul> <li>Links<ul> <li>MG' Flask Web App Testing</li> <li>MS's Testing code for Life Game</li> <li>Stackoverflow why to use PyTest</li> <li>Flask Docs - Pure Pytest</li> <li>RealPython - Flask testing has a different package, so differs</li> <li>Codethechange Stanford - Guides Guide_flask_unit_testing</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/flask/#error-handling","title":"Error Handling","text":"<ul> <li>Try.. Except..<ul> <li>You can use <code>try except finally</code> block to handle errors that you think might occur. With requests, it is best to handle errors at last step , that is before making the response, because, if at any previous step an error has occured it will bubble up. In another scenario, use <code>try.. except</code> at the step where you have another option to do in case of error. Eg, handle error in view when you make a db call that is last function before returning response. Do not handle it in model or db connections unless you have another database to fall over to or another table to ping.</li> </ul> </li> </ul> <ul> <li>Email / Log error<ul> <li>Error can be emailed to admin automatically.</li> </ul> </li> </ul> <ul> <li>Error Templates<ul> <li>You can have templates for exceptions and errors so they don't go out to end users.</li> <li> <p>These templates only work, when <code>FLASK_ENV=production</code> and <code>FLASK_DEBUG=0</code> in your environment.</p> <pre><code># blueprint handler\n@bp.app_errorhandler(404)\ndef internal_error(error):\n    return render_template('errors/404.html'), 404\n\n# app handler\n@app.errorhandler(500)\ndef internal_error(error):\n    return render_template('errors/500.html'), 500\n</code></pre> </li> </ul> </li> </ul> <ul> <li>Links<ul> <li>RealPython Flask App Part III</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/flask/#log-in-flask","title":"Log in Flask","text":"<ul> <li>A standard Python logging component <code>logger</code> is available on the Flask object at <code>app.logger</code>.</li> <li>404 is autologged by server, so skip.</li> <li>request logs are autologged by proxy server, so skip.</li> <li>Exceptions, 400 and 500 can be logged to look back in time.</li> </ul> <ul> <li>Setup<ul> <li>configure logger as early as possible</li> <li>add path and log configs in <code>config.py</code> even before app is created. You can add logging config to any env config class. They are all called before app is created.</li> </ul> </li> </ul> <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n\napp = Flask(__name__)\napp.logger.info('some log')\n\n# prj/config.py\nimport logging\n\nclass Config:\n    logging.basicConfig(level=logging.DEBUG, format='[%(asctime)s] - %(name)s - %(levelname)s - %(module)s -: %(message)s')\n\n# prj/app/models.py\nfrom flask import current_app\ncurrent_app.logger.debug('Some message')\n</code></pre> <ul> <li> <p>Explore here</p> <pre><code>log = logging.getLogger(__name__)\n\ndef init_log():\n    logging.basicConfig(level=logging.DEBUG)\n    log.info(\"Logging enabled\")\n    # Set the log level for werkzeug to WARNING because it will print out too much info otherwise\n    logging.getLogger(\"werkzeug\").setLevel(logging.WARNING)\n</code></pre> </li> </ul> <p>Links</p> <ul> <li>FlaskDocs - Config Logger</li> <li>MG - Logging to a File Legacy</li> <li>RealPython - Flask Part III - Logging. Eg, shows to Log Errors, keep 7 days history.</li> </ul>"},{"location":"0-Information-Technology/flask/#cli-in-flask","title":"CLI in Flask","text":"<p>how to build CLI commands in flask</p> <p>Test - add a CLI command to run tests</p> <p>You can build a CLI command to run  tests automatically. Add following code to files where you build app, usually <code>app/__init__.py</code></p> CLI command for running tests<pre><code>@app.cli.command()\ndef test():\n    \"\"\"Run the unit tests.\"\"\"       # help msg on cli\n    import unittest\n    tests = unittest.TestLoader().discover('tests')\n    unittest.TextTestRunner(verbosity=2).run(tests)\n</code></pre> <p>To run this CLI command do following and it will run all the rest cases.</p> <pre><code>flask --app flasky.py test\n</code></pre>"},{"location":"0-Information-Technology/flask/#jupyter-notebook-with-flask","title":"Jupyter Notebook with Flask","text":"<p>You can use Jupyter notebook with your flask app for rapid development and testing. Benefits:</p> <ul> <li>use your flask modules in notebook like models, db-connections etc.</li> <li>quickly develop the logic required in model / service layer.</li> </ul> <p>Ensure correct path</p> <pre><code>import os\n\n# make cwd same as where app is\nos.chdir(os.path.join(os.getcwd(),'..'))\n</code></pre> <p>Lets say you are in <code>./dev/my.ipynb</code> and your flask app is in <code>./app/__init__.py</code>, then your notebook execution should be in <code>.</code> that is folder that has <code>app</code> so that notebook can import the <code>app</code> as a module.</p> <p>Import app and build context:</p> <pre><code>from app import create_app\n\napp = create_app()\napp_context = app.app_context()\napp_context.push()\n</code></pre> <p>Import Flask App modules you want to work on</p> <pre><code>import json\nimport pandas as pd\n\nimport app.config as config\nfrom app.bp.user.model import Users\nfrom app.utils.db_conn import get_sqlite_db, get_postgres_db\n\nfrom app.bp.postapi.services import get_posts, set_posts\n</code></pre>"},{"location":"0-Information-Technology/flask/#security-flaws-checks","title":"Security Flaws Checks","text":"<p>You can use <code>bandit</code> package to check security flaws in app. It scans code and lets you know any possible security flaw.</p> <p>Install and Run</p> <pre><code># install the package\npython -m pip install bandit\n\n# run on the app module\nbandit -r app\n</code></pre> <p>The second command above, runs the checks on the whole app and lists the issues by severity and confidance.</p> <p>Links</p> <ul> <li>Realpython - Python Testing</li> </ul>"},{"location":"0-Information-Technology/flask/#make-the-project-installable","title":"Make the Project Installable","text":"<ul> <li>Makes the project distributable like a library, so people can do <code>pip install</code> and use it.</li> <li>Deploying is same as installing any other library. like you deploy <code>mkdocs</code> by installing it.</li> <li> <p><code>setup.py</code> outside <code>app</code> is where we can define this.</p> <pre><code>from setuptools import find_packages, setup\n\nsetup(\n    name='your-project-name',\n    version='1.0.0',\n    packages=find_packages(),\n    include_package_data=True,\n    install_requires=[\n        'flask',\n    ],\n)\n</code></pre> </li> </ul> <ul> <li>also add <code>MAINFEST.in</code> to tell what other files to include in package. Eg, <code>some.sql</code> <code>static</code> or any other.</li> <li>VY Stem - Packaging</li> <li>more here</li> <li>RealPython Flask App part III</li> </ul>"},{"location":"0-Information-Technology/flask/#deployment-fundamentals","title":"Deployment Fundamentals","text":"<ul> <li>WSGI or \"Web Server Gateway Interface\"<ul> <li>is a protocol (calling convention) to forward requests from a web server (Apache or NGINX) to a backend Python web application or framework. Python then builds response which is passed back to the webserver which shares it to the requestor.</li> <li>it sits between Web Server and Python App. <code>Client -&gt; Webserver -&gt; WSGI -&gt; Python</code></li> <li>WSGI containers are Gunicorn, uWSGI. They invoke python callable object, such as a route in flask.</li> <li>WSGI container is required to be installed in the project so that a web server can communicate to a WSGI container which further communicates to the Python application and provides the response back accordingly.</li> </ul> </li> </ul> <ul> <li>Development Web Server<ul> <li>most frameworks come with development web server which serves requests. but this needs to be replaced on PROD.</li> </ul> </li> </ul> <ul> <li>Links<ul> <li>What is WSGI</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/flask/#deployment-windows-iis-server","title":"Deployment - Windows IIS Server","text":"<ul> <li><code>HTTP -&gt; IIS -&gt; ISAPI -&gt; FastCGI -&gt; WSGI -&gt; (Python Flask Application)</code></li> </ul> <ul> <li>IIS site will trigger a <code>wfastcgi.py</code> script and will use <code>web.config</code> file which calls module that has flask app (or creates using app factory).</li> <li>WFastCGI is a Py package, and <code>wfastcgi.py</code> provides a bridge between IIS and Python using WSGI and FastCGI. It is same as mod_python is for Apache.</li> <li>you can impersonate as a user in appPoolIdentity</li> <li>you can disable anonymous auth and can keep only windows authenticated access to get remote user in request.</li> </ul> <ul> <li>Links<ul> <li>Detailed deployment process on Medium</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/flask/#deployment-pythonanywhere-flask","title":"Deployment - PythonAnywhere Flask","text":"<ul> <li> <p>WSGI configuration</p> <ul> <li>On your Web App Configuration page, open \"WSGI configuration file\", and ensure you add your project folder to code below.</li> </ul> <pre><code>import sys\n\n# add your project directory to the sys.path\nproject_home = u'/home/username/mysite'\nif project_home not in sys.path:\n    sys.path = [project_home] + sys.path\n\n# import flask app but need to call it \"application\" for WSGI to work\nfrom flask_app import app as application  # noqa\n</code></pre> </li> </ul> <ul> <li>more here</li> </ul>"},{"location":"0-Information-Technology/flask/#static-site-with-flask-frozen","title":"Static Site with Flask-Frozen","text":"<ul> <li>Static site can be generated and hosted on Netlify or GitHub pages.</li> </ul> <ul> <li>link<ul> <li>td.io - Frozen</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/flask/#access-localhost-flask-app-on-network","title":"Access localhost flask app on Network","text":"<ul> <li>Suppose on an Ubuntu VM a flask app is running on localhost and you want to access it from you host machine that is Mac.</li> </ul> <ul> <li>Run flask app with <code>app.run(host='0.0.0.0', debug=True)</code></li> <li>This tells your operating system to listen on all public IPs.</li> <li>then access <code>192.168.10.33:5000</code> from host machine.</li> </ul> <p>Now that our app is running we can add a database to this app. We will use FlaskSQLAlchemy package for this. Or Pandas.</p>"},{"location":"0-Information-Technology/flask/#machine-learning-pandas-app-in-flask","title":"Machine Learning Pandas App in Flask","text":"<p>You can use Flask it with Pandas, Matplot and other ML libraries to make it easily usable for end users.</p> <ul> <li>Import all your libs in flask app that you have used in Jupyter NB.</li> <li>Add code and functions to read data and perform tasks.</li> <li>Flask routes are executed for each request, so keep data reads outside to read them once.</li> <li><code>return render_template( 'search.html', data=df_result.to_html(classes='table table-striped table-hover')</code> - to_html makes html table that can be passed to html page.</li> <li><code>{{ data|safe }}</code> - safe makes it as markup and browser renders it.</li> </ul> <p>Reference:</p> <ul> <li>https://sarahleejane.github.io/learning/python/2015/08/09/simple-tables-in-webapps-using-flask-and-pandas-with-python.html</li> </ul>"},{"location":"0-Information-Technology/flask/#charts-graphs-visualization-in-flask","title":"Charts Graphs Visualization in Flask","text":"<ul> <li>Requirements<ul> <li>HTML5 instead of PNG</li> <li>dynamic, shows info on hover</li> <li>interactive, filters modify charts</li> <li>dashboard, filters update multiple charts</li> <li>streaming dataset</li> <li>animation</li> </ul> </li> </ul> <ul> <li>Plotly<ul> <li>dynamic &amp; interactive charts</li> <li>handle data and build-chart in view function, then send JSON to template, use JSON in JS.</li> </ul> </li> </ul> <ul> <li>Plotly Express<ul> <li><code>import plotly.express as px</code></li> <li><code>fig = px.bar()</code> lets build bar</li> <li><code>fig.show()</code> makes PNG</li> </ul> </li> </ul> <ul> <li>Plotly Graph Objects<ul> <li><code>import plotly.graph_objects as go</code> builds figure objects.</li> <li>has class for object like <code>go.Bar()</code></li> <li>objects need to be added to figure <code>fig = px.Figure()</code></li> </ul> </li> </ul> <ul> <li>Bokeh<ul> <li>beautiful charts, simple plot to complex dashboard with streaming dataset!</li> <li>dynamic &amp; interactive</li> <li><code>bokeh.plotting</code> has plotting functions</li> <li><code>bokeh.models</code> has data handling functions</li> <li><code>bokeh.embed</code> has component that return HTML with JS ready to embedd, when python <code>fig</code> is passed.</li> </ul> </li> </ul> <ul> <li>Dash<ul> <li>React front-end - yes</li> <li>dashboard - yes</li> <li>HTML in Python - NOooo</li> <li><code>from dash import Dash, html, dcc</code><ul> <li>Dash is app</li> <li>html lets build html components <code>Div()</code> H1</li> <li>dcc is Dash Core Components - lets build <code>Graph() Dropdown()</code>, graph has figues, from px.</li> </ul> </li> </ul> </li> </ul> <ul> <li>Flask, Plotly &amp; AJAX<ul> <li>Flask app and html template</li> <li>use <code>json.dumps()</code> to get fig JSON and send to template</li> <li>use list of <code>chartJSON[]</code> for sending multiple charts</li> <li>template can use plotly js to plot chart with the json.</li> <li>js <code>Plotly.plot('chart',graphs,{});</code> where <code>chart</code> is <code>id</code> of <code>div</code></li> <li>to extend, send graphJSON, header, description</li> <li>AJAX<ul> <li><code>onchange=cb(this.value)</code> to invoke callback function, that passes value to python and python returns updated chartJSON</li> </ul> </li> </ul> </li> </ul> <ul> <li>Data Handling<ul> <li>mostly libraries use list, which has series from DataFrame</li> <li>px takes in dataframe as <code>data</code></li> </ul> </li> </ul> <ul> <li>Altair<ul> <li>py library</li> </ul> </li> </ul> <ul> <li>Chart.js<ul> <li>No Python wrapper</li> <li>is dynamic</li> </ul> </li> </ul> <ul> <li>Highchart, google charts, d3.js ?</li> </ul> <ul> <li>anychart<ul> <li>can be drawn from JSON, JSON needs to build without python wrapper</li> </ul> </li> </ul> <ul> <li>Matplot, ggplot<ul> <li>static chart, save fig as png, return file from view_function</li> </ul> </li> </ul> <ul> <li>Links<ul> <li>TDS - Web Visualization with Plotly and Flask</li> <li>TSD - Flask plotly AJAX</li> <li>Dash offical</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/flask/#restapi-in-flask","title":"RestAPI in Flask","text":"<p>What is REST?</p> <ul> <li>Client and Server are separate</li> <li>Stateless: No information from a request is stored on client to be used in other requests, eg, no session can be started, if authentication is required, username and password need to be sent with every request.</li> </ul> <p>What is RESTful API:</p> <ul> <li>There is a resource, eg, tasks</li> <li>It has endpoints for CRUD operations</li> <li>HTTP methods, GET PUT POST DELETE, are used for these operations.</li> <li>Data is provided with these requests in no particular format but usually as:<ul> <li>JSON blob in request body, or</li> <li>Query String arguments as portion of URL.</li> </ul> </li> </ul> HTTP Method URI Action GET http://hostname/todo/api/v1.0/tasks Retrieve list of tasks GET http://hostname/todo/api/v1.0/tasks/[task_id] Retrieve a task POST http://hostname/todo/api/v1.0/tasks Create a new task PUT http://hostname/todo/api/v1.0/tasks/[task_id] Update an existing task DELETE http://hostname/todo/api/v1.0/tasks/[task_id] Delete a task <p>Data of a task can be, JSON blob, as:</p> <pre><code>{\n  'id': 1,\n  'title': 'Title of a to do task',\n  'description': 'Description of to do task', \n  'done': False\n}\n</code></pre> <ul> <li>This API can be consumed by client side app which can be single page HTML.</li> <li>Note, JSON object is defined in python as dict, <code>jonify</code> converts and send as JSON Object.</li> </ul>"},{"location":"0-Information-Technology/flask/#api-architecture","title":"API Architecture","text":"<p>API can have following modules:</p> <ul> <li><code>views.py</code> - handle requests and call required service, validates request params</li> <li><code>service.py</code> - takes request param, calls database model, to do db crud and data processing, business logic implementation</li> <li><code>model.py</code> - data orm and data structures</li> <li><code>utils/api.py</code> - handles api key management</li> <li><code>utils/db.py</code> - handles DB connections</li> <li>JSON in and JSON out.</li> </ul> <p>Use <code>DataClasses</code> for API data structure and JSON conversions.</p>"},{"location":"0-Information-Technology/flask/#jwt-authentication-in-flask","title":"JWT Authentication in Flask","text":"<ul> <li>JWT Authentication<ul> <li><code>jwt</code> python library is used to make a <code>token</code> that can be send in every request instead of sending username and password.</li> <li>Token is encoded string which has a valid time and it expires after that time.</li> <li><code>ExpiredSignatureError</code> is raised if you <code>decode</code> and expired token string.</li> <li> how to add remember me.</li> </ul> </li> </ul> <ul> <li>how to register?<ul> <li>comment <code>token_authentication</code> for <code>create_user</code></li> <li><code>curl -i -X POST -H \"Content-Type: application/json\" -d '{\"username\":\"admin\",\"password\":\"admin\"}' http://127.0.0.1:5000/admin</code></li> </ul> </li> </ul> <ul> <li>CURL Requests<ul> <li>Send Username and password to get token<ul> <li><code>curl -u username:password -i -X GET http://127.0.0.1:5000/login</code> returns token and duration</li> </ul> </li> <li>Send token in header to access protected resources<ul> <li><code>curl -H \"x-access-token: token\" -i -X GET http://127.0.0.1:5000/users</code></li> <li><code>curl -H \"x-access-token: token\" -i -X GET http://127.0.0.1:5000/users/9d8c738b-3a39-482d-8a17-0c1b755f9a23</code></li> <li><code>curl -H \"x-access-token: token\" -i -X GET http://127.0.0.1:5000/api/v1.0/tasks</code></li> <li><code>curl -H \"x-access-token: token\" -i -X GET http://127.0.0.1:5000/api/v1.0/tasks/19</code></li> </ul> </li> <li> will this be more secure and beneficial?<ul> <li><code>curl -u username_or_token:password_or_unused -i -X GET http://127.0.0.1:5000/users</code></li> </ul> </li> </ul> </li> </ul>"},{"location":"0-Information-Technology/flask/#flask-snippets","title":"Flask Snippets","text":"<p>URL Redirect to previous</p> <pre><code>def redirect_url(default='index'):\n    return request.args.get('next') or \\\n           request.referrer or \\\n           url_for(default)\n\n# Use it in in the view\ndef some_view():\n    # some action\n    return redirect(redirect_url())\n</code></pre>"},{"location":"0-Information-Technology/flask/#app-restful-api-in-flask","title":"App - RESTful API in Flask","text":"<p>to be added</p>"},{"location":"0-Information-Technology/flask/#serving-over-https","title":"Serving over HTTPS","text":"<ul> <li>generate certificate key using <code>openssl req -x509 -newkey rsa:4096 -nodes -out cert.pem -keyout key.pem -days 365</code></li> <li>allow insecure connection to localhost in chrome, paste <code>chrome://flags/#allow-insecure-localhost</code></li> <li>https://blog.miguelgrinberg.com/post/running-your-flask-application-over-https</li> </ul>"},{"location":"0-Information-Technology/flask/#app-social-blogging-app-in-flask","title":"App - Social Blogging App in Flask","text":"<ul> <li> <p>User Authentication</p> <ul> <li>Password hashing<ul> <li>extensions - <code>from werkzeug.security import generate_password_hash, check_password_hash</code> this is tried and tested lib that is safe to use.</li> <li>model - implement <code>password</code> as write-only property of <code>User</code> class to set <code>password_hash</code></li> </ul> </li> </ul> <ul> <li>Blueprint - structure it as a sub-module <code>auth</code> Blueprint. It has view having login-route</li> </ul> <ul> <li> <p>Normal auth</p> <ul> <li>Login - <code>session['user_id'] = user['id']</code> once credentials are verified, save <code>user_id</code> in <code>session</code> dictionary, it makes it available across requests for the session.<ul> <li> <p><code>g.User</code> can hold user object. Using Blueprints decorator <code>@bp.before_app_request</code> register a function that sets <code>g.user</code>. If there is no <code>user_id</code> in session, <code>g.User</code> will be <code>None</code></p> <pre><code>@bp.before_app_request\ndef load_logged_in_user():\n    user_id = session.get('user_id')\n\n    if user_id is None:\n        g.user = None\n    else:\n        g.user = get_db().execute(\n        'SELECT * FROM user WHERE id = ?', (user_id,)\n        ).fetchone()\n</code></pre> </li> </ul> </li> </ul> <ul> <li>Logout - <code>session.clear()</code></li> </ul> <ul> <li> <p>Login Required Decorator - lets you use <code>@login_required</code> on view function so the following code is run before view-route code.</p> <pre><code>def login_required(view):\n    @functools.wraps(view)\n    def wrapped_view(**kwargs):\n        if g.user is None:\n            return redirect(url_for('auth.login'))\n\n        return view(**kwargs)\n\n    return wrapped_view\n</code></pre> </li> </ul> </li> </ul> <ul> <li> <p>Flask-login is ext having functions and decorators that make authentication easy.</p> <ul> <li>model - few required class members can either be declared in <code>User</code> class or can be importe from <code>UserMixin</code> class of Flask-login.</li> <li> <p>initialize and instantiate extension with required conf</p> <pre><code>from flask_login import LoginManager\n\nlogin_manager = LoginManager()\nlogin_manager.login_view = 'auth.login'\n\ndef create_app(config_name):\n  # ...\n  login_manager.init_app(app)\n  # ...\n</code></pre> </li> </ul> <ul> <li> <p>model implement user_loader in <code>User</code> class</p> <pre><code>from . import login_manager\n\n@login_manager.user_loader\ndef load_user(user_id):\n  return User.query.get(int(user_id))\n</code></pre> </li> </ul> <ul> <li><code>login_required</code> decorator lets protect route.</li> <li>Flask-Login\u2019s <code>login_user()</code> logs user in once verified. It setts user session.</li> <li><code>logout_user()</code> logs user out.</li> </ul> </li> </ul> <ul> <li>Register User<ul> <li>build a form class in new <code>auth/forms.py</code>, add unique email and username validator using <code>validate_</code> function</li> <li>build a template that uses form <code>templates/auth/register.html</code></li> <li>build a register route in <code>auth/views.py</code><ul> <li>get - render form</li> <li>post - validate and add user to db</li> </ul> </li> </ul> </li> </ul> <ul> <li>account confirmations<ul> <li>use expiry token to validate email url.</li> <li>model - add token generation and validation function.</li> <li>view - send email on registration</li> <li>view - <code>@auth.route('/confirm/&lt;token&gt;')</code></li> </ul> </li> </ul> <ul> <li>Links<ul> <li>RealPython Flask-Login</li> </ul> </li> </ul> </li> </ul> <ul> <li>Roles and Permissions<ul> <li>database implementation<ul> <li>add <code>role</code> table</li> <li>add <code>permission</code> column to role table as integer<ul> <li>multiple permission can be binary numbers, 1,2,4,8,16</li> <li>add them and subtract them to get unique number as total permission of user. 2+4=6</li> <li>do bit wise and operation to match permission. 6&amp;2=2, 6&amp;4=4</li> </ul> </li> </ul> </li> <li>add decorator function to make it easy to protect route to access only if permission is checked.</li> </ul> </li> </ul> <ul> <li>User Profiles</li> </ul>"},{"location":"0-Information-Technology/flask/#app-mega-tutorial-by-mg","title":"App - Mega Tutorial by MG","text":"<p>This is understanding of a tutorial by Miguel Grinberg, we are learning to create a micro-blogging site using flask and other dependencies.</p> <ul> <li> UserMixin?</li> <li> Why we pass Classes as param to Class?</li> </ul>"},{"location":"0-Information-Technology/flask/#elastic-search-in-flask","title":"Elastic Search in Flask","text":"<ul> <li>You can install elastic search by <code>brew install elasticsearch</code> on mac.</li> <li>Access <code>http://localhost:9200</code> to view service JSON output.</li> <li>Also, install in python <code>pip install elasticsearch</code></li> <li>To have launched start elasticsearch now and restart at login: <code>brew services start elasticsearch</code></li> <li>Or, if you don't want/need a background service you can just run: <code>elasticsearch</code></li> </ul>"},{"location":"0-Information-Technology/flask/#advanced-flask","title":"Advanced Flask","text":"<ul> <li> <p>Request Processing</p> <ul> <li> <p>Following diagram show the cycle of request-response</p> <pre><code>graph LR\nRequest --&gt;|GET\\n1| Web_Server\nWeb_Server --&gt; WSGI_Server\nWSGI_Server --&gt;|Spawns\\n2| Flask_App\nFlask_App --&gt;|Pushes\\n3| Application_Context\nFlask_App --&gt;|Pushes| Request_Context\nApplication_Context --&gt;|current_app\\n4| View_Function\nRequest_Context --&gt;|request| View_Function\nsubgraph Global\nApplication_Context\nRequest_Context\nend\nsubgraph Worker\nFlask_App --&gt; View_Function\nend\nView_Function --&gt;|5| Response</code></pre> </li> </ul> <ul> <li>Step 1 - Handle request<ul> <li>Web_server - Apache &amp; NginX</li> <li>WSGI_Server - Gunicorn, uWSGI, mod_wsgi</li> </ul> </li> <li>Step 2 - Spawn a Worker<ul> <li>it can be a thread, process or coroutine.</li> <li>one worker handles one request at a time.</li> <li>hence for multiple concurrent request multiple worker can be spawned.</li> </ul> </li> <li>Step 3 - pushes context<ul> <li>worker pushes context to global-stack as <code>context-local</code> which uses <code>thread-local</code> data, which means, a worker, which is one thread, has data specific to a thread and can be only accessed by worker that created it. So its global-memory but worker-unique data as <code>global LocalStack()</code>.</li> </ul> </li> <li>Step 4 - local context, proxy<ul> <li>this context data is basically an object, stored as stack data structure. To make it available in view-function it is not passed as a parameter, neither is imported as a global object, rather it is made available using proxy</li> </ul> </li> <li>Step 5 - Clean Up<ul> <li>req and app context are removed from stack</li> </ul> </li> </ul> </li> </ul> <ul> <li>Threading</li> </ul> <ul> <li>Link<ul> <li>td.io - Request processing</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/flask/#links","title":"Links","text":"<ul> <li>Official Flask Blogging Tutorial - Flaskr<ul> <li>app factory, blueprint</li> <li>Plane DB operations, no ORM SQLAlchemy</li> </ul> </li> <li>RealPython Flask Tutorial - Part I</li> <li>Flask Mega Tutorial I - Miguel Grinberg</li> <li>Flask Mega Tutorial I Legacy - Miguel Grinberg<ul> <li>no blueprint.</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/flutter/","title":"Flutter","text":"<p>Google Flutter is a framework for developing applications for iOS, Android and Web with same code base. It uses Dart as programming language. The dart code gets compiled to java/javascript/swift for native performance. It has rich open source libraries to plug in.</p> <p>Everything is a widget here :)</p>"},{"location":"0-Information-Technology/flutter/#quickstart","title":"Quickstart","text":"<ul> <li><code>flutter create --org com.codeo myapp</code> create a basic app</li> <li><code>flutter pub get</code> gets all packages</li> <li><code>flutter run -d Chrome</code> runs flutter app on device chrome</li> <li><code>pod setup</code> makes ios deploy faster</li> <li><code>flutter build web/apk</code> builds app for publishing</li> </ul>"},{"location":"0-Information-Technology/flutter/#structure","title":"Structure","text":"<ul> <li>assets - hold images/fonts</li> <li>lib - all the dart code<ul> <li>we can make folders specific for activity</li> <li>models - hold class for data in the app like users.dart, deserialize json</li> <li>pages - hold screens, each .php page or route</li> <li>widgets - component on pages, like progress bar</li> </ul> </li> <li>all folders are packages, eg, lib package, src package, models package.</li> </ul>"},{"location":"0-Information-Technology/flutter/#basic-workflow","title":"Basic Workflow","text":"<ul> <li>make product class, init constructor, add factory, the get and set functions</li> <li>link this to sqflite<ul> <li>have a db_provider</li> <li>and an init function</li> <li>crud functions</li> </ul> </li> <li>or firebase<ul> <li>get reference of a collection in instance.</li> <li>from ref get the snapshots and build.</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/flutter/#development","title":"Development","text":"<ul> <li><code>add(a,b) =&gt; a + b</code> creates function inline and returns valuel, can be without name.</li> <li><code>setState(){}</code> - rebuilds the app. usually used in onTap(){}.</li> <li>wrapping parameters in {} make them named params and we have to specify the name when passing them in call. eg: <code>emp({String name, Int age})</code>, when called, <code>emp(name: 'Jon', age: 34)</code>.</li> <li><code>initState() {}</code> function gets executed in every State ful widget. can be used to call all what we want to initialize, like db fetch etc.</li> <li><code>&lt;Future&gt;</code> needs to be handled. Either we can use <code>.then((data) {...})</code> or we can add keywords <code>async...await</code> to the functions.</li> <li><code>factory</code> before a func declaration makes it accessible outside class just like static.</li> <li>To preserve state of app, use mixin keep alive.</li> <li>await and async are good to wait for a process to finish and the execute the rest.</li> <li>Null-aware Operators in Dart</li> </ul> <p>Navigation:</p> <ul> <li>We can generate route on the go with Navigator.pop or push.</li> </ul>"},{"location":"0-Information-Technology/flutter/#function-calling","title":"Function calling","text":"<ul> <li>When work needs to be done on call, then pass refrence.</li> <li>When function builds/returns then call with ()s.</li> <li>Here is the difference:<ul> <li>onPressed: _incrementCounter is a reference to an existing function is passed. This only works if the parameters of the callback expected by onPressed and _incrementCounter are compatible.</li> <li>onPressed: _incrementCounter() _incrementCounter() is executed and the returned result is passed to onPressed. This is a common mistake when done unintentionally when actually the intention was to pass a reference to _incrementCounter instead of calling it.</li> </ul> </li> </ul> <p>External Links:</p> <ul> <li>appicon.co - app icons</li> <li>icons8.com - use icons for free</li> <li>vecteezy.com - icons</li> <li>canva.com - create own design</li> </ul>"},{"location":"0-Information-Technology/flutter/#flutter-state-management","title":"Flutter State Management","text":"<p>Flutter allows to use many kind of state management architectures like Redux, BLoC, MobX and many more. These all are commonly used architecture to layer out UI from Database/WebAPIs.</p>"},{"location":"0-Information-Technology/flutter/#bloc-business-logic-component","title":"BLoC - Business Logic Component","text":"<ul> <li>separates UI from Business logic (Database and Network).</li> <li><code>Sink&lt;data&gt;</code> - data in - events</li> <li><code>Stream&lt;data&gt;</code> - data out - state</li> </ul> <ul> <li>BLoC component converts a stream of incoming events into a stream of outgoing states.</li> <li>Close bloc references in despose() methods.</li> <li>It has:<ul> <li>Bloc</li> <li>BlocBuilder</li> <li>BlocProvider</li> </ul> </li> </ul> <ul> <li>Reactive Programming, whenever there is new data coming from the server. We have to update the UI screen</li> </ul> <ul> <li>KeyNote: never make any network or db call inside the build method and always make sure you dispose or close the open streams.</li> </ul> <ul> <li>Single Instance - all screens have access to bloc, there is one instance in the app</li> <li>Scoped Instance - only widget to which the bloc is exposed has access</li> </ul> <ul> <li>PublishSubject: Starts empty and only emits new elements to subscribers. There is a possibility that one or more items may be lost between the time the Subject is created and the observer subscribes to it because PublishSubject starts emitting elements immediately upon creation.</li> <li>BehaviorSubject: It needs an initial value and replays it or the latest element to new subscribers. As BehaviorSubject always emits the latest element, you can\u2019t create one without giving a default initial value. BehaviorSubject is helpful for depicting \"values over time\". For example, an event stream of birthdays is a Subject, but the stream of a person's age would be a BehaviorSubject.</li> </ul> <ul> <li>We pass the blocProvider to MaterialRoute and then it houses all the variables to be passed. This acts as inheritedWidget.</li> <li>MovieApp - Part 2</li> </ul> <p>The workflow of the Counter App:</p> <ul> <li>add packages</li> <li>create events as enum.</li> <li>create state - in this app, state is <code>int</code> so we don't create state class.</li> <li>create bloc to take events, map it, and return state,<ul> <li><code>class CounterBloc extends Bloc&lt;CounterEvent, int&gt; {...}</code></li> <li>here override init state</li> <li>and override mapEventsToState</li> </ul> </li> <li>instantiate bloc in main using <code>BlocProvider&lt;Bloc&gt;{}</code>.</li> <li>create Page, get bloc,</li> <li>get bloc, <code>final CounterBloc counterBloc = BlocProvider.of&lt;CounterBloc&gt;(context);</code></li> <li>body will be <code>BlocBuilder&lt;CounterBloc, int&gt;();</code> to build UI based on state.</li> <li>action event, <code>onPressed: () {counterBloc.add(CounterEvent.increment);}</code></li> <li>based on Flutter Counter App tutorial by @felangel.</li> </ul> <p>Redux:</p> <ul> <li>provides routing as well</li> <li>works with store reducer concept</li> </ul> <p>ScopedModel</p> <ul> <li>Updates only model in scope, not the whole widget tree</li> <li>Have to notifyListeners() on each state change</li> </ul> <p>Links:</p> <ul> <li>Flutter Architecture Samples to build ToDo apps</li> <li>The Essential Guide to UI Engineering</li> </ul>"},{"location":"0-Information-Technology/flutter/#firestore-and-flutter","title":"Firestore and Flutter","text":"<p>Connecting apps:</p> <ul> <li>iOS:<ul> <li>add GoogleService-Info.plist in xCode</li> <li>add reverseClientId in key CFBundleURLSchemes Runner/info.plist</li> </ul> </li> <li>android:<ul> <li>add google-services.josn and</li> <li>make sure applicationId is correct in app/build.gradle</li> </ul> </li> </ul> <p>Reading data (Flutter):</p> <ul> <li>Create a reference to instance of a collection, <code>userRef = Firestore.instance.collection('users')</code>.</li> <li>The above has functions to fetch records as snapshot of data</li> <li>It returns future which need to be handled.</li> <li>use StreamBuilder</li> <li>stream: Firestore.instance.collection('users').snapshots(),</li> <li>reference object accepts chain of functions like <code>where()..orderBy()..limit()</code> etc. for compound queries.</li> <li><code>FutureBuilder</code> - reads data in database</li> <li><code>StreamBuilder</code> - provides stream to data, shows new user added.</li> </ul> <p>NoSQL Structuting:</p> <ul> <li>do not nest collections</li> <li>keep data flatten</li> <li>copy data but mostly stored in the way it is best to be fetched.</li> </ul> <p>Create/Update/Delete data (Flutter):</p> <ul> <li>onTap: () =&gt; record.reference.updateData({'key': new_value})</li> <li>now its all linked from front to back</li> <li>all sync and updates offline and online.</li> <li>reference has function like <code>add()..updateData()..delete()</code>. They all return <code>&lt;Future&gt;</code>.</li> </ul> <p>Triggers:</p> <ul> <li>user firestore triggers to listen and act on the change in a collection.</li> <li>It uses node js to write and deploy the functions.</li> <li>Triggers can be created by using Trigger functions. Steps:<ul> <li><code>firebase login</code> - login to your google account to use CLI.</li> <li><code>firebase init fucntions</code> - create fucntions folder in flutter project.</li> <li><code>firebase deploy --only functions</code> - deploy functions to firebase cloud.</li> </ul> </li> </ul> <p>References:</p> <ul> <li>Felix Angelov - https://www.youtube.com/watch?v=knMvKPKBzGE</li> <li>Brian Egan - https://www.youtube.com/watch?v=Wfc5LMgWaRA</li> </ul>"},{"location":"0-Information-Technology/gen-ai/","title":"Generative Artificial Intelligence","text":"<p>Gen AI</p> <p>GenAI models are like engines of car. They are very complex engines built by FANG. They are private or public. Eg, ChatGPT, Llama, MidJourney, StabilityAI-StableDiffusion etc.</p> <p>Usages</p> <ul> <li>Business Leader - Propose to build a super car. Use GenAI model in their product. Either purchase license or use public offering.</li> <li>Engineer - Builds car by using per-built engine. Use the model in code to generate content. But do not build the model. They can train the model?</li> <li>User - Uses car. No tech knowledge needed, eg, user of chatGPT.</li> </ul> <p>So, the entities are:</p> <ul> <li>Model - algo trained on dataset.</li> <li>Notebook - tool to write code and play.</li> <li>App - Creative use case of gen AI</li> <li>Outcome - Generated content.</li> </ul> <p>Engineering Usage:</p> <ul> <li>You can use model via their app or directly pick from github and use in notebook.</li> <li>When using in notebook, you can tweak model to be more creative.</li> </ul> <p>Text to Image models:</p> <ul> <li>MidJourney - artistic like Mac</li> <li>DALL-E - common like windows</li> <li>Stable Diffusion - open like linux</li> </ul> <p>GANs or Generative Adversarial Networks:</p> <ul> <li>It has generator and discriminator. Generator creates content, discriminator will tell if it is good or not, and this continues until generator creates good content.</li> <li>Same model can be used for variety of things.</li> </ul> <p>Variational Auto-encoders (VAE)</p> <ul> <li>They help in anomaly detection</li> </ul> <p>Effect of GenAI</p> <ul> <li>Jobs shift, some will go, new will come.</li> <li>Everyone will be creative, no more repetitive or boring job (dull dirty dangerous difficult will be gone, 4D).</li> <li>To make the most, improve your, interpersonal emotional and creative skills.</li> <li>So, stop focusing on solving difficult things, focus on being creative, leadership.</li> </ul> <p>Models:</p> <ul> <li>X Grok</li> <li>Meta Llama</li> <li>Google Gemini</li> <li>Nvidia Megatron Turing</li> </ul> <p>API lets you give prompt to model and it returns model response. This way you can use any model in your app.</p>"},{"location":"0-Information-Technology/gen-ai/#llms-large-language-models","title":"LLMs Large Language Models","text":"<ul> <li>it uses transformers, theory researched by Google in 2017. GPT-3 is first model released in 2020 May (175B param trained)</li> <li>Different models use different techniques to train model<ul> <li>some increase the dataset</li> <li>some increase the tokens</li> <li>some change the logic</li> </ul> </li> <li>all have different effect on different scenario, it is not true that if you keep increasing the dataset size or tokens you will have better model.</li> <li>finally in 2022, Google PaLM is best model. (540B params, 780b tokens) possible as it uses logic that can train model in parallel that is it can make better use of hardware.</li> </ul> <ul> <li>Open models<ul> <li>OPT from Meta and BLOOM are open models. You can see the weights and dataset it is trained on.</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/gen-ai/#embeddings-using-openai-for-vector-search-on-text","title":"Embeddings Using OpenAI for Vector Search on Text","text":"<ul> <li>Embedding is conversion of text in to vector space.</li> <li>To convert, you can use OpenAI Embedding via API, using API Key, using Credit Card.</li> <li>Once done, you can send a string to OpenAI and it will return you a vector (array of numbers) this will be 1536 in length.</li> <li>You can imagine this as a vector space having 1536 dimensions, and your text is somewhere in that space.</li> </ul>"},{"location":"0-Information-Technology/gen-ai/#openai-model-selection-and-integration","title":"OpenAI Model Selection and Integration","text":"<ul> <li>you can use any model offered by OpenAI. You need to pay for token sent and received.</li> <li>in chat, all old prompt and response from assistant is sent in each request. because rest-api is stateless and it does not know anything about your last conversation.</li> <li>you can use python library or postman to use models.</li> </ul>"},{"location":"0-Information-Technology/google-cloud-platform/","title":"Google Cloud Platform","text":"<p>Google Cloud Platform is cloud service from Google just like AWS and Azure. It provides SaaS, PaaS and IaaS.</p>"},{"location":"0-Information-Technology/google-cloud-platform/#gcp-services","title":"GCP Services","text":"<ul> <li>GCP Firebase as data storage engine.</li> <li>GCP App Engine is PaaS for deploying web apps on cloud:<ul> <li>App Engine also helps us deploy dockers and containers</li> <li>hands on co-lab.</li> </ul> </li> <li>GCP Compute Engine provides VMs, which is like IaaS.</li> <li>GCP Cloud Machine Learning:<ul> <li>Offers pre trained models with biggest library.</li> <li>Cloud Vision API, Video Intelligence</li> <li>Identify objects, landmarks, celebrities, colors, million other entities</li> <li>NLP API, Translations,  Text2Speech, Speech2Text API</li> <li>Auto ML trains on your data using expertise from already trained neurals</li> <li>Provides interface to train, evaluate and proof objects on your own data.</li> <li>SaaS with latest TensorFlow, PyTorch and SKLearn on VMs with TPU and GPU support</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/google-cloud-platform/#gcp-compute-engine","title":"GCP Compute Engine","text":"<p>Google Compute Engine is a cloud service from GCP which offers Infrastructure as a Service. You can get a machine with configurations as required and it can be easily scaled.</p> <p>Example, start a Micro Machine <code>f1-micro</code>, with Ubuntu Server installed on  20gb HDD, in region 'us-central', and can allow traffic 'http and https'. This is also free for lifetime.</p>"},{"location":"0-Information-Technology/google-cloud-platform/#free-ubuntu-server-on-gce","title":"Free Ubuntu Server on GCE","text":"<p>Get an instance on GCE as per requirement or as in example above. We will configure it and add swap memory, then we will make it web server by installing Apache. We will also install MySQL database and PHP/Python as backend languages to serve web apps.</p>"},{"location":"0-Information-Technology/google-cloud-platform/#install-gcloud-on-mac","title":"Install gcloud on mac","text":"<ul> <li>Follow this guide.</li> <li>Install <code>gcloud</code> on workstation machine, mac, <code>wget &gt; tar -xf &gt; install.sh &gt; gcloud init</code></li> <li>Connect to the VM machine using ssh gcloud command, get it from the SSH dropdown on GCP console near VM.</li> <li>Command: <code>gcloud beta compute ssh --zone \"us-central1-a\" \"vm_name\" --project \"project_name\"</code> this adds to known hosts.</li> <li>This allows you to ssh to GCE host machine from your terminal on workstation.</li> </ul> <p>Congratualations, you have your own linux machine on cloud, free for lifetime and is scalable. It is time to get your hand dirty.</p>"},{"location":"0-Information-Technology/google-cloud-platform/#transferring-files","title":"Transferring files","text":"<ul> <li>You can transfer files using various options mentioned in this guide.</li> <li>we are using <code>gcloud</code> cli to transfer files between workstation and gce instance.</li> <li>Upload <code>gcloud compute scp local-file-path instance-name:dir-on-instance</code><ul> <li><code>instance-name</code> is name given during creation of instance</li> <li><code>dir-on-instance</code> is address where you need to copy, eg, <code>~</code></li> </ul> </li> <li>Download <code>gcloud compute scp --recurse instance-name:remote-dir local-dir</code></li> <li>to login to server using command <code>gcloud beta compute ssh --zone \"us-central1-a\" \"instance-name\" --project \"project-name\"</code>.</li> </ul>"},{"location":"0-Information-Technology/google-cloud-platform/#gcp-firebase","title":"GCP Firebase","text":"<p>Firebase is cloud based, app-backend service that is scalable and it helps in authentication, database, file storage, hosting, crashlytics, messeging, adMob, analytics, campaigns etc.</p> <p>It has following components:</p> <ul> <li>ML Kit<ul> <li>Has in build ML models for text analytics, image recongnition etc.</li> <li>Works on device or on cloud</li> <li>Can add Tensor flow functions/models to firebase functions and it will be hosted and serverd.</li> </ul> </li> </ul> <ul> <li>Firebase Authentication<ul> <li>Google, fb, twitter etc</li> <li>Account based</li> <li>Gives back user information, unique_id, name, photo,</li> <li>manages sessions</li> </ul> </li> </ul> <ul> <li>Cloud functions<ul> <li>responses to event, like welcome email on sign in</li> <li>triggers on database events</li> <li>modify files uploaded</li> <li>send cloud messaging messages to other users.</li> <li>build API from database</li> <li>All written in JS using node and then deployed using CLI</li> </ul> </li> </ul> <ul> <li>Firebase Hosting<ul> <li>Static files hosting</li> <li>on SSD, serves SSL,</li> <li>can host PWA</li> </ul> </li> </ul> <ul> <li>Firebase Storage<ul> <li>Store files, secure them, reliable.</li> </ul> </li> </ul> <ul> <li>Firebase Realtime Database<ul> <li>Store and sync data in realtime, even offline</li> <li>It is NoSql database.</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/google-cloud-platform/#firestore-notes","title":"Firestore Notes","text":"<p>It is NoSQL database storage engine in firebase</p> <ul> <li>Works online and offline</li> <li>Stores data in collections</li> </ul> <p>Initialization:</p> <ul> <li>create database</li> <li>create table, called collection, users</li> <li>create rows, called document, doc_id</li> </ul> <p>More on Flutter notes.</p>"},{"location":"0-Information-Technology/google-cloud-platform/#app-script","title":"App Script","text":"<p>Create app script https://developers.google.com/apps-script/add-ons/translate-addon-sample</p>"},{"location":"0-Information-Technology/htmx-flask/","title":"HTMX and use with Flask","text":"<p>HTMX lets you do AJAX stuff with few words in HTML.</p>"},{"location":"0-Information-Technology/htmx-flask/#htmx","title":"HTMX","text":"<p>Init:</p> <pre><code>&lt;script src=\"https://unpkg.com/htmx.org@1.5.0\"&gt;&lt;/script&gt;\n\n...\n\n&lt;button hx-post=\"/clicked\"\n    hx-trigger=\"click\"\n    hx-target=\"#parent-div\"\n    hx-swap=\"outerHTML\"\n&gt;\n    Click Me!\n&lt;/button&gt;\n</code></pre> <p>Trigger</p> <p>When to send the request to server? Use <code>hx-trigger=\"click\"</code> to trigger request on an event like click.</p> <p>HTTP Verb (required)</p> <p>How to do the request? POST, PUT, DELETE, PATCH or GET.</p> <p>Target</p> <p>Where to use the response from server. <code>hx-target=\"#parent-div\"</code> tells to use at DOM elem with id <code>parent-div</code></p> <p>Swap</p> <p>What to do on the target? <code>hx-swap=\"outerHTML\"</code> tells to replace the target with response.</p> <p>Loading</p> <p>To show user that request is being server, use:</p> <ul> <li><code>hx-indicator=\"#indicator\"</code> on any element, or</li> <li><code>hx-indicator=\"#indicator\"</code> to specify particular element to show on request and hide on response.</li> </ul> <p>Pros</p> <ul> <li>Any element can be used, not just <code>a</code> or <code>form</code></li> <li>Any event can be used, not just <code>click</code> or <code>submit</code></li> <li>Any http-verb can be used, not just <code>get</code> or <code>post</code>.</li> </ul> <p>Cons</p> <p>After using for 2 days, it is good for very small asks else it is a pain, things can be done in a way and not as you want to. Or else it needs too much complexity</p> <ul> <li>updating two things from response is not easy, you can use oob-swap but it adds complexity</li> <li>restricting user clicks is not easy, you can use abort but adds complexity</li> <li>canceling form request requires a new get request to server.</li> <li>any change on client side is now no more a javascript, instead it is a server request. which is not good.</li> </ul> <p>so it is not a replacement for js, but can be used along with to reduce some code, but adds on to code complexity on both client and server code base. (2 days feedback)</p> <p>Working</p> <ul> <li>Verb is required, get post etc.</li> </ul> <ul> <li>If no trigger, it will be natural like, like click on a, submit on form, change on input/select, click on everything else.</li> <li>Trigger can be specified, like <code>mouseenter</code> or <code>keyup</code>, more on hx-trigger attribute.</li> </ul> <ul> <li>If no target then source is replaced with response.</li> <li>Else, target can be CSS selector, more on tagets and css ext</li> </ul> <ul> <li>Swapping can be done on in around the target, more on swapping</li> </ul> <ul> <li>Swap OOB from response. Rather than defining in source, you can define swap in response, it loads and replaces in source. Use <code>hx-swap-oob=\"beforeend:#contacts-table\"</code></li> </ul> <ul> <li>Select, you can select an element from response dom, and swap it in source. Use <code>hx-select=\"#info\"</code>. More on select</li> </ul> <ul> <li>Sync - you can syncajax requests more on syncing</li> </ul> <ul> <li>Confirm Box - you can add confirm before ajax is done, <code>hx-confirm=\"Sure?\"</code> more on Confirming Requests</li> </ul> <ul> <li>CSRF with HTMX - use <code>hx-vals='{{ '{\"csrf_token\": \"' + csrf_token() +'\" }'|safe }}'</code>, more on CSRF with HTMX</li> </ul> <ul> <li>Response header in flask with retarget and reswap <code>return hx_res, {\"HX-Retarget\": \"closest a\", \"HX-Reswap\": \"innerHTML\"}</code></li> </ul> <ul> <li>Flask-HTMX is a python library that<ul> <li>tells flask if the request is via HTMX or normal so you can act accordingly in view-route, that return partial or full page.</li> <li>let you prepare response with headers modified for HTMX behaviours.</li> <li>more on Flask Htmx Quickstart</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/htmx-flask/#flask-snippets","title":"Flask Snippets","text":"<p>Add flash placeholder and loading indicator</p> <pre><code>&lt;div class=\"container\"&gt;\n  ...\n  &lt;div id=\"htmx-activity\"&gt;\n    &lt;span id=\"htmx-none\"&gt;&lt;span&gt;\n  &lt;/div&gt;\n  ...\n&lt;/div&gt; &lt;!-- container --&gt;\n\n&lt;div id=\"overlay-wrapper\" class=\"htmx-indicator\"&gt;\n    &lt;div class=\"overlay-content\"&gt;\n        &lt;h1&gt;Working in background!&lt;/h1&gt;\n        &lt;p&gt;Please do not close the window&lt;/p&gt;\n        &lt;span class=\"glyphicon glyphicon-refresh spinning\"&gt;&lt;/span&gt;\n    &lt;/div&gt;\n&lt;/div&gt;\n</code></pre> <p><code>htmx-none</code> is used when you do not have to change anything on dom. <code>htmx-activity</code> is used to append flash messages after it.</p> <p>Add CSS</p> <pre><code>/* HTMX */\n#overlay-wrapper.htmx-indicator{\n    display:none;\n    opacity: 100!important;\n    z-index: 10;\n}\n.htmx-request #overlay-wrapper.htmx-indicator{\n    display:inline;\n}\n.htmx-request#overlay-wrapper.htmx-indicator{\n    display:inline;\n}\n\n/* Spinner */\n.glyphicon.spinning {\n    animation: spin 1s infinite linear;\n    -webkit-animation: spin2 1s infinite linear;\n}\n\n@keyframes spin {\n    from { transform: scale(1) rotate(0deg); }\n    to { transform: scale(1) rotate(360deg); }\n}\n\n@-webkit-keyframes spin2 {\n    from { -webkit-transform: rotate(0deg); }\n    to { -webkit-transform: rotate(360deg); }\n}\n/* HTMX ends */\n\n/* Overlay */\n.overlay-content {\n    min-height: 100px;\n    width: 500px;\n    background: #fffffff0;\n    text-align: center;\n    padding: 54px 5px;\n    margin: 100px auto;\n}\n\n#overlay-wrapper {\n    display: none;\n    position: fixed;\n    height: 100vh;\n    width: 100vw;\n    top: 0;\n    left: 0;\n    background: rgba(0, 0, 0, 0.5);\n}\n/* Overlay ends */\n</code></pre> <p>Add flash message build method</p> <pre><code>def hx_flash(message,category):\n    category = 'info' if category == 'message' else category\n    return f\"\"\"\n    &lt;div hx-swap-oob=\"afterbegin:#htmx-activity\" &gt;\n    &lt;div class=\"alert alert-{category}\"&gt;\n        &lt;button type=\"button\" class=\"close\" data-dismiss=\"alert\"&gt;&amp;times;&lt;/button&gt;\n        {message}\n    &lt;/div&gt;\n    &lt;/div&gt;\n    \"\"\"\n</code></pre> <p>See how <code>hx-swap-oob=\"afterbegin:#htmx-activity\"</code> will add flash message to particular dom, this will be appended to another dom in response.</p> <p>Delete Button</p> <pre><code>        &lt;td&gt;\n          &lt;a href=\"#\"&gt;Edit&lt;/a&gt;\n          &lt;a\n            hx-vals='{{ '{\"csrf_token\": \"' + csrf_token() +'\" }'|safe }}'\n            hx-delete=\"{{ url_for('items.delete', id=item.id) }}\"\n            hx-indicator=\"#overlay-wrapper\"\n            hx-target=\"closest tr\"\n            hx-swap=\"outerHTML\"\n            hx-confirm=\"Are you sure?\"\n            &gt;Del&lt;/a&gt;\n          &lt;a href=\"#\"&gt;Resonses&lt;/a&gt;\n        &lt;/td&gt;\n</code></pre> <p>HTMX strategy here is for success that the closest <code>tr</code> which row that is having delete button will be deleted with response that will have blank <code>&lt;tr&gt;&lt;/tr&gt;</code></p> <p>Delete Method</p> <pre><code>@bp.route('/item/&lt;int:id&gt;', methods=[\"DELETE\"])\ndef delete(id):\n    item = db.get_or_404(Item, id)\n    error = False\n\n    ... # check for errors\n\n    if error == False:\n        db.session.delete(item)\n        db.session.commit()\n        hx_res = hx_flash(f\"Item {id} Deleted\",\"success\")\n        hx_res = hx_res + \"&lt;tr&gt;&lt;/tr&gt;\"\n        return hx_res\n    else:\n        # on ERROR\n        hx_res = hx_flash(f\"Cannot delete item {id} as either email is sent out or response is submitted.\", \"warning\")\n        hx_res = hx_res + \"\"\n\n        return hx_res, {\"HX-Retarget\": \"#htmx-none\", \"HX-Reswap\": \"innerHTML\"}\n</code></pre> <p>Here, on error you see that we have changed the HTMX swap and target strategy by modifying response headers. as there is error, no change is required on dom current <code>tr</code> row, rather we will utilize <code>#htmx-none</code> and add <code>\"\"</code> nothing there.</p> <p>Links</p> <ul> <li>Codecapsules - Tutorial Building A Full Stack Application With Flask And Htmx</li> <li>Testdriven - Active Search with Flask Htmx</li> </ul>"},{"location":"0-Information-Technology/javascript-notes/","title":"JavaScript","text":"<p>all about JavaScript, it's frameworks and variations</p>"},{"location":"0-Information-Technology/javascript-notes/#javascript-concepts","title":"JavaScript Concepts","text":"<p>How Functions behave:</p> <p>We can make a function in JS by defining it like we define a variable.</p> <pre><code>myFunction = function(arg1, arg2, arg3) {\n  // all that you want to do\n  // use args or may be they are optional\n  //...\n  return myResult;\n}\n</code></pre> <p>another way is, arrow notation:</p> <pre><code>myFunction = arg1 =&gt; {\n  // all that you want to do\n  // use args or may be they are optional\n  //...\n  return myResult;\n}\n</code></pre> <p>now this function can return a variable which could possibly be any data/object/json etc.</p> <p>Function can now be used to get the result</p> <pre><code>myFunction(arg1, arg2).done( function (data) {\n  // Now do what you want to do with result in data\n});\n</code></pre> <p>Now we see that, done would be triggered once myFunction is completed. All jQuery promises provide a done method that takes a callback.</p> <p>JS Objects</p> <ul> <li>JS Object can hold anything, they can even hold another function.</li> <li>They are accessed using . DOT notation.</li> </ul>"},{"location":"0-Information-Technology/javascript-notes/#node-js","title":"Node JS","text":"<p>Overview:</p> <ul> <li>node js is like asp/php/python, it makes use of JS for backend lang.</li> <li>you can run .js files outside browser</li> <li>npm is package manager for node js, like pip</li> <li>pakages are nothing but js libraries.</li> <li><code>npm install</code> downloads a package and it's dependencies defined in a <code>package.json</code> file and generates a node_modules folder with the installed modules.</li> </ul>"},{"location":"0-Information-Technology/javascript-notes/#angular-js","title":"Angular JS","text":"<p>Overview:</p> <ul> <li>We can define a resource in app folder eg: hero.ts to include the class with it's members.</li> <li>We can make components for various behaviors of our app. eg:<ul> <li>heroes - to list all heroes</li> <li>hero-detail - to keep functionality for one hero</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/javascript-notes/#typescript","title":"Typescript","text":"<p>It is superset of javascript, basically used to make use of new features of JS coming with ES6, ES7 and compile them back to old ES3 that can be used with IE and Safari as well.</p>"},{"location":"0-Information-Technology/javascript-notes/#js-snippets","title":"JS Snippets","text":""},{"location":"0-Information-Technology/javascript-notes/#looping","title":"Looping","text":"<pre><code>// array\nfor (let i = 0; i &lt; myArray.length; i++) {\n  console.log(myArray[i].age)\n}\n\n// array values, in list of dictionary\nfor (let item of items) {\n    console.log(item); // Will display contents of the object inside the array\n}\n\nfor (const [key, value] of items) {\n  console.log(value);\n}\n\n// fixed length\nvar toAdd = document.createDocumentFragment();\nfor(var i=0; i &lt; 11; i++){\n  var newDiv = document.createElement('div');\n  newDiv.id = 'r'+i;\n  toAdd.appendChild(newDiv);\n}\ndocument.appendChild(toAdd);\n</code></pre> <ul> <li>More on Stackoverflow - Questions 16626735 How To Loop Through An Array Containing Objects And Access Their Properties</li> </ul>"},{"location":"0-Information-Technology/javascript-notes/#add-and-modify-elements","title":"Add and Modify Elements","text":"<pre><code>var newDiv = document.createElement('div');\nnewDiv.id = 'roam';\nnewDiv.innerHTML = 'JavaScript DOM';\nnewDiv.className = 'ansbox';\nnewDiv.className = 'ansbox';\n\n// remove\nnewDiv.remove();\n</code></pre>"},{"location":"0-Information-Technology/javascript-notes/#action-on-dom-elements","title":"Action on DOM Elements","text":"<pre><code>// items selection\nconst items = document.querySelectorAll(\".some-class\");\n\n// item selection\nconst item = document.querySelector('#item-id');\n\nconst elem = document.getElementById(\"myDIV\")\n\n// Function definition\nvar showOverlay = function overlay() {\n  // do something\n};\n\n// if item exists\nif (item) {\n  // do something\n}\n\n// on click one item\nitem.addEventListener(\"click\", function () {\n    // do something\n});\n\n// on click to all\nitems.forEach( function (item) =&gt; {\n    item.addEventListener(\"click\", function () {\n        // do something\n    })\n});\n\n// CSS Display change\nitem.style.display = 'block';\n</code></pre>"},{"location":"0-Information-Technology/javascript-notes/#how-to-quickly-scrape-all-links-from-page","title":"How to quickly scrape all links from page","text":"<p>Use JS path to get all a tags you are interested in, do inspect, go to div having all a, then right click and copy <code>JS Path</code>. Then add path till <code>a</code> tag to this <code>div JS Path</code>. Now that you have JS Path to all the anchor tags, you can iterate over them and make a list then copy. You can execute this in console directly, eg:</p> <ul> <li>JS Path to div <code>#root &gt; div &gt; div.sc-AxjAm.tlQbp &gt; div &gt; main &gt; div &gt; div &gt; div</code></li> <li>JS Path to all a tags <code>article &gt; div:nth-child(2) &gt; div &gt; div:nth-child(5) &gt; a</code></li> <li>Join both together, then loop</li> </ul> <pre><code>links = '';\ndocument.querySelectorAll(\"#root &gt; div:nth-child(5) &gt; a\").forEach(\n  function (e) {\n    links+=\"yourCmd \"+e.href+\" \\n\";\n  }\n)\ncopy(links)\n</code></pre> <p>This copies the output to clipboard.</p>"},{"location":"0-Information-Technology/javascript-notes/#links","title":"Links","text":"<ul> <li>NativeScript Notes</li> <li>ECMA6 Notes</li> <li>React JS Notes</li> </ul>"},{"location":"0-Information-Technology/material-mkdocs/","title":"Material for MkDocs","text":"<p>all about MkDocs and Material theme</p>"},{"location":"0-Information-Technology/material-mkdocs/#mkdocs","title":"MkDocs","text":"<p>MkDocs is static site generator, more like <code>Pelican</code>. From markdown files, it can build static site.</p> <p>Static site can be deployed on Github Pages and builds can be automated using Github Actions.</p> <p>So one repo has your source markdown files and another repo has static site which is build and deployed using github actions.</p>"},{"location":"0-Information-Technology/material-mkdocs/#basics","title":"Basics","text":"<ul> <li><code>mkdocks.yml</code><ul> <li><code>site_name</code> and <code>site_url</code> are bare minimum to specify.</li> <li>new docs are auto picked and added to navs. If you specify pages in nav, you will have to do for all the pages you add, any new ones too..</li> </ul> </li> </ul> <ul> <li><code>mkdocs serve</code> live preview server on dev+</li> <li><code>mkdocs build</code> to create <code>site</code> folder</li> </ul>"},{"location":"0-Information-Technology/material-mkdocs/#deployment","title":"Deployment","text":"<ul> <li><code>mkdocs gh-deploy</code> to deploy to github<ul> <li>creates <code>gh-pages</code> branch on local and adds site to it.</li> <li>pushes this branch to remote as default.</li> </ul> </li> </ul> <ul> <li>Issues<ul> <li>pushes static site, needs build. Can be resolved with GitHub actions.</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/material-mkdocs/#github-actions","title":"Github Actions","text":"<ul> <li>GitHub Actions is yet another free option from GitHub, which is basically a build server in the cloud</li> <li>have a build server automatically pick up changes in Markdown source files and build the static website directly on the build server.</li> <li>More on git page</li> </ul> <ul> <li>Links<ul> <li>https://blog.elmah.io/deploying-a-mkdocs-documentation-site-with-github-actions/</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/material-mkdocs/#internal-links-to-pages","title":"Internal Links to Pages","text":"<p>You can link to internal pages by providing relative filename. On compiliting these are converted to HTML links. Eg:</p> <pre><code>Please view [documentation](documentation.md) for more information.\n\nPlease view [documentation](../about/documentation.md) for more information.\n\nPlease view [documentation](about.md#documentation) for more information.\n</code></pre> <p>Reference page MkDocs - linking-to-pages</p>"},{"location":"0-Information-Technology/material-mkdocs/#material-theme","title":"Material Theme","text":"<ul> <li>Requirements<ul> <li><code>pip install mkdocs-material</code></li> <li><code>pip install mkdocs-git-revision-date-localized-plugin</code></li> </ul> </li> </ul> <ul> <li> <p>add following to config file</p> <pre><code>theme:\n  name: material\n</code></pre> </li> </ul> <ul> <li>For Good Looks<ul> <li>Keep <code>title: Two-Three words</code></li> <li>Can have H1-H5 as proper english sentences, keep small to a few words.</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/material-mkdocs/#jupyter-notebook-rendering","title":"Jupyter Notebook Rendering","text":"<p>You can add support to render <code>*.ipynb</code> files in your site. Please follow docs on Use Jupyter Notebooks in mkdocs.</p> <p>Few things to note:</p> <ul> <li>Update requirement file so that github actions pick it.</li> <li>The include/ignore paths are within <code>./docs/</code> folder.</li> </ul>"},{"location":"0-Information-Technology/material-mkdocs/#markdown-extensions","title":"MarkDown Extensions","text":"<p>Markdown is rendered in MkDocs using Python-Markdown. This supports basic markdown with some strict formats. It can be extended using Python Markdown Extensions and PyMdown Extensions.</p> <p>Extensions might need to be installed and added to configuration file.</p>"},{"location":"0-Information-Technology/material-mkdocs/#list-intent","title":"List Intent","text":"<p>Default is to keep 4 spaces for list intent. Extension <code>mdx_truly_sane_lists</code> allows to use 2 spaces.</p>"},{"location":"0-Information-Technology/material-mkdocs/#admonitions","title":"Admonitions","text":"Collapsible <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> Collapsible Open <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>More: https://squidfunk.github.io/mkdocs-material/reference/admonitions/#collapsible-blocks</p>"},{"location":"0-Information-Technology/material-mkdocs/#code-blocks","title":"Code Blocks","text":"<p>Inline <code>code</code> and block.</p> Code Title: bubble_sort.py<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre> <p>More: https://squidfunk.github.io/mkdocs-material/reference/code-blocks/</p>"},{"location":"0-Information-Technology/material-mkdocs/#content-tabs","title":"Content Tabs","text":"CC++ <pre><code>#include &lt;stdio.h&gt;\n\nint main(void) {\n  printf(\"Hello world!\\n\");\n  return 0;\n}\n</code></pre> <pre><code>#include &lt;iostream&gt;\n\nint main(void) {\n  std::cout &lt;&lt; \"Hello world!\" &lt;&lt; std::endl;\n  return 0;\n}\n</code></pre> <p>More: https://squidfunk.github.io/mkdocs-material/reference/content-tabs/</p>"},{"location":"0-Information-Technology/material-mkdocs/#diagrams-mermaid","title":"Diagrams Mermaid","text":"<pre><code>graph LR\n  A[Start] --&gt; B{Error?};\n  B --&gt;|Yes| C[Hmm...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Yay!];</code></pre> <p>Adding title to Diagram</p> <p>Use markdown latext <code>text</code> to give a center aligned title.</p> <p><code>$$\\text{Fig: Sticky Note Color}$$</code>, which come up like:</p> \\[\\text{Fig: Sticky Note Color}\\] <p>Adding Style to Diagram</p> <p>Use SVG styling CSS to style nodes, links etc.</p> <pre><code>flowchart\ns1[Event\\nOf interest to the\\nBusiness]\nstyle s1 fill:#ffa930,stroke-width:0,color:#fff,width:150px,height:150px\n\ns2[Action\\n What do we want\\nto have happen]\nstyle s2 fill:#a9edf1,stroke-width:0,width:150px,height:150px\n\ns3[Question?\\nThings we have to\\ndo]\nstyle s3 fill:#ff32b2,stroke-width:0,color:#fff,width:150px,height:150px</code></pre> <pre><code>mermaid\n\nflowchart TD\n\ns1[Event\\nOf interest to the\\nBusiness]\nstyle s1 fill:#ffa930,stroke-width:0,color:#fff,width:150px,height:150px\n\ns2[Action\\n What do we want\\nto have happen]\nstyle s2 fill:#a9edf1,stroke-width:0,width:150px,height:150px\n\ns3[Question?\\nThings we have to\\ndo]\nstyle s3 fill:#ff32b2,stroke-width:0,color:#fff,width:150px,height:150px\n</code></pre> <p>More on mermaid styling.</p> <p>Links</p> <ul> <li>Mermaid JS - Flowchart 101</li> <li>MKDocs Diagrams</li> </ul>"},{"location":"0-Information-Technology/material-mkdocs/#mathjax","title":"MathJax","text":"<p>Inline \\(\\alpha\\) math.</p> \\[ \\operatorname{ker} f=\\{g\\in G:f(g)=e_{H}\\}{\\mbox{.}} \\] <p>More: https://squidfunk.github.io/mkdocs-material/reference/mathjax/</p>"},{"location":"0-Information-Technology/material-mkdocs/#lists-as-tasks","title":"Lists as Tasks","text":"<ul> <li> Lorem ipsum dolor sit amet, consectetur adipiscing elit</li> <li> Vestibulum convallis sit amet nisi a tincidunt<ul> <li> In hac habitasse platea dictumst</li> <li> In scelerisque nibh non dolor mollis congue sed et metus</li> <li> Praesent sed risus massa</li> </ul> </li> <li> Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque</li> </ul> <p>More: https://squidfunk.github.io/mkdocs-material/reference/lists/</p>"},{"location":"0-Information-Technology/material-mkdocs/#third-party-tweaks","title":"Third Party Tweaks","text":"<p>Modify theme and interact with core Python Plugins</p>"},{"location":"0-Information-Technology/material-mkdocs/#last-10-updated-pages","title":"Last 10 updated pages","text":"<p>Link https://timvink.github.io/mkdocs-git-revision-date-localized-plugin/howto/override-a-theme/#example-list-last-updated-pages</p>"},{"location":"0-Information-Technology/material-mkdocs/#iyv-wiki-specifics","title":"iYV Wiki Specifics","text":"<p>Decisions</p> <ul> <li>Don't need tabs</li> <li>need sidebar sections</li> <li>need blog</li> </ul> <p>Example repo wiki - https://github.com/barnumbirr/wiki</p> <p>view all page items</p> <pre><code># all page items\n{% for key,value in page.__dict__.items() %}\n    {{ key,value }}\n{% endfor %}\n\n\n{% for key,value in page.meta.items() %}\n    {{ key,value }}\n{% endfor %}\n</code></pre>"},{"location":"0-Information-Technology/material-mkdocs/#markdown-101-rules","title":"Markdown 101 - Rules","text":"<p>Based on a year of work, following structure has emerged and has worked in arranging notes all around, for work logs, meetings and knowledge base. make a <code>~/notes</code> folder, which should be a GIT repo and can have following files:</p> <ul> <li>Project Specific - markdown file <code>project_name.md</code> - H1s<ul> <li><code># Understanding</code> - add notes as you gain understanding of process etc. Can have all sort of h2, h3 etc.</li> <li><code># Work Log</code> - add dated work that you do, or steps you follow, kind of rough log. <code>## 22-11 - tu - adding exception handling</code></li> <li><code># Meeting Log</code> - add dated meeting rough notes. <code>## 23-04 - mo - Show and tell with James</code></li> </ul> </li> <li>Meetings markdown - <code>meetings.md</code> -  a file to have all meeting that you do - H1s<ul> <li><code># People Log</code> - this can have <code>## John Doe</code> which can have log of conversation, facts, or actions with a person.<ul> <li><code>## Others</code> - log of people you interact less.</li> </ul> </li> <li><code>Meeting Log</code> - logs of meeting, can have actions [ ] with date to be completed on. <code>## 12-03 - tu - Sales Pitch Overview</code></li> </ul> </li> <li>Daily Logs markdown - <code>daily_log.md</code> - H1s<ul> <li><code># ToDo - Backlog - Minddump</code> has any thing that comes to ming, todos, actions to be taken, read later etc.</li> <li><code># Daily Log - 2022</code> - followed by week numbers - latest at top<ul> <li><code>## Week 48</code> -  followed by daily log of work.<ul> <li><code>28-11 - mo</code> probably add hourly burns</li> <li><code>week review</code> - reflect how your week went, goods and bads, what can be improved, what gave you appreciation. Are you aligning to a wider goal?</li> </ul> </li> </ul> </li> </ul> </li> <li>Notepad - <code>notepad.md</code> the huge notepad where you build your knowledge base, H1s<ul> <li><code># Team DNA - Business &amp; Vision</code> - knowlege base specific to your team, unit. Add business understanding, projects, team hierarchy, team's vision.</li> <li><code># ABC Inc - Business &amp; Vision</code> - build knowledge base aroung your company, what it does, entities, relations, public info, customers, products, business processes.</li> <li><code># Tools/Links/Processes</code> - this has info of all tools, portals, how tos, third parties, databases. All specific to your company.</li> <li>Notes in general - add H1s to have generic notes that are around a technology, but not specific to company. These can later go to your personal knowlege base e.g.<ul> <li><code># Python</code> - all things you learn in python</li> <li><code># Git</code> - all knowlege you gain in git</li> </ul> </li> </ul> </li> </ul> <ul> <li>Writing Rules and basics<ul> <li>Add date where possible.</li> <li> to have todo actions. can be searched.</li> <li> to have action done. - resolution.<ul> <li>or resolution in next line, tabbed like this.</li> </ul> </li> <li>avoid adding h3, h4 to rough notes.</li> </ul> </li> </ul> <ul> <li>How to arrange notes for easy add on and updates and follow ups<ul> <li>Notes should have section of action 'what you want to do'. Eg, create, install, read, write etc.</li> <li>So when you learn something new about an action, you know the section to add to. Similarly, when you have to do an action, you know what to refer to.</li> <li>Keep knowledge at one place, like sqlite to be on its page and other pages can refer to it. Do not repeat on separate pages.</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/material-mkdocs/#links","title":"Links","text":"<ul> <li>Sphinx - Furo - theme</li> <li>Sphinx - Pydata-sphinx-theme</li> <li>Github - Using YAML + Markdown format in documentation comments #878</li> <li>Auto format Markdown Tables</li> </ul>"},{"location":"0-Information-Technology/nativescript-notes/","title":"NativeScript","text":"<p>NativeScript ( or tns Telerik NativeScript) is used to make native iOS and Android apps using Angular/TypeScript or JavaScript.</p>"},{"location":"0-Information-Technology/nativescript-notes/#setting-up-the-environment","title":"Setting up the environment","text":"<p>Required:</p> <ul> <li>node</li> <li>nativescript cli</li> <li>native script cli uses npm</li> </ul> <p>Installation:</p> <ul> <li>do npm install script and then add package</li> </ul> <ul> <li><code>$ sudo npm install nativescript -g</code> to install globally</li> <li><code>nativescript --version</code> to confirm installation</li> <li>tns is alias to nativescript</li> </ul> <p>iOS prerequisites</p> <ul> <li>Xcode</li> <li>Xcode CLT</li> </ul> <p>NS CLI</p> <ul> <li>component</li> <li>module</li> <li>html</li> </ul>"},{"location":"0-Information-Technology/nativescript-notes/#application-architecture","title":"Application Architecture","text":"<p>How the application is architectured.</p>"},{"location":"0-Information-Technology/nativescript-notes/#javascript","title":"JavaScript","text":"<ul> <li>app.ts is the entry point of application. We can do app level initialization here. But this is basically used to pass control to first module.</li> <li>we can have app.css to keep our global css rules.</li> <li>A folder for each module/view<ul> <li>eg: home</li> </ul> </li> <li>home folder can have three files.<ul> <li>home-page.xml having xml for UI</li> <li>home-page.ts code behind file for xml above. Can call functions</li> <li>home-view-model.ts having data, binding and other logics.</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/nativescript-notes/#angular-script","title":"Angular Script","text":"<p>for each module we need:</p> <ul> <li>home.component.html it has all html ui defined</li> <li>home.component.css has css rules related to this module</li> <li>home.component.ts has code behind the html ui</li> <li>home.module.ts imports everything to one place</li> <li>home-routing.module.ts routing for module ui</li> </ul> <p>Component is building block of all angular nativescript</p> <ul> <li>Component defines UI elem and screens</li> <li>root - app.component</li> <li>child - pt_backlog</li> <li>backlog module</li> </ul> <p>Modules</p> <ul> <li>use one module per file as an ES15 standard</li> </ul>"},{"location":"0-Information-Technology/nativescript-notes/#notes","title":"Notes","text":"<ul> <li>Every UI element should be inside a layout, else only last UI element takes whole screen.</li> <li>Making Angular Groceries App<ul> <li>All UI elements come in stack layout in app.component.ts and add their css to css files</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/nativescript-notes/#uninstalling","title":"Uninstalling","text":"<ul> <li><code>$ sudo npm uninstall nativescript -g</code> to uninstall globally</li> </ul> <p>References:</p> <ul> <li>Refer: https://docs.nativescript.org/angular/start/tutorial/ng-chapter-1#11-what-youre-building</li> </ul>"},{"location":"0-Information-Technology/pelican/","title":"Pelican","text":"<p>Pelican is a static site generator in Python.</p>"},{"location":"0-Information-Technology/pelican/#quickstart","title":"Quickstart","text":"<ul> <li>make a dir</li> <li>create virtual env <code>python3 -m venv venv</code></li> <li>upgrade pip <code>./venv/bin/pip install --upgrade pip</code></li> <li>activate env <code>source venv/bin/activate</code></li> <li>install pelican <code>pip install pelican</code></li> <li>create basic files and dir <code>pelican-quickstart</code></li> <li>start server <code>make devserver</code> go to <code>http://localhost:8000/</code></li> <li>It builds the <code>output</code> dir</li> </ul>"},{"location":"0-Information-Technology/pelican/#structure","title":"Structure","text":"<ul> <li><code>content</code> holds all contents,<ul> <li>articles can be created as <code>.md .html or .rst</code></li> <li><code>content/pages</code> dir can have pages</li> <li><code>content/downloads</code> can serve static content for pages</li> <li><code>{static}/downloads/logo.jpg</code> copies file to output and links to path</li> <li>Samples can be found on docs and eg site</li> </ul> </li> </ul> <ul> <li><code>output</code> has all the static pages built using content and theme.</li> </ul>"},{"location":"0-Information-Technology/pelican/#adding-theme","title":"Adding Theme","text":"<ul> <li>Make a themes dir anywhere</li> <li>add this dir to <code>pelicanconf.py</code> as <code>THEME = 'themes_dir/my_theme</code></li> <li><code>my_theme</code> should have two folders<ul> <li><code>static</code> all your css, js and images can go here, copied as-is to output</li> <li><code>templates</code> templates for various mandatory pages<ul> <li><code>base.html</code> starting point, links <code>href=\"{{ SITEURL }}/theme/css/my.css\"&gt;</code></li> <li><code>page.html</code> imports base and adds content for pages</li> <li><code>index.html</code> lists all blog articles</li> <li>other pages are mandatory like article, author, tag etc can be copied from here.</li> </ul> </li> </ul> </li> </ul>"},{"location":"0-Information-Technology/pelican/#dev-sprints","title":"Dev Sprints","text":"<ul> <li>Go to folder</li> <li>activate env <code>source venv/bin/activate</code></li> <li>start server <code>make devserver</code> go to <code>http://localhost:8000/</code></li> <li>this will launch the site and will autoupdate site on page modifications.</li> </ul>"},{"location":"0-Information-Technology/pelican/#production-push","title":"Production Push","text":"<ul> <li>Delete the output dir</li> <li>run <code>make devserver</code> go to <code>http://localhost:8000/</code> verify all changes</li> <li>then make an archive of <code>output</code> folder and upload to your host.</li> <li>for GCP please follow steps here.</li> </ul>"},{"location":"0-Information-Technology/pelican/#tips-and-tricks","title":"Tips and Tricks","text":"<ul> <li>to replace blog index page, add following metas to any new <code>homepage.html</code></li> </ul> <pre><code>  &lt;meta name=\"save_as\" content=\"index.html\" /&gt;\n  &lt;meta name=\"url\" content=\"index.html\" /&gt;\n</code></pre> <ul> <li>sorting menu items<ul> <li>in <code>pelicanconf.py</code> add <code>PAGE_ORDER_BY = 'reversed-date'</code></li> <li>then in pages meta add <code>&lt;meta name=\"date\" content=\"2020-07-26 07:00\" /&gt;</code></li> <li>you can then change time to sort pages in menu bar.</li> </ul> </li> </ul> <p>Minimum html required for pages:</p> <pre><code>&lt;html&gt;\n    &lt;head&gt;\n        &lt;title&gt;My super title&lt;/title&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        This is the content of my super blog post.\n    &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>To do:</p> <ul> <li>exclude some pages from menu</li> <li>add articles</li> </ul>"},{"location":"0-Information-Technology/plotly-dash/","title":"Plotly and Dash","text":"<p>plotly js python dash and flask</p>"},{"location":"0-Information-Technology/plotly-dash/#plotly-express","title":"Plotly Express","text":"<p>High level api for common charts</p> <p>Pie - Plotly - Python Pie Charts</p> <pre><code>fig = px.pie(df, values='id', names='response_type', title='Response Type',\n                 hole=0.3, template=my_template)\n</code></pre> <p>Bar</p> <pre><code>fig = px.bar(df_graph, x='response_type', y = 'count', title = period, color='response_type',\n                 template='none', width=1000, height=500)\n</code></pre>"},{"location":"0-Information-Technology/plotly-dash/#plotly","title":"Plotly","text":"<ul> <li>poltly built on top of D3</li> <li>plotly python api uses plotly.js</li> </ul> <p>Plotly Library:</p> <ul> <li>data - result of go.chartType(x=, y=, others=....)</li> <li>layout - title, axis, annotations<ul> <li>has param <code>updatemenus</code></li> <li>There are four possible update methods:<ul> <li>\"restyle\": modify data or data attributes</li> <li>\"relayout\": modify layout attributes</li> <li>\"update\": modify data and layout attributes</li> <li>\"animate\": start or pause an animation</li> </ul> </li> </ul> </li> <li>frames: used for animations<ul> <li>we can add different frames to a chart</li> <li>this can be used to produce the animations</li> <li>Example can be found here.</li> </ul> </li> <li>figure - final object combining data and layout.</li> </ul>"},{"location":"0-Information-Technology/plotly-dash/#plotly-and-flask","title":"Plotly and Flask","text":"<ul> <li>Flask <code>dash()</code> route has a html form to show filters</li> <li>JS on form change, <code>POST</code>s to a <code>callback</code> url with form data and csrf.</li> <li><code>callback()</code> sends form data, filters-json to <code>get_graph</code> and returns figure-json.</li> <li><code>get_graph()</code> reads data, filters, builds figure, returns json.</li> </ul> <pre><code>@bp.route('/dash', methods=[\"GET\", \"POST\"])\ndef dash():\n    header = \"Global Temperature\"\n    subheader = \"Global Temperature changes over the last few centuries\"\n    description = \"\"\"The graph shows the increase in temperature year on year.\n    The data spans the years 1881 to 2022 and includes temperature anomalies for periods of each year as indicated.\n    \"\"\"\n    menu_label = \"Select a period\"\n    params = {\n        'title': header,\n        'subtitle': subheader,\n        'content' : description,\n        'menu_label': menu_label,\n        'options' : [{'code':'50', 'desc':'Latest Week'},\n                     {'code':'48','desc':'Week 48'},\n                     {'code':'49','desc':'Week 49'},\n                     {'code':'50','desc':'Week 50'},\n                     {'code':'0','desc':'All'}],\n        'graph'   : get_graph()\n    }\n    return render_template('viz/dash.html', params=params)\n\n@bp.route('/callback', methods=['POST'])\ndef callback():\n    \"\"\"Ajax Request Handler for Charts\n    Reads filter json\n    Calls get_graph with filters\n\n    Returns:\n        json: figure json\n    \"\"\"\n    if request.is_json:\n\n        data = request.get_json()\n        # print(f\"{list(data.keys())}\")\n\n        # do something with the incoming data and return the appropiate data\n        return get_graph(filter=data['dropdown'])\n\n    else:\n        return jsonify({\"error\": \"Invalid JSON data\"}), 400\n\ndef get_graph(filter = '50'):\n    import pandas as pd\n    import plotly.express as px\n\n    df = pd.read_csv(...)\n\n    df_filtered = df[ df['dim1'] == filter ]\n\n    fig = px.bar(df_filtered, x='dim2', y='measure1')\n\n    graphJSON = fig.to_json()\n\n    return json.dumps(graphJSON)\n</code></pre> <p>HTML Template</p> <pre><code>&lt;form&gt;...&lt;/form&gt;\n&lt;div id=\"graph\"&gt;&lt;/div&gt;\n\n{% block scripts %}\n{{ super() }}\n&lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/plotly.js/2.27.1/plotly.min.js\"&gt;&lt;/script&gt;\n&lt;script&gt;\n  function getFormValues(f) {\n      const form = document.forms.namedItem(f);\n      const formData = new FormData(form);\n      const value = Object.fromEntries(formData.entries());\n      postJSON(value);\n  }\n\n  async function postJSON(data) {\n      var csrf_token = \"{{ csrf_token() }}\";\n      try {\n          const response = await fetch(\"{{ url_for('viz.callback') }}\", {\n              method: \"POST\",\n              headers: {\n                  \"Content-Type\": \"application/json\",\n                  \"X-CSRFToken\": csrf_token\n              },\n              body: JSON.stringify(data),\n          });\n\n          const result = await response.json();\n          console.log(\"Success:\");//, result);\n\n          drawGraph(result);\n      }\n      catch (error) {\n          console.error(\"Error:\", error);\n      }\n  }\n\n  function drawGraph(graph) {\n      var figure = JSON.parse(graph);\n      // console.log(figure);\n      Plotly.newPlot(id='graph', figure, {});\n  }\n\n  getFormValues('form1');\n\n&lt;/script&gt;\n{% endblock %}\n</code></pre>"},{"location":"0-Information-Technology/plotly-dash/#sub-plots","title":"Sub Plots","text":"<p>You can add graphs to one figure using <code>subplots</code>.</p> <p>More on Plotly - Python Api Reference Plotly</p>"},{"location":"0-Information-Technology/plotly-dash/#multi-plots","title":"Multi Plots","text":"<p>Often the requirement is to plot more than one graphs. Use below logic for same:</p> <ul> <li>Build each fig in <code>get_graph()</code>.</li> <li>Create a list, <code>graphsJSON</code> and append <code>fig.to_json()</code> for each figure.</li> <li>Now you can return this list of json of figures to the callback. In JS you can iterate over this list and build graphs.</li> <li>In JS,<ul> <li>Iterate over list</li> <li>Create <code>div</code>s for each figure and draw plot.</li> </ul> </li> <li>Json handling remains the same</li> </ul> <pre><code>def get_graph(period = '50'):\n    ...\n\n    fig1 = px.bar(...)\n    fig2 = px.pie(...)\n    fig3 = px.pie(...)\n\n    figures = [fig1, fig2, fig3]\n\n    graphsJSON = []\n\n    for fig in figures:\n        graphsJSON.append(fig.to_json())\n\n    return graphsJSON\n</code></pre> <pre><code>function drawGraph(graphsJSON) {\n    const container = document.querySelector('#dash');\n    for (let i = 0; i &lt; graphsJSON.length; i++) {\n        graphJSON=graphsJSON[i];\n        let element = document.createElement('div');\n        element.id='graph'+i;\n        container.appendChild(element);\n        var figure = JSON.parse(graphJSON);\n        Plotly.newPlot(id='graph'+i, figure, {});\n    }\n}\n</code></pre>"},{"location":"0-Information-Technology/plotly-dash/#theme-and-templates","title":"Theme and Templates","text":"<ul> <li>More on Plotly - Python Templates</li> <li>and on Geeksforgeeks - Python Plotly How To Set Up A Color Palette</li> </ul>"},{"location":"0-Information-Technology/plotly-dash/#dash-and-flask","title":"Dash and Flask","text":"<p>Integration Philosophy - Things should be as decoupled as possible, that is, database models should work standalone without flask or dash, that means as python module. Flask should enter only when route has to render template. Dash should only enter when a route has to show dashboard. So, every thing is Python, then all pages are flask, then all dashboards are Dash.</p> <p>AnnMarieW does eveything dash. even login pages, which is adding complexity.</p> <p>Dash as main app prevents using flask capability.</p> <p>So, build Flask app and withing that have Dash dashboard.</p> <p>Best example is of okomarov.</p>"},{"location":"0-Information-Technology/plotly-dash/#dash","title":"Dash","text":"<p>Dash is putting and linking many plotly charts together.</p> <ul> <li>Dash is Python framework for building analytical web applications.</li> <li>It is built on Flask, Plotly.js and React.js</li> <li>Just like flask, we define <code>app = dash.Dash()</code> and then at end <code>app.run_server()</code></li> <li>We can create complete site with links.</li> <li>It has intractable story.</li> </ul> <p>Adding controls</p> <pre><code># Add controls to build the interaction\n@callback(\n    Output(component_id='controls-and-graph', component_property='figure'),\n    Input(component_id='controls-and-radio-item', component_property='value')\n) # decorator ends here and below is its function\ndef update_graph(col_chosen):\n    fig = px.histogram(df, x='continent', y=col_chosen, histfunc='avg')\n    return fig\n</code></pre> <p>Here, <code>component_property</code> of the <code>Input</code> is <code>col_chosen</code> arg to <code>update_graph()</code> function. So whenever the <code>value</code> is changed in radio-button, <code>update_graph()</code> is triggered, it builds the histogram with updated value and returns the figure.</p>"},{"location":"0-Information-Technology/plotly-dash/#chart-studio","title":"Chart Studio","text":"<ul> <li>is like Tableau web edit and public.</li> <li>Can create and host data, charts and dashboards.</li> <li>can explore other people's work.</li> <li>charts are interactable and linked together.</li> <li>can be reverse engineered.</li> <li>can host notebooks as well.</li> </ul>"},{"location":"0-Information-Technology/plotly-dash/#links","title":"Links","text":"<ul> <li>How and why I used Plotly (instead of D3)</li> <li>4 interactive Sankey diagrams made in Python</li> <li>https://hackersandslackers.com/plotly-dash-with-flask/</li> <li>https://github.com/AnnMarieW/dash-multi-page-app-demos - moves everything to dash including core service features like login.</li> </ul>"},{"location":"0-Information-Technology/pwa/","title":"PWA with Flask","text":"<p>making PWA and Flask work best togther</p>"},{"location":"0-Information-Technology/pwa/#pwa-notes","title":"PWA Notes","text":"<p>As John Price explained in his article, How to Turn Your Website into a PWA, the advantages of a PWA are:</p> <ul> <li>Offline capable</li> <li>Installable</li> <li>Improved performance</li> <li>App-like experience</li> <li>Push notifications</li> <li>Discoverable</li> <li>Linkable</li> <li>Always up to date</li> <li>Safe</li> <li>Cost-effective</li> </ul> <p>PWA Requirements</p> <p>In order to convert a web application to a PWA, there are three main requirements.</p> <ul> <li>Run it over HTTPS.</li> <li>For Installation: Create and serve a manifest file in JSON format.</li> <li>For Offline and more: Create and serve a JavaScript file to be registered as a service worker file.</li> </ul>"},{"location":"0-Information-Technology/pwa/#manifest","title":"Manifest","text":"<p>Web App Manifest Generator to create <code>manifest.json</code>.</p>"},{"location":"0-Information-Technology/pwa/#service-worker","title":"Service Worker","text":"<p>Now depending on your use-case you will expand you <code>serviceWorker.js</code> to add more listeners, opening the cache, activating the cache, and adding/fetching URLs and responses to/from the cache and handling offline state.</p>"},{"location":"0-Information-Technology/pwa/#notifications","title":"Notifications","text":"<p>It does not require Service-Worker. There are two main things to notification:</p> <ul> <li>Is supported?</li> <li>Have permission?</li> </ul> <p>If both are <code>True</code> you can send notifications.</p>"},{"location":"0-Information-Technology/pwa/#push-api","title":"Push API","text":"<p>Push notifications extend the notifications and help push notification from server to user. Each client installation gets a <code>push-endpoint</code> which device vendor gives.</p> <p>It requires service worker and public key from server and much more to push.</p> <pre><code>Eg:\nhttps://fcm.googleapis.com/fcm/send/ctqg6w78iYE:APA9...srGoQ\nhttps://web.push.apple.com/fjthuwerh53iv4mn...dshnfu4\n</code></pre>"},{"location":"0-Information-Technology/pwa/#flask-and-pwa","title":"Flask and PWA","text":"<p>For a Progressive Web App in Flask you need a minimum of 3 files:</p> <ul> <li>Static - A service worker, this must contain a fetch event listener.</li> <li>Static - A complete manifest.json file in your root web directory.</li> <li>Static - A pwa-app.js to register service worker on document load.</li> <li>Route - A landing page (index page) to have pwa-app.js and manifest. May be static as well.</li> </ul> <p>Your app must be served with HTTPS.</p>"},{"location":"0-Information-Technology/pwa/#handon-flask-pwa-by-uwi-info3180","title":"Handon flask-pwa by uwi-info3180","text":"<p>Link https://github.com/uwi-info3180/flask-pwa</p>"},{"location":"0-Information-Technology/pwa/#service-worker_1","title":"Service Worker","text":"<p>This is based on the First Progressive Web App Tutorial by Google.</p> <p>The files are cached (stored in browser on device) to make the app-shell work offline. App Shell is basic app, may be just landing page. This may include caching the html, javascript, css, images and any other files important to the operation of the application shell.</p> <p>Non JS App - It is good to avoid your index page as app shell specially if it contains new information, because it may show cached page. Instead you can catch a landing page and an offline page. So landing page is assumed to not change frequently with data. It can show instantly form cache and the other offline page can show when the user is offline and is requesting a n/w reasource.</p> <p>In <code>sw.js</code> (service worker), the cache has a name, <code>cacheName</code> and a list of files to cache, <code>filesToCache</code>. Note: <code>sw.js</code> is not cached.</p> <p>Working Offline: On install, the files from list are cached with the name in <code>cacheName</code>. In order to get the web app working offline, the browser needs to be able to respond to network requests and choose where to route them. Use event <code>fetch</code>.</p> <p>Updating Cache: To update the cache, on sw activation, we check if the cacheName in sw and cache stored name are same or not. If not, the files are updated, else used from the cache.</p> <p>The name is like <code>projectname-v1</code>. In new release the version can be changed in then name.</p> <p>Now whenever the app is started, it will request the files from the network. The sw will use cache and will not do network request. If any of the file is not in cache, it can be pulled from network. If no network then it will display an offline page.</p> <p>You have control at each of these steps using sw. You can specify what to cache, you can use cache or not. You can specify when to update cache. You can specify what to do when offline, you use cache and show old content, or show offine page.</p> <p>Case Offline: Show basic app, with an optional 'You are offline' banner and for any n/w reqeust show your custom offline page.</p> <p>Case Online: Show basic app from cache, fast. Show rest of pages via n/w fetch.</p> <p>Case Update: Update the cache, show optional 'Updating' banner.</p> <p>So sw does following, in <code>sw.js</code>, github link:</p> <pre><code>const cacheName = 'pwa-with-flask-v1';\n\nconst filesToCache = [\n    '/',\n    '/static/pwa-app.js',\n    '/static/my.css',\n    '/offline.html',\n    '/static/images/pwa-light.png'\n];\n\nself.addEventListener('install', (e) =&gt; {\n    // When the 'install' event is fired we will cache\n    // the html, javascript, css, images and any other files important\n    // to the operation of the application shell\n});\n\nself.addEventListener('activate', (e) =&gt; {\n    // We then listen for the service worker to be activated/started.\n    // Once it is, ensures that your service worker updates its cache\n    // whenever any of the app shell files change.\n    // In order for this to work, you'd need to increment the cacheName variable\n    // at the top of this service worker file.\n});\n\nself.addEventListener('fetch', (e) =&gt; {\n    // Serve the app shell from the cache\n    // If the file is not in the cache then try to get it via the network.\n    // otherwise give an error and display an offline page\n    // This is a just basic example, a better solution is to use the\n    // Service Worker Precache module https://github.com/GoogleChromeLabs/sw-precache\n});\n</code></pre>"},{"location":"0-Information-Technology/pwa/#responses-from-server","title":"Responses from Server","text":"<p>In each response sent from server, its header tells browser not to cache. Github link</p> <pre><code>...\n\n@app.route('/service-worker.js')\ndef sw():\n    return app.send_static_file('service-worker.js')\n\n...\n\n@app.after_request\ndef add_header(response):\n    \"\"\"\n    Add headers to tell the browser not to cache the rendered page. If we wanted\n    to we could change max-age to 600 seconds which would be 10 minutes.\n    \"\"\"\n    response.headers['Cache-Control'] = 'public, max-age=0'\n    return response\n</code></pre>"},{"location":"0-Information-Technology/pwa/#manifest-json","title":"Manifest JSON","text":"<p>It has metadata like namme, short name, icons, id, start_url, app_protocol, orientations, theme_color. github link</p>"},{"location":"0-Information-Technology/pwa/#pwa-register-js-pwa-appjs","title":"PWA Register JS - pwa-app.js","text":"<p>This will have few things related to PWA. github link</p> <p>Register Service Worker (Mandatory) - This will tell browser to register the sw.</p> <pre><code>navigator.serviceWorker.register('/sw.js')\n</code></pre> <p>Display Network Banner - Use online and offline event listener to show/hide the banner.</p> <p>Install to Home Screen Buttons - A2HS Button lets user install app to home screen on one click. This is implemented differently.</p>"},{"location":"0-Information-Technology/pwa/#layout","title":"Layout","text":"<p>Add following not let user zoom in out and make it feel like app. In <code>templates/layout.html</code></p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0\"&gt;\n    &lt;meta name=\"theme-color\" content=\"#34A5DA\"&gt;\n    &lt;meta name=\"description\" content=\"A demo flask progressive web app\"&gt;\n    &lt;link rel=\"icon\" sizes=\"192x192\" href=\"{{ url_for('static', filename='images/android-launchericon-192-192.png') }}\"&gt;\n    &lt;link rel=\"manifest\" href=\"{{ url_for('static', filename='manifest.json') }}\" /&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div class=\"offline-notification\"&gt;No Network!&lt;/div&gt;\n    &lt;header&gt;&lt;/header&gt;\n    &lt;main&gt;&lt;/main&gt;\n    &lt;footer&gt;&lt;/footer&gt;\n\n    &lt;script type=\"module\" src=\"{{ url_for('static', filename='app.js') }}\"&gt;&lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"0-Information-Technology/pwa/#handson-dzone","title":"Handson Dzone","text":"<p>Understanding Dzone - Flask Game as PWA</p> <p>The manifest.json and serviceworker.js files are placed in the static folder, from which Flask serves public assets without requiring server-side routing.</p>"},{"location":"0-Information-Technology/pwa/#pwa-register-js-pwa-appjs_1","title":"PWA Register JS - pwa-app.js","text":"<p>This has code to register the service worker when app loads</p> <pre><code>if ('serviceWorker' in navigator) {\n    window.addEventListener('load', function() {\n        navigator.serviceWorker\n        .register('/wtgw/static/serviceworker.js', {scope: '/wtgw/'})\n        .then(function(registration) {\n            console.log('ServiceWorker registration successful with scope: ', registration.scope);\n        }, function(err) {\n            console.log('ServiceWorker registration failed: ', err);\n        });\n    });\n}\n</code></pre>"},{"location":"0-Information-Technology/pwa/#service-worker_2","title":"Service Worker","text":"<p>Very similar to uwi-info3180. Github link. A cacheName and fileToCache var.</p> <p>In this case, SW checks if request is to a specific URL, which is new word, it goes to server, else, it is tried from cache, if not, it is fetched and added to cache.</p> <p>In <code>activate</code> event, where cache is activated, user has list of caches to be kept. [ ] Why?</p>"},{"location":"0-Information-Technology/pwa/#responses-from-server_1","title":"Responses from Server","text":"<p>Nothing specific in this case. No response header, no sw as route.</p>"},{"location":"0-Information-Technology/pwa/#handson-flask-pwa-by-umluizlima","title":"Handson flask-pwa by umluizlima","text":"<p>Link Github umluizlima flask-pwa</p> <p>This one uses workbox by chrome, which is ready to use sw modules.</p>"},{"location":"0-Information-Technology/pwa/#pwa-blueprint","title":"PWA Blueprint","text":"<p>It server two files which make them location agnostic. Alos you can set no-caching</p> <pre><code>from flask import (\n    Blueprint, make_response, send_from_directory\n)\n\nbp = Blueprint('pwa', __name__, url_prefix='')\n\n\n@bp.route('/manifest.json')\ndef manifest():\n    return send_from_directory('static', 'manifest.json')\n\n\n@bp.route('/sw.js')\ndef service_worker():\n    response = make_response(send_from_directory('static', 'sw.js'))\n    response.headers['Cache-Control'] = 'no-cache'\n    return response\n</code></pre>"},{"location":"0-Information-Technology/pwa/#service-worker_3","title":"Service worker","text":"<p>It uses workbox for caching, updating and installing.</p>"},{"location":"0-Information-Technology/pwa/#handson-conclusion","title":"Handson Conclusion","text":"<p>Use logic of umluizlima as it has blueprint and simplicity Use SW of uwi-info3180</p>"},{"location":"0-Information-Technology/pwa/#links","title":"Links","text":"<ul> <li>WebDev article and codelabs</li> <li>Mozilla Game Build codelab</li> <li>Web.dev PWA</li> <li>Handon - Flipkart</li> <li>Notifications - Mozilla</li> <li>pwafire.org</li> <li>tool - realfavicongenerator.net</li> <li>what pwa can do today</li> <li>Github IO PWA Bug Fix Gist</li> <li>Github IO PWA Guide</li> <li>Github CM Minimal Demo</li> </ul>"},{"location":"0-Information-Technology/python-notes/","title":"Python","text":"<p>all learnings here, keep one file for easier updates</p>"},{"location":"0-Information-Technology/python-notes/#python-interpreters","title":"Python Interpreters","text":"<ul> <li>there may be many python version and interpreter installed on your machine. eg, <code>/bin/python3</code> or <code>~/anaconda/bin/python</code></li> <li>identify current the default, <code>which python</code></li> <li>find all installed binaries, <code>locate \"bin/python\"</code> - this lists all the binaries including venvs.</li> <li>to set anaconda python as default python, add following to <code>~/.bashrc</code><ul> <li><code>export PATH=\"/home/username/anaconda3/bin:$PATH\"</code></li> <li>then restart terminal or do <code>source ~/.bashrc</code></li> <li>check with <code>which python</code>, should be using anaconda.</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/python-notes/#virtual-environments","title":"Virtual Environments","text":"<ul> <li>Why use virtual environment - It gives different apps isolation and personal environment so that modules don't interfere and it is easy when we have to productionize the app.</li> </ul> <ul> <li>What is Python Virtual Environment  <ul> <li>Basically a folder having python core binaries and use this new installation to install libraries that will be specific to this installation (or environment).</li> <li>It is an isolated environment</li> <li>When you activate a virtual environment, your PATH variable is changed. The Scripts directory of <code>venv_app</code> is put in front of everything else, effectively overriding all the system-wide Python software.</li> </ul> </li> </ul> <ul> <li>Create<ul> <li><code>python -m venv venv_app</code> - creates folder venv_app</li> </ul> </li> </ul> <ul> <li>Usage<ul> <li><code>source venv_app/bin/activate</code> - activates this environment</li> <li><code>&gt;venv_app\\Scripts\\activate</code> in Windows</li> <li>check using <code>python -V</code> or <code>which python</code></li> <li>now do <code>python -m pip install &lt;package-name&gt;</code> - this installs the package only in this environment.</li> <li>add <code>#!venv_app/bin/python</code> on top of main.py file to make it use this python.</li> <li>python or python3 depends on your installation.</li> </ul> </li> </ul> <ul> <li>Deactivate<ul> <li><code>deactivate</code> to deactivate the current env.</li> </ul> </li> </ul> <ul> <li>View &amp; Share<ul> <li><code>python -m pip list</code> to see installed libs.</li> <li><code>python3 -m pip freeze</code> to share libs.<ul> <li><code>python3 -m pip freeze &gt; requiremeents.txt</code> to make a file.</li> <li><code>python3 -m pip install -r requirements.txt</code> to install all libs from file</li> </ul> </li> </ul> </li> </ul> <ul> <li>Delete<ul> <li><code>rm -r venv_app</code> delete</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/python-notes/#offline-virtual-environment-setup","title":"Offline Virtual Environment Setup","text":"<ul> <li>On machine with internet<ul> <li><code>pip download -r requirements.txt -d path/pip-packages</code> downloads requirements and its dependencies to a folder <code>path/pip-packages</code></li> </ul> </li> </ul> <ul> <li>On machine without internet</li> <li><code>python -m venv venv</code> create virtual environment</li> <li><code>venv\\Scripts\\activate</code> activate environment</li> <li><code>pip install -r requirements.txt --find-links=pip-packages --no-index</code> install requirements<ul> <li><code>--no-index</code> tells to not use any repository like pypi</li> <li><code>--find-links</code> tells a path to find all packages, or <code>-f</code></li> </ul> </li> <li><code>flask --app app:create_app('testing') run</code></li> </ul> <ul> <li>Links<ul> <li>Stackoverflow answer</li> <li>PIP Docs - pip install</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/python-notes/#conda-miniconda-anaconda","title":"Conda Miniconda Anaconda","text":"<p>Conda is package and virtual environment manager, like pip, for any language\u2014Python, R, Ruby and more. It is a CLI and can</p> <ul> <li>install packages like flask, jupyter, pandas etc.</li> <li>can manage envs, virtual environment is separate python and its packages. This means each project you work on can have its own set of packages.</li> </ul> <p>Anaconda is toolkit for Data Science. Along with conda it includes ds and ml libraries (500Mb) installed.</p> <p>Anaconda Navigator is GUI to use conda.</p> <p>Miniconda includes conda and python but not much libraries.</p> <p>So we can use conda alone to create a development virtual environment for new data science project or use miniconda.</p> <p>Installing Conda on Linux</p> <ul> <li><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh</code></li> <li><code>bash Miniconda3-latest-Linux-x86_64.sh</code> this installs python, conda, vevn and others, all in virtual environment.</li> <li>After installation, restart terminal, and it will load new environmnet called 'base'. Now python is not the default system python.<ul> <li>If you'd prefer that conda's base environment not be activated on startup, set the auto_activate_base parameter to false: <code>conda config --set auto_activate_base false</code></li> <li>more: https://www.projectdatascience.com/step-by-step-guide-to-setting-up-a-professional-data-science-environment-on-a-linux/</li> </ul> </li> </ul> <p>Quick Start using Conda for a new project</p> <pre><code>conda deactivate\nmkdir prj1\ncd prj1\nconda create -n prj1env pandas numpy jupyter scikit-learn matplotlib seaborn\nconda activate prj1env\ntouch README.md\ncode .\n</code></pre> <p>Run <code>conda activate prj1env</code> in code shell.</p> <p>Undo <code>conda deactivate &amp;&amp; conda remove --name prj1env --all</code> and remove files if any.</p> <p>Conda Basic Commands</p> <ul> <li><code>conda update conda</code> updates conda</li> <li><code>conda install PACKAGENAME</code> installs pkg to default/active env</li> <li><code>conda update PACKAGENAME</code> updated pkg</li> <li><code>pip install pkg</code> aslo intalls to active env</li> </ul> <p>Conda Environments</p> <p>Environment let you use multiple versions of python</p> <pre><code># list all env\nconda env list\n</code></pre> <ul> <li><code>conda create --name py35 python=3.5</code> created new env called 'py35' and installs py 3.5</li> <li><code>conda activate py35</code> activates</li> <li><code>conda list</code> lists all packages installed in active env.</li> <li><code>conda remove --name my-env --all</code> deletes the env.</li> </ul> <pre><code># Windows Create Python 3.8 env\nconda create -y --name py38 python=3.8\npython -m ensurepip\npython -m pip install --upgrade pip\npython -m pip install virtualenvwrapper-win\nactivate py38\n</code></pre>"},{"location":"0-Information-Technology/python-notes/#python-programming","title":"Python Programming","text":"<ul> <li>with<ul> <li>it is keyword, used to handle unmanaged resources like database and filestreams. It gurantees closing of resources once used.</li> <li>eg, <code>with open(filepath) as f:</code> - do something, and as the block ends, the resource is closed.</li> </ul> </li> </ul> <ul> <li>next(iterable, default)<ul> <li>returns - next item in iterable</li> <li>params<ul> <li><code>default</code> - optional, value to return if end of list is reached.</li> </ul> </li> <li>eg, <code>next(mylist, \"orange\")</code></li> </ul> </li> </ul> <ul> <li>Dictionary - data type<pre><code>for key, value in data_py_dict.items():\n    print(key, \":\", value)\n\ndata_py_dict['some_key'] # also shows data\n</code></pre> </li> </ul> <p>Decorators are a standard feature of the Python language. A common use of decorators is to register functions as handler functions to be invoked when certain events occur.</p> <ul> <li>Global Local Variables and its scope<ul> <li>If you use a var with same name in function (local) and module (global) then you need to take caution</li> <li>when you assign a var in function, python create that local var irrespective of it being present in global context. To use the global var in function:</li> <li>either pass as param, or</li> <li>you can use <code>global</code> keyword before var to let python interpreter know to use global var and not redeclare it.</li> <li>more on this on StackOverflow - UnboundLocalError: local variable referenced before assignment</li> </ul> </li> </ul> <ul> <li>Packages &amp; Modules<ul> <li><code>Package</code> is usually a folder with <code>__init__.py</code> in it.</li> <li>Other python files are <code>modules</code>.</li> </ul> </li> </ul> <ul> <li>Public, Private, Protected in python<ul> <li>public - every member of class in python is public by defaut, can be accessed outside class using object.</li> <li>protected attribute needs to be prefixed with underscore, <code>_name</code>. It can be accessed, just a convension.</li> <li>private members can be <code>__name</code> prefixed with double underscore, this makes them non accessible outside \"directly\". though can be accessed using <code>_Classname__attrname</code></li> <li>Python provides conceptual implementation but not exactly like java or C++. As the underscores tell what is protected and private but does not make them non accessible.</li> <li> <p>To implement a \"write-only\" attribute use \"property\"</p> <ul> <li>getter - use <code>@property</code> decorator with function, <code>def pvt_prop_name(self):</code> raise err in this func so no one can get this property.</li> <li>setter - use <code>@pvt_prop_name.setter</code> with function <code>def pvt_prop_name(self, value):</code> to implement setting some values. You can set value to another public property. Thus this makes <code>pvt_prop_name</code> as write-only.</li> </ul> <pre><code>class Book():\n    scrambled = None # public, can be get or set\n    __safe = None    # pvt, can't be directly get or set\n\n    @property\n    def secret(self):\n        raise AttributeError()\n\n    @secret.setter\n    def secret(self,value):\n        self.scrambled = value[::-1]\n\n    # `secret` is write_only member\n</code></pre> </li> </ul> </li> </ul> <ul> <li>JSON handling in python<ul> <li>JSON is comma separated key-value pairs.</li> <li> <p>Python has most of the things in form of Object which is similar to dictionary. It is similar to JSON but not same. Some differences are None - null, True true.</p> <pre><code>import json \n\n# takes string and returns Python serialized object\ndata_py_dict = data.loads(some_json_as_str)\nwith open(\"data.json\", \"r\") as read_file:\n    data_py_dict = data.load(read_file) # reads from file\n\n# converts py_obj (as dict) to std-JSON-string\nstd_json_string = json.dumps(data_py_dict)\nwith open(\"data.json\", \"w\") as write_file:\n    json.dump(data_py_dict, write_file) # encode dict into JSON\n</code></pre> </li> </ul> </li> </ul>"},{"location":"0-Information-Technology/python-notes/#exception-handling-with-try-except-else-and-finally","title":"Exception handling with try, except, else, and finally","text":"<p>Try: This block will test the excepted error to occur Except:  Here you can handle the error Else: If there is no exception then this block will be executed Finally: Finally block always gets executed either exception is generated or not</p> <pre><code># minimum\ntry:\n  2/0\nexcept:\n  print('err')\n\n# nesting, finally and multiple exception example\ntry:\n  connection = pyodbc.connect(con_uri)\n  cursor = connection.cursor()\n\n  # nesting of try\n  try:\n    cursor.execute(query)\n    n_rows = cursor.rowcount\n    cursor.commit()\n    cursor.close()\n\n  # in case of any exception\n  except Exception as e:\n    cursor.rollback()\n    logger.error(f'sql_execute - Query failed!. Error \"{str(e)}\".')\n\n  # this executes irrestive of exception occuring\n  finally:\n    connection.close()\n\n# Excepting a custom error\nexcept pyodbc.OperationalError as e:\n  logger.error(f'sql_execute - No connection to \"{service}\". Message: \"{str(e)}\"')\n\n# Excepting all other errors\nexcept Exception as e:\n  logger.error(f'sql_execute - No connection to \"{service}\". Message: \"{str(e)}\"')\n</code></pre>"},{"location":"0-Information-Technology/python-notes/#files-handling-in-python","title":"Files Handling in python","text":"<p><code>f  = open(filename, mode)</code> Where the following mode is supported:</p> <ul> <li>r: open an existing file for a read operation.</li> <li>w: open an existing file for a write operation. If the file already contains some data then it will be overridden but if the file is not present then it creates the file as well.</li> <li>a:  open an existing file for append operation. It won\u2019t override existing data. creates if not exists.</li> <li>r+:  To read and write data into the file. The previous data in the file will be overridden.</li> <li>w+: To write and read data. It will override existing data.</li> <li>a+: To append and read data from the file. It won\u2019t override existing data.</li> </ul> <pre><code>## read\nf = open('notes/abc.txt', 'r') # returns handle to file\nf.readline() # returns one line, keep using for next lines\n\ncontent = f.read() # returns whole file, arg is size. Not efficient.\nline_list = f.readlines() # read all lines as list, each list item is a line. Not efficient.\n\nf.close()\n\n# peak large file\nwith open(filepath) as f:\n    head = [next(f) for _ in range(10)]\nprint(head)\n\n\n## write\nfile = open('note.txt','a')\nfile.write(\"quick brown\")\nfile.write(\"munde, not fox\")\nfile.close()\n\n# if exists\nos.makedirs(pdf_dir, exist_ok=True)\n</code></pre>"},{"location":"0-Information-Technology/python-notes/#snippets-file-handling","title":"Snippets File Handling","text":"<p>Find all files of a type in a dir recursively</p> <pre><code>def find_images(dir_path):\n    \"\"\"finds all jpg,png in given path\n\n    Args:\n        dir_path (str): path to search in\n\n    Returns:\n        list: files list\n    \"\"\"\n\n    files = []\n    for root, dirs, files in os.walk(dir_path):\n        for file in files:\n            if file.lower().endswith('.jpg') or file.lower().endswith('.png'):\n                files.append(os.path.join(root, file))\n\n    print(f\"{len(files)} images found.\")\n\n    return files\n\nfiles = find_images(\"/home/user/project\")\n</code></pre> <p>Non-recursive replace <code>_</code> with <code>-</code> in file names</p> <pre><code>[os.rename(f, f.replace('_', '-')) for f in os.listdir('.') if not f.startswith('.')]\n</code></pre> <p>Replace <code>_</code> with <code>-</code> in filenames recursively</p> <pre><code>directory = '.'\nfor subdir, dirs, files in os.walk(directory):\n  for filename in files: # !! use dirs instead of files to rename dirs first\n    new_filename = filename.replace('_','-')\n    subdirectory_path = os.path.relpath(subdir, directory) #get the path to your subdirectory\n    file_path = os.path.join(subdirectory_path, filename) #get the path to your file\n    new_file_path = os.path.join(subdirectory_path, new_filename) #create the new name\n    # os.rename(file_path, new_file_path) #rename your file\n    print(file_path, new_file_path) #rename your file\n</code></pre> <ul> <li> <p>Handling CSV and JSON</p> <ul> <li> <p>use python std <code>csv</code> n <code>json</code> library for data handling with file.</p> <pre><code># pandas iter rows\ndef CSVToJson():\n  df=pd.read_CSV('/home/paulcrickard/data.CSV')\n  for i,r in df.iterrows():\n  print(r['name'])\n  df.to_JSON('fromAirflow.JSON',orient='records')\n</code></pre> </li> </ul> </li> </ul> <ul> <li> <p>zip a folder</p> <pre><code>import zipfile, os\ndir_to_zip = 'path to dir'\n\ndef zipdir(path, ziph):\n    # ziph is zipfile handle\n    for root, dirs, files in os.walk(path):\n        for file in files:\n            ziph.write(os.path.join(root, file), \n                      os.path.relpath(os.path.join(root, file), \n                                      os.path.join(path, '..')))\n\nwith zipfile.ZipFile(dir_to_zip+'.zip', 'w', zipfile.ZIP_DEFLATED) as zipf:\n    zipdir(dir_to_zip, zipf)\n</code></pre> </li> </ul>"},{"location":"0-Information-Technology/python-notes/#logging-in-python","title":"Logging in Python","text":"<ul> <li>Logging is done to keep record of events of software. They can be logged to console or a file or emailed.</li> <li><code>level</code> is set to filter logs, default is <code>warning</code> so anything below it is not logged.</li> <li><code>basicConfig()</code> set in one module works for all modules used in a session. You can pass<ul> <li><code>level</code> to filter</li> <li><code>filename</code> to save logs</li> <li><code>format</code> - what to log, it can have, log level, message, time, location etc.<ul> <li>all format strings here</li> </ul> </li> <li><code>datefmt</code> to set date format of <code>asctime</code></li> </ul> </li> </ul> <ul> <li>When to use What<ul> <li><code>print()</code> is good for small script to display on console. Else use logging.</li> <li><code>raise exception</code> when run time error occurs, that need to stop software.</li> <li><code>logging.exception() or logging.error() or logging.critical()</code> when error is to be logged and application can continue</li> <li> <p>Following table show when to use which level of logging</p> Level When it is used DEBUG detailed info, typically for problem analysis INFO confirmation that event is working as expected WARNING default. unexpected behaviour, but system will work. Eg, low space ERROR serious problem, that software has not performed an action CRITICAL serious error that may bring system down </li> </ul> </li> </ul> <ul> <li>Advanced<ul> <li>loggers, handlers, filters, and formatters are components that can be used to have control on the functionality.<ul> <li>loggers- these have name, like SqlAlchemy logger, <code>werkzeug</code> logger, app.logger etc. Each of these can be formatted and added to handler.</li> <li>handles - these are places to log to, eg, file, stream (stdout), email etc.</li> </ul> </li> <li>more here on PythonDocs - Advanced Logging</li> <li>Configuring Logging - configs can be in code, in file or in dictionary.</li> </ul> </li> </ul> <pre><code>import logging\n\n## Very Basic\nlogging.warning('Watch out!')  # will print a message to the console\nlogging.info('I told you so')  # will not print anything as it is below default level\n\n## To a file\nlogging.basicConfig(filename='example.log',level=logging.DEBUG)\nlogging.debug('This message should go to the log file')\n# DEBUG:root:This message should go to the log file\n\n## Formatting\nlogging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.DEBUG, datefmt='%m/%d/%Y %I:%M:%S %p')\nlogging.debug('Something is happenning')\n# 02/16/2023 01:50:17 PM - root - DEBUG - Something is happenning\n\n\n## New filename for each run\nimport os, time\nfrom time import localtime\n\nbasedir = os.path.abspath(os.path.dirname(__file__))\n\nlog_dir = os.path.join(basedir, 'logs')\nparent_process_id=os.getppid()\nprocess_id=os.getpid()\nlog_time=time.strftime('%Y%m%d_%H%M', localtime())\nlog_filename=str(log_time)+'_app_'+str(parent_process_id)+'_'+str(process_id)+'.log'\nLOG_FILE_PATH = os.path.join(log_dir, log_filename)\n\nlogging.basicConfig(level=logging.DEBUG, \\\n                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', \\\n                    datefmt='%Y/%m/%d %H:%M:%S %p', \\\n                    filename=LOG_FILE_PATH, \\\n                    filemode='w')\n\nlogging.info(\"New Working directory is: \" + str(os.getcwd()))\n\n# or use RotatingFileHandler\n\n\"\"\"Using Components. Using Both File and Stream Handlers\"\"\"\n\nlogger = logging.getLogger()\n\nformatter = logging.Formatter('[%(asctime)s] %(levelname)s in %(name)s.%(module)s: %(message)s')\n\n# Setup file handler\nfhandler  = logging.FileHandler('my.log')\nfhandler.setLevel(logging.DEBUG)\nfhandler.setFormatter(formatter)\n\n# Configure stream handler for the cells\nchandler = logging.StreamHandler()\nchandler.setLevel(logging.DEBUG)\nchandler.setFormatter(formatter)\n\n# Add both handlers\nlogger.addHandler(fhandler)\nlogger.addHandler(chandler)\nlogger.setLevel(logging.DEBUG)\n\n# Show the handlers\nlogger.handlers\n\n# Log Something\nlogger.info(\"Test info\")\nlogger.debug(\"Test debug\")\nlogger.error(\"Test error\")\n</code></pre> <p>Links</p> <ul> <li> RealPython - Logging in Python</li> <li>PythonDocs - When to use what level</li> <li>TD.io - logging in python</li> </ul>"},{"location":"0-Information-Technology/python-notes/#datetime-and-time-in-python","title":"Datetime and Time in Python","text":"<p><code>datetime</code> and <code>time</code> are separate python packages</p> <pre><code>import datetime\nimport time\n# they are different\n</code></pre> <p>Datetime package</p> <p><code>datetime</code> has most methods to work with date and time, some sub modules are. more datetime docs</p> <pre><code>import datetime\n\n# modules\ndatetime.datetime\ndatetime.date     # date specific methods\ndatetime.time     # time specific methods\n</code></pre>"},{"location":"0-Information-Technology/python-notes/#datetimedatetime-as-dt-module","title":"Datetime.datetime as dt module","text":"<p>Mostly can handle date and time. eg geek4geek</p> <ul> <li>DateTime as String - <code>time.strftime('%Y-%m-%d %H:%M:%S')</code></li> </ul> <pre><code>from datetime import datetime as dt\n\nmy_dt_obj = dt(2001, 12, 9, 14, 28, 55)\nmy_dt_obj\n# obj: datetime.datetime(2001, 12, 9, 14, 28, 55)\n\nmy_dt_obj.hour\n# int: 14\n\nstart = dt.now() \n# obj: datetime.datetime(2022, 10, 20, 8, 14, 44, 277307)\n# it is datetime object, not a string\n\ndt.now().ctime() \n# str: Thu Oct 20 08:16:51 2022\n\nend = dt.now()\nend - start \n# obj: datetime.timedelta(seconds=11, microseconds=129035)\n\ndelta = (end - start).seconds \n# int: 11\n\n# Str to Obj\ndt.strptime('2023-22-03 12:28:11', \"%Y-%d-%m %H:%M:%S\")\n# obj: datetime.datetime(2023, 3, 22, 12, 28, 11)\n\n# Obj to Str\ndt.now().strftime('%Y-%m-%d %H:%M:%S')\n# str: '2023-10-27 17:43:23'\n</code></pre> <p>date time formats - w3 school</p> <p>TimeZone Hell</p> <pre><code>dt.now()\n# obj: datetime.datetime(2023, 10, 27, 17, 33, 58, 183305)\n# This is GMT + 01:00 hrs\n\ndt.utcnow()\n# obj: datetime.datetime(2023, 10, 27, 16, 34, 7, 908839)\n# This is GMT\n</code></pre> <p>GMT (Greenwich Mean Time) or UTC (Universal Time Coordinated) is same. This is time at 00:00 timezone or at 0\u00b0 latitude (basically -7.5\u00b0 to 7.5\u00b0). Now depending where your server is or your client is, there their time will be different depending on the timezone set in the system. This makes difficult to store the time in database in one timezone, show in another and then include daylight difference, and show timezone save by one time zone to another timezone and things like person moving in timezones, welcome to timezone hell.</p> <p>Best way to save datetime in database, is to save time at UTC in format, this makes datetime on server, location independent and all time are in one timezone UTC.</p> <p>To show on browser, send UTC from server and covert on browser using JS or lib like moment.js</p> <p>To store datetime of event from browser, pick UTC time from server.</p> <p>To store datetime enterd by user, also get Timezone of browser and then convert it to UTC on server and store it.</p>"},{"location":"0-Information-Technology/python-notes/#timezone-hell-to-heaven","title":"Timezone Hell to Heaven","text":"<p>Firstly, know the international standards. IANA time zone names are international standard timezone names that are same in Javascript and Python. Eg, <code>\"Asia/Kolkata\"</code></p> <p>IST, BST is not standard as not standard, IST is Indian/Israel/Irish Standard time. BST is British or Bangladesh? It is not defined.</p> <p>All IANA Timezones</p> <pre><code># to show all timezones\nimport pytz\npytz.all_timezones\n</code></pre> <p>Get Client Timezone</p> <pre><code>const tz = Intl.DateTimeFormat().resolvedOptions().timeZone;\nconsole.log(tz);\n// 'Europe/London'\n</code></pre>"},{"location":"0-Information-Technology/python-notes/#python-timezones","title":"Python timezones","text":"<p>In Python, datetime object can be aware or naive. aware has timezone info, naive does not.</p> <p>Timezone info is stared in class <code>class datetime.tzinfo</code>, this is None for naive object.</p> <p>Naive Datetime Object</p> <pre><code># Naive object, not timezone aware\n\ndt_str = '15 April 2024 | 10:45:02'\ndt_obj = dt.strptime(dt_str, \"%d %B %Y | %H:%M:%S\")\n\ndt_obj\n# datetime.datetime(2024, 4, 15, 10, 45, 2)\n\nprint(dt_obj)\n# 2024-04-15 10:45:02\n\ndt_obj.tzinfo      # this is, None\n#\n</code></pre> <p>Now lets add timezone info to this. Eg, this is gym entry time of a client. You can get client tz using JS, lets say it is <code>\"Asia/Kolkata\"</code> which is 'UTC +05:30'.</p> <p>To add tzinfo and make it tz-aware, you can use <code>pytz</code> module.</p> <p>Add Timezone Info</p> <pre><code>import pytz\n\n# Add time zone to naive object\nclient_tz = pytz.timezone(\"Asia/Kolkata\")\nclient_db_obj = dt_obj.replace(tzinfo=client_tz)\nprint(client_db_obj)\n# 2024-04-15 10:45:02+05:53\n</code></pre> <p>Now the client_db_obj is tz aware, you can convert it to any timezone.</p> <p>Convert to Timezones</p> <pre><code>print(client_db_obj.astimezone())  # local system timezone time, I am in London BST +1hr UTC\n# 2024-04-15 05:52:02+01:00\n\nprint(client_db_obj.astimezone(pytz.utc))   # to store in db, UTC 00\n# 2024-04-15 04:52:02+00:00\n\nprint(client_db_obj.astimezone(pytz.timezone(\"US/Pacific\")))\n# 2024-04-14 21:52:02-07:00\n</code></pre> <p>Create TZ Aware Object</p> <pre><code>dt_str = '15 April 2024 | 10:45:02 +0530'\ndt_obj = dt.strptime(dt_str, \"%d %B %Y | %H:%M:%S %z\")\n\nprint(dt_obj)\n# 2024-04-15 10:45:02+05:30\n\nprint(dt_obj.astimezone())    # my localtime, London BST +1hr UTC\n# 2024-04-15 06:15:02+01:00\n\nprint(dt_obj.astimezone(pytz.utc))\n# 2024-04-15 05:15:02+00:00\n\nprint(dt_obj.astimezone(pytz.timezone(\"US/Pacific\")))\n# 2024-04-14 22:15:02-07:00\n\nprint(dt_obj.astimezone(pytz.timezone(\"Asia/Kolkata\")))\n# 2024-04-15 10:45:02+05:30\n</code></pre> <p>ISO Formats</p> <pre><code>dt_str = '2024-04-15T08:12:48Z'\ndt.strptime(dt_str, \"%Y-%m-%dT%H:%M:%S%z\")\n# datetime.datetime(2024, 4, 15, 8, 12, 48, tzinfo=datetime.timezone.utc)\n\ndt_str = '2024-04-15T08:12:48+0530'\ndt.strptime(dt_str, \"%Y-%m-%dT%H:%M:%S%z\")\n# datetime.datetime(2024, 4, 15, 8, 12, 48, tzinfo=datetime.timezone(datetime.timedelta(seconds=19800)))\n\ndt_str = '2024-04-15T08:12:48+0530'\ndt.strptime(dt_str, \"%Y-%m-%dT%H:%M:%S%z\").isoformat()\n# '2024-04-15T08:12:48+05:30'\n</code></pre> <p>This is ISO 8601 standard. Here, <code>Z</code> in string is parsed at UTC. and <code>\u00b1hhmm</code> at end is where timezone offset is parsed, if specified as in the second example.</p> <p>Links: wiki ISO 8601</p> <p>Date Parser</p> <p>When unsure of format, but confident that it would be common format and correct  you may use</p> <pre><code>from dateutil import parser\nparser.parse(dt_str)\n</code></pre> <p>Exceptions</p> <pre><code>dt_str = '15 April 2024 | 10:45 UTC'\n\ndt_obj = dt.strptime(dt_str, \"%d %B %Y | %H:%M %Z\")\nprint(dt_obj)\n# '%Z' only takes local timezone, GMT or UTC. Noting else.\n# for someone in India it will only accept, IST/GMT/UTC\n</code></pre> <p>Link:</p> <ul> <li>Timezone Hell</li> <li>SO - JS timezone</li> <li>Wiki - Abbr are duplicates</li> <li>SO - py tz play</li> <li>Py Docs - datetime</li> </ul>"},{"location":"0-Information-Technology/python-notes/#timedelta","title":"Timedelta","text":"<pre><code>from datetime import datetime as dt\nfrom datetime import timedelta\n\n# Add 10 minutes to now\ndt.now() + timedelta(minutes=10)\n# obj: datetime.datetime(2023, 10, 27, 17, 41, 50, 848715)\n</code></pre>"},{"location":"0-Information-Technology/python-notes/#snippets-datetime","title":"Snippets Datetime","text":"<pre><code># 1. Get last week start and end\nimport datetime\ntoday = datetime.date.today()\n# my_date = datetime.date(2023,4,6)\nmy_date = today\nstart = my_date - datetime.timedelta(days=my_date.weekday(), weeks=1)\nend = start + datetime.timedelta(days=6)\nprint(start, end)\n</code></pre>"},{"location":"0-Information-Technology/python-notes/#time-package","title":"Time Package","text":"<pre><code>import time\n\ntime.time() # timestamp\n\n# pause program execution\ntime.sleep(2) # sleeps for two seconds\n\ntime.strftime('%Y-%m-%d %H:%M:%S') # '2023-06-12 11:18:06'\n</code></pre>"},{"location":"0-Information-Technology/python-notes/#testing-in-python","title":"Testing in Python","text":"<p>Manual testing is done to check if all functionalities are performed as expected.</p> <p>Now if you change the code you have to check all the functionalities again. To avoid this, you can do Automated Testing.</p> <p>There is <code>test step</code> and <code>test assertion</code>. Eg, step is that button will turn on light, to see that the light is on is assertion.</p> <p>An integration test checks that components in your application operate with each other. Eg, switch turns on light.</p> <p>A unit test checks a small component in your application. Eg, switch, battery, wire, bulb etc.</p> <p>You can check a output against a <code>known output</code></p> <p><code>assert</code> keywork is used to check a logical statement. In case of False, error is raised.</p> <pre><code>def test_sum():\n    assert sum([1, 2, 3]) == 6, \"Should be 6\"\n</code></pre> <p><code>AssertionError</code> is raised in case of failure, else nothing.</p> <p><code>unittest</code> is a test runner, based on JUnit from java. It requires that:</p> <ul> <li>You put your tests into classes as methods</li> <li>:TODO https://realpython.com/python-testing/</li> </ul>"},{"location":"0-Information-Technology/python-notes/#documenting-code-in-python","title":"Documenting Code in Python","text":"<ul> <li>Why - when you revisit after months, it saves time to pick back<ul> <li>when it is public or team work, it helps others contribute</li> </ul> </li> </ul> <ul> <li>Documenting is making it understandable to users, like react-docs</li> <li>Commenting is for developers, to understand why code is written. It can be to understand, reasons, description or<ul> <li>Tagging, <code># todo: some work</code>, <code># bug: fix the bug</code>, <code># FIXME: some fix</code>.</li> </ul> </li> </ul> <ul> <li>Docstrings - these are structured string format. They can be parsed by parser like Sphinx, and can autogenerate documentation from code.<ul> <li>everything in Python is an object. And that object has a property <code>__doc__</code> that stores the docstring that can be printed when using help.</li> <li>you can set this as <code>my_func.__doc__ = \"Some string\"</code></li> <li>or the next line after function in <code>\"\"\"Some string\"\"\"</code> automatically sets the docstring for the function.</li> <li>docstring structures are of three types<ul> <li>Google - google's way (mostly used)</li> <li>reStructured - python style</li> <li>em - same as done in java</li> </ul> </li> </ul> </li> </ul> <ul> <li>Sphinx lets you write documentation using markdown and can auto-generate documentation from docstrings.<ul> <li>Installation - <code>pip install sphinx</code></li> <li>Initialization<ul> <li>In the project root folder, <code>my_project/</code></li> <li><code>sphinx-quickstart docs</code>, creates docs dir <code>my_project/docs</code></li> </ul> </li> <li>Configuration</li> <li>Build - <code>sphinx-build -b html docs/source/ docs/build/html</code></li> </ul> </li> </ul> <ul> <li>Links<ul> <li>TD.io using Sphinx</li> <li>PythonHosted.ORG - Examples</li> <li>RealPython - Doc Guide</li> <li>Sphinx Google Example</li> <li>Shinx Autogen from docstring</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/python-notes/#concurrency-and-parallelism-in-python","title":"Concurrency and Parallelism in Python","text":"<ul> <li>wait is when you have I/O or N/W or processing operation.</li> <li>you can at same time do other things while you wait, concurrency</li> <li>you can also do things simultaneously, parallelism</li> <li>Thread lets break a program into small pieces that execute separately. You can create multiple threads in a program, they all start one after other (not in parallel). This can be faster compared non-thread execution because when a thread waits another starts execution. Hence, it enables continuous execution.</li> <li>Python has slightly different approach for parallelism, because <code>threading</code> module lets create thread but can't execute in parallel, <code>multiprocessing</code> module is similar and enables parallel execution.</li> </ul> <ul> <li>Asyncio is another better way to do tasks concurrently. It lets you perform tasks in non-blocking way using <code>async</code> / <code>await</code> syntax.<ul> <li>Non-blocking means other tasks can exucute while a task is waiting. Synchronous operations suffer to wait and execute in sync.</li> <li><code>aiohttp</code> for non-blocking requests, <code>aiofiles</code> for non- blocking file operations. <code>asyncio</code> is python standard library.</li> </ul> </li> </ul> <ul> <li>Parallelism<ul> <li>it can be done using <code>multiprocessing</code> or <code>concurrent.futures</code> library.</li> <li>it lets distribute compute over multiple processors.</li> </ul> </li> </ul> <ul> <li>CRUX<ul> <li>Threading enable concurrency, execute tasks independently without wait</li> <li>Multiprocessing enables parallelism, execute with more compute power</li> <li>Asyncio enables asynchronous execution, let long running task be handled in a nice way.</li> </ul> </li> </ul> <ul> <li>When to use Multiprocessing or AsyncIO or Threading<ul> <li>When doing compute heavy task use multiprocessing. Eg, heavy math operation, string comparision.</li> <li>Use asyncio or threading when using network, like request response read write.</li> <li>Use both multiprocessing and asyncio tohether when using doing both high compute and n/w task. But, good rule of thumb is to fork a process before you thread(use) asyncio.</li> <li>threads are cheap compared to processes.</li> </ul> </li> </ul> <ul> <li>Link<ul> <li>Tstdriven.io - Concurrency  Parallelism AsyncIO</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/python-notes/#cryptography-encode-encrypt-decrypt-decode","title":"Cryptography - Encode Encrypt Decrypt Decode","text":"<p>Encode/Decode or Transcode or Endec is a method of representing data in a different format to efficiently transfer information through a network or the web and make it consumable by different systems. Eg, same data can be in string format or bytes format. Encoded data has the same meaning as original.</p> <p>Encrypt - Encryption is a process that transforms information to scrambled and meaningless data, and can only be deciphered by using a key (kept safely).It maintains data confidentiality.</p> <p>Hashing is for validating the integrity of content by detecting all modification thereof via obvious changes to the hash output.</p> <p>Obfuscation is used to prevent people from understanding the meaning of something, and is often used with computer code to help prevent successful reverse engineering and/or theft of a product\u2019s functionality.</p> <pre><code># Obscuring\nfrom zlib import compress\nfrom base64 import urlsafe_b64encode as b64e\n\ndef obscure(data: bytes) -&gt; bytes:\n    return b64e(compress(data, 9))\ntxt = 'random'\n\nobscured = obscure(txt.encode())  # bytes\n\n# write\nobs_loc = r\"./obs_random.txt\"\nfile = open(obs_loc,'w')\nfile.write(obscured.decode())\nfile.close()\n\n## One liner obscuring\nb64e(compress('Secret'.encode(), 9)).decode()\n\n\n\n# Unobscuring\nfrom zlib import decompress\nfrom base64 import urlsafe_b64decode as b64d\n\nobs_loc = r\"C:\\code\\repos\\obs_random.txt\"\nf = open(obs_loc, 'r')\n\n# One liner\ntxt2 = decompress(b64d(f.readline().encode())).decode()\n\n# OR, multi use\ndef unobscure(obscured: bytes) -&gt; bytes:\n    return decompress(b64d(obscured))\n\nobscured = f.readline().encode()    # bytes\nunobscured = unobscure(obscured)    # bytes\ntxt2 = unobscured.decode()          # str\n</code></pre> <p>Links:</p> <ul> <li>StackOverflow Ways of security and Py code</li> </ul>"},{"location":"0-Information-Technology/python-notes/#snippets-ways-python","title":"Snippets &amp; Ways Python","text":"<ul> <li>Taking input - <code>msg = str(input(\"Message? : \"))</code></li> <li>use <code>//</code> to divide and return only integer part of division or Quotient</li> <li>you can pass function as an argument to another function.</li> <li>you can define a function in an function. the inner function has access to arguments in outer function.</li> </ul> <p>View Python Shell History</p> <pre><code>import readline; print('\\n'.join([str(readline.get_history_item(i + 1)) for i in range(readline.get_current_history_length())]))\n</code></pre>"},{"location":"0-Information-Technology/python-notes/#web-scraping-selenium","title":"Web Scraping - Selenium","text":"<ul> <li>Selenium is browser automation tool</li> <li>BeautifulSoup is DOM parser</li> <li>Pandas for data handling</li> <li>Both can work together, bs4 is best for extraction while selenium is best for performing actions or interactions.</li> </ul> <ul> <li> <p>Selenium Setup </p> <ul> <li>browser - You need browser installed (Firefox or Chrome)</li> </ul> <ul> <li>driver - you need driver for browser installed and added to bath. Its a binary or exe.<ul> <li>visit <code>https://chromedriver.chromium.org/downloads</code> and download version same as your browser version.</li> <li>unzip and move <code>chromedriver</code> to <code>/usr/local/bin/chromedriver</code></li> </ul> </li> </ul> <ul> <li>python package - you need selenium installed in python<ul> <li><code>python -m pip install selenium</code></li> </ul> </li> </ul> <ul> <li>Install Chromium, ChromeDriver and Selenium on Ubuntu .sh</li> </ul> </li> </ul> <ul> <li>XPATH in Chrome<ul> <li>it is easy to find DOM elems in browser using console. It highlights the element and lets you hit and try.</li> <li>to inspect a xpath in chrome, write <code>$x('//div[@role=\"button\"]')</code>, it finds all elements and returns a list, expand the list the hover to see which element is where on page. then use it in Python.</li> <li>to get any attribute value in python use <code>elem.get_attribute('innerText')</code></li> </ul> </li> </ul> <ul> <li>Python Code Snippets for Web Scraping</li> </ul> <pre><code>import os, time, datetime, json\n\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By # search elem by\n\nfrom bs4 import BeautifulSoup   # to parse DOM\nimport pandas as pd             # to store data structure\n\nimport getpass                  # to take hidden password inputs\nimport pickle\nimport requests\n\nchrome_options = webdriver.ChromeOptions()\n\n# Download drive that is compatible to your chrome installation\ndriver_path = \"C:\\code\\chromedriver_win32\\chromedriver.exe\"\ndriver_path = \"/usr/local/bin/chromedriver\"\n\n# Optional Options and preferences\n\n# print PDF to file\npdf_dir = r\"C:\\code\\path-to-pdfs\"\n\n#change destination to save as pdf and save in required folder\npdf_settings = {\n  \"recentDestinations\": [{\"id\": \"Save as PDF\", \"origin\": \"local\", \"account\": \"\"}],\n  \"selectedDestinationId\": \"Save as PDF\",\n  \"version\": 2\n  }\n\nprefs = {\n  \"credentials_enable_service\": False,\n  \"profile.password_manager_enabled\": False,\n  \"printing.print_preview_sticky_settings.appState\": json.dumps(pdf_settings),\n  \"savefile.default_directory\": pdf_dir\n  }\n\nchrome_options.add_experimental_option(\"prefs\", prefs)\nchrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\nchrome_options.add_experimental_option(\"useAutomationExtension\", False)\n\nchrome_options.add_argument('--kiosk-printing')\n\ndriver = webdriver.Chrome(driver_path, options=chrome_options)\n\nurl_to_scrape = \"https://www.something.com\"\ndriver.get(url_to_scrape) # web page loads\ndriver.implicitly_wait(5)\n\n\n\"\"\" Cookie Handling \"\"\"\ncookies_path = username+\"_cookies.pkl\"\n\n# read pickle\nif(os.path.exists(cookies_path)):\n    cookies = pickle.load(open(cookies_path, \"rb\"))\n    print('Cookie exists')\n\n    # Check logged in by finding insta-home icon\n    driver.get(url_to_scrape)\n    sleep(randint(2,5))\n    # set cookie from pickle\n    try:\n        for cookie in cookies:\n            driver.add_cookie(cookie)\n        print('Cookies added')\n    except e:\n        print('Error adding existing cookie.'+e)\n\n# Save cookie to pickle\npickle.dump( driver.get_cookies() , open(username+\"_cookies.pkl\",\"wb\"))\n\n\"\"\" Doing actions \"\"\"\n\n# Check if a field exists\nxlogin = '//*[@id=\"loginForm\"]/div/div[3]/button'\nxlogin_exists = len(driver.find_elements(By.XPATH, value=xlogin)) != 0\n\n\n# find by name and send keys\nusername_box=driver.find_element(by=By.NAME, value=\"username\")\nusername_box.send_keys(\"some text\")\n\npassword = getpass.getpass('Enter your password:')\n\n# find by x-path and click\ndriver.find_element(By.XPATH,'//*[@id=\"submit-button\"]').click()\n\n# Get attribute\nxpath = '//span[@id=\"ctdat\"]'\nelem = browser.find_element(By.XPATH, value=xpath)\ndate_text = elem.get_attribute('innerText')       # use any JS attribute\n\n# scroll to click, Can not click on a Element: ElementClickInterceptedException\ncheckbox_xpath = f\"/html/body/div[4]/.../div\"\ncheckbox_elem = driver.find_element(By.XPATH,checkbox_xpath)\ndriver.execute_script(\"arguments[0].scrollIntoView(true);\", checkbox_elem)\n\ndriver.back()\ndriver.quit() \n\n# parse HTML\nsoup = BeautifulSoup(driver.page_source)\n\n# find all where\nitems = soup.find_all('li', attrs={'class': 'the-items'})\n\n# Request and Beautiful - Scrapping HashTags\nterm = \"nature\"\nhastag_url = 'https://best-hashtags.com/hashtag/'+term\npage = requests.get(hastag_url)\nsoup = BeautifulSoup(page.content, 'html.parser')\nhashtags = soup.select(\"body &gt; div.wrapper &gt; div.job-description &gt; div &gt; div &gt; div.col-md-8 &gt; div &gt; div &gt; div:nth-child(1) &gt; div:nth-child(4) &gt; p1\")[0].text.strip()\n\n\n# building lists of data\nrows = []\nfor i,item in enumerate(items):\n    row = []\n    row.append(i) # index of elem, can be used later for traversing\n    row.append(item.p.text)\n    row.append(item.find_all('li')[0].text)\n    row.append(item.find_all('li')[1].text)\n    row.append(item.find_all('li')[2].text)\n    row.append(item.h3.text)\n    rows.append(row)\n\n# build DataFrame\ndf = pd.DataFrame(columns=['id','date_','region','strength','source','title'], data=rows)\n</code></pre> <ul> <li>Links<ul> <li>Real Python</li> <li>Kiwidamien Github - WebScraping Beyond Beautiful-soup And Selenium</li> <li>Beautiful Soup 4 Readthedocs - En Latest Index</li> <li>StackOverFlow - PDF printing from Selenium with chrome-driver</li> <li>XPATH - Guide</li> <li>XPATH - text contains</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/python-notes/#data-science-setup","title":"Data Science Setup","text":"<p>Python installed in Ubuntu or Mac should not be used. Instead create a Virtual Environment for it.</p> <p>Virtual Environments can be created using conda or venv module. Each virtual environment has its own Python binary and can have its own independent set of installed Python packages in its site directories. More here.</p> <p>Quick Start on Linux more here.</p> <pre><code>sudo apt update\nsudo apt install python3 python3-dev python3-venv\n\nsudo apt-get install wget\nwget https://bootstrap.pypa.io/get-pip.py\nsudo python3 get-pip.py\n\npip --version\n\ncd your-project\npython3 -m venv env\n\nsource env/bin/activate\n</code></pre> <p>This will create a dir <code>env</code> and will have its own python, python3, pip and pip3. Now you can install any packages and this will not interfere with system.</p> <p>Install Jupyter in the venv. Now that we have an environment (base) you can use it, or create a new. Then</p> <ul> <li><code>pip install jupyter</code></li> <li><code>which jupyter</code> shows <code>/home/vaibhav/code/miniconda3/bin/jupyter</code> it does not effect the system python.</li> <li>it is pkg, same as flask</li> <li><code>jupyter notebook</code> runs a server to server jupyter notebooks at http://localhost:8888/tree</li> </ul>"},{"location":"0-Information-Technology/python-notes/#animation-and-modelling-in-python","title":"Animation and Modelling in Python","text":"<p>VPython GlowScript</p> <ul> <li>can be used to create objects and animate them</li> <li>VPython makes it unusually easy to write programs that generate navigable real-time 3D animations.</li> <li>https://www.glowscript.org/docs/VPythonDocs/videos.html</li> </ul> <p><code>Manim</code></p> <ul> <li>can animate equations and plots</li> <li>https://github.com/3b1b/manim</li> <li>https://www.youtube.com/watch?v=ENMyFGmq5OA</li> </ul> <p>Sage</p> <ul> <li>Allows equation animations and plotting</li> <li>https://www.sagemath.org/download-mac.html</li> </ul> <p><code>Povray</code></p> <ul> <li>The Persistence of Vision RayTracer is a high-quality, Free Software tool for creating stunning three-dimensional graphics. The source code is available for those wanting to do their own ports.</li> <li>http://www.povray.org/</li> </ul> <p>ImageMagick</p> <ul> <li>Create, edit, compose, or convert digital images.</li> <li>It can resize, flip, mirror, rotate, distort, shear and transform images, adjust image colors, apply various special effects, or draw text, lines, polygons, ellipses and <code>B\u00e9zier</code> curves.</li> </ul> <p>EdX</p> <ul> <li>https://learning.edx.org/course/course-v1:CornellX+ENGR2000X+1T2017/home</li> </ul>"},{"location":"0-Information-Technology/python-notes/#packaging-installable-packages","title":"Packaging - Installable Packages","text":"<p>You can make the package installable in an environment so that it can used in any program by simply importing like other packages.</p>"},{"location":"0-Information-Technology/python-notes/#using-setuptools","title":"Using SetupTools","text":"<ul> <li>Install setup tools in venv, <code>python -m pip install --upgrade setuptools</code></li> </ul> <ul> <li> <p>Ensure DIR structure as</p> <pre><code>.\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 Makefile\n\u251c\u2500\u2500 app\n\u2502   \u251c\u2500\u2500 __init__.py         # flask app\n\u2502   \u2514\u2500\u2500 bp\n\u251c\u2500\u2500 config.py\n\u251c\u2500\u2500 docker-compose.yaml\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 readme.md\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 tests\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 test_some.py\n\u2514\u2500\u2500 venv\n    \u2514\u2500\u2500 bin\n</code></pre> </li> </ul> <ul> <li> <p>Create and write to pyproject.toml file</p> <pre><code>[project]\nname = \"my-package\"\nversion = \"1.0.0\"\ndescription = \"My Package for.. this\"\n\ndynamic = [\"dependencies\"]\n\n[tool.setuptools.dynamic]\ndependencies = {file = [\"requirements.txt\"]}\n\n[build-system]\nrequires = [\"setuptools&gt;=65.5.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.setuptools.package-dir]\n\"\" = \"app\"\n\n[tool.setuptools.packages.find]\nwhere = [\n    \"app\",\n]\n</code></pre> </li> </ul> <ul> <li>Install the new package in editable mode, <code>pip install -e .</code></li> </ul> <p>Links:</p> <ul> <li>SetupTools - Quickstart</li> <li>Migule Example</li> </ul>"},{"location":"0-Information-Technology/python-notes/#using-flit_core","title":"Using Flit_Core","text":"<p>Folder Structure</p> <p>Inside a root folder like <code>vy_lib_package</code> you can have following structure</p> <pre><code>.\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 src\n\u2502   \u2514\u2500\u2500 vylib\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 example.py\n\u2502       \u2514\u2500\u2500 utils\n\u2502           \u2514\u2500\u2500 df_util.py\n\u2514\u2500\u2500 tests\n</code></pre> <p>Here, <code>example</code> and <code>df_util</code> are modules in <code>vylib</code> package.</p> <p>This will let you import in following way</p> <pre><code>from vylib import example\nfrom vylib.utils import df_util\n\n# then use any function in module\ndf_util.some_function(abc)\n</code></pre> <p>In <code>pyproject.toml</code></p> <pre><code>[project]\nname = \"vylib\"\nversion = \"1.0.0\"\ndescription = \"Common Utilites by Vaibhav\"\n\n[build-system]\nrequires = [\"flit_core&lt;4\"]\nbuild-backend = \"flit_core.buildapi\"\n</code></pre> <p>Package name should be same as folder name, <code>vylib</code>. This is a convention.</p> <p>You can also have your dependencies here.</p> <p>Install the package in venv or other env using:</p> <pre><code>pip install -e ./vy_lib_package/.\n</code></pre> <p>Baiscally, you need to give path where your <code>toml</code> file is. This will install in editable mode.</p> <p>Links</p> <ul> <li>Packaging Python Projects - Python Docs</li> <li>Example- miguelgrinberg/Flask-Moment</li> </ul>"},{"location":"0-Information-Technology/python-notes/#makefile","title":"Makefile","text":"<p>Make file lets run multiple shell commands easily with one command. It is useful and standard for testing, building resources and tearing down.</p> <p>Eg</p> <pre><code>test:\n  # source .env_test\n  python -m unittest tests.test_namecheckapi\n\nlint:\n  python -m flake8 --max-line-length=120 --ignore=E402,E501,W293,E302,E303,W391 ./app\n</code></pre> <p>Here you have <code>test</code> as phase which has one command. You can run this using</p> <pre><code>make test\n</code></pre> <p>Links:</p> <ul> <li>https://github.com/josephmachado/beginner_de_project/blob/master/Makefile</li> </ul>"},{"location":"0-Information-Technology/python-notes/#links","title":"Links","text":"<ul> <li>Python Coding Kaggle</li> <li>Pandas Kaggle</li> <li>Flask - back end web framework micro</li> <li>Python Official Tutorial</li> <li>pythonbasics.org</li> </ul>"},{"location":"0-Information-Technology/r-language/","title":"R and R-Studio","text":"<p>They are statistical language and ID</p> <ul> <li><code>r</code> or <code>R</code> - R console in terminal</li> <li><code>Rscript my.r</code> - executes file in terminal</li> <li><code>Rscript -e \"getwd()\"</code> - executes cmd in terminal, can quickly install a library.</li> <li><code>R CMD BATCH my.r</code> runs R script and saves output to <code>my.r.Rout</code></li> <li>To make R Script executable like <code>./my.r</code> then:<ul> <li>set permission to 755</li> <li>add correct <code>#!</code> to top of file</li> </ul> </li> </ul> <pre><code>#!/usr/bin/env Rscript\nsayHello &lt;- function(){\n   print('hello')\n}\n\nsayHello()\n</code></pre>"},{"location":"0-Information-Technology/react-notes/","title":"React JS Notes","text":"<p>notes on React JS Library</p>"},{"location":"0-Information-Technology/react-notes/#react-js-overview","title":"React JS Overview","text":"<p>added: 12-07-2022, updated: 26-07-2023</p> <p>What - It is a JS Library, it can be added to HTML page as other JS libraries (like jQuery). It is used to create single-page applications. It uses only one HTML page <code>index.html</code> and then all changes to page and routes are managed strictly by JS events.</p> <p>How - You build DOM elements using JS rather than defining them in HTML. To do this easily, use JSX, which is a format that is combination of HTML and JS and it lets write HTML in JS.</p> <p>Tools Setup:</p> <ul> <li>Build in Browser using https://react.new/</li> <li>Upgrade browser with <code>React Developer Tool</code> browser extension. It shows react components.</li> </ul>"},{"location":"0-Information-Technology/react-notes/#quickstart-traditional-style-use-without-nodejs","title":"Quickstart - Traditional Style - Use Without Node.js","text":"<p>You can use react as other JS library by adding script to HTML. Both React and ReactDOM are available over a CDN. Add following to the body of HTML.</p> <p>This makes <code>React</code> and <code>ReactDOM</code> classes available in JS. <code>React.createElement()</code> function is used to create new DOM elements. <code>ReactDOM.render()</code> function to render elements in DOM.</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n\n&lt;head&gt;\n    &lt;meta charset=\"utf-8\" /&gt;\n    &lt;meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;React without JSX&lt;/title&gt;\n\n    &lt;script src=\"https://unpkg.com/react@18/umd/react.development.js\" crossorigin&gt;&lt;/script&gt;\n    &lt;script src=\"https://unpkg.com/react-dom@18/umd/react-dom.development.js\" crossorigin&gt;&lt;/script&gt;\n\n&lt;/head&gt;\n\n&lt;body&gt;\n    &lt;div id=\"root\"&gt;&lt;/div&gt;\n\n    &lt;script type=\"text/javascript\"&gt;\n\n\n        const element = React.createElement(\n            \"h1\",\n            { className: 'greeting' },\n            \"Welcome User\"\n        );\n\n        const root = ReactDOM.createRoot(document.getElementById('root'));\n        root.render(element);\n\n    &lt;/script&gt;\n&lt;/body&gt;\n\n&lt;/html&gt;\n</code></pre> <p>Here, a DOM element, <code>element</code> is defined using <code>React.createElement</code> which takes <code>createElement(type, props, ...children)</code>. Type is DOM elem type like p, h2, div. props are properties of type. and then children. It can be multiple children, here we are only passing one child, which is text <code>\"Welcome User\"</code>.</p> <p>Next, we define <code>root</code> which is used to build rest of DOM tree. Finally, we render root by adding <code>element</code> to it.</p> <p>You see how HTML is now written in JS. To make it simple, JSX is introduced which lets write HTML and JS together.</p> <p>Adding JSX and Babel</p> <p>Traditionally, HTML is written as string, eg <code>\"&lt;h1&gt;\"</code>. Using JSX, you can write HTML in JS.</p> <p>JSX is not HTML, JSX is JavaScript XML, an extension of JavaScript that allows writing HTML in JS. To execute it, we need a compiler that can convert JSX to JS and HTML, because browsers still expect HTML and JS separately as in traditional way.</p> <p>Babel is a JS compiler. You can insert plane HTML without quotes. Also add variable names with JS code within curly braces. Eg: <code>&lt;p&gt;Hi {name.toUpperCase()}&lt;/p&gt;</code>.</p> <p>Babel converts JSX <code>&lt;p&gt;Hi&lt;/p&gt;</code> to JS <code>React.createElement(\"p\", null, \"Hi\");</code> on the fly.</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n\n&lt;head&gt;\n    &lt;meta charset=\"utf-8\" /&gt;\n    &lt;meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;React JSX&lt;/title&gt;\n\n    &lt;script src=\"https://unpkg.com/react@18/umd/react.development.js\" crossorigin&gt;&lt;/script&gt;\n    &lt;script src=\"https://unpkg.com/react-dom@18/umd/react-dom.development.js\" crossorigin&gt;&lt;/script&gt;\n    &lt;script src=\"https://unpkg.com/@babel/standalone@7.10.3/babel.min.js\" crossorigin&gt;&lt;/script&gt;\n\n&lt;/head&gt;\n\n&lt;body&gt;\n    &lt;div id=\"root\"&gt;&lt;/div&gt;\n\n    &lt;script type=\"text/babel\"&gt; // type is changed from javascript to babel\n\n        let msg = \"Tom\";\n\n        const root = ReactDOM.createRoot(document.getElementById('root'));\n\n        root.render(&lt;p&gt;Hi {msg}&lt;/p&gt;);\n\n    &lt;/script&gt;\n&lt;/body&gt;\n\n&lt;/html&gt;\n</code></pre> <p>Here, first we added babel script to html body. Next, in <code>&lt;script type=\"text/babel\"&gt;</code> type is changed from javascript to babel. This tells comiper to change this before run. Then, in <code>render()</code> you can see how html and js are written together without using  <code>React.createElement()</code> or HTML with quotes.</p> <p>JSX and JS go one in another and can be nested using <code>{}</code>s. When you start JS put in {} , then can again write HTML and again nest JS in {}. Babel can be said it is similar to Jinja in Flask or Blade in Laravel.</p> <p>Conceptually - Using React, HTML can be written in JS, which renders it and adds it to a HTML DOM element, eg <code>root</code>. Now that it is in JS, it can be programmed and be data driven and dynamic. To manage the HTML code we split the UI into independent and reusable <code>Components</code>.</p> <p>This completes the basic. Now that you have seen how react works and done it all in single <code>html</code> file, it's time to \"raise the bar\". In next section you will use <code>node</code> and create react app. Using single html file is okay for prototype but for production you should use node app. Lets GO....!</p>"},{"location":"0-Information-Technology/react-notes/#quickstart-with-nodejs","title":"Quickstart - with Node.js","text":"<p>Node.js creates structure for react project with Babel, file bundling etc. You can start from scratch as react-starter-pack or use an existing project.</p> <p>Create new</p> <pre><code>npx create-react-app app_name\n</code></pre> <p>This, creates a folder, <code>app_name</code> with all required libraries like react, react dom, babel, react scripts etc. NPX is a package in node that lets you run and execute packages without having to install them locally or globally. Now do</p> <pre><code>cd app_name\nnpm start\n</code></pre> <p>This will run a web server using node and serve this app on <code>http://localhost:3000</code> as a dev build.</p> <p>Use Existing Project</p> <p>To run an existing react project, that you have downloaded. It would already have <code>package.json</code>. You need to install all the dependencies locally and that can be done by</p> <pre><code>cd old_app_name\nnpm install\nnpm start\n</code></pre> <p>It uses <code>package.json</code> to install all dependencies. Then starts the web-server.</p> <p>Application Structure</p> <p>The folder above <code>app_name</code>, has following files and folders:</p> <ul> <li><code>package.json</code> - All dependencies can be seen in this file. Also, in <code>package-lock.json</code></li> <li><code>node_modules</code> dir has all dependencies installed by Node.js.</li> <li><code>src</code> is where you code, it has app code. It has all React Magic of JSX.</li> <li><code>public</code> dir has what goes to browser, it is used to serve app. It has index.html and other static files.</li> <li><code>public/manifest.json</code> - it is not react specific. Modern web standard adds it, it provides information about the app in JSON format. It is used by mobile and desktop when we install this app. We can update <code>short_name</code> and <code>name</code> here.</li> <li><code>build</code> dir will be added later when you create a prod build.</li> </ul> <p>Basics of website is <code>index.html</code>, it is the same here, <code>public/index.html</code>. It defines the old giants: html, head, meta tags, title and body.  It has <code>&lt;div id=\"root\"&gt;&lt;/div&gt;</code> which is picked by react and then your GREAT single-page app is built on top of it! React will build DOMs and render it in this \"root\" element. This will be \"root of your tree\" :) .</p> <p>The react magic starts in <code>src/index.js</code>, this picks the root div from html and builds the app. This is where you see new things. <code>index.js</code> is entry point to render app to DOM. <code>public/index.html</code> has <code>&lt;div id=\"root\"&gt;&lt;/div&gt;</code> that is used in <code>index.js</code>. The <code>render()</code> function renders the app and takes JSX tree, as argument, that structures the entire app.</p> <p>What is this new kinda JS code?</p> <p>Any code that is non-JavaScript standard like \"import of file\" and \"JSX\" are converted to standart-JavaScript then they are served on browser. Eg of non-std-js</p> <pre><code>import varName from 'filename.ext'\n// pull any file say image as var\n\nimport \"./styles.css\";\n// add css to head\n</code></pre> <p>The codes above are from starter-code, they are non-std code and are later compiled and transformed to make it standard JS code and execute on browser.</p> <p>JSX uses CamelCase while HTML uses lower case.</p>"},{"location":"0-Information-Technology/react-notes/#third-party-packages","title":"Third Party Packages","text":"<p>As project proceeds, you can make use of other packages that like UI framework BootStrap.</p> <pre><code>npm install bootstrap@5.2.0 react-bootstrap\n</code></pre> <p>Here, the it adds two packages:</p> <ul> <li><code>bootstrap</code>: UI framework.</li> <li><code>react-bootstrap</code>: a React component library wrapper for the bootstrap package.</li> </ul>"},{"location":"0-Information-Technology/react-notes/#react-components","title":"React Components","text":"<p>You know HTML gives you div/p/h1/span standard components. You use them to build structure of page and add your data to it. This builds your website, right?</p> <p>Now, using react, you build custom \"super-powered\" components like:</p> <pre><code>&lt;Header name=\"Sara\" /&gt;\n</code></pre> <p>Imagine, this is your styled header, with all branding and functionality you want and it changes with data you pass.</p> <p>To define it, react lets you define it as a function that accepts parameters or go OOPs style and define it as a class.</p> <pre><code>function Header(props) {\n  return &lt;h1&gt;Hello, {props.name}&lt;/h1&gt;;\n}\n</code></pre> <p>It is that simple...! Isn't it?</p> <p>This lets split whole page in to independent, dynamic, resuable components. Hence, it is eaisier to manage and control them using JS.</p> <p>React Component is a building block of app, one of the UI component (same as a template in Flask). Eg, <code>Header</code>, <code>Sidebar</code>, <code>Content</code> and <code>Footer</code>.</p> <p>Conceptually components are like JavaScript functions. They accept arbitrary inputs (called \u201cprops\u201d) and return React elements (JSX) describing what should appear on the screen. Here is complete code, within <code>src</code> each component is defined in its own file, eg, <code>app.js</code>, <code>header.js</code>.</p> <pre><code>function Header(props) {\n  return &lt;h1&gt;Hello, {props.name}&lt;/h1&gt;;\n}\n\nconst root = ReactDOM.createRoot(document.getElementById('root'));\nroot.render(&lt;Header name=\"Sara\" /&gt;;\n</code></pre> <p><code>export default App</code> means export this component (function) as default <code>App</code> component.</p> <p>So previously we could only render standard tags like <code>&lt;div&gt;</code>, <code>&lt;p&gt;</code> etc. but now can render custom tags like <code>&lt;Header /&gt;</code>, also pass properties (props) to it. Remember, every <code>ComponentFunction</code> follows CamelCase. It should return only one JSX tag like <code>&lt;div&gt;</code>, <code>&lt;p&gt;</code> or <code>&lt;Header /&gt;</code>. To return muliple elements, wrap them in div, and if you want to avoid extra divs added to dom, use <code>&lt;React.Fragment&gt;blah blah&lt;/React.Fragment&gt;</code> or shorthand <code>&lt;&gt; ...</code>.</p> <p>Props make components dynamic. You can use logic in any component, like hiding sidebar when loggin in. Props is basically an <code>JSON object</code> (key-value), its properties can passed in JSX in same way as we pass props in HTML <code>&lt;App name=\"Tom\" yob={1990}&gt;</code>, they are passed where we render a component. It can be recieved as <code>function App(props)</code> or as using ES6 as <code>function App ({name, yob})</code>.</p> <p>A component must return a representation of itself as an HTML element tree.</p> <p>Javascript tricks to use in React</p> <p>To loop lists of elements use <code>map()</code> of <code>Array</code> class. You can map a function to an array, that is, apply a functions to each element of array and return the applied value for each of them.</p> <pre><code>choices = ['Yes', 'No', 'May be']\n\nchoices.map(choice =&gt; {\n  return &lt;li&gt;{choice}&lt;/li&gt;\n});\n</code></pre> <p>The above returns three <code>&lt;li&gt;</code> elements.</p> <p>React required 'key' for every dom element so that it can maintain their state.</p> <p>and include a <code>key</code> attribute with a unique value per element. React requires it to efficiently rerender only part of list.</p> <p>eg, <code>&lt;p key={data.id}&gt;&lt;/p&gt;</code></p> <p>Conditional Logic: you may need to apply if-then or if-then-else when returning DOMs. Eg, if there are post then return post else return 'User has no posts.'. This can be implemented in React using <code>&amp;&amp;</code> (if-then) and <code>?:</code> (if-then-else) operators. Expression on right of &amp;&amp; is executed when left is true. Eg,</p> <pre><code>function Posts() {\n  const posts = [\n    {id: 1, text: 'Hello, world!'},\n    {id: 2, text: 'The Next Post'},\n  ];\n\n  return (\n    &lt;&gt;\n      {posts.length === 0 ?\n        &lt;p&gt;There are no blog posts.&lt;/p&gt;\n      :\n        posts.map(p =&gt; {\n          return (\n            &lt;p key={post.id}&gt;{post.text}&lt;/p&gt;\n          );\n        })\n      }\n    &lt;/&gt;\n  );\n}\n</code></pre> <p>Ways to handle conditional in JSX, JSX Conditional Expressions</p> <p>Good strategy is to build multiple components with each having one purpose. Eg, <code>Header</code> <code>Sidebar</code>. Add <code>ContentArea</code> that can be swapped based on navigation.</p> <p>Components can be nested to build hierarchy. Eg,</p> <pre><code>&lt;Body name=\"Tom\"&gt;\n  &lt;Posts /&gt;\n&lt;/Body&gt;\n</code></pre> <p>If the component is called with children, like above, then a <code>children</code> key is included in <code>props</code> object, which is equal to child components, here <code>&lt;Posts /&gt;</code>.</p> <pre><code>function Body({name, children}) {\n  return (...);\n}\n</code></pre> <p>Class Implementation</p> <p>Component can also be a ES6 Class, that <code>extends React.Component</code> and implements <code>render() {}</code>.</p> <pre><code>class Post extends React.Component {\n  render() {\n    return &lt;h2&gt;Post Title!&lt;/h2&gt;;\n  }\n}\n</code></pre> <p>React-Bootstrap</p> <p>React-Bootstrap provides <code>Container</code> and <code>Stack</code> components to design layout of website.</p> <pre><code>  return (\n    &lt;Container fluid className=\"App\"&gt; // className is HTML class\n      ...  // &lt;-- no changes to JSX content\n    &lt;/Container&gt;\n  );\n</code></pre> <p>Strategy of components</p> <ul> <li>build base components like <code>header</code>, <code>content</code> <code>footer</code>.</li> <li>use them in pages.</li> </ul>"},{"location":"0-Information-Technology/react-notes/#hooks","title":"Hooks","text":"<p>useState Hook</p> <p>useState is a function in React class that can be used to manage state of App.</p> <p><code>useState(arg)</code> arg can be any data type.</p> <p>it returns array having <code>[stateValue, setStateFunc] = setState(\"a Value\")</code> we can use them in app to manage states. We can have multiple states of app, so make multiple instance of this function.</p> <p>Whatever is the variable, the setter works for it, don't think of how this function is defined.</p> <p>useEffect Hook</p> <p>These are like eventListners that can listen to some variable and if there is a change in its state they can do something.</p> <p><code>useEffect(funcDoSomething, [varToListenTo]);</code> when arg2 changes, arg1 is executed. arg2 = [] then only run once, arg2 can have multiple vars to listen to.</p> <p>useReducer Hook can be used to manage state and effect at once.</p> <p>It takes two arguments, arg1 is function to execute on state change, and arg2 is initial state value.</p> <p>It returns, 1 state value, 2 function to change state. Eg, state is counterValue</p> <pre><code>const [counterValue, setCounterValue] = useReducer(\n  (a) =&gt; a++, //\n  0\n);\n</code></pre> <p>ToDo: Need some more understanding here on, how to use reducer to do different action based on param passed?</p>"},{"location":"0-Information-Technology/react-notes/#handling-forms","title":"Handling Forms","text":"<p>added: 14-07-2022, updated: 26-07-2023</p> <p>Uncontrolled Component: <code>useRef</code> we can create instance of this function and attach that to form inputs.</p> <pre><code>const empName = useRef();\n\n&lt;input ref={empName} ...&gt;\n\n// on Submit\n\na = empName.current.value; // GET\nempName.current.value = 'A'; // SET\n</code></pre> <p>We can get and set the values using <code>empName.current.value</code></p> <p>Controlled Component: <code>useState</code> and bind to input tag using <code>onChange</code>:</p> <pre><code>const [empName, setEmpName] = useState(\"\");\n\n&lt;input value={empName} onChange={ (e) =&gt; setEmpName(e.target.value) } ...&gt;\n\n// on Submit\n\na = empName; // GET\nsetEmpName(\"A\"); // SET\n</code></pre> <p>Custom Hooks</p> <p>We can create our own custom hooks that can be reused based on our requirements. They have to start with <code>use...</code>. They instantiate another hook within them. Eg, we can make a custom hook to handle form events like setting default value, getting current value, onChange events, validation etc.</p> <p>Others: formik.org, react-hook-form.com, usehooks.com</p>"},{"location":"0-Information-Technology/react-notes/#routes","title":"Routes","text":"<p>React makes SPA, only one page is served from server. So <code>react-router-dom</code> is library that can be used to manage routes on browser.</p> <p>In <code>index.js</code> or<code>App.js</code> you can create <code>&lt;Route&gt;</code> in <code>&lt;Routes&gt;</code> having <code>path</code> and <code>element</code>.</p> <pre><code>import { App, About, Contact } from \"./App\";\nimport { BrowserRouter, Routes, Route, Navigate } from react-router-dom\";\n\nReactDOM.render(\n  &lt;BrowserRouter&gt;\n    &lt;Routes&gt;\n      &lt;Route path=\"/\" element={&lt;App /&gt;} /&gt;\n      &lt;Route path=\"/about\" element={&lt;About /&gt;} /&gt;\n      &lt;Route path=\"/contact\" element={&lt;Contact /&gt;} /&gt;\n      &lt;Route path=\"*\" element={&lt;Navigate to=\"/\" /&gt;} /&gt;\n    &lt;/Routes&gt;\n  &lt;/BrowserRouter&gt;,\n  document.getElementById(\"root\")\n);\n</code></pre> <p>You can link routes to create hierarcy. Use <code>{ Link }</code> a XHR, to add link to component in DOM. It is like <code>url_for</code> in Flask.</p> <p>Routes with Bootstrap</p> <p>Bootstrap has <code>Nav.Link</code> component that creates nav-item, it has <code>as</code> and <code>to</code> props to work with React-Route's <code>NavLink</code>.</p> <pre><code>import Navbar from \"react-bootstrap/Navbar\";\nimport Nav from \"react-bootstrap/Nav\";\nimport { NavLink } from 'react-router-dom';\n\nexport default function Sidebar() {\n  return (\n    &lt;Navbar sticky=\"top\" className=\"flex-column Sidebar\"&gt;\n      &lt;Nav.Item&gt;\n        &lt;Nav.Link as={NavLink} to=\"/\"&gt;Feed&lt;/Nav.Link&gt;\n      &lt;/Nav.Item&gt;\n      &lt;Nav.Item&gt;\n        &lt;Nav.Link as={NavLink} to=\"/explore\"&gt;Explore&lt;/Nav.Link&gt;\n      &lt;/Nav.Item&gt;\n    &lt;/Navbar&gt;\n  );\n}\n</code></pre> <p>Parameters and Routes</p> <p>To define a route with a dynamic section, the path attribute of the Route component uses a special syntax with a colon prefix:</p> <pre><code>&lt;Route path=\"/user/:username\" element={&lt;UserPage /&gt;} /&gt;\n</code></pre> <p>The component referenced by the element attribute or any of its children can use the <code>useParams()</code> hook function to access the dynamic parameters of the current URL as an object.</p>"},{"location":"0-Information-Technology/react-notes/#development-and-structuring","title":"Development and Structuring","text":"<p>Following steps can help you build a React App:</p> <ol> <li>Plan the UI that you want to build as a wireframe or sketch.</li> <li>Break the UI into top level Components like header, sidebar, content and footer.<ol> <li>Build these components and place in <code>src/components</code></li> </ol> </li> <li>Think of logical pages that use the above components but have different low level componenets. Eg, ProfilePage and FeedPage, both have header, sidebar and content BUT the content will have different sub-components like profile or posts.<ol> <li>Build these components and place in <code>src/pages</code></li> </ol> </li> <li>Build routes for these Pages and update the header.</li> </ol> <p>State - use to set and get data, like responses, session variables and state (loading, error, done)</p> <p>Effect - use to do functionality like - fetch, or any function.</p> <p>If a function returns JSX then it is a react component.</p>"},{"location":"0-Information-Technology/react-notes/#testing","title":"Testing","text":"<p><code>Jest.js</code> is used to write test.</p>"},{"location":"0-Information-Technology/react-notes/#deployment","title":"Deployment","text":"<p>App can be deployed to Netlify.com</p> <p>Do <code>npm run build</code> and then <code>build</code> folder has prod ready code to deploy.</p>"},{"location":"0-Information-Technology/react-notes/#react-way","title":"React Way","text":"<ul> <li>there is a JSX tree which has components</li> <li>components are mostly UI blocks, or transparent utility like Routes, Context.</li> <li><code>Props</code> are variables that you can send to component to make them dynamic or reusable.</li> <li>url parameters are managed in <code>useParam</code> hook</li> <li>data is managed by <code>useState</code></li> <li>any work like, backend call, update state, re-render component is done using <code>useEffect</code>. It has dependencies, which can be param or prop.. so</li> <li>URL param and prop can trigger effect, effect can do calls and set state which re-renders. This is URL param, to trigger effect, to re-render, thus things get dynamic in react-way.</li> <li>if anything in useEffect arg2-dependency-var changes, it executes arg1-function which sets state var, which rerenders components.</li> <li>Eg, in code below, as soon as username changes in url, it updates <code>username</code> which is dependency vriable in <code>useEffect</code> so it runs async, which fetches new user and then sets user, which is re-rendered in component.</li> <li>The better your backend is structured, models, rest, pagination etc. the better it is to use react and bootstrap. So base has to be structured.</li> <li>Never render contents directly to the page with DOM APIs, as this introduces a risk of XSS attacks.</li> </ul> <pre><code>// url: /users/:username\nconst { username } = useParams();\nconst [user, setUser] = useState();\n\nconst api = useApi();\n\nuseEffect(() =&gt; {\n  (async () =&gt; {\n    const response = await api.get('/users/' + username);\n    setUser(response.ok ? response.body : null);\n  })();\n}, [username, api]);\n\nreturn (\n  &lt;p&gt;user.name&lt;/p&gt;\n);\n</code></pre>"},{"location":"0-Information-Technology/react-notes/#extras","title":"Extras","text":"<p>Notice here, that it is not only JS packages but also CSS frameworks like <code>bootstrap</code>. CSS are loaded to HTML using JS line</p> <pre><code>// Load only css file\nimport 'bootstrap/dist/css/bootstrap.min.css';\n\n// Alternatively, load entire library\nimport Container from 'react-bootstrap/Container'\n</code></pre>"},{"location":"0-Information-Technology/react-notes/#resources","title":"Resources","text":"<p>Links</p> <ul> <li>React Tutorial Miguel Grinberg - https://blog.miguelgrinberg.com/post/the-react-mega-tutorial-chapter-2-hello-react</li> <li>LinkedIn Learning - https://www.linkedin.com/learning/react-js-essential-training</li> <li>React Docs - https://reactjs.org/docs/hello-world.html</li> <li>ECMA 6+</li> </ul> <p>Next</p> <ul> <li>ReactNative</li> <li>GraphQL</li> </ul>"},{"location":"0-Information-Technology/text-editors/","title":"Text Editors","text":"<p>If you love coding text editors are your sharp tools. Customizing them to your needs will help you save time and increase productivity.</p>"},{"location":"0-Information-Technology/text-editors/#sublime-text","title":"Sublime Text","text":""},{"location":"0-Information-Technology/text-editors/#how-to-use-python-script-with-key-bindings","title":"How to use Python script with Key Bindings","text":"<p>We will create a sample package that will display time on status bar when we press keys <code>ctrl+shift+c</code>, just for demonstrating.</p> <ul> <li>Create a new python file inside Users Package Library. On Mac with Sublime Text 3 it can be created at: <code>Users/yourname/Library/Application Support/Sublime Text 3/Packages/User/any_name.py</code></li> </ul> <ul> <li>In the python file add following content:</li> </ul> <pre><code>import sublime\nimport sublime_plugin\nimport time\n\nclass MyCustomMessageCommand(sublime_plugin.WindowCommand):\n\n    # Command shows message on Status Bar\n    def run(self):\n        now = time.strftime(\"%c\")\n        message = \"The time is \" + now\n        sublime.status_message(message)\n</code></pre> <ul> <li>Add the key bindings:</li> </ul> <pre><code>[\n  {\n    \"keys\" : [\"ctrl+shift+c\"], \n    \"command\" : \"my_custom_message\" \n  }\n]\n</code></pre> <p>As you can see that the python class name becomes the command name with _ added.</p> <p>Now on pressing <code>ctrl+shift+c</code> you can execute this python file which displays the time on status bar in this case.</p> <p>You can use this feature to unlimited possibilities. I used it to add timestamp to file whenever it was saved.</p> <p>References:</p> <ul> <li>https://forum.sublimetext.com/t/automatically-updated-timestamp/7156/7</li> </ul>"},{"location":"0-Information-Technology/text-editors/#extending-sublime-text-for-markdown-support","title":"Extending Sublime Text for Markdown Support","text":"<p>If you want more syntax highlighting and better preview of what you write then you can extend Sublime Text by installing  package, follow steps below:</p> <ul> <li>Type: <code>Cmd + Shift + P</code> to open package manager.</li> <li>Then type <code>install package</code> and hit enter. This will provide you list of available packages from packagecontrol.io</li> <li>Next when you get dropdown type <code>Markdown</code> and this will list you all markdown related packages.</li> <li>You can select <code>Markdown Editing</code> to install the package. This provides much better highlighting and preview.</li> </ul> <p>I, personally, didn't like it much and was a bit distracting for me. So I removed this package. But you may like it.</p> <p>Removing a package from Sublime Text:</p> <ul> <li>press <code>Cmd + Shift + P</code> and</li> <li>then type <code>remove package</code>. This will give you list of packages installed and</li> <li>next select <code>Markdown Editing</code> to remove it.</li> </ul>"},{"location":"0-Information-Technology/text-editors/#vs-code","title":"VS Code","text":"<p>VS Code is general purpose light weight highly customizable text editor. Supports remote environment editing.</p>"},{"location":"0-Information-Technology/text-editors/#settings","title":"Settings","text":"<p>You can customize settings either via GUI or via modifying JSON <code>ctrl shift p</code> and type <code>preferences</code>. It has three levels:</p> <ul> <li>Workspace is specifically to a folder or project level and overrides all other settings. File is located at <code>.\\.vscode\\settings.json</code> in project folder can have all the modifications. It can be part of your git to keep others in sync.</li> <li>User apply to all the projects of a user. Path is <code>~/.config/Code/User/settings.json</code></li> <li>Global applies to all users on a system.</li> </ul> <p>Sync Settings - VS Code can be logged in using GitHub to start sync settings.</p> <ul> <li>Extensions can be disabled when not in use.</li> <li>open <code>~/.config/Code/User/settings.json</code> to add extension configurations</li> <li>add below codes within the curly braces</li> </ul> <p>Proxy Settings</p> <p>You may be behind a proxy server and need proxy settings to allow http requests to happen. To add proxy to vs code add following key-value in <code>settings.json</code>:</p> <pre><code>\"http.proxy\": \"http://&lt;username&gt;:&lt;password&gt;@&lt;url&gt;:&lt;port&gt;\",\n</code></pre> <p>Env Variables</p> <p>VS Code will autoload the <code>.env</code> file in root of workspace when the folder is opened. You can see this in OUTPUT tab, provided, the following default settings:</p> <pre><code>{\"applyAtShellIntegration\":true,\"applyAtProcessCreation\":true}\n</code></pre>"},{"location":"0-Information-Technology/text-editors/#extensions","title":"Extensions","text":"<p>They are developed to enhance the functionality of VS Code. Each extension has settings that can be added to VS Code JSON settings at all level workspace, user ot global.</p> Ext Desc <code>ms-python.autopep8</code> Python formatter <code>aaron-bond.better-comments</code> Comment formatter with todo, question and alert <code>njpwerner.autodocstring</code> Helps write doc-string <code>usernamehw.errorlens</code> Shows error on the line <code>ms-python.flake8</code> Linter for python <code>esbenp.prettier-vscode</code> -"},{"location":"0-Information-Technology/text-editors/#snippets","title":"Snippets","text":"<ul> <li>what - they are code template with place holders</li> <li>run - there can be invoked with <code>ctrl+space</code> or bound to key</li> <li> <p>create - it can language specific or global. It can be defined in <code>Code/User/snippets/markdown.json</code> to be language specific. Each snippet has a name (key), under which (nested key:value) we define prefix, body and description. prefix triggers and body is inserted. Body can have variables and placeholders: $1, $2 for tab stops, $0 for the final cursor position, and ${1:label}, ${2:another} for placeholders. Placeholders with the same ids are connected. Eg:</p> <pre><code>\"Print to console\": {\n    \"prefix\": \"log\",\n    \"body\": [\n        \"console.log('$1');\",\n        \"$2\"\n    ],\n    \"description\": \"Log output to console\"\n}\n</code></pre> </li> </ul> <ul> <li>more here Code Visualstudio - User defined snippets</li> </ul>"},{"location":"0-Information-Technology/text-editors/#key-bindings","title":"Key Bindings","text":"<p>Find and Select All</p> <ul> <li>Find the string using <code>cmd + F</code></li> <li>Then do, <code>Alt + Return</code> or <code>cmd + Shift + L</code> to select all matches/highlights.</li> </ul> <p>Add New Key Bindings</p> <ul> <li> <p>it can be updated in JSON. <code>ctrl shifp p &gt; shortcuts json</code>. In a list you can add your mappings. Eg:</p> <pre><code>[\n    {\n        \"key\": \"ctrl+alt+s\",\n        \"command\": \"macros.url2mkd\", // ext.command\n    },\n    {\n        \"key\": \"ctrl+alt+d\",\n        \"command\": \"editor.action.insertSnippet\",\n        \"when\": \"editorTextFocus &amp;&amp; editorLangId == 'markdown'\",\n        \"args\": {\n            \"snippet\": \"\\n\\n```${1:python}\\n$0\\n```\\n\", // or name of snippet\n        }\n    },\n]\n</code></pre> </li> </ul>"},{"location":"0-Information-Technology/text-editors/#testing","title":"Testing","text":"<p>VS Code, pulls all test cases and can show test results, passed/failed and let debug the test cases. It helps to run individual case, class or all test cases. Configure by clicking on beaker icon.</p> <p>Click \"Configure\" to specify test framework and test cases filename pattern. Once done, vs code will find and list all test cases.</p>"},{"location":"0-Information-Technology/text-editors/#debugging","title":"Debugging","text":"<p>You need to add <code>.vscode/launch.json</code> with settings to let you debug the python app. This tell vs-code to configure the app based on framework and let debug.</p> <pre><code>{\n  \"version\": \"0.2.0\",\n  \"configurations\": [\n\n    {\n      \"name\": \"Python Debugger: Flask\",                 # Flask\n      \"type\": \"debugpy\",\n      \"request\": \"launch\",\n      \"module\": \"flask\",\n      \"env\": {\n        \"FLASK_APP\": \"app/__init__.py\",\n        \"FLASK_DEBUG\": \"1\"\n      },\n      \"args\": [\"run\", \"--no-debugger\", \"--no-reload\"],\n      \"jinja\": true,\n      \"autoStartBrowser\": false,\n      \"cwd\": \"${workspaceFolder}/src/frontend\"          # optional, you can set a current working dir\n\n    },\n\n    {\n      \"name\": \"Python Debugger: UnitTest\",              # Unit Test\n      \"type\": \"debugpy\",\n      \"request\": \"launch\",\n      \"module\": \"unittest\",\n      \"args\": [\"tests.test_hello\"]               # Replace with test case file or remove to debug all\n    }\n  ]\n\n}\n</code></pre>"},{"location":"0-Information-Technology/text-editors/#ext-flake8-in-vs-code","title":"Ext: Flake8 in VS Code","text":"<p>A linter is a tool to help you improve your code style quality control by displaying warnings and errors. It helps keep the code style and formatting consistent. <code>Pylint</code> and <code>Flake8</code> are popular linters.</p> <p>Linters can be run on code to check formatting error, vs-code ext runs the linter as soon as you open/save the file and it displays error if any. Without ext, you will have to manually run flake8 on code. You can see <code>OUTPUT</code> log that flake8 is run and can see output. The error from this log is shown in <code>PROBLEMS</code> tab. So <code>flake8</code> is installed in the venv</p> <p>Formatting and linting are complementary and have a little overlap. <code>autopep8</code> is good formatter for python.</p> <p>Selecting, enabling and disabling linter is now automatic, install ext to enable linter, open/save file to run linter, uninstall ext to disable linting. No need for explicit settings.</p> <p>Configure - In VS Code settings</p> <pre><code>    \"flake8.args\": [\n        \"--max-line-length=120\",\n        \"--ignore=E402,E501,W293,E302,E303,W391\",\n    ],\n    // E402 - Module level import not at top of file\n    // E501 - Line lengths are recommended to be no greater than 79 characters\n    // W293 - Blank line contains whitespace\n    // E302 - 2 blank lines\n    // E303 - too many blank lines\n    // W391 - blank line at end of file\n</code></pre> <p>Similarly, <code>autopep8</code> if installed as formatter, can be enabled as:</p> <pre><code>    \"[python]\": {\n        \"editor.defaultFormatter\": \"ms-python.autopep8\"\n    },\n</code></pre> <p>Manual Linting</p> <pre><code># install\npython -m pip install flake8\n\n# run\nflake8 path/to/code/to/check.py\n# or\nflake8 path/to/code/\n</code></pre> <p>Links</p> <ul> <li>vs code linting</li> <li>manual linting command line</li> </ul>"},{"location":"0-Information-Technology/text-editors/#ext-better-jinja","title":"Ext: Better Jinja","text":"<p>Better code formatting, highlighting and intellisense for Jinja Templates. From select-language-mode <code>ctrl+k m</code> select <code>jinja-html</code>.</p> <p>Congigure the settings with following for better and auto association of files:</p> <pre><code>// Jinja Settings\n\"files.associations\": {\n    \"*.html\": \"jinja-html\"\n},\n\"emmet.includeLanguages\": {\n    \"jinja2\": \"html\",\n    \"jinja-html\": \"html\",\n},\n\"[jinja]\": {\n    \"editor.defaultFormatter\": \"vscode.html-language-features\",\n    \"editor.detectIndentation\": false,\n    \"editor.tabSize\": 2,\n    \"editor.formatOnSave\": true,\n},\n\"[jinja-html]\": {\n    \"editor.defaultFormatter\": \"vscode.html-language-features\",\n    \"editor.detectIndentation\": false,\n    \"editor.tabSize\": 2,\n    \"editor.formatOnSave\": true,\n},\n</code></pre> <p>Link: StackOverflow - jinja highlighting</p>"},{"location":"0-Information-Technology/text-editors/#ext-macro-in-vs-code","title":"Ext: Macro in VS Code","text":"<p>You can use Macro in VS Code using extention macro-commander</p> <ul> <li>You can run a sequence of vscode commands</li> <li>Run javascript with access to vscode object</li> <li>open terminal and run sequence of commands</li> <li>combine all above</li> </ul> <pre><code>var a=await vscode.env.clipboard.readText();\nawait window.showInformationMessage(`IN : ${a}`);\nb = await new URL(a);\nc = await b.hostname.split('.')\n        .slice(0,-1) // remove domain ext\n        .join(' ')\n        .replaceAll(new RegExp('[.,\\\\\\\\-,\\\\\\\\/,:]','g'),' ')+' -'\n    +b.pathname.replace(/\\\\..+/,'') // remove page extension from path\n     .replaceAll(new RegExp('[.,\\\\\\\\-,\\\\\\\\/,:]','g'),' '); // remove . - / : from path\n\nd = await c.split(' ')\n    .map(function(word) {return (word.charAt(0).toUpperCase() + word.slice(1))})\n    .join(' ');\n\nawait window.showInformationMessage(`OUT: ${d}`);\nawait vscode.env.clipboard.writeText(`[${d}](${a})`)\n</code></pre>"},{"location":"0-Information-Technology/text-editors/#ext-markdown-in-vs-code","title":"Ext: Markdown in VS Code","text":"<p>Markdownlint disable rules:</p> <pre><code>    \"markdownlint.config\": {\n        \"default\": true,\n        \"MD007\": { \"indent\": 4 }\n    }\n</code></pre> <p>cSpell disable code check in Markdown code blocks:</p> <pre><code>    \"cSpell.languageSettings\": [\n        {\n            // use with Markdown files\n            \"languageId\": \"markdown\",\n            // Exclude code inline and multiline both\n            \"ignoreRegExpList\": [\n                \"^\\\\s*```[\\\\s\\\\S]*?^\\\\s*```\",\n                \"\\\\s`[\\\\s\\\\S]*?\\\\s*`\",\n            ]\n        }\n    ],\n</code></pre> <ul> <li>Links<ul> <li>Code Visualstudio - Editing Tips-and-tricks</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/text-editors/#multi-root-workspace","title":"Multi-Root Workspace","text":"<p>open more than one repo, multi-repo</p> <p>Just do, <code>File</code> -&gt; <code>Add Folder to Workspace...</code> then <code>Save Workspace As...</code>.</p> <p>This will make a file, <code>somename.code-workspace</code>. This is a JSON file which contains what folders to include and also serves as workspace <code>settings.json</code>. Here, you can configure setting you want to apply for all folders. Note, this is not in git and is not version controlled.</p> <p>The folders settings.json are now deprioritised as active folder settings, depending on which folder you are working on. Common settings from folder is not considered and is greyed out, eg, zoom level.</p> <p>Link: Vscode Doc on Multi Repo</p>"},{"location":"0-Information-Technology/text-editors/#pycharm","title":"PyCharm","text":"<ul> <li>Keyboard Shortcuts<ul> <li>Back and forth - <code>ctrl-alt-left</code> ro <code>ctrl-alt-right</code></li> </ul> </li> </ul>"},{"location":"0-Information-Technology/tools-frameworks/","title":"Tools and Frameworks","text":"<p>tools, frameworks, libraries, systems, projects that help do IT Engineering work. Landing zone for new topics you learn that have no dedicated file</p> <ul> <li>Kubernetes - is an open-source container orchestration system for automating software deployment, scaling, and management.</li> </ul> <ul> <li>IAC - Infrastructure as a Code<ul> <li>Ansible - It can provision the underlying infrastructure of your environment, virtualized hosts and hypervisors, network devices, and bare metal servers.</li> <li>Terraform - It is used to automate various infrastructure tasks.</li> <li>AWS CloudFormation - lets you create and manage a collection of Amazon Web Services (AWS) resources by provisioning and updating them in a predictable way. more iyv</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/tools-frameworks/#gmail-tricks","title":"Gmail Tricks","text":"<ul> <li><code>is:unread category:primary</code> shows important unread emails</li> <li><code>is:unread -category:updates</code> shows unread but not updates</li> <li>Updates category is not clear, it has updates and primary both</li> </ul>"},{"location":"0-Information-Technology/tools-frameworks/#chat-gpt-tricks","title":"Chat GPT Tricks","text":"<p>Prompt Formula</p> <ul> <li>The better the prompt the better is response and so on.</li> <li>Include following in your prompt<ul> <li>Task <code>mandatory</code> - Write...</li> <li>Context <code>important</code> - the reader is non-technical...</li> <li>Exemplar <code>important</code> - It should include my last...</li> <li>Persona <code>nice to have</code> - you are a male data engineer</li> <li>Format <code>nice to have</code> - an email</li> <li>Tone <code>nice to have</code> - professional</li> </ul> </li> </ul> <ul> <li> <p>Eg:</p> <pre><code>You are a senior product marketing manager at Apple   # persona\nand you have\n\n# context\njust unveiled the latest Apple product in collaboration with Tesla, the\nApple Car, and received 12,000 pre-orders, which is 200% higher than target.\n# context\n\nWrite               # task\nan email            # format\n\nto your boss, Tim Cookie, sharing this positive news.\n\n# exemplar\nThe email should include a read section, project background, why this product came into existence, business results section, quantifiable business metrics, and end with a section thanking the product and engineering teams.\n# exemplar\n\n# tone\nUse clear and concise language and write in a confident yet friendly tone. # tone\n</code></pre> </li> </ul>"},{"location":"0-Information-Technology/virtual-box/","title":"Virtual Box","text":"<p>virtual box, vagrant, setting and networking</p>"},{"location":"0-Information-Technology/virtual-box/#enable-ssh-access-to-vm-from-remote","title":"Enable SSH access to VM from remote","text":"<p>guide to let you use vm via SSH</p> <p>Install ssh server on virtual machine</p> <p>The machine you need to connect to should have ssh installed and enabled so that it can allow remotes to connect to it.</p> <p>On ubuntu server or desktop</p> <pre><code># install ssh\nsudo apt install openssh-server\n\n# view status\nsudo systemctl status ssh\n\n# Ubuntu ships with a firewall configuration tool called UFW\nsudo ufw allow ssh\n\n# get IP address, something like 10.0.2.15\nip a\n</code></pre> <p>Default SSH port is 22. So now your VM will listen too this port.</p> <p>Configure Virtual Box Network</p> <p>VirtualBox creates a Network Address Translation (NAT) adapter for VMs. This allows VM to access the internet but prevents other devices from accessing it via SSH. To configure the network, you need to use VirtualBox port forwarding on the default NAT adapter your VM is attached to.</p> <p>Click on <code>Virtual Machine &gt; Settings &gt; Network &gt; Adapter 1 &gt; Advanced &gt; Port Forwarding</code>. Next, Add a Port Forwarding Rule. Click on the Plus (+) icon under the Port Forwarding Rules page. Give your rule a meaningful name (for example \"SSH port forwarding\"). Use the default protocol i.e. TCP. The host IP will be 127.0.0.1 or simply localhost and use 2222 as the Host Port.</p> <pre><code>Name: SSH port forwarding\nProtocol: TCP\nHost IP: 127.0.0.1\nHost Port: 2222\nGuest IP: 10.0.2.15\nGuest Port: 22\n</code></pre> <p>Finally, press the Ok button. You can read more in detail on enable network port forwarding on virtual box page.</p> <p>Another option that work directly without port forwarding is by using \"Bridged Network\" as network on virtual machine. This gives VM an IP on same LAN network as your host is, so if your host is on <code>192.168.1.10</code> then VM may get <code>192.168.1.15</code>.</p> <p>Connecting from Remote</p> <p>On remote to connect via ssh</p> <pre><code>ssh -p 2222 gues_vm_username@127.0.0.1\n</code></pre> <p>VM Web Server Access to Host</p> <p>Similarly you can add another port-forwarding for <code>80:8080</code> to let website hosted on VM be accessed from your host.</p> <pre><code>Name: Web Server port forwarding\nProtocol: TCP\nHost IP: 127.0.0.1\nHost Port: 8080\nGuest IP: 10.0.2.15\nGuest Port: 80\n</code></pre> <p>Access webhost using: <code>127.0.0.1:8080</code></p> <p>How does port forwarding work?</p> <p>When you do ssh on <code>127.0.0.1:2222</code> it is forwarded to <code>10.0.2.15:22</code> which lets the connection happen. Both IP are on separate network hence you cannot directly do <code>user@10.0.2.15:22</code> directly from host.</p> <p>Links</p> <ul> <li>https://www.makeuseof.com/how-to-ssh-into-virtualbox-ubuntu/</li> <li>https://linuxize.com/post/how-to-enable-ssh-on-ubuntu-20-04/?utm_content=cmp-true</li> <li>Nakivo - VM Network Settings</li> </ul>"},{"location":"0-Information-Technology/virtual-box/#command-line-control-of-virtual-box","title":"Command Line Control of Virtual Box","text":"<p>Ubuntu on Windows Virtual Box SSH</p> <pre><code># Start Ubuntu\n\"C:\\Program Files\\Oracle\\VirtualBox\\VBoxManage.exe\" startvm \"ubuntu22\" --type headless\n\n# SSH from Host\nssh -p 2222 vaibhav@127.0.0.1\n\n# Off Ubuntu\n\"C:\\Program Files\\Oracle\\VirtualBox\\VBoxManage.exe\" controlvm \"ubuntu22\" poweroff\n</code></pre>"},{"location":"0-Information-Technology/virtual-box/#vm-jupyter-access-on-host","title":"VM Jupyter access on Host","text":"<p>how to access jupyter on virtual box form your host</p> <p>You should be able to do SSH to VM from host. Once SSHed, do</p> <pre><code># In ubuntu or its shell\njupyter notebook --no-browser --port=9299\n</code></pre> <p>this starts jupyter in VM on <code>localhost:9299</code>. Now to access this from host:</p> <pre><code># SSH from host and link another port\nssh -p 2222 -L 9299:localhost:9299 vaibhav@127.0.0.1\n</code></pre> <p>Here, The -L option is used to bind a port on the local machine with a remote port at the remote destination IP address. Format is <code>ssh -L local_port:remote_destination:remote_port user@ssh_server</code></p> <p>You can read more on SSH tunnels and access guide.</p>"},{"location":"0-Information-Technology/virtual-box/#vagrant","title":"Vagrant","text":"<p>Vagrant is a CLI to create virtual box with all configurations in a file. </p> <ul> <li>Install vagrant by downloading from site. It installs package with CLI.</li> <li>Create following file in any folder, say <code>~/vagrant/vagrantfile</code>:</li> </ul> <pre><code>Vagrant.configure(\"2\") do |config|\n  config.vm.box = \"ubuntu/xenial64\"\n  config.vm.network \"private_network\", ip: \"192.168.33.10\"\n  config.vm.provider \"virtualbox\" do |vb|\n    vb.memory = \"1024\"\n  end\nend\n</code></pre> <ul> <li>Then run <code>vagrant up</code>. On first run it will download and install ubuntu 16.04, 1GB, at 192.168.33.10. In subsequent run it will just start the VM.</li> <li>do, <code>vagrant ssh</code> to ssh to new vm.</li> <li><code>vagrant halt</code> to stop a VM</li> <li>DANGER ZONE: <code>vagrant destroy</code> to delete a VM</li> </ul>"},{"location":"0-Information-Technology/web-server-config/","title":"Web Server Config","text":"<p>Web server can be a linux server that takes client requests and provides a response. It can host a web app. Web app can be backed by a programming language that and process data and store this data in  a database.</p> <p>In this article we would be covering all that is listed in contents below. It is assumed that you have a machine with Ubuntu server installed. It can be on cloud like GCP or AWS etc, or it can be on a virtual machine via Vagrant and Virtual Box.</p> <ul> <li>Do not remove this line (it will not be displayed) {:toc}</li> </ul>"},{"location":"0-Information-Technology/web-server-config/#ubuntu-server-configuration","title":"Ubuntu Server Configuration","text":"<p>Now that you are connected to host VM via SSH, let's start with:</p> <ul> <li>Updating Ubuntu: <code>sudo apt update &amp;&amp; sudo apt upgrade</code></li> <li>Check ram and CPU usage <code>htop</code> , we see that we do not have swap memory, this helps in low ram system when on high load.</li> </ul> <p>Adding the swap memory:</p> <ul> <li>Lets allocate 1GB swap memory: <code>sudo fallocate -l 1G /swapfile</code></li> <li><code>sudo dd if=/dev/zero of=/swapfile bs=1024 count=1048576</code></li> <li>Assign the correct permission to swapfile: <code>sudo chmod 600 /swapfile</code></li> <li>make the swap: <code>sudo mkswap /swapfile</code></li> <li>Turn on the swapfile: <code>sudo swapon /swapfile</code></li> <li>edit the fstab file: <code>sudo nano /etc/fstab</code></li> <li>add this line to the end of file: <code>/swapfile swap swap defaults 0 0</code></li> <li>mount the files: <code>sudo mount -a</code></li> <li>Check ram and cpu again to verify swap: <code>htop</code></li> </ul>"},{"location":"0-Information-Technology/web-server-config/#install-apache2-php-and-mysql-using-lamp","title":"Install Apache2 PHP and MySQL using LAMP","text":"<p>Install the softwares using below commands:</p> <ul> <li><code>sudo apt install tasksel</code></li> <li><code>sudo tasksel install lamp-server</code></li> <li><code>sudo apt install php-curl php-gd php-mbstring php-xml php-xmlrpc</code></li> <li>Get the external IP address: <code>curl ifconfig.me</code></li> <li>Type http://123.456.789.10 , the external IP address in browser to see served page.</li> </ul>"},{"location":"0-Information-Technology/web-server-config/#domain-configuration","title":"Domain Configuration","text":"<p>Now we need to point the domain to this webserver so that we can access files server. We will use <code>example123.com</code> in this example.</p> <p>Only if you do not have a domain to point to server and want to use vhost:</p> <ul> <li>Modify the host file to add virtual hosts: <code>sudo nano /etc/hosts</code></li> <li>To map 'example123.com' tp IP address add this line: <code>35.111.00.111 example123.com</code></li> </ul> <p>If you have a domain,</p> <ul> <li>edit DNS records and add 'A-record' with the external IP address, for eg:</li> <li>host: *</li> <li>IP: 35.111.00.111</li> <li>then, <code>example123.com</code> will open the page from GCP VM machine.</li> <li>You may also reserve static external IP address of VM on GCP:<ul> <li>From your GCP dashboard find 'Networking &gt; External IP addresses'.</li> <li>Now click the down arrow under the 'Type' column and select 'Static' for the External IP address which is connected to your instance of GCP Compute Engine.</li> <li>By reserving a Static IP Address you will not loose your access to website after server outages or restarts.</li> </ul> </li> </ul> <p>Point a domain to external IP Address:</p> <ul> <li>To Name servers, add DNS name server, eg, <code>dns1.india-to.com</code></li> <li>on DNS Management of the DNS server<ul> <li>add Host Name '@', record type 'A (Address)', then your IP Address</li> <li>add Host Name 'www', record type 'A (Address)', then your IP Address</li> </ul> </li> </ul>"},{"location":"0-Information-Technology/web-server-config/#add-sites-to-apache-server","title":"Add sites to Apache Server","text":"<p>Now we need to enable sites on apache we server to connect domain with dir of files to be served by this server.</p> <ul> <li><code>cd /etc/apache2/sites-available/</code></li> <li><code>ls -l</code></li> <li>Copy configuration file for new domain to be added: <code>sudo cp 000-default.conf example123.com.conf</code></li> <li>Let us switch to root user: <code>sudo su</code></li> <li>edit: <code>nano example123.com.conf</code> and add following content to file:</li> </ul> <pre><code>&lt;Directory /var/www/html/example123.com&gt;\n  Require all granted\n&lt;/Directory&gt;\n&lt;VirtualHost *:80&gt;\n  ServerName example123.com\n  ServerAlias www.example123.com\n  ServerAdmin webmaster@localhost\n  DocumentRoot /var/www/html/example123.com\n\n  ErrorLog ${APACHE_LOG_DIR}/error.log\n  CustomLog ${APACHE_LOG_DIR}/access.log combined\n&lt;/VirtualHost&gt;\n</code></pre> <ul> <li>disable default site: <code>a2dissite 000-default.conf</code></li> <li>enable new site: <code>a2ensite example123.com.conf</code></li> <li>restart apache service: <code>systemctl reload apache2</code></li> <li>go to web documents location: <code>cd /var/www/html</code></li> <li>create directory for new site: <code>mkdir example123.com</code></li> <li><code>cd example123.com</code> go in the directory.</li> <li><code>nano index.html</code> create an html file.</li> <li>Now open browser and goto 'example123.com' you should see the contents of index.html.</li> </ul>"},{"location":"0-Information-Technology/web-server-config/#mysql-database-configuration","title":"MySQL Database Configuration","text":"<p>Now we will configure MySQL database so that we can use that in our web apps.</p> <ul> <li><code>mysql -u root</code></li> </ul> <pre><code>&gt; CREATE DATABASE db123;  \n&gt; GRANT ALL ON db123.* TO 'db123_user' IDENTIFIED BY 'db123_pwd!';  \n&gt; quit;  \n</code></pre> <ul> <li>Secure the installation <code>mysql_secure_installation</code>, select Y for all or as per your need.</li> </ul> <p>Configure PHP:</p> <ul> <li><code>nano /etc/php/7.2/apache2/php.ini</code></li> <li>update:</li> </ul> <pre><code>upload\\_max\\_filesize = 20M  \npost\\_max\\_size = 21M\n</code></pre> <p>Congratulations, can you believe we are already done! We can create and host any webapp on one of the Worlds biggest cloud infrastructure, GCP, and its scalable!</p>"},{"location":"0-Information-Technology/web-server-config/#wordpress-installation-on-ubuntu-server","title":"Wordpress Installation on Ubuntu Server","text":"<p>Now that we have Apache, PHP and MySQL configured, connected and working together, we can start developing web apps. Wordpress is a easy to use CMS in PHP, lets get started with installing wordpress.</p> <ul> <li><code>cd /var/www/html/example123.com/</code></li> <li>download wordpress: <code>wget https://wordpress.org/latest.tar.gz</code></li> <li>Unzip the file <code>tar -xvf latest.tar.gz</code></li> <li><code>mv wp-config-sample.php wp-config.php</code></li> <li><code>nano wp-config.php</code></li> </ul>"},{"location":"0-Information-Technology/web-server-config/#web-server-tuning","title":"Web Server Tuning","text":"<p>Configure MPM_Prefork.conf to manage apache load performance:</p> <ul> <li><code>nano /etc/apache2/mods-enabled/mpm_prefork.conf</code></li> <li>Update to below:</li> </ul> <pre><code>&lt;IfModule mpm_prefork_module&gt;\n  StartServers    1\n  MinSpareServers   2\n  MaxSpareServers   5\n  MaxRequestWorkers 10\n  MaxConnectionsPerChild  1000\n&lt;/IfModule&gt;\n</code></pre> <p>Tune the new Apache install:</p> <ul> <li><code>cd ~</code></li> <li><code>wget https://raw.githubusercontent.com/richardforth/apache2buddy/master/apache2buddy.pl</code></li> <li><code>chmod +x apache2buddy.pl</code></li> <li><code>./apache2buddy.pl</code></li> <li>This perl script tells the health of Apache server.</li> </ul> <p>User guides:</p> <ul> <li>video used: YouTube</li> </ul>"},{"location":"0-Information-Technology/web-server-config/#how-to-add-multiple-domains-on-apache-web-server","title":"How to add multiple domains on Apache Web Server","text":"<p>We can host multiple sites on one server with vistual hosts in Apache.</p> <ul> <li>create conf file <code>cd /etc/apache2/sites-available/</code> then <code>sudo cp 000-default.conf addonDomain.com.conf</code></li> <li>open conf file using nano, then edit</li> </ul> <pre><code>&lt;Directory /var/www/html/addonDomain.com&gt;\n  Require all granted  \n&lt;/Directory&gt;\n&lt;VirtualHost *:80&gt;\n  ServerName example.com\n  ServerAlias www.example.com\n  ServerAdmin admin@example.com\n  DocumentRoot /var/www/example.com/html\n&lt;/VirtualHost&gt;\n</code></pre> <ul> <li>make folders <code>mkdir var/www/html/addonDomain.com/</code></li> <li>enable site:</li> </ul> <pre><code>a2ensite example.com.conf\nsystemctl reload apache2\n</code></pre> <ul> <li>More details here.</li> </ul>"},{"location":"0-Information-Technology/web-server-config/#serve-python-files-on-apache-web-server","title":"Serve Python files on Apache Web Server","text":"<p>Python files can be served via CGI or WSGI. Python is language, Apache2 is webServer, CGI and WSGI are protocol to help web server and language talk to each other.</p>"},{"location":"0-Information-Technology/web-server-config/#cgi-scripts","title":"CGI Scripts","text":"<p>Common Gateway Interface or CGI provides interface for web server to serve HTML from console programs like Python.</p> <ul> <li>By CGI we can directly open <code>index.py</code> in browser and it works like PHP file. It outputs the result of the script file.</li> <li>default directory is <code>/usr/lib/cgi-bin/</code>, you can add, <code>hello.cgi</code> here and open in browser.</li> <li>enable in apache: <code>a2dismod mpm_event</code></li> <li>enable cgi module: <code>a2enmod mpm_prefork cgi</code></li> <li>to make new dir for cgi files, add following to site conf file:</li> </ul> <pre><code>&lt;VirtualHost *:80&gt;\n  ...\n  &lt;Directory /var/www/html/cgi_dir&gt;\n    Options +ExecCGI\n    AddHandler cgi-script .py .cgi\n    # DirectoryIndex index.py\n  &lt;/Directory&gt;\n  ...\n&lt;/VirtualHost&gt;\n</code></pre> <ul> <li>CGI runs script when requested, where as WSGI runs script with start of WebServer.</li> </ul>"},{"location":"0-Information-Technology/web-server-config/#wsgi","title":"WSGI","text":"<p>Web Server Gateway Interface or WSGI is a simple calling convention for web servers to forward requests to web applications.</p> <ul> <li><code>mod_wsgi</code> is an Apache HTTP Server module that provides a WSGI compliant interface for hosting Python based web applications. It is an alternative to CGI.</li> </ul>"},{"location":"0-Information-Technology/web-server-config/#flask-wsgi-and-apache2-on-linux-hands-on","title":"Flask, WSGI and Apache2 on Linux [Hands On]","text":"<p>Assuming you have a working apache2 server and you have python installed. Next we need to install pip and WSGI module.</p> <p>Python 3:</p> <ul> <li><code>sudo apt-get install libapache2-mod-wsgi-py3</code></li> <li><code>sudo apt-get install python3-pip</code></li> </ul> <p>Python 2:</p> <ul> <li><code>sudo apt-get install libapache2-mod-wsgi</code></li> <li><code>sudo apt-get install python-pip</code></li> </ul> <p>Enable WSGI and install flask:</p> <ul> <li><code>a2enmod wsgi</code> enable the WSGI module in apache.</li> <li><code>pip3 install flask</code></li> </ul> <p>Set up directory and flask files:</p> <ul> <li><code>mkdir /var/www/apps</code> this contains all flask apps.</li> <li><code>mkdir /var/www/apps/blog</code> this is our first app.</li> <li><code>mkdir /var/www/apps/blog/lib</code> this contains code for our blog app.</li> <li><code>mkdir /var/www/apps/blog/lib/static</code> this will serve static files for our blog app.</li> </ul> <p>Make flask app:</p> <ul> <li><code>sudo nano /var/www/apps/blog/lib/main.py</code></li> </ul> <pre><code>from flask import Flask\napp = Flask(__name__)\n\n@app.route('/')\ndef hello_world():\n  return 'Hello from Flask blog app!'\n\nif __name__ == '__main__':\n  app.run()\n</code></pre> <p>Make this folder as python module, (important):</p> <ul> <li><code>touch /var/www/apps/blog/lib/__init__.py</code></li> <li>We created <code>__init__.py</code> as a blank file. This is important and is required to import our lib folder as a python module. Now we can import <code>lib.main</code> in wsgi file.</li> </ul> <p>Add the wsgi file:</p> <ul> <li><code>sudo nano /var/www/apps/blog/app.wsgi</code></li> </ul> <pre><code>import sys\nsys.path.insert(0, '/var/www/apps/blog')\n\nfrom lib.main import app as application\n</code></pre> <p>Configuring virtual hosts conf file to make it work:</p> <ul> <li>Imp, in exmple below, replace 'myapps.com' with your domain name.</li> <li><code>cd /etc/apache2/sites-available/</code></li> <li>add a site file <code>cp 000-default.conf myapps.com.conf</code></li> <li><code>nano myapps.com.conf</code></li> </ul> <pre><code>&lt;VirtualHost *:80&gt;\n  ServerName myapps.com\n  ServerAdmin webmaster@localhost\n\n  # This is path for homepage of myapps.com\n  DocumentRoot /var/www/html/myapps.com\n\n  # App: blog, URL: http://myapps.com/myblog\n  WSGIScriptAlias /myblog /var/www/apps/blog/app.wsgi\n  &lt;Directory /var/www/apps/blog&gt;\n    Order deny,allow\n    Allow from all\n  &lt;/Directory&gt;\n  Alias /myblog/static /var/www/apps/blog/lib/static\n\n  # Enables passing of authorization headers\n  WSGIPassAuthorization On\n\n  # logs configuration\n  ErrorLog ${APACHE_LOG_DIR}/error.log\n  CustomLog ${APACHE_LOG_DIR}/access.log combined\n\n&lt;/VirtualHost&gt;\n</code></pre> <ul> <li><code>a2ensite myapps.com.conf</code> enable the site.</li> <li><code>service apache2 restart</code> restart the service.</li> <li>Now visit <code>http://myapps.com/myblog/</code> to access app.</li> </ul> <p>If you see errors:</p> <ul> <li><code>tail -30 /var/log/apache2/error.log</code> shows you error logs from apache server.</li> </ul>"},{"location":"0-Information-Technology/web-server-config/#multiple-flask-apps-and-adding-user-and-group-to-process","title":"Multiple Flask apps and adding user and group to process","text":"<ul> <li>Add new user <code>adduser apps</code>, remember password here</li> <li>update primary group <code>usermod -g www-data apps</code></li> <li>give permission to group <code>chmod 775 /home/apps</code></li> </ul> <ul> <li>login with new user <code>apps</code></li> <li><code>git clone the_web_app.git</code></li> <li><code>python3 -m venv venv</code></li> <li><code>source venv/bin/activate</code></li> <li><code>pip3 install -r requirements.txt</code></li> <li><code>touch __init__.py</code></li> <li>add <code>app.wsgi</code></li> </ul> <pre><code>#!venv/bin/python3\nimport sys\nimport logging\nlogging.basicConfig(stream=sys.stderr)\n\nsys.path.insert(0, '/home/apps/todo')\n\nfrom lib.main import app as application\n</code></pre> <p>Apache v-host file:</p> <pre><code>&lt;VirtualHost *:80&gt;\n  ServerName py.ess.com\n  ServerAdmin webmaster@localhost\n\n  DocumentRoot /var/www/html/py.ess.com\n\n  WSGIScriptAlias /site1 /var/www/apps/app1/app1.wsgi\n  &lt;Directory /var/www/apps/app1&gt;\n    Order deny,allow\n    Allow from all\n  &lt;/Directory&gt;\n  Alias /site1/static /var/www/apps/app1/static\n\n  WSGIScriptAlias /flap /var/www/apps/flap/app.wsgi\n  &lt;Directory /var/www/apps/flap&gt;\n    Order deny,allow\n    Allow from all\n  &lt;/Directory&gt;\n\n\n        WSGIScriptAlias /todo /var/www/apps/todo/app.wsgi\n        &lt;Directory /var/www/apps/todo&gt;\n                Order deny,allow\n                Allow from all\n        &lt;/Directory&gt;\n\n  WSGIDaemonProcess todo-client user=apps group=www-data threads=5\n        WSGIScriptAlias /todo-client /home/apps/todo/app.wsgi\n        &lt;Directory /home/apps/todo&gt;\n    WSGIApplicationGroup todo-client\n    WSGIProcessGroup todo-client\n                Order deny,allow\n                Allow from all\n    Require all granted\n        &lt;/Directory&gt;\n  Alias /todo-client/static /home/apps/todo/lib/static\n  WSGIPassAuthorization On\n\n\n  # WSGIDaemonProcess site2 user=myserviceuser group=myserviceuser threads=5 python-home=/$\n  # WSGIScriptAlias /site2 /var/www/apps/app2/application.wsgi\n  # &lt;Directory /var/www/apps/app2&gt;\n  #     WSGIApplicationGroup site2\n  #     WSGIProcessGroup site2\n  #     Order deny,allow\n  #     Allow from all\n  # &lt;/Directory&gt;\n\n  # logs configuration\n  ErrorLog ${APACHE_LOG_DIR}/error.log\n  CustomLog ${APACHE_LOG_DIR}/access.log combined\n\n&lt;/VirtualHost&gt;\n</code></pre> <p>Note:</p> <ul> <li>ensure group has permission to database and log directories.</li> </ul> <p>To Do:</p> <ul> <li>WSGIDaemonProcess helloworldapp user=www-data group=www-data threads=5</li> <li>WSGIProcessGroup</li> <li>Multiple Apps: Apache virtualhost is only for domain/sub-domain, to add more apps with different directories, add directory tags to configuration file. If the two Flask apps are running on the same domain just as subfolders, then you only need one VirtualHost but you\u2019ll need multiple WSGIScriptAlias directives.</li> </ul> <p>Links:</p> <ul> <li>Muiltiple flask apps usgin Apache2 and Ubuntu.</li> </ul>"},{"location":"1-Software-Engineering/cs-se-basics/","title":"Software Engineering","text":"<p>all about standards, conventions, ethics, practices</p>"},{"location":"1-Software-Engineering/cs-se-basics/#coding-best-practices-ways-of-working","title":"Coding Best Practices - Ways of Working","text":"<ul> <li>Follow coding standards</li> <li>Use git</li> <li>Write Unit testing - use code to test the small units of programme, test standalone modules.</li> <li>Write integration testing - use code to test if modules work correclty when connected together.</li> <li>Do funcitonal testing - as a tester, test the functionality of the application.</li> <li>Test business outcomes, verify by business people</li> <li>Work with people smarter than you</li> <li>do pair programming</li> </ul> <ul> <li>Link<ul> <li>td.io - Software Development 101</li> <li>Open Sourcing a Python Project the Right Way</li> </ul> </li> </ul>"},{"location":"1-Software-Engineering/cs-se-basics/#open-source-contribution","title":"Open Source Contribution","text":"<ul> <li>Why? - It is a great way to grow and evovle, learn, build your skill and be part of community, grow your network, showcase your presence.</li> </ul> <ul> <li>Links - OpenSourceGuide</li> </ul>"},{"location":"1-Software-Engineering/cs-se-basics/#working-on-someone-elses-code","title":"Working on someone else's code","text":"<ul> <li>Start a debugger</li> <li>Step into and over, understand the variable values, add breakpoints.</li> </ul>"},{"location":"1-Software-Engineering/cs-se-basics/#links","title":"Links","text":"<ul> <li>3 EDX Soft engg - https://www.edx.org/course/software-engineering-basics-for-everyone</li> <li>2 edx Soft Engg MM - https://www.edx.org/micromasters/ubcx-software-development</li> <li>1 edx Algorithms and Data Structures MM - https://www.edx.org/micromasters/ucsandiegox-algorithms-and-data-structures</li> <li>g4g DSA - https://www.geeksforgeeks.org/complete-roadmap-to-learn-dsa-from-scratch/</li> </ul>"},{"location":"1-Software-Engineering/cs-se-basics/#coding-naming-conventions","title":"Coding Naming Conventions","text":"<ul> <li>Python - Use underscores</li> <li>Javascript - Camel Case</li> <li>files and folders - use hyphen. Link google style guide</li> </ul>"},{"location":"1-Software-Engineering/cs-se-basics/#computer-science","title":"Computer Science","text":"<ul> <li>Thread is the smallest sequence of instructions that can be managed independently. It is common for a process to have multiple active threads, sometimes sharing resources such as memory or file handles. Multithreaded web servers start a pool of threads and select a thread from the pool to handle each incoming request.</li> </ul> <ul> <li>Daemon - In multitasking computer operating systems, a daemon is a computer program that runs as a background process, rather than being under the direct control of an interactive user.</li> </ul> <ul> <li>Unix Domain Socket or Unix-Socket is endpoint for data communication between processes. Eg, Docket client (process-1) and Docker daemon (process-2) communicate using a REST API over Unix Sockets.</li> </ul>"},{"location":"1-Software-Engineering/cs-se-basics/#oops","title":"OOPS","text":"<ul> <li>Object is a Class and has<ul> <li><code>attributes</code> - variables</li> <li><code>methods()</code> - functions</li> </ul> </li> </ul>"},{"location":"1-Software-Engineering/data-structures-algorithms/","title":"Data Structures and Algorithms","text":"In\u00a0[6]: Copied! <pre>from scipy.special import factorial\nimport numpy as np\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nx = np.arange(1,5,1)\n\nfig = make_subplots(rows=1, cols=5, shared_yaxes=True)\n\nfig.add_trace(go.Line(x=x,y=np.log2(x), name='log n'), row=1, col=1)\nfig.add_trace(go.Line(x=x,y=x, name='n'), row=1, col=2)\nfig.add_trace(go.Line(x=x,y=x*np.log2(x), name='n log n'), row=1, col=3)\nfig.add_trace(go.Line(x=x,y=x*x, name='n&lt;sup&gt;2&lt;/sup&gt;'), row=1, col=4)\nfig.add_trace(go.Line(x=x,y=factorial(x), name='n!'), row=1, col=5)\n\nfig.update_layout(height=300, width=1000, title_text=\"Big O - Time Complexity\")\nfig.show()\n</pre> from scipy.special import factorial import numpy as np from plotly.subplots import make_subplots import plotly.graph_objects as go  x = np.arange(1,5,1)  fig = make_subplots(rows=1, cols=5, shared_yaxes=True)  fig.add_trace(go.Line(x=x,y=np.log2(x), name='log n'), row=1, col=1) fig.add_trace(go.Line(x=x,y=x, name='n'), row=1, col=2) fig.add_trace(go.Line(x=x,y=x*np.log2(x), name='n log n'), row=1, col=3) fig.add_trace(go.Line(x=x,y=x*x, name='n<sup>2</sup>'), row=1, col=4) fig.add_trace(go.Line(x=x,y=factorial(x), name='n!'), row=1, col=5)  fig.update_layout(height=300, width=1000, title_text=\"Big O - Time Complexity\") fig.show() <pre>\nflowchart TD\nSon --&gt; Mother --&gt; GrandFatherM\nSon --&gt; Father --&gt; GrandFatherF\nFather --&gt; GrandMotherF\nMother --&gt; GrandMotherM\n</pre> <p>Shortest Path</p> <pre>\nflowchart LR\nStart-.6.-&gt;A\nStart==2==&gt;B\nA==1==&gt;Finish\nB==3==&gt;A\nB-.5.-&gt;Finish\n</pre> <p>Code</p> <pre>graph = {\n    'start' : {         # all neighbours to start\n        'a': 6,\n        'b': 2\n    },\n    'a' : {             # all neighbours to a\n        'finish' : 1\n    },\n    'b' : {             # all neighbours to b\n        'a' : 3,\n        'finish' : 5\n    },\n    'finish' : {}       # finish has no neighbours\n}\n\ncosts = {               # at Iter 1\n    'a': 6,\n    'b': 2,\n    'final': float(\"inf\")\n}\n\nparents = {             # at Iter 1\n    'a': \"start\",\n    'b': \"start\",\n    'final': None\n}\n\nprocessed = []\n\n# Dijkstra's Algo\n\ndef find_lowest_cost_node(costs):\n    lowest_cost = float(\u201cinf\u201d)\n    lowest_cost_node = None\n    for node in costs:              # Go through each node.\n        cost = costs[node]\n        if cost &lt; lowest_cost and node not in processed:    # If it\u2019s the lowest cost so far and hasn\u2019t been processed yet ...\n        lowest_cost = cost                                  # ... set it as the new lowest-cost node.\n        lowest_cost_node = node\n    return lowest_cost_node\n\nnode = find_lowest_cost_node(costs)     # that you haven\u2019t processed yet.\nwhile node is not None:                 # If you\u2019ve processed all the nodes, this while loop is done. \n    cost = costs[node]\n    neighbors = graph[node]\n    for n in neighbors.keys():          # Go through all the neighbors of this node.\n        new_cost = cost + neighbors[n]  # If it\u2019s cheaper to get to this neighbor\n        if costs[n] &gt; new_cost:         # by going through this node ...\n            costs[n] = new_cost         # ... update the cost for this node.\n            parents[n] = node           # This node becomes the new parent for this neighbor\n    processed.append(node)              # Mark the node as processed.\n    node = find_lowest_cost_node(costs)\n</pre> <p>Summary</p> <ul> <li>Breadth-first search is used to calculate the shortest path for an unweighted graph.</li> <li>Dijkstra\u2019s algorithm is used to calculate the shortest path for a weighted graph.</li> <li>Dijkstra\u2019s algorithm works when all the weights are positive.</li> <li>If you have negative weights, use the Bellman-Ford algorithm.</li> </ul>"},{"location":"1-Software-Engineering/data-structures-algorithms/#data-structures-and-algorithms","title":"Data Structures and Algorithms\u00b6","text":"<p>Recursion</p> <ul> <li>Break big problem into smaller problem that are same and solve the smallest one, and built on top of it.</li> </ul> <p>Sorting Algorithms</p> <ul> <li>insertion</li> <li>merge</li> <li>quick sort</li> <li>understanding which to use depends on the data and how sorted it is in different scenarios</li> </ul> <p>Searching Algorithms</p> <ul> <li><p>Binary</p> </li> <li><p>Breadth first</p> </li> <li><p>Depth first</p> </li> <li><p>Use bradth-first search, when finding shortest path between two nodes in a tree</p> </li> <li><p>Don't use it on tree having high branching factor.</p> </li> </ul>"},{"location":"1-Software-Engineering/data-structures-algorithms/#dsa-overview","title":"DSA Overview\u00b6","text":"<ul> <li><p>Why is DSA important?</p> <ul> <li>In every problem there is a budget, which defines time and money. Hence, it is important to understand the cost of memory and time of execution.</li> </ul> </li> <li><p>Edge Cases</p> <ul> <li>extreme cases of input or situation</li> <li>like: empty array, bad format</li> </ul> </li> <li><p>Steps to solve a problem using DSA</p> <ol> <li>state problem clearly</li> <li>define inputs and outputs and their formats</li> <li>state test cases and edge cases in english</li> <li>state correct solution in english</li> <li>code and test</li> <li>repeat 2-5</li> <li>analyze algo's complexity or inefficiency</li> <li>refactor code, test. repeat 2-8</li> </ol> </li> <li><p>Best Practices in Coding using DSA</p> <ul> <li>create a function</li> <li>give meaningful name and follow standards</li> <li>do test driven development</li> <li>10 mins code built test repeat, small steps</li> <li>abstract code into functions and make them generic. eg, repetitive logic can be moved to a separate function and used in changing condition function. as in binary-search logic is same but matching condition may vary, so make a generic binary-search function and use it in other functions.</li> <li>The solving approach and coding both are important, approach &amp; right technique is more important that coding</li> <li>don't start coding directly, rather write clearly the problem statement and then solution in plain english, then code.</li> </ul> </li> <li><p>linear search algorithm or brute force algorithm</p> <ul> <li>simple, start from 0 and traverse whole array</li> </ul> </li> <li><p>Time calculation of code execution</p> <ul> <li><code>1Ghz</code> machine can execute <code>10^9</code> instructions in 1 second (giga = 9). Instructions are low level binary code. so a high level python statement can execute tens or thousands of instructions, but that's how you calculate execution time of once statement, and the set of statements in function and then number iterations of that function in worst case gives its complexity.</li> <li><code>iterations --of--&gt; function of statements --having--&gt; statement --build_of--&gt; instructions --processed_on--&gt; CPU</code></li> </ul> </li> <li><p>Analyze complexity and find inefficiency</p> <ul> <li>can you reduce the number of iterations in the loop? eg, use binary search instead of linear search on a sorted array.</li> <li>complexity of algo is the max time it will take to finish, that is total time in worst case. it is usually based on the size of array <code>n</code></li> <li>O(n) or Big O Notation or 'Order of n' means that a code-block is executed <code>n</code> number of times at max.</li> <li>O(log n) means that code will iterate <code>log n</code> times.</li> </ul> </li> </ul>"},{"location":"1-Software-Engineering/data-structures-algorithms/#time-complexity","title":"Time Complexity\u00b6","text":"<p>Time Complexity</p> <ul> <li><p>Amount of time, in worst case scenario, given the length of input.</p> </li> <li><p>O(1) simple var access - constant time complexity</p> <ul> <li>given a list, you do not iterate whole list, instead just access one element. even if list lenght iceases, the time will not increase as your function just prints one element.</li> </ul> </li> <li><p>O(n) loop over list - linear</p> </li> <li><p>O(log n) binary search, where you eliminate half result.</p> </li> <li><p>O(n^2), for loop within for loop, you iterate over each list element. - quadratic</p> </li> <li><p>O(n log n), O(2^n) and O(n!) (n factorial), not common.</p> </li> <li><p>You should know time complexity of solution.</p> </li> <li><p>If there are multiple steps, time complexity is worst step, that is what we want to know.</p> </li> <li><p>Big O</p> <ul> <li>O represents order of. O(n) means, the time complexity increases as order of n. If n increases the time increases by it.</li> <li>if you plot time and n on graph, let's say eq is $t = an + b$, where $a$ and $b$ are constant, then t we see that t increases majorly by $a x n$, which is order of n. hence, <code>O(n)</code>. You ignore the co-efficients from the equations.</li> <li>For $t = 20$ it is $t = 20 x 1$, hence <code>O(1)</code></li> <li>For $t = an^2 + c$, notation is, $O(n^2)$</li> <li>Big O does not depend on machine capability, twice fast machine will change coefficient, which is ignored. However, the function complexity will remain the same, that is, constant, linear or quadratic etc irrespective of machine's capability.</li> </ul> </li> <li><p>Evaluating Big O</p> <ul> <li>the assignment operation will have complexity constant, of 1 so $O(1)$</li> <li>Now, if this is in for loop, it repeats n times, hence time is $n \\times O(1)$ that will be $O(n)$</li> <li>Now, if there is another for loop outside this, time taken is $n \\times O(n)$, that will be $O(nn)$ or $O(n^2)$</li> </ul> </li> </ul> <p>Comparison of time complexity with common algo and common orders</p> Algo Binary Search Simple Search Quicksort Selection Sort The Traveling Salesman Complexity / Size $O(\\log n)$ $O(n)$ $O(n\\log n)$ $O(n^2)$ $O(n!)$ 10 0.3s 1s 3.3s 10s 4.2 days 100 0.6s 10s 66.4s 16.6m $2.9 \\times 10^{149}$ yrs 1000 1s 100s 996s 27.7 hrs $1.27 \\times 10^{2559}$ yrs <ul> <li>Link - YK Sugi CS Dojo</li> </ul>"},{"location":"1-Software-Engineering/data-structures-algorithms/#built-in-data-types","title":"Built In Data Types\u00b6","text":"<p>Tuples</p> <ul> <li>good when you don't have to change the data</li> <li>immutable, iterable, indexed, ordered</li> </ul> <p>Lists</p> <ul> <li>mutable, use when you have to change</li> <li>ordered</li> <li>userful, reverse and sort, append, pop, remove</li> </ul> <p>Dictionaries</p> <ul> <li>mutable, iterable, ordered (3.7+)</li> </ul> <p>Sets</p> <ul> <li>unique, mutable, fast, unordered cannot index, iterable.</li> <li>add update remove</li> <li>intersection, difference; math functions</li> </ul>"},{"location":"1-Software-Engineering/data-structures-algorithms/#abstract-data-types","title":"Abstract Data Types\u00b6","text":"<p>These are general and well known data types that exist in Computer Science domain, eg linked-list. People know what data type it can hold and how to access it. They are language agnostic.</p> <p>Advantages</p> <ul> <li>abstraction - you need not know inner working</li> <li>consistency - inner implementation can change but outer interface remains the same. So code using it need not to be changed.</li> </ul> <p>Data Structure</p> <ul> <li>Implementation of abstract data type.</li> <li>Implementation means how you implement a ADT in program. Eg, how you implement a Link-List in Python. Once you implement it becomes data-structure.</li> </ul> <p>List vs LinkedList</p> <ul> <li>Memory: List stores elements in contiguous (common border) memory, one next to another, so adding new elements should have contiguous memory available (otherwise cannot store it). Also, this means yoy have to block the memory even if you do not use it (eg in java you do initialize 10 size array). Linked Lists store the values randomly in memory where every it has free space, and it has pointer to next element.</li> <li>Accessibility: Arrays are good as there is sequential index, you can directly access last element, or fifth, forth last etc. But in LL you have to traverse whole list to access last element as LL has no index. Array is good for random access, LL is good for accessing whole list.</li> </ul> Worst Case Arrays Lists Reason Reading O(1) O(n) Reading last elem in LL is worst, so O(n) Insertion O(n) O(1) Insert at start of array then you have to move all elem, hence O(n) Deletion O(n) O(1) Del 1st elem in array, you move all 1 memory back <ul> <li>Use Case: Too many write, few read, eg, in expense entry system, you enter each day but read total at month end. You would prefer LL, as it has low order of execution for write. Remember, Arrays allow fast reads. Linked lists allow fast inserts and deletes.</li> </ul> <p>Linked List</p> <ul> <li><p>Has a pointer to next element's memory location</p> </li> <li><p>List having no pointer is empty linked list</p> </li> <li><p>Recursive Data Type [?]</p> </li> <li><p>Operations</p> <ul> <li>Add</li> <li>Remove</li> <li>Is empty</li> <li>how many nodes</li> <li>find node's index given its key</li> </ul> </li> <li><p>When to use</p> <ul> <li>insert in between other items</li> <li>collection size is unknown</li> <li>don't need random access</li> <li>not concerned about memory</li> </ul> </li> </ul> <p>References:</p> <ul> <li>Book - Problem Solving with Algorithms and Data Structures using Python</li> </ul>"},{"location":"1-Software-Engineering/data-structures-algorithms/#linear-data-structures","title":"Linear Data Structures\u00b6","text":"<p>Stacks</p> <ul> <li>Linear data structure</li> <li>LIFO</li> <li>maintains queue in which they were added</li> <li>when want quick access to latest item</li> </ul> <p>Queues</p> <ul> <li>FIFO</li> <li>Ordered</li> <li>Use when you have to process something in order</li> </ul> <p>Linked List</p> <ul> <li>Singly linked, ordered</li> <li>When you have unknown number of items</li> <li>One-direction movement is ok</li> <li>Insert in between</li> </ul> <p>Doubly Link list</p> <ul> <li>move both directions</li> <li>ordered</li> <li>when you need to move both ways</li> <li>sequential access of data</li> </ul>"},{"location":"1-Software-Engineering/data-structures-algorithms/#non-linear-data-structures","title":"Non-Linear Data Structures\u00b6","text":"<p>Non Linear Data Structures</p> <ul> <li>Trees</li> <li>Binary Search Trees</li> <li>Graphs</li> <li>Hash Maps</li> </ul> <p>Trees</p> <ul> <li>hierarchical, level, nodes</li> <li>when you need to store hierarchical</li> </ul> <p>Binary</p> <ul> <li>smaller on left, large on right</li> <li>root is midpoint</li> <li>lookup, o(log n) time, as it eliminates half</li> </ul> <p>Graphs</p> <ul> <li>A finite set of nodes connected by edges</li> <li>Edges can have optional values (attributes)</li> <li>when you need relationship between nodes</li> </ul> <p>Hash Maps</p> <ul> <li>key - value</li> <li>like dictionaries, name to id and id has phone numbers</li> </ul>"},{"location":"1-Software-Engineering/data-structures-algorithms/#binary-search","title":"Binary Search\u00b6","text":"<p>It searches a Sorted list. Starts from mid and compares the item, in each comparison it gets to know in which half the item would be, hence, ignores the rest half. This makes the search possible in maximum, $\\log n$ times.</p> <p>How, $log_2 n$:</p> <ul> <li>since in each search the number is halved, so 32 - 16 - 8 - 4 - 2 , this can be remembered as, total elements are getting chopped/logged in base of 2.</li> <li>mathematically, the total elements n, reduce in power of 2, hence,<ul> <li>numbers in each steps are power of 2, so how many steps are there,</li> <li>what power of 2 gives numbers,</li> <li>what power of 2 gives 32? it is 5</li> <li>log 32 base 2, is what? it is 5</li> </ul> </li> </ul> <p>Hence, the it is <code>O(log n)</code> complexity algorithm.</p> <ul> <li><p>Binary Search Algorithm</p> <ul> <li><p>if an array is sorted</p> </li> <li><p>find the number in middle</p> </li> <li><p>then look left or right</p> </li> <li><p>complexity is <code>log(n)</code> - that means that in worst case the number of iterations will be <code>log(n)</code> for an array of length <code>n</code></p> </li> <li><p>how?</p> <ul> <li>lets say we do <code>k</code> iterations (at max) to search a number in array of length <code>n</code>, i.e. loop will run no more than k times and is hence worst case scenario, so</li> <li>iter: 1, length: n/2</li> <li>iter: 2, length: n/4 = n/2^2</li> <li>iter: 3, length: n/4 = n/2^3</li> <li>...</li> <li>iter: k, length: n/2^k, this is last iteration and we know in worst case the length will be reduced to 1.</li> <li><code>n/2^k = 1</code> or solve it to get</li> <li><code>k = log(n)</code> which means that the code will have to run maximum <code>log(n)</code> number of times.</li> </ul> </li> </ul> </li> <li><p>Links</p> <ul> <li>Binary Search, Linked Lists and Complexity | Data Structures and Algorithms in Python - direct code, no plain algo dry run</li> </ul> </li> </ul>"},{"location":"1-Software-Engineering/data-structures-algorithms/#selection-sort","title":"Selection Sort\u00b6","text":"<p>Sorting simply by comparing a item in list by other items is easy. Its complexity is item1 compared to n others, item 2 compared to n others and so on until item_n compared to n others, which is, n x O(n) or $O(n^2)$</p> <p>Complexity: $O(n^2)$</p> <p>You would be comparing one less item in each step, but that one would come up as constant with n, and we ignore constant in Big O.</p>"},{"location":"1-Software-Engineering/data-structures-algorithms/#recursion","title":"Recursion\u00b6","text":"<p>Recursion is when a function calls itself.</p> <p>Every recursive function has two parts: the base case, and the recursive case. Base case tells when to break the recursion and stop the execution.</p> <p>Recursive case calls the function itself, and in base case, it doesn't hence ends. Eg:</p> <pre>def countdown(i):\n    print (i)\n    if i &lt;= 0:          # Base case\n        return\n    else:               # Recursive case\n        countdown(i-1)\n</pre> <p>Example 2: Factorial of a number</p> <pre>def fact(x):\n    if x == 1:              # Base case\n        return 1\n    else:                   # Recursive case\n        return x * fact(x-1)\n</pre> <p>Call Stacks</p> <ul> <li>In programming execution, there is call stack, in which each item of stack is a function call and this item is popped from stack when it returns or ends.</li> <li>So if a function B is called from a line A, then B is stacked on top of A until B returns.</li> <li>In recursion, lets say, A calls B - factorial function, then stacks builds as, A B(5) B(4) B(3) B(2) B(1). The top item in stack, B(1) returns $1$, and then other stack returns $2*1$ and so on, so you lastly get, $1*2*3*4*5$</li> <li>Tip: When you\u2019re writing a recursive function involving an array, the base case is often an empty array or an array with one element.</li> </ul>"},{"location":"1-Software-Engineering/data-structures-algorithms/#quicksort","title":"Quicksort\u00b6","text":"<p>Divide &amp; Conquer</p> <p>D&amp;C algorithms are recursive algorithms. To solve a problem using D&amp;C, there are two steps:</p> <ol> <li>Figure out the base case. This should be the simplest possible case.</li> <li>Divide or decrease your problem until it becomes the base case.</li> </ol> <p>D&amp;C isn\u2019t a simple algorithm that you can apply to a problem. Instead, it\u2019s a way to think about a problem.</p> <p>Quicksort Algorithm</p> <ul> <li>pick an element in array, called <code>pivot</code></li> <li>make new array having all elements less than pivot, <code>less[]</code></li> <li>make new array having all elements more than pivot, <code>more[]</code></li> <li>recursively do same thing on <code>less</code> and <code>more</code>.</li> </ul> <p>Base Case You need not sort an empty array or array with one element. It is sorted. eg, <code>[]</code> or <code>[20]</code>.</p> <p>Recursive Case You need to build less and more array and then do same.</p> <p>Code</p> <pre>def quicksort(array):\n    if len(array) &lt; 2:\n        return array        # Base case: arrays with 0 or 1 element are already \u201csorted\u201d\n    else:\n        pivot = array[0]    # Recursive case\n        less = [i for i in array[1:] if i &lt;= pivot]\n        greater = [i for i in array[1:] if i &gt; pivot]\n        return quicksort(less) + [pivot] + quicksort(greater)\n</pre> <p>Complexity</p> <p>$O(n) = n \\log n$ , average case</p> <p>$O(n) = n^2$ , worst case</p>"},{"location":"1-Software-Engineering/data-structures-algorithms/#hash-tables","title":"Hash Tables\u00b6","text":"<p>Lets build somethings that searches in O(1) complexity, that is instantly. No matter how long the array is, you get the result instantly.</p> <p>They\u2019re also known as hash maps, maps, dictionaries, and associative arrays.</p> <p>You\u2019ll probably never have to implement hash tables yourself. Any good language will have an implementation for hash tables. Python has hash tables; they\u2019re called dictionaries.</p> <p>Hash Functions</p> <ul> <li>It is a function where you put a string and you get back a number. That number tells index where the value is stored.</li> <li>Dict in Py is such implementation, you put a <code>key</code> it knows index, reads and returns a <code>value</code>.</li> </ul> <p>Code</p> <pre># create a hash table\nbook = dict()\n</pre> <p>Usage</p> <ul> <li>Relationship of one thing to another, or key-value pair</li> <li>Filtering out duplicates</li> <li>They have no ordering</li> </ul> <p>Implementation</p> <ul> <li>Hash function is important, it should not do collision (same index for different key).</li> <li>Reading array is O(1), as you can simply read from memory on the index you provide, So 1 elem or 1billion, just read that index from memory, hence O(1).</li> <li>Insert and Delete in LinkedList is fast O(1).</li> <li>Hash is best of both, for average case.</li> <li>If load in hast table (occupied / empty slots) is more than 0.7, resize it to maintain optimal performance.</li> <li>Real implementation of good hash function is beyond scope for now!</li> </ul>"},{"location":"1-Software-Engineering/data-structures-algorithms/#breadth-first-search","title":"Breadth-First Search\u00b6","text":"<p>When trying to solve problem like shortest-distance, try to model problem as graph and solve it using Breadth-first Search (BFS).</p> <p>Graphs</p> <ul> <li>Rama ---owes----&gt; Bob ; or shows rama owes money to Bob. this can have many relations where 1 owes to 2 and 3, and 3 owes to 1, 4 etc.</li> <li>It has node and edge</li> <li>The directed graph has edge pointing to node, undirected has no direction of edge.</li> </ul> <p>BFS</p> <ul> <li>Lets say you want to search who works in Google in your friend list.</li> <li>you build a list of your friends and search one by one if they work in Google.</li> <li>If not, you add their friend (friend of friend, 2nd degree relation) to end of list, so that you can find someone working in google in your network.</li> <li>But first, you search all your 1st degree friends, this is Breadth-First Search. Because you want to find the shortest path to Google working friend, which why you first search all your 1st connection. (search in order they are added to search list), data structure for this is queue. and for representing relations is hash-tables</li> <li>BFS finds path from A to B, and if so, it finds the shortest path.</li> </ul> <p>Queues</p> <ul> <li>FIFO - First in first out (like queue at bus stop), opposite of stack which is LIFO.</li> </ul> <p>Code</p> <p>You can implement using <code>List</code> for Queue and <code>Dictionary</code> for hashmap. And maintain another <code>list</code> for Array of searched nodes to avoid circular relation and sending into infinite loop.</p> <p>Time Complexity</p> <ul> <li>Running all edges will take $O(E)$, number of edges</li> <li>Running queue for all person will take $O(V)$ that is number of vertices</li> <li>Running breadth-first search that is above two, will be</li> <li>$O(V+E)$</li> </ul> <p>Tree</p> <ul> <li>let say you see a family tree showing parents and child, edges represent parent, so they can point only one way, son ---parent---&gt; father, as son can't be parent of father. These graphs are trees.</li> <li>A tree is a special type of graph, where no edges ever point back.</li> </ul>"},{"location":"1-Software-Engineering/data-structures-algorithms/#dijkstras-algorithm","title":"Dijkstra\u2019s Algorithm\u00b6","text":"<p>Weighted graphs, edges that carry weight.</p> <pre>\nflowchart LR\nStart--6--&gt;A\nStart--2--&gt;B\nA--1--&gt;Finish\nB--3--&gt;A\nB--5--&gt;Finish\n</pre><p>Dijkstra\u2019s algorithm</p> <p>Lets say you have to find path from start to finish, start is connected to nodes, and they to other nodes and so on until finish, each node has travel time.</p> <ul> <li>From all neighbors to start, find the cheapest node. This is the node you can get to in the least amount of time*.</li> <li>Update the costs of the neighbors of this node.</li> <li>Repeat until you\u2019ve done this for every node in the graph.</li> <li>Calculate the final path.</li> </ul> <p>You can do this by maintaining a table with three columns and update them in each iteration by traversing all nodes and edges.</p> <p>Iter 1</p> Parent Node Cost Start Stop A 6 Start Stop B 2 Stop A Finish infinity <p>...</p> <p>Keep updating cost and parent, for each node in each iteration and finally you can find the path.</p> <p>Iter Last</p> Parent Node Cost Stop B Stop A 5 Start Stop B 2 Stop A Finish 6 <ul> <li>there is no other way to get to the cheapest node other than the direct link to start. Also this is only true when you have positive weights.</li> </ul> <p>Negative-weight edges</p> <p>The edge can have negative weight value, eg, you give A to B and B returns you money. A -- -2$ ---&gt; B</p> <p>Now the Dijkstra Algo does not work, as \"the only shortest path to cheapest node from start is direct link\" does not stand true. Hence, you can\u2019t use Dijkstra\u2019s algorithm if you have negative-weight edges.</p> <p>The Bellman-Ford algorithm can be used in this case (out of scope for now!).</p> <p>Implementation of Dijkstra</p> <p>You need three hash-tables to represent the weighted graph data (and the structure in table above that keeps track to find shortest path)</p> <ul> <li><code>graphs</code> - dict - represents the weighted graph (see graph)</li> <li><code>costs</code> - dict - keeps track of lowest cost in each run (see table)</li> <li><code>parents</code> - dict - keeps track of parent having lowest cost in each run (see table)</li> <li><code>processed</code> - list - keeps track of nodes processed, so we do not repeat</li> </ul> <pre>graph = {\n    'start' : {         # all neighbours to start\n        'a': 6,\n        'b': 2\n    },\n    'a' : {             # all neighbours to a\n        'finish' : 1\n    },\n    'b' : {             # all neighbours to b\n        'a' : 3,\n        'finish' : 5\n    },\n    'finish' : {}       # finish has no neighbours\n}\n\ncosts = {               # at Iter 1\n    'a': 6,\n    'b': 2,\n    'final': float(\"inf\")\n}\n\nparents = {             # at Iter 1\n    'a': \"start\",\n    'b': \"start\",\n    'final': None\n}\n</pre> <p>Once, you keep executing, you will reach the final iter, which will have costs and parents hash-table having shortest path route.</p>"},{"location":"1-Software-Engineering/data-structures-algorithms/#greedy-algorithms","title":"Greedy Algorithms\u00b6","text":"<p>Some problems do not have fast algo solution. You can identify them and use greedy strategy to find approximate solution. We use Greedy Algorithm</p>"},{"location":"1-Software-Engineering/data-structures-algorithms/#dynamic-programming","title":"Dynamic Programming\u00b6","text":"<p>When there is a hard solution to a problem, we break down problem into small tasks and solve them, this is dynamic programming.</p>"},{"location":"1-Software-Engineering/data-structures-algorithms/#links","title":"Links\u00b6","text":"<ul> <li>Python Data Structures and Algorithms</li> <li>Python Data Structures: Power of Linked Lists for Technical Interviews</li> <li>Grokking Algorithms: An Illustrated Guide for Programmers and Other Curious People - Book by Aditya Y. Bhargava</li> </ul>"},{"location":"1-Software-Engineering/data-structures-algorithms/#university-of-california-berkeley","title":"University of California, Berkeley\u00b6","text":"<ul> <li><p>UC Berkeley (UCB) is \"University of California\" located in Berkeley, California, USA. It has CS lectures that have open syllabus, course material with lecture slides/videos, assignments, available for free online in an organized manner.</p> <ul> <li>Courses have a page and professor. Either videos are on course page, or go to prof youtube channel to find lectures.</li> </ul> </li> <li><p>EECS UCB is Electrical Engineering and Computer Science department.</p> </li> <li><p>EECS Courses cover all CS fundamental and advance courses. Each course has a page where you can see prerequisites which will be another course.</p> <ul> <li>CS61A/B/C (alternatives CS47ABC)<ul> <li>61A: Teaches you how to think like a programmer</li> <li>61B: The go-to class that turns a lot of people into great programmers/interested in software.</li> <li>61C: Everything left in CS that isn't taught in 61A/B: Low-level things. Clocks, CPUs, Assembly, C, etc.</li> <li>skip/skim 61A all together and just watch/do 61B, then 61C stuff</li> <li>Those who have taken these courses claim that after completing this series they feel like they can achieve or learn almost anything if they wanted because they are already well versed on the lingo and tools of CS that is programming, problem solving and low level stuff.</li> <li>DA DSA</li> <li>CS 61A: Structure and Interpretation of Computer Programs<ul> <li>Prof JohnDeNero<ul> <li>mostly teaches in Python</li> <li>youTube channel has all lecture assignment videos.<ul> <li>go to playlist to see all lectures</li> </ul> </li> </ul> </li> </ul> </li> <li>DataStructur.es<ul> <li>info about all CS61 courses</li> <li>Resources Algo Visual</li> </ul> </li> </ul> </li> <li>C9A/B/C/D/E/F/G/H<ul> <li>Basic programming courses</li> <li>CS 9C. C for Programmers</li> <li>CS 9D. Scheme and Functional Programming for Programmers</li> <li>CS 9E. Productive Use of the UNIX Environment</li> </ul> </li> <li>CS 186. Introduction to Database Systems<ul> <li>https://cs186berkeley.net/</li> </ul> </li> <li>CS 261. Security in Computer Systems</li> <li>CS 162: Operating Systems and System Programming</li> </ul> </li> <li><p>Programming Systems (PS) - Area in UCB where many discoveries were done. It includes following courses</p> <ul> <li>CS 61A. The Structure and Interpretation of Computer Programs</li> <li>CS 61B. Data Structures</li> <li>CS 164. Programming Languages and Compilers</li> <li>CS 169. Software Engineering</li> <li>CS 263. Design of Programming Languages</li> <li>CS 264. Implementation of Programming Languages</li> <li>CS 265. Compiler Optimization and Code Generation</li> <li>CS C267. Applications of Parallel Computers</li> </ul> </li> <li><p>Seasons of the United States</p> <ul> <li>Spring - March to May. Q2</li> <li>Summer - June to August. Q3</li> <li>Fall/Autumn - September to November. Q4</li> <li>Winter - December to February. Q1</li> </ul> </li> <li><p>Books Recommended</p> <ul> <li>A Philosophy of Software Design by John Ousterhout - recommended by student on reddit</li> </ul> </li> <li><p>Links</p> <ul> <li>Ask HN: CS61 series from Berkeley, the best set of foundational CS courses?</li> </ul> </li> </ul>"},{"location":"1-Software-Engineering/data-structures-algorithms/#cs-61a-structure-and-interpretation-of-computer-programs","title":"CS 61A: Structure and Interpretation of Computer Programs\u00b6","text":"<p>prof John DeNero</p> <p>CS61A - Fall 2023 - 37 Lectures have playlist on Youtube. Text on site.</p>"},{"location":"1-Software-Engineering/data-structures-algorithms/#cs-61b-data-structures-fall-2023","title":"CS 61B Data Structures, Fall 2023\u00b6","text":"<ul> <li>Prof Josh Hug  has lecture playlist. Other playlists can be found on course page</li> </ul>"},{"location":"1-Software-Engineering/data-structures-algorithms/#explore-study-material","title":"Explore Study Material\u00b6","text":"<p>UCB has best and open material based on reddit and your search.</p> <p>It will take 2-3 months to complete all lectures. So be patient and just do it without 2nd thought of result. It will benefit you.</p>"},{"location":"1-Software-Engineering/data-structures-algorithms/#links","title":"Links\u00b6","text":"<ul> <li>DSA Guide - Quora</li> <li>DSA https://www.udacity.com/course/data-structures-and-algorithms-nanodegree--nd256</li> <li>DSA Python https://www.udemy.com/course/data-structures-and-algorithms-bootcamp-in-python/</li> <li>Coding FAANG https://www.udemy.com/course/master-the-coding-interview-big-tech-faang-interviews/</li> </ul>"},{"location":"1-Software-Engineering/design-principle/","title":"Software Design Principles and Patterns","text":"<p>Software Design Process, Principle, Patterns and System Design</p> <p>Software design principles and patterns help developers make a good system design. It helps in better code design, maintainability, and extendability (adding new code won't break the existing one).</p> <p>It was introduced in 2000 in paper published by Robert C. Martin (Uncle Bob).</p> <p>Coding can be done without pattern knowlednge but to make something big, like frameworks (flask, laravel) and for you to be senior developer, it needs good understanding of patterns like decorator Pattern, Strategy Pattern etc. These frameworks apply best required design principle wisely. Eg, Laravel 5 use the Dependency Inversion Principle (IoC Container, Dependency Injection) for their core.</p> <p>Open-source software often uses a lot of Design Patterns: Singleton, Factory Method, Decorator Pattern, Strategy Pattern, Proxy, and so on. Hence contribution to these require good understanding of design patterns and process.</p> <p>Ever worked on bug in big system? You can quickly find the part that has bug, read and understand the code, change it to fix the bug and other parts of system stay unaffected.</p>"},{"location":"1-Software-Engineering/design-principle/#software-design-process","title":"Software Design Process","text":"<p>It is process that lets convert customer requirements in to programmable code. It includes following phase of design:</p> <ul> <li>Interface Design - Events, Messages, Data Format and relationship.</li> <li>Architectural Design - Components and communication</li> <li>Detailed Design - DSA, UI, State</li> </ul> <p>In each phase you design the components it is responsible for.</p>"},{"location":"1-Software-Engineering/design-principle/#software-design-principle","title":"Software Design Principle","text":"<p>Software Design Principles are guidelines. Basically, Sofware Design Principles are Object-Oriented Design Principles which includes:</p> <ul> <li>SOLID Principles</li> <li>DRY (Don\u2019t repeat yourself) Principle</li> <li>KISS (Keep it simple, stupid!!) Principle</li> <li>YAGNI (You ain\u2019t gonna need it) Principle</li> <li>PHAME</li> </ul>"},{"location":"1-Software-Engineering/design-principle/#solid-principles","title":"SOLID Principles","text":""},{"location":"1-Software-Engineering/design-principle/#single-responsibility-principle","title":"Single Responsibility Principle","text":"<p>Class (or function) should have one job and should change only if the purpose of that job changes. Decorator Pattern is useful to comply with this principle. Like, in a room it is easy to keep one type of items at one place rather dump anything anywhere.</p> <p>To implement this, write small classes with very specific names.</p> <p>For example, instead of throwing everything inside an <code>Teacher</code> class, separate the responsibilities into <code>TeacherTimeLog</code>, <code>TeacherTimeOff</code>, <code>TeacherSalary</code>, <code>TeacherDetails</code>, etc.</p> <p>This gives designated place for everyting rather than code lying all at once, it will be easier to read and fix after one year.</p>"},{"location":"1-Software-Engineering/design-principle/#open-close-principle","title":"Open / Close Principle","text":"<p>A module should be Open for extension but closed for modification. This lets add new feature by extending module, rather than modifying it. It is done so that the tested code is not changed.</p> <p>For example, VS Code is open for extensions but closed for modification without extension.</p> <p>Example 2, if you make a <code>Delivery</code> class that delivery methods, if you have to add/modify a method the class changes, which will open the existing code. Rahter make a <code>DeliveryInterface</code> which lets implement different option classes. This lets add new methods without modifying the core system.</p>"},{"location":"1-Software-Engineering/design-principle/#liskov-substitution-principle","title":"Liskov Substitution Principle","text":"<p>This principle states that when a parent class is replaced with a child class it should not break the system.</p> <p>Eg, In prototype, you make a class <code>DataProviderFile</code> that reads from file and provide array. Now when you upgrade, you make another class <code>DataProviderDatabase</code> that reads from database and returns object. You system will break because now you are getting object whereas array is expected. One option is use to make interface <code>DataProvider</code> which has return type-hinting so that when it is implemented developer uses same return type.</p> <p>Another example, is all the method of parent class should be implemented in child class, this will make child class replace parent class.</p>"},{"location":"1-Software-Engineering/design-principle/#iterface-segregation-principle","title":"Iterface Segregation Principle","text":"<p>The client (or user of a class) should not be dependent on code it doesn't use.</p> <p>Eg, if a client uses <code>payment</code> module of ecommerce system, it should not be depended on <code>ordering</code> or <code>inventory</code> module code. There should not be on <code>Ecommerce</code> class with order, inventory and payment as methods, rather it should be broken down into separate classes / interfaces like <code>Order</code> <code>Inventory</code> and <code>PaymentInterface</code>. Now when a client uses <code>PaymentInterface</code>, it does not depend on <code>Order</code> code.</p>"},{"location":"1-Software-Engineering/design-principle/#dependency-inversion-principle","title":"Dependency Inversion Principle","text":"<p>High level module should not depend on low level module - Both should depend on abstraction.</p> <p>Eg, when you switch database driver from MSSQL to NoSQL, it should not break the system. It is possible by using abastract interface.</p>"},{"location":"1-Software-Engineering/design-principle/#link-solid-principles","title":"Link SOLID Principles","text":"<ul> <li>Adevait - Software Solid Design Principles The Guide To Becoming Better Developers</li> <li>Freecodecamp - News Solid Design Principles In Software Development</li> </ul>"},{"location":"1-Software-Engineering/design-principle/#yagni","title":"YAGNI","text":"<p>This principle states that always implement things when you actually need them never implements things before you need them.</p>"},{"location":"1-Software-Engineering/design-principle/#phame","title":"PHAME","text":"<p>Principles of Hierarchy, Abstraction, Modularisation, and Encapsulation</p> <p>Links:</p> <ul> <li>Medium - Software Design Principles Every Programmer Should Know</li> </ul>"},{"location":"1-Software-Engineering/design-principle/#software-design-patterns","title":"Software Design Patterns","text":""},{"location":"1-Software-Engineering/devops/","title":"DevOps","text":"<p>It is a methodology that brings, development, QA and IT operations close together, automated and test driven, container based to have isolated similar environment so that the changes are minimal and are tested and hence don\u2019t break.</p>"},{"location":"1-Software-Engineering/devops/#cicd-pipeline","title":"CI/CD Pipeline","text":"<ul> <li>CI/CD stands for Continuous Integrationa and Continuous deployment.</li> <li>it is DevOps methodology to being IT and Ops together.</li> <li>It is a series of steps to deliver new version of software. Improves delivery throughout SDLC (software development life cycle) which is development, testing, production and monitoring. These steps can be automated to make it error free and fast.</li> <li>Steps may include - compiling code, unit tests, code analysis, security and binaries creations. And/or packaging code into container image.</li> <li>benefits, includes early integrating and testing, enhancing developer productivity, accelerating delivery, and finding/fixing bugs faster.</li> <li>commit to production is continuous and automated.</li> <li>CI/CD pipelines are completely tailor-made based on the needs and requirements and could have multiple stages and jobs, and could be complex and comprehensive.</li> </ul> <pre><code>graph LR\n\nBuild --&gt; Test --&gt; Merge --&gt; Release[Automatically Release to Repository] --&gt; Deploy[Automatically Deploy to Production]\n\nsubgraph a[Continuous Integration]\n  Build\n  Test\n  Merge\nend\n\nsubgraph b[Continuous Delivery]\n  Release\nend\n\nsubgraph c[Continuous Deployment]\n  Deploy\nend</code></pre> <ul> <li>CI-CD has increased over years because on cloud-native development it is much more efficient way and is more required way. Compared to traditional Virtual-Machine deployment where it could had been left to be done manually</li> </ul> <ul> <li>CI-CD Implementation<ul> <li>Typically building a CI/CD pipeline consists of the following phases/stages.<ul> <li>Code: Checked into the repository.</li> <li>Build: Build is triggered and deployed in a test environment.</li> <li>Test: Automated tests are executed.</li> <li>Deploy: Code is deployed to stage, and production environments.</li> </ul> </li> </ul> </li> </ul>"},{"location":"1-Software-Engineering/devops/#containers","title":"Containers","text":"<ul> <li>Containers are packages of code together with necessary elements (like runtimes, libraries) required to run a software on any environment.</li> <li>application is abstracted from environmant on which they run. It makes software run anywhere, be it on-prem, cloud or personal-laptop. container is packaged in a way that it can run on any OS and it makes shared use of resources like CPU, Memory, Storage and Network at OS level.</li> <li>Separation of Responsibilities - if you use containers, developers only code and containerize without worrying about deploment env, IT-Ops only deploy container without worrying about version, dependencies, OS-requirements.</li> <li>Compared to Virtual-Machines, Containers are lightweight, use less resource and virtualize at the OS level while VMs virtualize at the hardware level, use more resouse and are heavy.</li> </ul> <ul> <li>Cloud Google - Learn What Are Containers</li> </ul>"},{"location":"1-Software-Engineering/devops/#kubernetes","title":"Kubernetes","text":"<ul> <li>lets you manage containers</li> <li>automated container orchestration project</li> <li>manages containers, machine and services</li> <li>improves reliability and reduces time on devops</li> <li><code>Google Kubernetes Engine</code> (GKE) - Is Google cloud kubernetes service</li> <li><code>Kubernetes cluster</code> is a set of nodes that run containerized applications.</li> <li><code>Edge computing</code> is a distributed computing paradigm that brings computation and storage closer to the sources of data. Often called 'The Edge' or 'at the edge'. Good for time-sensitive data.</li> </ul>"},{"location":"1-Software-Engineering/devops/#docker","title":"Docker","text":"<p>moved to docker notes in Information Technology. Link: Docker Notes</p>"},{"location":"1-Software-Engineering/devops/#jenkins","title":"Jenkins","text":"<ul> <li>open source automation server that facilitates automating CI (Continuous Integration) and DevOps by automating build, test, deploy.</li> <li>it is orchestration tool, that manages 'chain of actions' to acieve CI.</li> <li>it is used to implement CI/CD workflows as pipelines.</li> <li>it is written in Java</li> <li>automation reduces time, minizes error, makes release frequent.</li> </ul> <ul> <li>CI - on commit, code is build, then tested. if test is passed, build is tested for deployment. if deployment is successful on UAT, code is pushed to PROD.</li> <li>Jobs (collection of steps) is called stages.</li> <li>Alternatives - <code>Github Actions</code></li> </ul>"},{"location":"1-Software-Engineering/devops/#github-actions","title":"Github Actions","text":"<ul> <li>GitHub Actions is a continuous integration and continuous delivery (CI/CD) platform that allows you to automate your build, test, and deployment pipeline.</li> <li>You can create workflows that has jobs, and its trigger.</li> <li>on an event, job(s) gets triggered, that has steps, which can be actions or script to execute.</li> </ul> <ul> <li>Docs Github - Actions Learn Github Actions Understanding Github Actions</li> </ul>"},{"location":"1-Software-Engineering/devops/#chef-devops","title":"Chef DevOps","text":"<ul> <li>for Automating Infrastructure Management</li> <li>Chef is an automation tool that provides a way to define infrastructure as code. Infrastructure as code (IC) simply means that managing infrastructure by writing code.</li> </ul>"},{"location":"1-Software-Engineering/devops/#aws-cloudformation","title":"AWS CloudFormation","text":"<ul> <li>manage resource with code, AWS CloudFormation Notes</li> </ul>"},{"location":"1-Software-Engineering/domain-driven-development/","title":"Domain Driven Design","text":"<p>INSTRUCTOR: Allen Holub | Linkedin Learning</p> <p>Domain-Driven Design or DDD is good to manage change and agile way of development.</p>"},{"location":"1-Software-Engineering/domain-driven-development/#what-is-ddd","title":"What is DDD?","text":"<p>Agile - The philosophy of DDD is similar to Agile manifesto, \"Developers and Business People must work together everyday throughout the project\".</p> <p>Modeling - DDD works on model and says that the structure of code models 1-1 to domain you are solving. So it makes sense to business and change is domain can be mapped to change in code.</p> <p>Incremental - Code for MVP (only enough to solve immediate problem), then evolve as you learn more about the problem.</p> <p>DD, follows the Agile Cycle</p> <pre><code>flowchart LR\n    code --&gt; assess --&gt; adapt --&gt; improve --&gt; code</code></pre>"},{"location":"1-Software-Engineering/domain-driven-development/#microservices-and-monoliths","title":"Microservices and Monoliths","text":"<p>Monoliths have problem with changes, deployment, maintainance.</p> <p>Microservives split it but has to be architectured nicely. It should be</p> <ul> <li>small: as big as something you can hold in your head</li> <li>Independently deployable</li> <li>Changing one should have no change required on other</li> <li>Modeled around business</li> <li>decentralized</li> <li>observable - you can see errors</li> <li>autonomous - runs on its own</li> </ul>"},{"location":"1-Software-Engineering/domain-driven-development/#bounded-contexts-and-entities","title":"Bounded Contexts and Entities","text":"<p>Contexts means boundary or scope in business. Eg, for 'A book Company, you have two contexts, Warehouse and Store, both are speparate from each other in real world and have different definition, attributes, properties, roles and functions. However, they have similarities too like product, cost etc. To manage this difference and similarities we use DDD which keeps them separate using microservice and links them only via single channel. Lets understand more..</p> Details / Context Warehouse Store Handles shipping of books sales of books Responsibilities only ship no sales only sales no shipping Ubiquitous Language Properties Weight, Height Author, Review, Length, Readability Ubiquitous Language Actions Pick books, Box Books Sell, Shelf, Organize Actors Warehouse Manager Store Cashier Roles Ship on time Balancesheet updates \\[\\text{Fig: Monolith Design}\\] <pre><code>flowchart TD\nsubgraph Warehouse\n    Trolley --&gt; Picker\n    Trolley ---&gt; Shipping\nend\n\nsubgraph Store\n    Cart --&gt; Product --&gt; Search\nend\n\nsubgraph Finance\n    Payments\nend\n\nProduct --&gt; Order\nPicker --&gt; Order\nOrder --&gt; Cutomer --&gt; Finance</code></pre> <p>In DDD, every entity should be associated with some context and same model in Database may be broken up by context, so in below, Trolley, Product can be broken into separate entity based on context like store or warehouse.</p> \\[\\text{Fig: Domain Driven Design}\\] <pre><code>flowchart LR\n\nsubgraph Warehouse\n    Trolley --&gt; Picker\n    Trolley ---&gt; Shipping\n    Order ---&gt; Customer\\nAddress\n    Order --&gt; p[Product\\nSKU\\nShelf\\nSize\\nWeight]\nend\n\nsubgraph Store\n    Cart --&gt; p1\n    o[Order] ---&gt; c[Customer\\nID]\n    o --&gt; p1[Product\\nSKU\\nimage\\nprice]\n    p1 --&gt; s[Search\\nsub service]\nend\n\nsubgraph Finance\n    Customer\n    Payments\nend\n\nWarehouse &lt;--&gt; Store\nFinance &lt;--&gt; Store</code></pre> <p>Hence, when we get into a domain driven design approach, we're going to move away from the relational database thinking of trying to make giant components, if you will, that could be used everywhere. It is considered a bad thing in the domain driven design world to have a single product-object for example, a single product component, that could be used in multiple context. Because that product component is going to be too big. It's going to be hard to maintain. It's going to be containing lots of internal dependencies and so forth that you just don't want to have. So, in the DDD world there will be two different product entities. One that makes sense in the store context, another one which makes sense in the warehouse context and the fact that they're different is good actually. You do not try and put them together. Now this leads to something that's important in microservices that people often don't understand, which is that if we are implementing a DDD design in microservices, we'll have two different product services. One product service will be warehouse focused, the other product will be store focused. Those two services will have completely different databases. You will not have a single table that contains everything that a product has to have in it. So everything stays context sensitive, context specific, withinside a domain driven design.</p>"},{"location":"1-Software-Engineering/domain-driven-development/#reactive-vs-declarative-system","title":"Reactive vs Declarative System","text":"<p>How entities communicate with each other.</p> <p>In Monolith System, entity sends instruction the same way as in real-world, like shopping card issues invoice to billing service and tells warehouse service to ship items. It declares it, hence Declarative System, they are all tightly-coupled.</p> \\[\\text{Fig: Declarative System in Monolith System}\\] <pre><code>flowchart LR\n\nss[Shopping Cart\\nService] --\"issueInvoice()\"--&gt; bs[Billing Service]\nss --\"queueItemForShipping()\"--&gt; ws[Warehouse Service]\nss --\"emailCustomer()\"--&gt; es[Email Service]\n\nsubgraph us[Upstream Services]\n    ss\nend\n\nsubgraph ds[Downstream Services]\n    bs\n    ws\n    es\nend</code></pre> <p>In Microservice System these function calls are replaced by Network Calls, which adds a complexity. Upstream serive needs to know downstream service. So any change in downstream affects upstream which is a problem.</p> <p>Solution is Reactive Systems which make Asynchronous Services, which takes away the dependecies of communication and relying on other service, rather the service announces, eg, shopping-cart-service announces that order has been placed, now other services may listen to it and do the work. So email-servive sends email, shipping-service ships and billing-service generates invoice. All this is done by decoupling the services. So services need not know each other, and hence we can change them or add new downstream service. This is implemented using publisher-subscriber model.</p> <p>In Publisher-Subscriber Model, the publishers don't know who the subscribers are and vice-versa. More on implementation details in messegins systems like Kafka, RabbitMQ or ZeroMQ.</p>"},{"location":"1-Software-Engineering/domain-driven-development/#event-storming","title":"Event Storming","text":"<p>Best way to do DDD is using technique Event Storming by Alberto Brandolini</p> <p>It is working with Business People to come up with events that mimic the real world events. Do not do all at once, pick a story, model it and repeat.</p> <p>Event is something that happens at business level that customers care about. Eg, 'Order Submitted', 'Payment Received', 'Nightly Reconciliation Done'. Nightly Reconciliation is different as in real world it might not happen but is an automated event in modelling. Keeping in past tense is good convension.</p> <p>How to do Event Storming?</p> <p>Start with a sticky note, write an event, and then arrange these events in timeline. You can start anywhere.</p> \\[\\text{Fig: Sticky Note Color and Meaning}\\] <pre><code>flowchart\ns1[Event\\nOf interest to the\\nBusiness]\nstyle s1 fill:#ffa930,stroke-width:0,color:#fff,width:150px,height:150px\n\ns2[Action\\n What do we want\\nto have happen]\nstyle s2 fill:#a9edf1,stroke-width:0,width:150px,height:150px\n\ns3[Question?\\nThings we have to\\ndo]\nstyle s3 fill:#ff32b2,stroke-width:0,color:#fff,width:150px,height:150px</code></pre> <pre><code>flowchart\ns1[Policy\\nControl how the\\naction\\nplays out]\nstyle s1 fill:#74ed4b,stroke-width:0,width:150px,height:150px\n\ns2[Human Activity\\nWho does the\\nevent?\\nClerk, Cashier etc.]\nstyle s2 fill:#f1f58f,stroke-width:0,width:150px,height:150px\n\ns3[External Systems?\\nOccured in\\noutside\\nworld]\nstyle s3 fill:#d9b8ff,stroke-width:0,width:150px,height:150px</code></pre> <p>You can do this digitally on miro.com and do these steps:</p> <ul> <li>Just think you are working in a physical world and see events that are happening, add them.</li> <li>Add action that are taking place due to the events.</li> <li>Add external Systems in these actions like Payment handler, shipping handler.</li> <li>Add human roles who do these actions, like ,cashier, clerk etc. Entities.</li> <li>Add questions that you have no answers for, like how to talk to payment gateway.</li> <li>Add policies that govern the flow of actions, like cannot ship to certain geo.</li> <li>Add context for entities, like cashier is working in warehouse or store.</li> </ul> <p>This will map the whole business process and makes us ready to think if it would be monolith or microservice, but is where we have mapped domain and can begin domain driven development.</p> <p>Lastly list down all the contexts, eg:</p> <ul> <li>warehouse</li> <li>finance</li> <li>order processing</li> <li>store</li> <li>customer service</li> <li>shipping</li> </ul> <p>Then see how entities in contexts talk to each other. This will give a final map.</p> <p>In case of monolith, these would be classes and objects. In case of microservices these can be individual services.</p>"},{"location":"1-Software-Engineering/domain-driven-development/#abreviations","title":"Abreviations","text":"<ul> <li>DDD - Domain Driven Design</li> </ul>"},{"location":"1-Software-Engineering/domain-driven-development/#links","title":"Links","text":"<ul> <li>Linkedin - Learning Software Architecture Domain Driven Design</li> </ul>"},{"location":"1-Software-Engineering/domain-driven-development/#user-journey","title":"User Journey","text":"<ul> <li>Objective<ul> <li>What are you trying to accomplish?</li> <li>What information to gather and what insight to present?</li> </ul> </li> </ul> <ul> <li>Personas<ul> <li>Who is the customer?</li> <li>Actual user with genuine interest?</li> </ul> </li> </ul> <ul> <li> <p>Stages: Usually any journey can have four stages</p> <ol> <li>Awareness: What is customer input</li> <li>Consideration: What is available in website</li> <li>Decision: Accept the offer or reject</li> <li>Retention: Create loyality.</li> </ol> </li> </ul> <ul> <li>Touchpoints</li> <li>Feedback</li> <li>Pain Points</li> <li>Areas of improvement.</li> </ul> <p>Links</p> <ul> <li>https://uxstudioteam.com/ux-blog/user-journey-design-flows/</li> </ul>"},{"location":"1-Software-Engineering/git/","title":"Git and GitHub","text":"<p>Git is version control software to track changes in source code. GitHub is cloud storage for Gits. We check-in and check-out files to git and it keeps a track of the history.</p>"},{"location":"1-Software-Engineering/git/#installation","title":"Installation","text":"<ul> <li><code>git --version</code> to check the version of git installed.</li> <li><code>where git</code> to find path<ul> <li>to fix this, add the location of folder having <code>git.exe</code> to <code>$PATH</code>.</li> </ul> </li> <li>On mac it was pre installed as part of Xcode Command Line Tools.</li> </ul>"},{"location":"1-Software-Engineering/git/#terminology","title":"Terminology","text":"<ul> <li><code>HEAD</code> variable that points to default branch.</li> <li><code>checkout</code> switch or activate a branch</li> <li><code>remote</code> - Git repo on server. Git stores remote URLs in objects called \"remotes\"<ul> <li><code>origin</code> origin is the default remote for almost all repositories. It has info on fetch/push url, remote branches, track status etc.</li> </ul> </li> <li><code>track</code> - local-branch needs a link to remote-branch to do push and pull.<ul> <li>as a <code>tracking reference to the upstream server</code></li> <li>set <code>the remote as upstream</code> </li> <li>to <code>track remote branch</code></li> <li>current branch <code>has no upstream branch.</code></li> <li><code>-u</code> or <code>--set-upstream</code> or <code>--set-upstream-to=origin/master</code> or <code>--track</code> all are same.</li> <li><code>upstream branch</code> - remote-branch to which local-branch tracks to</li> </ul> </li> <li><code>fetch</code> look for changes but don't download</li> <li><code>pull</code> download changes to local</li> <li><code>fork</code> - copies the code to your-remote-space. Keeps link to original upstream, in case you want to pull future changes.</li> <li><code>clone</code> to local-system for making changes</li> <li><code>pull request</code> Ask remote-repo owner to pull your branch into main branch. This will merge your updates. Do follow developer guidelines for this to be accepted.</li> </ul>"},{"location":"1-Software-Engineering/git/#configuration","title":"Configuration","text":"<ul> <li>You can get/set/edit/unset variables that configure git. Values can be set at global or system or local (repo) level.</li> <li>Global configs are stored in <code>.gitconfig</code> file in <code>home</code> dir usually. It holds YAML data. You can set values using commands and see the values set.<ul> <li><code>~/.gitconfig</code> global</li> <li><code>./.git/config</code> local in repo</li> </ul> </li> <li>username and email are required to be correctly set locally as they are written in each commit (who did it?). This also helps to associate correct account when pushed to remote.</li> <li>format is <code>git config --flag name.key value</code></li> </ul> <ul> <li>read config<ul> <li><code>git config user.name</code> shows name  </li> <li><code>git config user.email</code> shows email</li> <li><code>git config --list --global</code> get all global options, --list or -l</li> <li><code>git config --list --local</code> get all local options</li> <li><code>git config --get remote.origin.url</code> - remote URL, --get is optional</li> </ul> </li> </ul> <ul> <li>write config<ul> <li><code>git config --global user.name \"Your Name\"</code> sets the user name in global file</li> <li><code>git config user.name \"Your Name\"</code> sets user name in local repo file</li> <li><code>git config --global credential.helper 'cache --timeout=72000'</code> caches. so enter credentials once every 20 hours</li> <li><code>git config --global credential.helper store</code> - stores the username and password in store utility</li> <li><code>git config --global --unset http.proxy</code> to unset a variable</li> </ul> </li> </ul>"},{"location":"1-Software-Engineering/git/#read-local-repo","title":"Read Local Repo","text":"<ul> <li><code>git status</code> - current local branch and changes</li> <li><code>git branch</code> - local branches</li> <li><code>git branch -r</code> - remote branches</li> <li> <p><code>git branch -a</code> - all, local and remote branches</p> <ul> <li> <p>output</p> <pre><code>* PRJ-454\ndevelop\nremotes/origin/PRJ-454\nremotes/origin/PRJ-508\nremotes/origin/HEAD -&gt; origin/develop\nremotes/origin/origin/develop\n</code></pre> </li> </ul> <ul> <li>first two are local and then remotes, see <code>HEAD</code> points to one of the remote branch, this is checkout and acts as default.</li> </ul> </li> </ul>"},{"location":"1-Software-Engineering/git/#write-local-repo","title":"Write Local Repo","text":"<ul> <li>create repo<ul> <li><code>git clone https://...git</code> - clone remote repository</li> <li><code>git init</code> - start repository local</li> </ul> </li> </ul> <ul> <li>commits<ul> <li><code>git add --all</code></li> <li><code>git commit -m \"Initial Commit\"</code></li> </ul> </li> </ul> <p>Remotes</p> <pre><code># add remote to existing local repo\ngit remote add origin https://repo.git\n\n# update remote\ngit remote set-url origin https://repo.git\n\n# View remotes added to local repo\ngit remote -v\n\n# View URL of a remote\ngit remote get-url --all &lt;remote-name&gt;\n</code></pre> <ul> <li>branch/pull<ul> <li><code>git checkout -b &lt;new-branch&gt;</code> - creates new-local-branch and checksout. It has no upstream to track.</li> <li><code>git checkout &lt;branch-name&gt;</code> - checksout existing-local-branch</li> <li><code>git checkout head</code> - checksout detached default branch</li> <li><code>git checkout HEAD</code> - checksout default branch, usually master.</li> <li><code>git branch -u origin/remote-branch-name</code> to set-upstream on current-local-branch to track. remote-branch-name should exist.</li> <li><code>git branch -u origin/remote-branch-name local-branch-name</code> to set upstream on another-local-branch.</li> <li><code>git pull</code> - downloads changes from remote to current-local-branch</li> <li><code>git pull origin remote_branch_name</code> - pulls an existing remote branch to local repository, local branch with same name should exist, else do <code>git checkout -b &lt;new-branch&gt;</code></li> <li>If you want to create a new branch to retain commits you create, you may do so (now or later) by using -c with the switch command. Example:<ul> <li><code>git switch -c &lt;new-branch-name&gt;</code></li> </ul> </li> </ul> </li> </ul> <ul> <li>merge - changes from one branch to another, say from <code>hotfix</code> to <code>master</code><ul> <li><code>git checkout branch-merge-into</code> - master if you have to merge changes into master</li> <li><code>git merge branch-merge-from</code> - say hotfix</li> <li><code>git rebase master</code> - if you have new changes in master that you want in your branch. more here.</li> <li>it can go smooth or can have conflicts, then resolve conflicts</li> <li>more details here</li> </ul> </li> </ul> <ul> <li>stash<ul> <li>stash means keep safely. When you have to switch from branch A to B but not commit changes in branch A, then stash changes in A, switch to B, do work, back to A, then stash pop to return to your uncommited changes.</li> <li><code>git stash</code> on branch A, to not commit but keep changes safely</li> <li><code>git checkout B</code> - do changes, commit.. push.. whatever</li> <li><code>git checkout A</code></li> <li><code>git stash pop</code> - to return your uncommited changes in A.</li> <li>more here</li> </ul> </li> </ul> <ul> <li>delete<ul> <li><code>git branch -d my-branch-name</code> - use <code>-D</code> to force delete</li> <li><code>git remote prune origin</code> - removed deleted remote branch from local history in branch -a.</li> </ul> </li> </ul> <p>Remove Files and Folders</p> <p>If a file is tracked in git then adding it to <code>.gitignore</code> will not ignore it. So you will have to do following for it.</p> <pre><code># to remove dir\ngit rm --cached -r the_project/dev_dir\n\n# to remove a file\ngit rm --cached the_project/secrets.py\n</code></pre>"},{"location":"1-Software-Engineering/git/#read-remote-repo","title":"Read Remote Repo","text":"<ul> <li><code>git remote show origin</code> all the information about a remote called origin. Output:<ul> <li>fetch and push URL</li> <li>All remote branches, their tracking status and HEAD branch</li> <li>who pulls from whom</li> <li>who pushes to whom<pre><code>* remote origin\nFetch URL: URL/user/repo.git\nPush  URL: URL/user/repo.git\nHEAD branch: master\nRemote branches:\n  PRJ-001 tracked\n  PRJ-000 stale (use 'git remote prune' to remove)\n  master  tracked\nLocal branches configured for 'git pull':\n  PRJ-001 merges with remote PRJ-001\n  master  merges with remote master\nLocal refs configured for 'git push':\n  PRJ-001 pushes to PRJ-001 (up to date)\n  master  pushes to master  (up to date)\n</code></pre> </li> </ul> </li> </ul> <ul> <li><code>git fetch</code> - reads from remote if changes are available to pull, does not pull</li> </ul>"},{"location":"1-Software-Engineering/git/#write-remote-repo","title":"Write Remote Repo","text":"<ul> <li><code>--set-upstream</code> or <code>-u</code> to set upstream</li> <li><code>git push</code> writes current-local-branch to remote<ul> <li><code>git push -u origin &lt;local-branch-name&gt;</code> - sets upstream as origin/local-branch-name and pushes current-local-branch to remote git. New remote-branch \"local-branch-name\" is created, if not exists.</li> <li><code>git push -u origin local-branch:remote-branch</code> - uses different branch names. Creates new on remote if does not exist.<ul> <li>Output <code>Branch 'local-branch' set up to track remote branch 'remote-branch' from 'origin'.</code></li> </ul> </li> <li><code>git push -u origin HEAD</code> need not write</li> <li><code>git push -u origin</code> - sets upstream as origin and pushes current-local-branch to remote.</li> </ul> </li> <li><code>git push origin</code> pushes all branches to remote</li> </ul>"},{"location":"1-Software-Engineering/git/#pull-requests-pr","title":"Pull Requests - PR","text":"<p>Is a request made by developer (coder) to other developers (reviewers) so that reviewers can review the feature-branch and up on satisfaction they can approve it. On required approvals coder can <code>merge</code> the feature branch to main branch. This will pull the feature branch to main branch, hence named pull request.</p> <p>Review</p> <p>To approve a Pull Request you need to create the same on your local machine. So on terminal do:</p> <pre><code>$ git checkout -b JIRA-123\nSwitched to a new branch 'JIRA-123'\nbranch 'JIRA-123' set up to track 'origin/JIRA-123'.\n\n$ git pull\nremote: Enumerating objects: 22, done.\nremote: Counting objects: 100% (22/22), done.\n...\nAlready up to date.\n</code></pre> <p>If you accidently pull a wrong remote branch to local,eg in local-develop you pulled remote-JIRA-123 , you can reset using</p> <pre><code>git reset --hard origin/develop\n</code></pre> <p>Create a Pull Request</p> <p>When you develop a new feature, you can create a Pull Request so that reviewers can review and approve it.</p> <p>Pull Request needs a source (your feature branch) and destination (the main branch).</p> <p>It is good to have a template for the documentation of Pull Request that reviews can use to review.</p> <pre><code>**Title**\n\nJIRA-123 - The Story Name\n\n\n**Info**\n\nI have done ...\n\nSo that...\n\n**Steps for reviewers**\n\n- Ensure that ...\n- Copy this to ...\n- Add env ...\n- Run below commands ...\n\n`Insert Code`\n\n- Check that ...\n- Verify that ...\n- Approve PR.\n</code></pre>"},{"location":"1-Software-Engineering/git/#how-to-clone-a-repository-from-githubcom","title":"How to clone a repository from GitHub.com","text":"<ul> <li><code>git clone https://github.com/YOUR-USERNAME/YOUR-REPOSITORY</code></li> <li>eg, <code>git clone https://github.com/miguelgrinberg/microblog.git</code></li> <li>This will bring all the files from remote to local directory with git repository on local folder.</li> <li>Now if you have permission to commit to this repo then you can authenticate to push, else change the remote to another repo that you can push to.</li> </ul>"},{"location":"1-Software-Engineering/git/#git-diff-patch-bundle-text-sync","title":"Git Diff Patch &amp; Bundle - Text Sync","text":"<p>all about syncing in network restrictions</p> <p>Git Patch</p> <p>It is changes in text-format. It lets collaborate using text files instead of remote servers.</p> <p>\"A patch in Git is a textual representation of the changes in a commit, formatted in a way that Git can reconstruct the commit and apply it on a branch in another repository.\" - InitialCommit</p> <p>It should be used when you want to share one or more commits over email. It was used to when push/pull to remote was not used, rather email was used to sync changes. So text having information of changes done was sent to another user via email and they can apply to branch in their repository.</p> <p><code>git format-patch</code> command builds patch from multiple commits. It generates the patch file. To generate a patch file is called \"format a patch\".</p> <p>Git  Diff</p> <p><code>git diff</code> command shows difference between two things (files/commits/branches).</p> <p>The output <code>format-patch</code> and <code>diff</code> is same, except that</p> <ul> <li>the <code>format-patch</code> has additional information about the commits, author details, timestamp and file change meta data. This extra information is also merged when applying the patch to another repository.</li> <li>the <code>format-patch</code> can be used over multiple-commits and generates one patch for each commits. You can then apply these patches in another repository to one-by-one to reach each commit.</li> </ul> <p>You can use diff to create patch</p> <pre><code>git diff &lt;old-commit&gt; &lt;new-commit&gt; &gt; diff.patch\n\ngit diff branch1 branch2 &gt; diff.patch\n# diffs branch1 with branch2 and stores output in diff.patch file\n</code></pre> <p>Here, branch2 should be new to list latest changes in patch, else the patch will have reverse changes.</p> <p>Applying Patch</p> <p><code>git apply</code> is command that lets apply patch to a branch. You can also use <code>patch</code> which is separate command for updates and not git specific.</p> <pre><code>git apply diff.patch\n</code></pre> <p>Git Bundle</p> <p><code>git bundle</code> is a command that builds one file having whole repository with all commits. It allows easy file sharing without need of central server. Note: this file is binary not text.</p> <pre><code>git bundle create &lt;bundle_file&gt; &lt;refs&gt;\n\ngit bundle create my_prj.repo HEAD\n</code></pre> <p><code>&lt;refs&gt;</code> can be HEAD or branch name or tags.</p> <p>Links</p> <ul> <li>Initialcommit - git-format-patch</li> <li>Git bundle on Lindedin</li> </ul>"},{"location":"1-Software-Engineering/git/#rename-git-branch","title":"Rename git branch","text":"<ul> <li><code>git branch --move main master</code> - moves main to master, <code>-m</code> is same as <code>--move</code></li> <li><code>git push -u origin master</code> - pushed master to remote as master (new on remote if not exists)</li> <li><code>git branch -a</code> - to view all branches</li> <li>finally make <code>master</code> as default on remote and delete <code>main</code>.</li> <li>or simply rename on github.com.</li> </ul>"},{"location":"1-Software-Engineering/git/#ssh-authentication-for-git","title":"SSH Authentication for Git","text":"<p>You can pull using HTTP/SSH. HTTP lets you pull without authentication and is recommended for public repos.</p> <p>For push you always need authenticated n/w, and recommended method is SSH. Also, to pull private repo you need auth.</p> <pre><code>cd ~/.ssh/\nls -la\n\n# If no id_rsa.pub key, generate\nssh-keygen\n\n# copy public key\ncat ~/.ssh/id_rsa.pub\n</code></pre> <p>Now you would have id_rsa.pub and id_rsa files.</p> <p>The file <code>id_rsa.pub</code> has your public key. This is secret, but can be givent to server/bitbucket/github, so that they have your public key and can authenticate you without your username password.</p> <p>Add key to Github</p> <p>Copy the content, Open GitHub, click your profile icon, settings, SSH and GPC Keys, Click on the new ssh key button. Enter any title and key that you copied.</p> <p>Checking SSH Auth</p> <ul> <li>Check using <code>ssh -T git@github.com</code></li> <li>Output should say <code>Hi &lt;user_name&gt;! You've successfully authenticated, but GitHub does not provide shell access.</code></li> </ul> <p>Fixing SSH issue</p> <ul> <li>error - <code>ssh: connect to host github.com port 22: Connection refused</code></li> <li>change ssh config to use new url and port, Override SSH settings <code>gedit ~/.ssh/config</code> and add</li> </ul> <pre><code># Add section below to it\nHost github.com\n  Hostname ssh.github.com\n  Port 443\n</code></pre> <ul> <li>save and try again.</li> <li>Change your git remote to use SSH URL instead of HTTPS <code>git remote set-url origin git@github.com:YOUR-USERNAME/REPO-NAME.git</code></li> </ul> <p>Links - https://docs.github.com/en/authentication/connecting-to-github-with-ssh</p>"},{"location":"1-Software-Engineering/git/#multiple-remote-repo-username-setup","title":"Multiple Remote Repo Username Setup","text":"<p>You may want to push code to different Remote Repo, eg, tom@github and ram@github. One rsa key does not work on multiple accounts. You need to have different rsa keys. Git Config needs to know which rsa key to use. Also git config need to know name and email.</p> <pre><code># Generate SSH Key for new user\nssh-keygen -t rsa -C \"tom@gmail.com\"\n# name: id_rsa_tom\n</code></pre> <p>Here, -t defines type of key to generate. -C defines comment to the key, this helps see the comment in the key.</p> <p>Now copy content of <code>id_rsa_tom.pub</code> and use it on server to let you authenticate.</p> <p>On local, you need to set git config to use correct user details:</p> <pre><code># Change SSH Key to be used in Git Repo\ngit config --global core.sshcommand 'ssh -i /home/tom/.ssh/id_rsa_tom -F /dev/null'\n\ngit config --global user.name \"Tom Guy\"\ngit config --global user.email \"tom@gmail.com\"\n</code></pre> <p>You can modify above based on which user you want to use for remote push/pull. You can use <code>--local</code> if you are in git dir.</p> <p>In case you have commited by wrong user, you can edit the commit before push.</p>"},{"location":"1-Software-Engineering/git/#fix-commit-author-for-last-commit","title":"Fix Commit Author for Last Commit","text":"<p>Ensure you have correct git config, user.name and user.email. Then amend the comment</p> <pre><code># Configre new username\ngit config user.name username\ngit config user.email user.name@gmail.com\n\n# Amend last commit if done wrong\ngit commit --amend --author=\"User Name &lt;user.name@gmail.com&gt;\"\n\n# Check if last commit is updated\ngit log\n\n# Change SSH Key to be used in Git Repo\ngit config --local core.sshcommand 'ssh -i /home/username1/.ssh/id_rsa_username -F /dev/null'\n\n# Pull GitHub content to rebase\ngit config pull.rebase true \n\n# Pull to avoid conflict\ngit pull origin main\n\n# Push to GitHub with new username\ngit push --set-upstream origin main\n</code></pre>"},{"location":"1-Software-Engineering/git/#fix-commit-author-for-multiple-commits","title":"Fix Commit author for multiple commits","text":"<p>First, update the git config with correct username and email.</p> <p>Find commit number which is older than the commits you want to update author, eg, <code>91d5062</code></p> <pre><code>git rebase -r 91d5062 \\\n    --exec 'git commit --amend --no-edit --reset-author'\n</code></pre> <p>Link: Stackoverflow - How do I change the author and committer name/email for multiple commits?</p> <p>For only last commit message update\"</p> <pre><code>git commit --amend -m \"New commit message\"\n```## Fix Github Commit author for multiple commits\n\nFirst, update the git config with correct username and email.\n\nFind commit number which is older than the commits you want to update author, eg, `91d5062`\n\n```sh\n\ngit rebase -r 91d5062 \\\n    --exec 'git commit --amend --no-edit --reset-author'\n</code></pre> <p>Link: Stackoverflow - How do I change the author and committer name/email for multiple commits?</p> <p>For only last commit message update\"</p> <pre><code>git commit --amend -m \"New commit message\"\n</code></pre>"},{"location":"1-Software-Engineering/git/#handling-conflicts","title":"Handling Conflicts","text":"<p>If you push to git from two different repositories then there may be conflict. eg, you push from mac repo and a cloud repo or ubuntu repo. To handle conflict:</p> <ul> <li>Open conflicted file in editor and look for <code>&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;</code> .</li> <li>You'll see the changes from the HEAD or base branch (github usually) after the line <code>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</code></li> <li><code>========</code>, it divides your changes from the other branch as <code>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;YOUR_BRANCH_NAME</code></li> <li>You can decide if you want keep your branch changes or not. If you want to keep the changes what you did, delete the conflict marker they are, <code>&lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, &gt;&gt;&gt;&gt;&gt;&gt;&gt;</code> and then do a merge.</li> <li>Once done, <code>add commit push</code> :)</li> </ul>"},{"location":"1-Software-Engineering/git/#version-controlling-in-git","title":"Version controlling in GIT","text":"<p>You can see previous versions of file in your git repository.</p> <ul> <li>to see the checkins done</li> </ul> <pre><code>&gt; git reflog\n044cf0e (HEAD -&gt; master) HEAD@{0}: commit: updates\naae1995 HEAD@{1}: commit (initial): first commit\n</code></pre> <ul> <li><code>git show HEAD@{1}:path/to/file.ext</code> show file on terminal</li> <li>press down arrow to navigate and <code>q</code> to quit</li> </ul> <p>or</p> <ul> <li><code>git show -1 filename</code> - shows difference with last revision</li> <li>use -1 or -2 or -3 and so on for going into history.</li> </ul>"},{"location":"1-Software-Engineering/git/#git-tags","title":"Git Tags","text":"<p>First commit and then add a tag, <code>git tag -a v1.4 -m \"my version 1.4\"</code> to add new annotated tag.</p> <p><code>git push origin --tags</code> to push tags to remote.</p> <p><code>git tag -d v0.6</code> to delete tag</p> <p>Remote tag example: `git push -d origin v0.6``</p> <p>Links - Git Tags</p> <ul> <li>https://git-scm.com/book/en/v2/Git-Basics-Tagging</li> </ul>"},{"location":"1-Software-Engineering/git/#fix-corrupt-git","title":"Fix Corrupt Git","text":"<p>Following works on ubuntu</p> <pre><code>git repair\n</code></pre>"},{"location":"1-Software-Engineering/git/#guide-disconnected-sync-when-network-is-restricted","title":"Guide - Disconnected Sync when Network is Restricted","text":"<p>Scenario is that you need to sync repo1 and repo2 but there is no central server or connectivity between them. You can't do git push/pull as there is no remote. In this case we use text file having changes called patch.</p> <p>Here, repo1 is network restricted, repo2 is not.</p> <p>Build Patch</p> <p>On repo1, ensure you are on <code>dev</code> branch and that all changes are commited.</p> <pre><code># Ensure\ngit checkout dev\ngit add .\ngit commit -m \"ready_to_diff\"\n\n# Build Patch\ngit diff master dev &gt; diff.patch\n</code></pre> <p>This generates a patch with all changes done compared to master branch. Now copy the contents of this file to another machine having <code>repo2</code></p> <p>Applying patch on Repo2</p> <p>Now on repo2, ensure you are on master branch and all changes are commited. Create a new branch to patch:</p> <pre><code>git checkout -b master_patched\ntouch diff.patch\n</code></pre> <p>Copy and paste contents to <code>diff.patch</code></p> <pre><code>git apply --reject --whitespace=fix diff.patch\n</code></pre> <p>Once you verify that patch is applied and all changes are in place, delete the patch file</p> <pre><code>rm diff.patch\ngit add .\ngit commit -m \"patched\"\n\n# merge to master\ngit checkout master\ngit merge master_patched\n\n# commit and push\ngit add .\ngit commit -m \"patched_merged\"\ngit push\n</code></pre> <p>Now you have synced changes from repo1 to repo2. Lastly, bring these changes to repo1 so that both are in perfect sync.</p> <p>Resetting Repo 1</p> <p>On repo1</p> <pre><code>git checkout master\ngit branch -D dev\n</code></pre> <p>Now delete all files and folders except <code>.git</code>. Then download zip, extract.</p> <pre><code>mv .git ../tmp.git/\nrm -rf *.*\nrm -rf *\nrm -rf .*\nmv ../tmp.git ./.git\ncurl -L http://github.com/iYadavVaibhav/stem/archive/master.zip &gt; master.zip\nunzip master.zip\nmv ./stem-master/* .\nrm master.zip\nrm -rf stem-master\n</code></pre> <p>Then, commit and create dev branch</p> <pre><code>git add .\ngit commit -m \"downloaded patched\"\ngit checkout -b dev\n</code></pre> <p>Now your repo1 is updated with all changes merged and is ready to work.</p> <p>Link - https://gist.github.com/nepsilon/22bc62a23f785716705c</p>"},{"location":"1-Software-Engineering/git/#guide-git-local-to-remote-basics","title":"Guide - Git Local to Remote Basics","text":"<p>Setup Git</p> <ul> <li>On any folder, do this once</li> <li>eg, <code>mkdir myProject</code> then <code>cd myProject</code></li> <li><code>git init</code> this will create a local git repository on your local drive. Now if you need to add this to a remote git repository, for example, a repository on github.com or bitbucket then you need to add remote to this folder.</li> </ul> <p>Now once you have written your code, you can add and commit new code to local git:</p> <p>Add and Commit code</p> <ul> <li><code>git add .</code> adds all files to git. To add one file, pass filename.</li> <li><code>git diff</code> shows changes made to files.</li> <li><code>git commit -m \"Message\"</code> commits to git with message.</li> <li>optional, <code>cat .gitignore</code> add files that you want git to ignore</li> </ul> <p>Add Remote</p> <ul> <li>Create a new repository on GitHub.com</li> <li>on your local folder, <code>git remote add  [name] [url]</code> will add remote. Here, <code>name</code> can be origin and <code>url</code> is https/ssh url of git repo created online on GitHub.com. Use SSH only if you have SSH authentication setup.</li> <li>Once remote is added to your local git then you can push or pull the files based on the commands below.</li> </ul> <p>Syncing local and remote</p> <ul> <li><code>git pull</code> pulls updates from remote to local</li> <li><code>git push</code> pushes the committed changes from local to remote. We can also specify remote name and branch here. eg:<ul> <li><code>git push -u origin master</code>.</li> </ul> </li> </ul>"},{"location":"1-Software-Engineering/git/#todo","title":"Todo","text":"<ul> <li> - Merge guide to operations</li> </ul>"},{"location":"1-Software-Engineering/git/#links","title":"Links","text":"<ul> <li>Github RSA by Karl Broman</li> </ul>"},{"location":"1-Software-Engineering/js-ecma6-notes/","title":"ECMA 6+","text":"<p>ECMAScript is a JavaScript standard meant to ensure the interoperability of web pages across different web browsers.</p> <p>ECMA 6 or ES 6 or ECMAScript 2015, added new feature to javascript which are highlighted below.</p>"},{"location":"1-Software-Engineering/js-ecma6-notes/#new-changes-in-es6","title":"New Changes in ES6","text":"<p>Some changes in ES6 compared to ES5</p>"},{"location":"1-Software-Engineering/js-ecma6-notes/#variables","title":"Variables","text":"<p><code>let</code> is used to allow block scope of variables.</p> <p><code>const</code> is used to define a fixed value to variable, if it changes, it throws error and does not allow to change.</p> <p>Template strings are used to print formatted strings. use <code>Name is $(fName)</code>, here fName is a variable. it cna be multiline too.</p> <pre><code>let greeting = `Hello, ${name}!`;\n</code></pre>"},{"location":"1-Software-Engineering/js-ecma6-notes/#functions","title":"Functions","text":"<p><code>for ... of</code> new for loop.</p> <pre><code>for (name of names) {\n  console.log(name)\n}\n</code></pre> <p><code>new Symbol()</code> can be added to an object (dict) to give it a unique identifier that never conflicts with its other keys.</p> <p><code>new Map()</code> is new data type that can be used to hold key and value of any type, we can have non-string keys. We can mix the data types. both key and value can be value or key. it is iterable in order of insersion.</p> <p><code>new Set()</code> have unique values. it has index 0,1...n.</p> <p><code>...</code> is spread operator and is used to flatten an array.</p> <p>Object's key/values can be functions as well. In this case objects behave as class having static instance.</p> <p>Desctucturing - If in function argumnet we see <code>{}</code> thats destructuring of object. We can only recieve key of object using this syntax. Eg, <code>function App ( {emp} ) {...}</code></p> <pre><code>// destructuring array\nconst [a,b] = [1,2]; // a=1, b=2\n\n// destructuring object\nconst {p,q} = {p:1,q:2} // use same key name\n// p=1, q=2\n</code></pre>"},{"location":"1-Software-Engineering/js-ecma6-notes/#import-and-export","title":"Import and Export","text":"<p>Now you can import functions from modules that export it. This is more robust dependency management compared to using <code>&lt;script&gt;</code> tag.</p> <p>A JavaScript module is a JS file. It can have a function with <code>export</code> to make use of it in another file. Or import entire JS file.</p> <pre><code>export GRAVITY = 9.81;\nexport default function abc() {\n  console.log('from abc in file1.js !');\n}\n</code></pre> <p>Another file can import it. As it is <code>default</code>, it can be imported with any name.</p> <pre><code>import abc from './file1.js';\nimport { GRAVITY } from './file1.js';\nimport './index.css';\n</code></pre> <p>Provide relative path of file (module). <code>.js</code> is optional. <code>index.css</code> has no export and hence entire file is imported without using <code>from</code> keyword.</p> <p>Other way is to use library, like <code>react</code>. You can import without absolute path. <code>Module</code> is a unit of software that is basically a file that you can refer. It usually has variables and functions. <code>Library</code> is collection of modules that is distributed as <code>package</code> and is managed by a manager like npm or pip. Library has multiple files.</p> <pre><code>import React from 'react';\n</code></pre>"},{"location":"1-Software-Engineering/js-ecma6-notes/#arrow-functions","title":"Arrow Functions","text":"<p><code>=&gt;</code> arrow funcitons.</p> <pre><code>let funcName = (arg1, arg2) =&gt; {\n  statement1;\n  return statement2;\n}\n</code></pre> <p>If there is one argument then remove the parantheses brackets, if there is only one statement the remove the curly barckets and the return keyword. Eg,</p> <pre><code>const square = x =&gt; x * x; // no  ()\n\nconst mult = (x, y) =&gt; x * y; // no {}\n\nconst mult = (x, y) =&gt; {\n  const result = x * y;\n  return result;\n};\n\ndata.map(element =&gt; \n  return (console.log(element))\n);\n</code></pre> <p><code>map</code> is function of <code>Array</code> class.</p> <p><code>generator</code> functions can be used to add a pause to execution of function and make it execute in parts using <code>yield</code> keyword. That is a funciton can yield many outputs or returns. It can be iterated calling <code>funcName.next()</code></p>"},{"location":"1-Software-Engineering/js-ecma6-notes/#promises-and-asynchronous","title":"Promises and Asynchronous","text":"<p>Promises and Asynchronous behaviour - Async means that there is delay in response when requested. A promise is a proxy object that is returned to the caller of an asynchronous operation running in the background. This object can be used by the caller to keep track of the background task and obtain a result from it when it completes.</p> <p>In JS code, promise is executed in background and the execution moves to next statement. Later promise can either resolve or reject and the chained methods gets executed.</p> <p>A Promise is in one of these states:</p> <ul> <li>pending: initial state, neither fulfilled nor rejected.</li> <li>fulfilled: meaning that the operation was completed successfully.</li> <li>rejected: meaning that the operation failed.</li> </ul> <p>When we define then Promise takes a callBackFunction as an argument.</p> <p><code>myPromiseObj.then(funcName() {}</code> anything in this funcName is passed as <code>resolve</code> to Promise.</p> <p>This object can be used by the caller to keep track of the background task and obtain a result from it when it completes. We can use <code>then</code> in chain to execute one func on top of other.</p> <pre><code>const myPromise = new Promise((resolve, reject) =&gt; {\n  setTimeout(() =&gt; {\n    resolve('foo');\n  }, 300);\n});\n\nmyPromise\n  .then()\n  .catch(e =&gt; console.log(e)); // prints error if \n\n// Example\n\nfetch('http://example.com/data.json') // returns apromise\n  .then(r =&gt; r.json()) // output of promise is passed to r\n  .then(data =&gt; console.log(data)) // chained functions\n  .catch(error =&gt; console.log(`Error: ${error}`)); // Error cached\n</code></pre> <p>This is good, but all execution has to be chained which makes it different than a normal function. To make the syntax same as normal funciton we can use <code>async... await</code></p>"},{"location":"1-Software-Engineering/js-ecma6-notes/#async-and-await","title":"Async and Await","text":"<p>Async/Await - we can define an async function, which executes a Promise(), and then await until the promise is resolved. This makes synchronous execution for a asynchronous call.</p> <pre><code>async function f() {\n  const response = await fetch('https://example.com/data.json');\n  // response has header, body, status codes\n  const data = await response.json(); // extract json from body\n  console.log(data);\n}\n</code></pre> <p>Error handling can be done using <code>try.. catch</code> block in this.</p>"},{"location":"1-Software-Engineering/js-ecma6-notes/#classes","title":"Classes","text":"<p>Now you can use classes in ES6.</p>"},{"location":"1-Software-Engineering/js-ecma6-notes/#jsx-javascript-xml","title":"JSX - JavaScript XML","text":"<p>This is not part of ES6, but is an extension to make it easier to use HTML in JavaScript. It lets us write HTML inline and this templates are eaisier to maintain. More in [[React]]</p> <p>Links</p> <ul> <li>Modern JavaScript - https://blog.miguelgrinberg.com/post/the-react-mega-tutorial-chapter-1-modern-javascript</li> <li>Learning ECMAScript 6+ (ES6+) - https://www.linkedin.com/learning/learning-ecmascript-6-plus-es6-plus/using-modern-javascript-today</li> </ul>"},{"location":"1-Software-Engineering/linux-terminal/","title":"Linux Terminal","text":"<p>Here are some basic understandings and commands that can be used on UNIX terminal and eventually on Mac.</p>"},{"location":"1-Software-Engineering/linux-terminal/#mac-specific","title":"Mac Specific","text":""},{"location":"1-Software-Engineering/linux-terminal/#macbook-air","title":"Macbook Air","text":"<ul> <li>Text input source <code>British PC</code> aligns correctly to british keyboards with<ul> <li>left to 1 as backtick and negation</li> <li>left to Z as backslash and pipe <code>\\\\ |</code></li> <li>left to enter as hash and tilde, <code>~ #</code></li> <li>\" above 2, and @ above ?.</li> </ul> </li> </ul>"},{"location":"1-Software-Engineering/linux-terminal/#mac-tools","title":"Mac Tools","text":"<pre><code># SQL Client for MySQL, Postgre\nbrew install --cask sequel-ace\n</code></pre>"},{"location":"1-Software-Engineering/linux-terminal/#homebrew","title":"Homebrew","text":"<ul> <li>package manager for mac,<ul> <li>cask are usually GUIs apps like Sublime.</li> <li>formulae are packages, CLIs, like node.</li> </ul> </li> </ul> <p>Brew Global Commands:</p> <ul> <li><code>brew update</code> - Update brew and cask list, not packages</li> <li><code>brew upgrade</code> -  Upgrade all packages</li> <li><code>brew list</code> - List installed packages and casks</li> <li><code>brew outdated</code> - List outdated packages?</li> <li><code>brew doctor</code> - Diagnose brew issues</li> <li><code>brew cleanup</code> - cleans all packages</li> <li><code>brew services list</code>- lists all services installed</li> </ul> <p>Brew Commands:</p> <ul> <li><code>brew install git</code> -  Install a package</li> <li><code>brew uninstall git</code> -  Remove/Uninstall a package</li> <li><code>brew upgrade git</code> -  Upgrade a package</li> <li><code>brew switch git 2.5.0</code> -  Change versions</li> <li><code>brew list --versions git</code>  See what versions you have</li> <li><code>brew cleanup git</code> Remove old versions</li> </ul> <p>Brew Cask (GUI) commands:</p> <ul> <li><code>brew install --cask firefox</code> Install the Firefox browser</li> <li><code>brew list --cask</code>  List installed applications</li> </ul>"},{"location":"1-Software-Engineering/linux-terminal/#mac-command-line","title":"Mac Command Line","text":"<p>Check defaults before altering hidden settings in OS X</p> <pre><code># defaults action target variable value\ndefaults write com.apple.Finder AppleShowAllFiles true\ndefaults read com.apple.Finder AppleShowAllFiles\n\n# clear defaults\n# defaults delete target variable\ndefaults delete com.apple.Finder AppleShowAllFiles\n\n# dock size\ndefaults read com.apple.Finder AppleShowAllFiles\ndefaults read com.apple.dock tilesize\n# 64\n\n# on 1920x1200, set dock size 41 to capture 1080p without mac menu bar and app title bar\ndefaults write com.apple.dock tilesize -int 41; killall Dock\n\n# on 1920x1200, set dock size 41 to capture 1080p from top\ndefaults write com.apple.dock tilesize -int 99; killall Dock\n</code></pre> <ul> <li>Link: Check defaults before altering hidden settings in OS X</li> </ul>"},{"location":"1-Software-Engineering/linux-terminal/#others","title":"Others","text":"<p><code>diskutil list</code> - lists all disks</p> <p>Format a disk from Mac terminal:</p> <ul> <li><code>diskutil eraseDisk FILE_SYSTEM DISK_NAME DISK_IDENTIFIER</code></li> <li>eg: <code>diskutil eraseDisk FAT32 VY_Disk /dev/disk2</code> or use ExFAT</li> </ul> <ul> <li><code>/Volumes/PenDrive</code> location of usb mounts</li> </ul> <p>Copy to clipboard</p> <ul> <li><code>$ pbcopy &lt; my_filename.ext</code> it copies the content of file to clipboard.</li> <li>It is helpful to quickly copy RSA key to clipboard which you need to paste on, may be, GitHub.</li> </ul> <p>Androids</p> <ul> <li><code>~/.android</code> - google utility folder</li> <li><code>avdmanager list avd</code> - lists all android virtual devices installed.</li> </ul> <p>Uninstalling: Usually check for following dirs and remove:</p> <ul> <li><code>sudo rm /usr/local/mypkg</code></li> <li><code>sudo rm -rf /usr/local/var/mypkg</code></li> <li><code>sudo rm -rf /usr/local/mypkg*</code></li> <li><code>sudo rm -rf /Library/StartupItems/mypkg*</code></li> <li><code>sudo rm -rf /Library/PreferencePanes/MyPkg*</code></li> <li><code>rm -rf ~/Library/PreferencePanes/MyPkg*</code></li> <li><code>sudo rm -rf /Library/Receipts/mypkg*</code></li> <li><code>sudo rm -rf /Library/Receipts/MyPkg*</code></li> </ul>"},{"location":"1-Software-Engineering/linux-terminal/#ubuntu-specific","title":"Ubuntu Specific","text":"<ul> <li> <p>what? - Ubuntu is debain based os, others are Mint, Elementary and PoP OS.</p> <ul> <li>Packages Installation &amp; Managemet - apt pkg dpkg<ul> <li>Packages are maintained in repositories, <code>Main</code>, <code>Universe</code>, <code>Restricted</code> and <code>Multiverse</code>. You can enable the repo based on your requirement.</li> <li>Debain uses <code>dpkg</code> packaging system, for install/uninstall software.</li> <li>PPA - Personal Package Archive - allows application developers to create their own repositories to distribute.</li> </ul> </li> </ul> <ul> <li>APT - Advanced Package Tool<ul> <li>it is CLT UI that works with core libraries to handle the installation and removal of software on Debian, Ubuntu. IT manages dependencies, config files and upgrades/downgrades. <code>apt-get</code> performs installation, search, updates to pkg available on system. works with <code>sudo</code> only.</li> <li>apt=most common used command options from apt-get and apt-cache. It is high level wrapper on old apt-get. Use apt for better UI and info like summary and progress bar.</li> <li><code>dpkg</code> does not handle dependency, while <code>apt</code> does. apt under the hood uses dpkg.</li> </ul> </li> </ul> </li> </ul> <p>Tasks</p> <ul> <li>Add repository<ul> <li><code>sudo add-apt-repository universe</code></li> <li>`` add a PPA repo</li> </ul> </li> </ul> <pre><code># Enable a repo\nsudo add-apt-repository universe\n\n# Add a repo\nsudo add-apt-repository ppa:whatever/ppa\n\n# list the repos added to system\nsudo add-apt-repository --list\n\n# Remove repo\nsudo add-apt-repository --remove ppa:whatever/ppa\n</code></pre> <p>Here, when you enable a repo, itmeans packages can be searched in this repository</p> <ul> <li>Update and Upgrade<ul> <li><code>sudo apt-get update</code> - updates local copy of packages database. The result has :<ul> <li>Hit: no change in pkg</li> <li>Get: update available, downloads details but not the update</li> <li>Ign: ignores.</li> </ul> </li> <li><code>sudo apt-get upgrade</code> updates core system and apps installed.</li> <li>Update one package - <code>sudo apt-get upgrade [package_name]</code>.</li> </ul> </li> </ul> <ul> <li> <p>Install and remove / uninstall</p> <ul> <li><code>sudo apt-get install [pkg1] [pkg2]</code> if you know the name of apps.</li> <li><code>sudo apt-get remove [package_name]</code> to uninstall. but kepps config files.</li> <li><code>sudo apt-get autoremove</code> cleans up unwanted pkg.</li> <li><code>apt list --installed</code> see all that's installed.</li> </ul> <ul> <li>Install a <code>.deb</code> file, eg, Chrome. You can use apt or dpkg. apt is manager and takes care of dependecies.<ul> <li><code>sudo apt install ./name.deb</code> to install with dependencies, or</li> <li><code>sudo dpkg -i /path/to/foo.deb</code> installs, then <code>sudo apt-get install -f</code> fix-broken dependencies.</li> </ul> </li> </ul> </li> </ul>"},{"location":"1-Software-Engineering/linux-terminal/#first-steps","title":"First Steps","text":"<p>Do following in a new install</p> <ul> <li>update and upgrade <code>sudo apt update &amp;&amp; sudo apt upgrade</code></li> <li>codecs flash and fonts <code>sudo apt install ubuntu-restricted-extras</code></li> <li>vlc - <code>sudo apt install vlc</code></li> <li>chrome <code>wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb</code> then <code>sudo dpkg -i google-chrome-stable_current_amd64.deb</code></li> <li>more - https://itsfoss.com/things-to-do-after-installing-ubuntu-20-04/</li> </ul> <p>Clean up</p> <ul> <li>Delete apps - <code>sudo apt purge thunderbird</code></li> <li>remove sw dependencies <code>sudo apt autoremove</code></li> <li>remove partially installed packages <code>sudo apt autoclean</code></li> <li>remove cache <code>sudo apt clean</code></li> </ul> <p>Speed up</p> <ul> <li>disable animations <code>gsettings set org.gnome.desktop.interface enable-animations false</code></li> </ul> <p>Dev Softwares</p> <ul> <li>sublime <code>sudo snap install sublime-text --classic</code></li> <li>vs code <code>sudo snap install code --classic</code></li> </ul> <p>Install Git and Gh:</p> <ul> <li>install git and essentials <code>sudo apt-get install build-essential procps curl file git</code></li> <li>set up git - <code>git config --global user.name \"YOUR NAME\"</code></li> <li>set up git - <code>git config --global user.email \"YOUR EMAIL ADDRESS\"</code></li> </ul> <p>Install Jupyter notebook, see Python Notes https://iyadavvaibhav.github.io/python-notes/</p> <p>Install nsepa and Citrix Workspace:</p> <ul> <li><code>wget http://ftp.br.debian.org/debian/pool/main/n/network-manager/libnm-util2_1.6.2-3+deb9u2_amd64.deb http://ftp.br.debian.org/debian/pool/main/n/network-manager/libnm-glib4_1.6.2-3+deb9u2_amd64.deb</code> download debs</li> <li><code>sudo apt install ./libnm-util2_1.6.2-3+deb9u2_amd64.deb ./libnm-glib4_1.6.2-3+deb9u2_amd64.deb</code> install the debs downloaded</li> <li>Download .deb file from Citrix, amd 64</li> <li><code>cd ~/Downloads</code></li> <li><code>sudo dpkg -i icaclient_21.4.0.11_amd64.deb</code></li> <li><code>sudo apt-get install -f</code></li> </ul>"},{"location":"1-Software-Engineering/linux-terminal/#others_1","title":"Others","text":"<ul> <li><code>lsblk</code> lists disk</li> </ul> <p>Check graphics card installed</p> <ul> <li>check hardware using lshw (list hardware) is a small Linux/Unix tool which is used to generate the detailed information of the system's hardware configuration from various files in the /proc directory. E.g. to see graphics driver - <code>sudo lshw -c video</code></li> <li>check loaded modules using lsmod - it shows which loadable kernel modules are currently loaded. <code>lsmod | grep radeon</code></li> <li>The glxinfo program shows information about the OpenGL and GLX implementations running on a given X display. <code>sudo apt install mesa-utils</code> and <code>glxinfo -b</code></li> <li>Check boot message for graphics card in use <code>dmesg | grep -i radeon</code></li> </ul> <p>Windows on Linux</p> <ul> <li><code>sudo apt-get install playonlinux</code></li> <li>installs wine too, 32 bit</li> <li>to install a program, create a virtual machine and install it.</li> <li>to install nfsmw<ul> <li>create a machine, 32bit</li> <li>add drivers dcdx9 and vcrun6</li> <li>more on install https://www.youtube.com/watch?v=lUqU_uf-o9E</li> <li>more on download https://www.youtube.com/watch?v=no8-fB4MX00&amp;t=1s</li> </ul> </li> </ul>"},{"location":"1-Software-Engineering/linux-terminal/#users-and-groups","title":"Users and Groups","text":"<p>List all users</p> <ul> <li><code>getent passwd</code></li> <li><code>compgen -u</code></li> <li><code>cut -d: -f1 /etc/passwd</code></li> </ul> <p>List all groups</p> <ul> <li><code>compgen -g</code></li> </ul> <p>Add user to group - <code>sudo adduser username group</code></p>"},{"location":"1-Software-Engineering/linux-terminal/#os-setup-and-virtualization","title":"OS Setup and Virtualization","text":"<p>USB Installation</p> <ul> <li>works like a charm,</li> <li>make a bootable live usb/cd - https://linuxhint.com/create_bootable_linux_usb_flash_drive/</li> <li>boot from it and install to another USB - https://www.fosslinux.com/10212/how-to-install-a-complete-ubuntu-on-a-usb-flash-drive.htm</li> <li>space and drive speed is a issue.</li> <li>Clean grub of mac - https://apple.stackexchange.com/questions/337189/unwanted-grub-on-macos-high-sierra</li> <li>Tripe boot mac - https://www.youtube.com/watch?v=B0EuYHFeLAA</li> <li>First steps on Ubuntu - https://www.youtube.com/watch?v=GrI5c9PXS5k</li> </ul> <p>Virtual box add on:</p> <ul> <li><code>sudo apt update</code></li> <li><code>sudo apt install virtualbox-guest-dkms virtualbox-guest-x11 virtualbox-guest-utils</code></li> </ul>"},{"location":"1-Software-Engineering/linux-terminal/#securing-ubuntu-server","title":"Securing Ubuntu Server","text":"<p>Firewall</p> <pre><code># install firewall\n$ sudo apt-get install -y ufw\n\n# enable ports required\n$ sudo ufw allow ssh\n$ sudo ufw allow http\n$ sudo ufw allow 443/tcp\n\n# Add to startup\n$ sudo ufw --force enable\nFirewall is active and enabled on system startup\n\n# Check status\n$ sudo ufw status\n</code></pre> <p>This will install ufw, the Uncomplicated Firewall. Allow external traffic on port 22 (ssh), 80 (http) and 443 (https) only, rest ports are declined.</p> <p>Fail2ban</p> <pre><code># Install fail2ban\n$ sudo apt install fail2ban\n# keeps logs of login attempts and bans if multiple failed attempts are done```\n</code></pre> <p>SSH Only Login</p> <p>Restric to ssh login and no root login. Do <code>nano /etc/ssh/sshd_config</code> and add</p> <pre><code>PubkeyAuthentication yes\nPermitRootLogin no\n</code></pre> <p>and do</p> <pre><code>systemctl restart ssh\n</code></pre>"},{"location":"1-Software-Engineering/linux-terminal/#user-management","title":"User Management","text":"<pre><code># Add user\n$ adduser john\n</code></pre> <p><pre><code># make user root user\n$ usermod -aG sudo john\n</code></pre> Here, it adds user <code>john</code> to sudo <code>group</code>. <code>-a, --append</code> is to add group, use with -G. And <code>-G, --groups GROUP1[,GROUP2,...[,GROUPN]]</code> is to add groups.</p> <pre><code># switch user\n$ su john\n</code></pre> <p>Links</p> <ul> <li>Securing Ubuntu Prod - Miguel</li> </ul>"},{"location":"1-Software-Engineering/linux-terminal/#ssh-secure-shell-protocol","title":"SSH - Secure Shell Protocol","text":"<p>Secure Shell Protocol (SSH) is a network protocol that lets a user access a computer securely over an unsecured network. It uses cryptography to secure communication. It also lets connect using keys and thus avoiding to provide username and password/token on each request.</p> <p>View your RSA Keys</p> <p>RSA keys are stored in your user-directory</p> <pre><code>$ ls ~/.ssh\nconfig  id_rsa  id_rsa.pub  known_hosts\n</code></pre> <p>if <code>id_rsa</code> and <code>id_rsa.pub</code> exist then you have rsa keys, else</p> <p>Generate Your SSH Keys</p> <p>Every machine need to have SSH public and private key to authenticate. To generate key:</p> <pre><code>$ ssh-keygen\n</code></pre> <p>RSA Public Key</p> <p>File <code>id_rsa.pub</code> has your public key, it represents you as a user in crytic key. This is secret, but can be givent to servers (vm, github etc.), so that they have your public key and can authenticate you without password. If a public key of a client-machine is in servers authorized keys, then connection from client-machine is authorized without requiring password of user.</p> <p>Adding your Key to Server</p> <p>On Server your have logged in with password. Copy your public key from your client machined, then on the server do:</p> <pre><code>$ echo &lt;your-copied-public-key&gt; &gt;&gt; ~/.ssh/authorized_keys\n$ chmod 600 ~/.ssh/authorized_keys\n</code></pre> <p>This adds your key to server. Then <code>600</code> changes permission to <code>-rw-------</code> from <code>-rw-rw-r--</code> so that it is secure and only current user can read write it. More information can be found here on Miguel's Linux Deployment Guide</p> <p>On Github.com - Copy the content of Public RSA Key, then open GitHub, click your profile icon, settings, SSH and GPC Keys, Click on the new ssh key button. Enter any title (usually your user and machine) and key that you copied. This key lets the user and machine be identified without password. More on this here</p> <p>Once added, the concept is that <code>ssh</code> command on your terminal will do a cryptographic operation with your private key then this magic is sent to server which uses your copied public key to validate and authorize you. GULP it for now..!</p> <p>What is the known_hosts file in SSH?</p> <p>The known_hosts file stores the public keys of the server accessed by a user (on client). It stores identity of server so that user accidently does not connect to unknown host and get attacked. They are stored in <code>/etc/ssh/known_hosts</code> for system hosts or in <code>~/.ssh/known_hosts</code> for each user.</p> <p>If the host is unknow you see following message:</p> <pre><code>$ ssh -p 2222 vaibhav@127.0.0.1\nThe authenticity of host '[127.0.0.1]:2222 ([127.0.0.1]:2222)' can't be established.\nED25519 key fingerprint is SHA256:UystFEOFRcH1YLdhYgr543JgrecGeZ2jmg02naQQZVw.\nThis key is not known by any other names\nAre you sure you want to continue connecting (yes/no/[fingerprint])? yes\nWarning: Permanently added '[127.0.0.1]:2222' (ED25519) to the list of known hosts.\n</code></pre> <p>Otherwise it simply lets connect.</p> <p>Install SSH Server and Enable on Ubuntu</p> <p>Any server needs to run SSH server and enable SSH port on itself so that it can accept SSH connections.</p> <pre><code># install ssh\nsudo apt install openssh-server\n\n# view status\nsudo systemctl status ssh\n\n# allow ssh port on firewall\nsudo ufw allow ssh\n\n# get IP address of ubnutu machine\nip a\n</code></pre> <p>You need to secure Ubunut server as well to prevent misuse, see \"Securing Ubuntu Server\" for more details.</p> <p>Now this machine can accept SSH connections.</p>"},{"location":"1-Software-Engineering/linux-terminal/#scp-file-transfer-via-ssh","title":"SCP - File Transfer via SSH","text":"<p>To copy a file:</p> <pre><code># Syntax\nscp &lt;source&gt; &lt;destination&gt;\n\n# To copy a file from B to A while logged into B:\nscp /path/to/file username@a:/path/to/destination\n\n# To copy a file from B to A while logged into A:\nscp username@b:/path/to/file /path/to/destination\n</code></pre> <p>If you want to copy a directory from machine a to b while logged into a:</p> <pre><code>scp -r /path/to/directory user@machine_b_ipaddress:/path/to/destination\n</code></pre> <p>If you want to copy a directory from machine a to b while logged into b:</p> <pre><code>scp -r user@machine_a_ipaddress:/path/to/directory /path/to/destination\n</code></pre> <p>Example:</p> <pre><code>scp -r .\\abc\\ john@192.168.10.10:/tmp/abc\n</code></pre>"},{"location":"1-Software-Engineering/linux-terminal/#linux-ways","title":"Linux Ways","text":"<ul> <li><code>alias vynote=\"subl ~/path/to/file/notepad.txt\"</code> add to <code>.bash_profile</code> to make shortcut</li> </ul> <p>ENVs:</p> <ul> <li><code>source activate [path to env]</code> activates env</li> <li><code>source deactivate</code> deactivates enn</li> </ul> <p>Emac Basic</p> <ul> <li>Press <code>ctrl + c + x</code> to save and exit a file.</li> </ul> <p>Other</p> <ul> <li>everything global is installed in <code>/usr/local/bin/</code></li> <li>use PostMan for http requests to REST routes</li> </ul> <ul> <li><code>rmdir</code> removed empty dir</li> <li><code>rm -rf</code> removes non/empty dir and files forcefully</li> <li><code>rm</code> removes files not directories.</li> </ul> <ul> <li>ls - shows file in dir<ul> <li><code>ls -l</code> as a list</li> <li><code>ls -h</code> human readable sizes</li> <li><code>ls -r</code> reverse order</li> <li><code>ls -t</code> sorts by last modified date and time, default is latest at top</li> <li><code>ls -a</code> shows all, includes hidden files</li> <li>Examples<ul> <li><code>ls -lt | head</code> shows last 10 modified files.</li> </ul> </li> </ul> </li> </ul> <ul> <li>head/tail - show first or last lines<ul> <li><code>head -n 2</code> - <code>--lines</code> - shows first 2 lines.</li> <li>Examples<ul> <li><code>ls -lt | head -n 5</code> shows only 5 files from prev command.</li> <li><code>head -5 notes.txt</code> - shows top 5 lines from file.</li> </ul> </li> </ul> </li> </ul> <p>Find and Delete</p> <pre><code># find and delete files\nfind ./path/to/dir -name \"*.db\" -type f -exec rm \"{}\" +\n</code></pre>"},{"location":"1-Software-Engineering/linux-terminal/#shell-configuration","title":"Shell Configuration","text":"<p>.zprofile is same as .bash_profile is for bash shell. Also same as .zshenv, this file is to set path and env.</p> <p>.zshrc is to set look and feel.</p> <p>To make zsh use auto-suggestiona and syntax highlight</p> <pre><code>brew install zsh-syntax-highlighting zsh-autosuggestions\n</code></pre> <p>Then, add to <code>~/.zshrc</code>:</p> <pre><code># &gt;&gt;&gt; Syntax Highlight Begins &gt;&gt;&gt;\nautoload -Uz colors\nautoload -Uz compinit\nautoload -Uz down-line-or-beginning-search\nautoload -Uz up-line-or-beginning-search\nautoload -Uz vcs_info\n\ncolors\ncompinit\n\nzle -N down-line-or-beginning-search\nzle -N up-line-or-beginning-search\n\nbindkey \"^[[A\" up-line-or-beginning-search\nbindkey \"^[[B\" down-line-or-beginning-search\nbindkey \"${terminfo[kcuu1]}\" up-line-or-beginning-search\nbindkey \"${terminfo[kcud1]}\" down-line-or-beginning-search\nbindkey \"${terminfo[kcbt]}\" reverse-menu-complete\nbindkey \"${terminfo[kdch1]}\" delete-char\n\nprecmd_functions+=(vcs_info)\nvcs_info_format=\"%{$fg_bold[cyan]%}(%{$fg_bold[red]%}%b%{$fg_bold[cyan]%}) \"\n\nzstyle ':completion:*' menu select\nzstyle ':vcs_info:*' enable git\nzstyle ':vcs_info:*' formats \"$vcs_info_format\"\nzstyle ':vcs_info:*' actionformats \"$vcs_info_format\"\n\nexport PROMPT='%(?:%{$fg_bold[green]%}:%{$fg_bold[red]%})\u279c %{$fg_bold[cyan]%}%c %{$vcs_info_msg_0_%}%{$reset_color%}'\nsetopt PROMPT_SUBST\n\nexport HISTFILE=~/.zsh_history\nexport HISTSIZE=10000\nexport SAVEHIST=1000000\nsetopt SHARE_HISTORY\nsetopt HIST_FIND_NO_DUPS\n\nsetopt AUTOCD\n\nsource /opt/homebrew/share/zsh-autosuggestions/zsh-autosuggestions.zsh\nsource /opt/homebrew/share/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh\n\n# &lt;&lt;&lt; Syntax Highlight Ends &lt;&lt;&lt;\n</code></pre>"},{"location":"1-Software-Engineering/linux-terminal/#youtube-dl","title":"youtube-dl","text":"<ul> <li><code>youtube-dl --extract-audio --audio-format mp3 -o \"%(title)s.%(ext)s\" http://www.youtube.com/watch?v=fdf4542t5g</code> -o is --output of filename.</li> </ul>"},{"location":"1-Software-Engineering/linux-terminal/#termux-linux-on-android","title":"Termux - Linux on Android","text":"<ul> <li><code>pkg</code> is similar to <code>apt</code>, it is another package manager in termux, it is even higher wrapper on apt. You can use both.</li> </ul> <ul> <li>Installing termux<ul> <li>Termux is no more updated on play store, so you can use old or download latest from F-droid.org</li> <li> <p>Once installed do following on phone</p> <pre><code>termux-setup-storage\ntermux-change-repo # (I selected Albatros)\npkg update &amp;&amp; pkg upgrade\nexit\n</code></pre> </li> </ul> </li> </ul> <ul> <li> <p>Enabling SSH</p> <ul> <li> <p>SSH lets you use phone linux from another laptop. Do this on phone:</p> <pre><code>pkg update &amp;&amp; pkg upgrade\npkg install openssh -y # ssh server is now installed\npasswd # set a password to login from another device\nsshd  # It's time to run the server\nifconfig # to check the IP of phone, ensure wifi is enabled\n</code></pre> </li> </ul> <ul> <li>Now open cmd or terminal on a machine connected to same wifi and do <code>ssh 192.168.1.17 -p 8022</code>, where IP is address of phone.</li> </ul> </li> </ul> <ul> <li>Termux - working with packages<ul> <li>Search:  <code>pkg search &lt;query&gt;</code></li> <li>Install: <code>pkg install &lt;package&gt;</code></li> <li>Upgrade: <code>pkg upgrade</code></li> </ul> </li> </ul> <ul> <li>Termux - subscribing to additional repositories<ul> <li>Root:    <code>pkg install root-repo</code></li> <li>X11:     <code>pkg install x11-repo</code></li> </ul> </li> </ul> <ul> <li> <p>Setting Python and Git</p> <pre><code>pkg install python -y\npkg install git -y\n</code></pre> </li> </ul> <ul> <li> <p>Crontab on Termux</p> <ul> <li> <p>it is used to schedule jobs in linux and make them run at certain intervals.</p> <pre><code>pkg install cronie termux-services\nsv-enable crond\n\n# `service-daemon start` or restart terminal \n\n~/code$ crontab -l\nno crontab for u0_a234\n\ncrontab -e \n\n0 * * * * cd /data/data/com.termux/files/home/code &amp;&amp; /data/data/com.termux/files/usr/bin/python tbd_logger.py\n</code></pre> </li> </ul> <ul> <li>Example run logs<ul> <li><code>time 2023-04-22 19:39 - battery 85% - cron at 1m</code></li> <li><code>time 2023-04-23 12:22 - battery 68% - cron switched to 1hr</code></li> </ul> </li> </ul> </li> </ul> <ul> <li>Links<ul> <li>Termux - Docs</li> <li>Termux - Installing</li> <li>Termux - Backing up</li> <li>Gist - How to install termux</li> <li>Youtube - installing linux on android Medium - Installing Linux</li> <li>Use Ubuntu in Termux</li> </ul> </li> </ul>"},{"location":"1-Software-Engineering/linux-terminal/#alpine-linux","title":"Alpine Linux","text":"<p>Installation detailed steps can be found in this Alpine Linux Virtual box Guide</p>"},{"location":"1-Software-Engineering/linux-terminal/#mobile-linux","title":"Mobile Linux","text":"<p>Everything related to iOS and Android Linux.</p> <p>Install Termux App on Android.</p> <pre><code>termux-setup-storage\ntermux-change-repo\npkg update\n\npkg install git\ngit --version\n\npkg install python\n\npkg install openssl\n\npip install --upgrade youtube-dl\n\nyoutube-dl -i PLyAyDdlMr3GOHFBt0IzgvED-n3uhwyrhd\n</code></pre> <p>Install on <code>a-Shell</code> iOS App</p> <pre><code>cd ~/code\nwget -qO- http://dl-cdn.alpinelinux.org/alpine/v3.12/main/x86/apk-tools-static-2.10.6-r0.apk | tar -xz sbin/apk.static &amp;&amp; ./sbin/apk.static add apk-tools &amp;&amp; rm sbin/apk.static\napk add python3\napk add py3-pip\npip install youtube-dl\n</code></pre>"},{"location":"1-Software-Engineering/linux-terminal/#links","title":"Links","text":"<ul> <li>terminal cheat book - mac</li> <li>Setting cronjob on mac</li> <li>vim Getting Started</li> <li>cheat book - https://github.com/0nn0/terminal-mac-cheatsheet#english-version</li> </ul>"},{"location":"1-Software-Engineering/networking/","title":"Networking","text":"<p>Modem connects to internet. Router connects devices to modem.</p> <pre><code>graph LR;\n\nrouter[Router\nDHCP Server]\n\nd1[Laptop\nPvt IP: 192.168.1.10] &lt;--&gt; router\nd2[Mobile\nPvt IP:  192.168.1.22] &lt;--&gt; router\nd3[TV\nPvt IP:  192.168.1.14] &lt;--&gt; router\nd4[Tablet\nPvt IP:  192.168.1.12] &lt;--&gt; router\n\nrouter &lt;--&gt; Modem[Modem\nPublic IP\n82.129.70.44]\n\nModem &lt;--ISP--&gt; Internet\n\nsubgraph LAN[Local Area Network]\n  d1\n  d2\n  d3\n  d4\n  router\nend</code></pre> <p>Router lets you connect multiple devices together to share internet connection. It gives each device a private IP address like <code>192.168.1.xxx</code>. The router (or modem) itself has a public IP address like <code>82.129.70.xxx</code> which is given by ISP. So your device requests are uniquely addressed on internet by the comnination of both public and private addresses.</p> <p>No two routers on the internet can have the same Public IP address.</p> <p>Eg, Your flat has a wifi network <code>Wifi-Flat-101</code> has public IP <code>82.123.123.101</code> and has two devices connected, a laptop with private IP address <code>192.168.1.10</code> and a mobile with private ip address <code>192.169.1.14</code>. You neighbour's flat will a separate network with say wifi name <code>Wifi-Flat-105</code> and may have public IP address <code>82.123.123.205</code> and has a TV connected to wifi having private IP address <code>192.168.1.10</code>. Notice, how in separate networks private IP can be same and how two public IP address on separate router can't be same.</p> <p>Now these private IP address are not magically made available to any new device connecting to router. It is done by DHCP Server on the routers. It has an IP address and port and can be found using <code>ipconfig /all</code> on windows. DHCP Server's IP is like <code>192.168.1.1</code> and port <code>68</code>.</p> <p>Similarly router has DNS Server which lets map custom domain name to address. Eg, type <code>http://hyperhub/</code> and you are taken to <code>http://192.168.1.1:80/</code> which is GUI to manage router. DNS IP is like <code>192.168.1.1</code> and port <code>53</code>.</p> <p>DHCP Server - \"A DHCP Server is a network server that automatically provides and assigns IP addresses, default gateways and other network parameters to client devices\". If you do not enable DHCP on router, then only one machine can connect to internet.</p> <p>DNS Server or domain name server lets map domain name to IP address and port.</p> <p>Localhost or <code>127.0.0.1</code> is the IP address of local computer that is the one you are working on and is used by machine to connect and communicate with itself.</p> <p>A machine can have more than one IP address and can be done using multiple network card.</p> <p>Starting a server means it can listen to requests. It will listen to a particular Address and Port. So if a server is started at <code>127.0.0.1:5000</code> it listens to request from local machine only.</p> <p>if a server is started at <code>0.0.0.0:5000</code> it listens to request from all IP addresses a machine has. Usually machine has a private IP address if connected to router and an address of itself. So this server will listen to requests on <code>127.0.0.1:5000</code> and say <code>192.168.1.15</code>.</p>"},{"location":"1-Software-Engineering/oops-python/","title":"OOPs in Python","text":"<ul> <li>Object is a Class and has<ul> <li><code>attributes</code> - variables</li> <li><code>methods()</code> - functions</li> </ul> </li> </ul>"},{"location":"1-Software-Engineering/oops-python/#instance-class-or-static-methods","title":"Instance, Class or Static Methods","text":"<ul> <li>Instance - method is specific to instance of class. First argument is <code>self</code> (referece to object) and is auto passed.</li> <li>Class - method can be called without instantiating the class. First argument is <code>cls</code> (referece to class itself) and is auto passed.</li> <li>Static - similar to class but can be called without passing <code>cls</code></li> </ul> <pre><code>class Book:\n\n    # Class attribute, common for all instances\n    # executes when class is imported\n    kind = \"Novels\"\n\n    def __init__(self, row):\n        # executes when object is created\n        # row is db record\n\n        # instance attribute\n        self.title = row.title\n        self.author = row.author\n        self.year = getattr(row, 'year', None) # if year is not in row\n\n    def age(self):\n        # return age of book\n        # this is per instance of book\n        return (year_today - self.year)\n\n    @classmethod\n    def fetch_book(cls, id):\n        # fetch the row from db or api\n        row = fetch_service(id) # some other database function\n        obj = cls(row)\n        return obj\n\n    @staticmethod\n    def fetch_books():\n        # fetch the row from db or api\n        objs = []\n        rows = fetch_service('all')\n        for row in rows:\n            # create object and add to objcts list\n            objs.append(Book(row))\n        return objs\n\nbook_obj = Book.fetch_book(21) # class method\nbook_obj.age() # instance method\n\nbook_objects = Book.fetch_books() # static method, no auto arg passed\n</code></pre> <ul> <li>More about different class methods here on stackoverflow</li> </ul>"},{"location":"1-Software-Engineering/oops-python/#inheritance","title":"Inheritance","text":"<ul> <li>to be added</li> </ul>"},{"location":"1-Software-Engineering/oops-python/#links","title":"Links","text":"<ul> <li>RealPython OOPS - https://realpython.com/python3-object-oriented-programming/</li> </ul>"},{"location":"1-Software-Engineering/regex/","title":"Regex","text":"<p>Regex are in simple terms \"a sequence of characters that define a search pattern\"</p> <ul> <li>basic is <code>*.gif</code> where * means anything.</li> </ul> <ul> <li><code>Position</code> of search<ul> <li><code>^</code> - starts with - <code>^Simple</code> - any line that start with this</li> <li><code>$</code> - ends with - <code>park$</code> - any line that end with this</li> </ul> </li> </ul> <ul> <li><code>Frequency</code> of occurrence<ul> <li><code>*</code> - 0 or more - <code>ab*</code> a followed by zero or more b's</li> <li><code>+</code> - 1 or more - <code>ab+</code> - the letter a followed by one or more b's</li> <li><code>?</code> - 0 or 1 - ab? - zero or just one b</li> <li><code>{n}</code> - n exactly - ab{2} - the letter a followed by exactly two b's</li> <li><code>{n,}</code> - n or more - ab{2,} - the letter a followed by at least two b's</li> <li><code>{n,y}</code> - n to y occurrence - ab{1,3} - the letter a followed by between one and three b's</li> </ul> </li> </ul> <ul> <li><code>Class</code> of characters - list or range, and are case-sensitive<ul> <li>Range - <code>[a-z]</code> or <code>[A-Z]</code> or <code>[0-9]</code></li> <li>List - <code>[a,d,p,w]</code> or <code>[adpwKRM]</code> - match any single character</li> <li>Mix - <code>[A-Z2-4pw]</code> - matches range A-Z, 2-4 and literally p, and literally w</li> <li>Except - <code>[^abXyP-Q]</code> and character except what is in</li> </ul> </li> </ul> <ul> <li><code>Flags</code> to search for special types of characters without needing to list them in a range<ul> <li><code>.</code> m any character</li> <li><code>\\s</code> - whitespace - <code>\\S</code> opposite</li> <li><code>\\w</code> - word - <code>\\W</code> not a word</li> <li><code>\\d</code> - digit (number) - <code>\\D</code> not a digit</li> <li><code>\\b</code> - word boundary - <code>ate\\b</code> finds ate at end of word, eg, plate but not gates.- <code>\\B</code> not boundary</li> </ul> </li> </ul> <ul> <li>Other<ul> <li><code>\\n</code> <code>\\r</code> <code>\\t</code> <code>\\0</code> - new line, carriage, tab, null</li> <li><code>|</code> - or - <code>The (?:cat|dog) jumps</code> matches cat or dog</li> </ul> </li> </ul> <ul> <li>Regex Groups<ul> <li>In Find: you can use regex with \"capturing groups,\" e.g. <code>I want to find (group1) and (group2)</code>, using parentheses</li> <li>In Replace: you can refer to the capturing groups via $1, $2 etc.</li> <li>Eg, VS Code - to use what you found in replace use <code>$1</code>, eg, find <code>&lt;h1&gt;(.+)&lt;/h1&gt;</code>, replace <code>&lt;b&gt;$1&lt;/b&gt;</code>. Replaces <code>&lt;h1&gt;Todo&lt;/h1&gt;</code> with <code>&lt;b&gt;Todo&lt;/b&gt;</code></li> <li>more here.</li> </ul> </li> </ul>"},{"location":"1-Software-Engineering/regex/#regex-in-javascript","title":"Regex in JavaScript","text":"<pre><code>// find and replace with space globally for all occurance\nsomeString.replaceAll(new RegExp('[:,/,.]','g'),' ')\n\n// replace an occurance in string\nsomeString.replace(/h1/,'h2')\n</code></pre>"},{"location":"1-Software-Engineering/regex/#regex-in-python","title":"Regex in Python","text":"<pre><code>import re\n\ntext = 'aaa@xxx.com bbb@yyy.net ccc@zzz.org'\n\n# print(re.sub('pattern', 'replacement', text))\nprint(re.sub('[a-z]+@', 'ABC@', s))\n\n# ABC@xxx.com ABC@yyy.net ABC@zzz.org\n</code></pre>"},{"location":"1-Software-Engineering/regex/#snippets","title":"Snippets","text":"<ul> <li>multiline with one line <code>\\s\\n+</code> <code>\\n</code></li> <li>remove end whitespace <code>\\s+\\n</code> <code>\\n</code></li> </ul> <ul> <li> <p>Remove single line comments // from code:</p> <ul> <li><code>\\/\\/.*$\\n</code> - Finds all single line comments starting with //.<ul> <li><code>\\/\\/</code> - string has //</li> <li><code>.*</code> - then has anything after that</li> <li><code>$\\n</code> - then matches next line as well.</li> </ul> </li> </ul> </li> </ul> <p>Remove Trailing whitespace</p> <ul> <li>Find: <code>\\s+$</code></li> <li>Replace: ``</li> </ul> <p>Remove trailing whitespace from only those lines that have text</p> <ul> <li>Find: <code>(\\S+)(\\s+)$</code></li> <li>Replace: <code>$1</code></li> <li>It finds trailing text as first group <code>(\\S+)</code>, then trailing whitespace just after text as second group <code>(\\s+)</code> and at end of line <code>$</code>.</li> <li>Then replaces both groups by just the first group <code>$1</code>.</li> </ul> <p>Whatsapp Text from Markdown</p> <ul> <li>This snippet lets you convert markdown to be formatted in most correct way for whatsapp chat.</li> <li>Find <code>**</code> Replace <code>*</code>. Finds bold, makes them bold for Whatsapp.</li> <li>Find <code>#+ (.+)\\n</code> Replace <code>*$1*\\n</code>. Finds headings, replaces them with bold line.</li> </ul>"},{"location":"1-Software-Engineering/regex/#links","title":"Links","text":"<ul> <li>Check and Validate on Regex101</li> </ul>"},{"location":"1-Software-Engineering/security/","title":"Security, IAM, RBAC","text":"<p>role permissions policies users groups rbac iam and more</p>"},{"location":"1-Software-Engineering/security/#access-and-authorization","title":"Access and Authorization","text":"<p>Access give permission to connect.</p> <p>Authorization gives permission to a do an action on a resource.</p> <p>Usually, a user first needs access to system (which is when user logs in) and then needs proper authorization to do things (like add a post, edit a post, delete a comment) etc.</p>"},{"location":"1-Software-Engineering/security/#user-roles-and-permissions","title":"User Roles and Permissions","text":"<p>There are two methods to implement authorization:</p> <ul> <li>Role-based access control (RBAC)</li> <li>Attribute-based access control (ABAC)</li> </ul>"},{"location":"1-Software-Engineering/security/#role-based-access-control","title":"Role Based Access Control","text":"<p>Broadly speaking, in a system, Actions are performed on Resources by User within a Session. These information are enought to implement Permission to control access.</p> <p>In a small system, user is stored in a <code>table</code>, action is a <code>method</code> and permission are applied with method to control access. Eg,</p> <pre><code>@role_required('Author')    # User: Role should be Author\ndef edit_post(id):          # resource: Post, action: edit\n    ...\n</code></pre> <p>To do this in more systematic way for big system, you can do, Model-Driven Role Base Access Control (MD-RBAC)</p>"},{"location":"1-Software-Engineering/security/#model-driven-role-base-access-control","title":"Model-Driven Role Base Access Control","text":"<p>Role, Permission, Resource, Action are added as table in database. Mapping tables like User-Role and Role-Permission are also added to handle m-m mappings. Besides this, the system has User table and individual resources tables like Post, Comments etc.</p> <p>Permission Table - It stores information about resources and actions that can be performed on resources.</p> <p>For more complex appliaction, you can have <code>user_group</code> table to implement Hierarchial role based access control with/out model tables.</p>"},{"location":"1-Software-Engineering/security/#attribute-based-access-control","title":"Attribute Based Access Control","text":"<p>Everything has associated attributes to them, or every object has attributes, eg:</p> <ul> <li>User has name, role, age, location</li> <li>Product has name, category, price, quantity, origin</li> <li>Action has what (read, edit), when, where</li> </ul> <p>So, on a request, there is a model which checks these entities and decides to grant or deny the request.</p> <p>Eg, User from Delhi wants to buy a GTA-5 CD from China. Here attributes are:</p> <ul> <li>Subject: Name - Rahul</li> <li>Subject: Age - 15</li> <li>Subject: Location - Delhi, IN</li> <li>Action: Buy</li> <li>Action Time: 12:55 am on Sunday 21 January 2024</li> <li>Object: Name: GTA-5</li> <li>Object: Category: Game 16+</li> <li>Object: Price: USD 20</li> <li>Object: Origin: New York, US</li> </ul> <p>Now based on these attributes in the request, a Policy Based Desicion Making System can either grant or deny the request. Policy defines 5Ws - what, where, when, who and why.</p> <p>More on RBAC and ABAC at Nikhilajain - Post User Role Permission Model</p>"},{"location":"1-Software-Engineering/security/#links","title":"Links","text":"<ul> <li>Aalpha - Blog Best Practice For Designing User Roles And Permission System</li> <li>Bootcamp Uxdesign - Designing Roles And Permissions Ux Case Study B1940f5a9aa</li> </ul>"},{"location":"1-Software-Engineering/software-diagrams/","title":"Software Diagrams","text":"<p>Diagrams are often used for plannig and sharing ideas and understanding in a group. Software engineering you can have following</p>"},{"location":"1-Software-Engineering/software-diagrams/#type-of-diagrams","title":"Type of diagrams","text":"<ul> <li>Flowcharts / Process Diagrams - Process Flow Diagram</li> <li>UMLs - Unified Modeling Language. This general-purpose modeling language helps you visualize a system\u2019s design. Can show:</li> <li>structural information (such as class diagrams),</li> <li>behavioral information (like use case diagrams), or</li> <li>display interactions (a sequence diagram).</li> <li>ORG Charts - herarchal organization mapping in parent-child form.</li> <li>ER Diagram - shows an entity\u2013relationship model which describes interrelated things of interest in a specific domain of knowledge. A basic ER model is composed of entity types (tables) and specifies relationships that can exist between entities.</li> </ul>"},{"location":"1-Software-Engineering/software-diagrams/#methodology-generalization","title":"Methodology / Generalization","text":"<ul> <li>C4model is a technique to stucturize your diagrams. It is an \"abstraction-first\" approach to diagramming software architecture. What are the 4C\u2019s?</li> <li>Context - people and system, user and app</li> <li>Container - containers in a system. app has front end, middle, db and api.</li> <li>Component - components in a container. middle has user management and loggin system.</li> <li>Code - code or physical view of components. Class diagram, ER diagram.</li> </ul>"},{"location":"1-Software-Engineering/software-diagrams/#tools-to-build","title":"Tools to build","text":"<ul> <li>Draw.io</li> <li>Visio</li> <li>LucidChart</li> <li>Excalidraw - Article</li> </ul>"},{"location":"1-Software-Engineering/software-diagrams/#excalidraw","title":"Excalidraw","text":"<p>You can draw sketches, it is web based PWA.</p> <p>VS Code extension can be used to build png svg that are editable.</p> <ul> <li>Animate</li> <li>Slide show and GIF</li> </ul>"},{"location":"1-Software-Engineering/software-diagrams/#draw-io","title":"Draw io","text":"<p>On the web https://draw.io/.</p> <p>VS Code extension can be used to build png svg that are editable.</p> <p>Animations</p> <ul> <li>can be enabled on web app, on extension you can change property.</li> <li>to make all arrows animate<ul> <li>go to <code>Extras &gt; Edit Diagram...</code></li> <li>Find, <code>entryX</code>, Replace with <code>flowAnimation=1;entryX</code></li> </ul> </li> </ul> <p>Mermaid</p> <ul> <li>diagram can me imported using <code>Arrange &gt; Insert &gt; Advanced &gt; Mermaid...</code>.</li> <li>However, in extension, mermaid is imported as image.</li> </ul> <p>Examples</p> <p></p>"},{"location":"1-Software-Engineering/software-diagrams/#entity-relationship-diagrams","title":"Entity Relationship Diagrams","text":"<p>ER Diagrams show relationship between entities. Entities are usually table / view in database.</p> <p>It is often good to show, at minimum:</p> <ul> <li>table name</li> <li>primary keys</li> <li>foreign keys</li> </ul> <p>Type of relationships</p> <ul> <li>1-1</li> <li>1-m</li> <li>m-m</li> </ul> <p>In addition to relationships, it can alos be mandatory or optional (that relates to null and not null). These are shown with different notations on the line, as shown below:</p> <ul> <li>1 - perpendicular</li> <li>mandatory - perpendicular</li> <li>many - crow foot</li> <li>optional - o</li> <li>0 - nothing</li> </ul> <p>So two perpendiculars on a line means 1 mandatory. More on notations on Vertabelo - Blog Crow S Foot Notation.</p> <p>Examples can be found at Vertabelo - Blog How To Use Er Diagram</p> <p>Drawsql - Beautiful database diagrams</p>"},{"location":"1-Software-Engineering/software-diagrams/#links","title":"Links","text":"<ul> <li>https://c4model.com/</li> </ul>"},{"location":"1-Software-Engineering/web-development/","title":"Web Development","text":"<p>Web development may need:</p> <ul> <li>OS and Infra - linux mostly, hosting/gcp/aws - machine connected to internet</li> <li>Web Server - Apache/NginX - connects domain to DIR/Process.</li> <li>Web Cache - optional - Squid</li> <li>Database - to store, mysql/sqlite/mongo/postgres</li> <li>Backend - lang &amp; framework - this will take requests and return response and can connect to DB.</li> <li>Frontend - is on the client side, JS, can have framework.</li> </ul> <p></p> <p>Web Servers / Proxy Servers</p> <ul> <li>Is a server application that acts as an intermediary between a client requesting a resource and the server providing that resource.</li> <li>Can help configure DIR in case of static site and process in case of python/perl/php and use <code>WSGI</code> or CGI. Uses:<ul> <li>Monitoring and filtering</li> <li>Filtering of encrypted data</li> <li>Bypassing filters and censorship - Geo restriction, firewall etc</li> <li>Logging</li> <li>Improving performance</li> </ul> </li> </ul> <ul> <li>Gunicorn, is a Web Server Gateway Interface (WSGI) server implementation that is commonly used to run Python web applications.</li> </ul> <ul> <li>Apache</li> </ul> <ul> <li>NginX</li> </ul> <ul> <li>more on web server</li> </ul> <p>Databases:</p> <ul> <li>mongo - no sql - JSON like document based</li> <li>mysql / PostgreSQL - sql</li> <li>neo4j - graph</li> </ul> <p>Web App Architectures</p> <p>A Web application system design architecture can vary based on various requirements. Some of the ways are</p> <ul> <li>Static Sites - optional backend in serverless lambda - Jekyll site.</li> <li>Containers - using docker images, or Heroku.</li> <li>VMs - fully available OS.</li> <li>3-tier web app, monolithic app, modular app, or a group of web services.</li> </ul>"},{"location":"1-Software-Engineering/web-development/#backend","title":"Backend","text":"<p>It is the server side part of the app. It provides services which clients can request or consume.</p> <p>Frameworks and languages</p> <ul> <li>Flask - Python</li> <li>Laravel - PHP</li> <li>Elastic.js - JS</li> </ul> <p>Auth is required for all request, so decouple and make a service of it. The gateway can take req and the authenticate it or reject it. Once authenticated it can send it to the correct service.</p> <p>APIs or Headless or RESTful backend can be build when we need to completely separate client from server.</p> <ul> <li>request payload is JSON data sent with req. In GET req we don't have to send payload we just send URL params.</li> <li>avoid actions in payload. like func name in payload, instead keep action in route</li> <li>avoid doing everything in one route, eg, if not exist then create else add, instead make the actions atomic to routes.</li> <li>for huge responses, like get_orders() returning all orders with all details, add pagination or fragmentation (in microservices)</li> </ul> <p>Load Balancing or Consistent Hashing</p> <ul> <li>when requests to a server increases we need to add new servers, then redirect requests to different servers. This can be done using hashing. Hash gives a random number to a request, then that number can be used to direct a request to a particular server, eg, hash_number mod servers is the server ID, 14%4=2, so req hashed as 14 goes to server 2, if we have 4 servers.</li> <li>now when we add new server, say 5, then the mod operation changes, every req gets mod of 5, this makes huge operational difference, as in there is a shift in all requests, so all cache that we built becomes useless, to avoid this, we use consistent hashing.</li> <li>more - https://www.youtube.com/watch?v=K0Ta65OqQkY</li> </ul> <p>DjangoREST vs FastAPI, ???</p> <p>WebSockets vs HTTP - Unlike req-res architecture of HTTP, in web sockets server can also send updates to client, updates are sent immediately when they are available. WebSockets keeps a single, persistent connection open while eliminating latency problems that arise with HTTP request/response-based methods. Can be used in messaging and notifications. Protocol is:</p> <ul> <li>XMPP on TCP</li> </ul> <p>Microservices - ???</p> <p>Gateway - ???</p>"},{"location":"1-Software-Engineering/web-development/#frontend","title":"Frontend","text":"<p>It makes the UI or client for an app. It can talk to API and do CRUD. It need to send authorization and access key/token to API.</p> <p>Must have features:</p> <ul> <li>Modular Layout - cards, templates</li> <li>Search</li> <li>Sort</li> <li>Filter</li> <li>Pagination</li> <li>Favourites</li> <li>Enhancements:<ul> <li>Event Bubling / Capturing</li> <li>Debouncing</li> </ul> </li> </ul> <p>Frameworks</p> <ul> <li>React</li> <li>Angular</li> <li>Vue</li> </ul> <p></p> <p>Object-Relational Mapping (ORM)is a technique that lets you query and manipulate data from a database using an object-oriented paradigm. It abstracts Object oriented code from database and hence we can switch DBs. eg, Eloquent in Laravel, SQLAlchemy is an open-source SQL toolkit and object-relational mapper for the Python programming language</p> <p>Headless is backend app with no frontend, it only has API endpoints.</p> <p>12-Factor Application - The Twelve-Factor App methodology is a methodology for building software-as-a-service applications. These best practices are designed to enable applications to be built with portability and resilience when deployed to the web. More - https://12factor.net/</p> <p>PWA - Progressive Web Apps</p> <p>Stacks:</p> <ul> <li>LAMP - Linux Apache MySql PHP</li> <li>MEAN - mongo express angular node</li> <li>MERN - mongo express react node</li> </ul> <p>What is a docker??</p> <p>FastAPI</p> <p>Flask Mega Tutorial</p> <p>Saleor - Python eCom open-source framework:</p> <ul> <li>headless ecom SDK</li> <li>can create category hierarchy</li> <li>can have attributes for products like size cost etc</li> </ul> <p>References:</p> <ul> <li>https://www.fullstackpython.com/</li> <li>https://yalantis.com/blog/tech-stack-for-web-app-development/</li> <li>https://testdriven.io/blog/fastapi-mongo/</li> </ul>"},{"location":"1-Software-Engineering/web-development/#mobile-app-development","title":"Mobile App Development","text":"<p>Data is sourced from RESTAPI or can be local storage.</p> <p>Apps can be:</p> <ul> <li>WebApp/HTML5 App - it is nothing but a webpage, optmized for mobile experience. Frameworks: Bootstrap, jQuery Mobile, Onesen UI.<ul> <li>HTML5 - Same as website but with Local Storage, Video Streaming, Drag and Drop.</li> <li>Progressive Web App - Website with Push Notifications, Offline Work, Splash Screen and Installable with icon.</li> </ul> </li> <li>Hybrid - Native container, installable, running webapp in webview in native container app. PhoneGap/Cordova can build container that can run webApp.</li> <li>Native Cross Platform - One code base, multi platform, native like most experience. React Native, Flutter, Xamarin.</li> <li>Native - Pure iOS and Android app. Different codebase. Full functionality.</li> </ul> <p>More:</p> <ul> <li>https://www.mobiloud.com/blog/native-web-or-hybrid-apps</li> </ul>"},{"location":"1-Software-Engineering/web-development/#python-web-development-stack","title":"Python Web Development Stack","text":"<p>Python has extensive libraries and choices when it comes to do a specific part of web developemnt. Below, it helps understand the library and list reason when to use it and when not to</p> <ul> <li>Flask to handle requests</li> <li>SQLAlchemy to handle DB requests</li> <li>Pandas to do data-analysis including reads, grouping, bulk-downloads. It is bad for insert/update/delete.</li> <li>Bokeh/Plotly/Dash for visualization. TBC.</li> </ul>"},{"location":"1-Software-Engineering/web-development/#user-journey-rest-resources-and-routes","title":"User Journey, REST Resources and Routes","text":"<p>architecture patterns, models, routes, rest end points, crud, philosophy</p> <p>User Journey should be well thought of before doing any hands on. It will define the tables of your app, the models, relationships and finally routes. This is most important part of SaaS development.</p> <p>Step by Step - Think of user journey for immediate goal, future goal. Good to have and best possible. Translate user journey into action map for now and for future enhancements.</p> <p>Routes</p> <p>They are endpoints and should have a pattern, like:</p> <ul> <li>TIA - <code>x.com/&lt;Type&gt;/&lt;Identifier?&gt;/&lt;Action?&gt;</code>, eg, <code>x.com/posts/1/edit</code></li> <li>Nested TIA - <code>/&lt;Type&gt;/&lt;Identifier?&gt;/&lt;Type&gt;/&lt;Identifier?&gt;/&lt;Action?&gt;</code>. Eg, <code>/posts/1/comments/create</code></li> </ul> <p>API URLs should have versioning and api prefix like <code>http://hostname/todo/api/v1.0/tasks</code></p> HTTP Method URI Action GET http://hostname/todo/api/v1.0/tasks Retrieve list of tasks GET http://hostname/todo/api/v1.0/tasks/new HTML Form to add new resource GET http://hostname/todo/api/v1.0/tasks/[task_id] Retrieve a task GET http://hostname/todo/api/v1.0/tasks/[task_id]/edit HTML form to edit resource POST http://hostname/todo/api/v1.0/tasks Create a new task PUT http://hostname/todo/api/v1.0/tasks/[task_id] Update an existing task DELETE http://hostname/todo/api/v1.0/tasks/[task_id] Delete a task <p>Flask Web Routes Methods and Template naming</p> HTTP Method URI Method Template Action GET tasks/ all() all.html View all tasks GET tasks/new new() new.html or form.html HTML Form to add new task <code>POST</code> tasks/ new() Create / Insert a new task GET tasks/[task_id] one() one.html Read View one task GET tasks/[task_id]/edit edit() edit.html or form.html HTML form to edit a task <code>POST</code> tasks/[task_id]/edit edit() Update an existing task <code>POST</code> tasks/[task_id]/delete delete() Delete a task <p>In above, <code>new.html</code> and <code>edit.html</code> can be combined together to be <code>form.html</code> as fields are same. Also, same form, <code>TaskForm</code> can be used for add and edit. The names are kept generic so that it is easy to replicate across resources (or you can name one:task and all:tasks).</p> <p>In <code>tasks</code> <code>all()</code> method, which shows all resources, you need to implement filters, pagination, sorting and searches. These are usually URL query parameters. Eg:</p> <ul> <li><code>&lt;tasks/?tag=shop&amp;sort=asc&amp;done=true&amp;page=2&gt;</code> this will provide you param to filter the result returned and you need to do<ul> <li><code>tag</code> - <code>where tag='shop'</code></li> <li>sort - order by</li> <li>done - where done=true</li> <li>page - use limit or offset</li> </ul> </li> </ul> <p>To show all resource, there are few options:</p> <ol> <li> <p>As Table: List all items as table. If you have to show under 500 records, you can use client side data-tables, this deals with search, sort and pagination. Template, <code>all_table.html</code> on view opens <code>one.html</code> which uses <code>_task.html</code> which shows detailed view.</p> </li> <li> <p>As List of Card: List all items as card snippet. A summary view of item is shown as small card with most relevant details. You need to implement pagination, search and sort. This gives more freedom and control but requires more work. URL params are usually used to control the content on page. Template is <code>all.html</code> and uses <code>_task_snippet.html</code>, on view it opens <code>one.html</code> which uses <code>_task.html</code> which shows detailed view.</p> </li> </ol> <p>Flask HTMX or AJAX Routes Methods and Template naming</p> <p>HTML verbs - new, edit, delete, one, all</p> <p>AJAX verbs - add, create, modify, update, remove, item, all</p> <p>HTML Components:</p> <ul> <li><code>_item.html</code> : Row Details with Edit and Delete buttons</li> <li><code>_modify_form.html</code> : Row Details in form with Save and Cancel buttons</li> <li><code>_add_form.html</code>: Blank form with Create and Cancel buttons</li> </ul> HTTP Method URI Method Template Action Retu GET tasks/ all() all.html View all tasks table with <code>_item.html</code> loop GET tasks/[task_id] item() _item.html Read One task HTML <code>_item.html</code> htmcomponent GET tasks/add add_form() _add_form.html HTML Form to add task <code>_add_form.html</code> htmcomponent <code>POST</code> tasks/ create() Create / Insert a add task `_item.htm GET tasks/[task_id]/modify_form modify_form() _modify_form.html HTML form to edit task html form <code>_modify_form.html</code> <code>PUT</code> tasks/[task_id] update() Update an existing task <code>_item.html</code> with updatedetails <code>DELETE</code> tasks/[task_id] remove() Delete a task <code>NULL</code>, to be add <p>Issues is that not always new item is same, it may have different hx attributes depending on when it is added to DOM.</p> <p>Links</p> <ul> <li>https://medium.com/@goldhand/routing-design-patterns-fed766ad35fa</li> <li>Codecapsules - Tutorial Building A Full Stack Application With Flask And Htmx</li> </ul>"},{"location":"1-Software-Engineering/web-development/#architecture-design-of-flask-webapp","title":"Architecture &amp; Design of Flask Webapp","text":"<p>Handling data at different levels in your application involves a mix of responsibilities. Here's a breakdown of where you can handle specific logic related to data in a Flask application:</p> <p>1 Database</p> <ul> <li>Responsibility: Defining the structure of your data, relationships between tables, and ensuring data integrity.</li> <li>Tasks:<ul> <li>Define tables and relationships using a database schema.</li> <li>Ensure proper indexing and constraints for performance and data integrity.</li> </ul> </li> </ul> <p>2 SQLAlchemy Models</p> <ul> <li>Responsibility: Representing the database entities in your application, defining how data is retrieved and manipulated.</li> <li>Tasks:<ul> <li>Define SQLAlchemy models corresponding to your database tables.</li> <li>Implement methods in models for data retrieval, manipulation, and business logic.</li> <li>Handle complex queries or data transformations in the model layer.</li> </ul> </li> </ul> <p>3 Flask Routes</p> <ul> <li>Responsibility: Handling HTTP requests, interacting with the database through models, and preparing data for presentation.</li> <li>Tasks:<ul> <li>Retrieve data from the database using SQLAlchemy models.</li> <li>Perform any necessary data manipulation or business logic.</li> <li>Pass the processed data to the template for rendering.</li> </ul> </li> </ul> <p>4 Jinja Templates</p> <ul> <li>Responsibility: Rendering HTML and presenting data to the user.</li> <li>Tasks:<ul> <li>Display data received from Flask routes.</li> <li>Implement conditional logic in templates for role-based rendering.</li> <li>Keep templates focused on presentation, avoiding complex business logic.</li> </ul> </li> </ul>"},{"location":"1-Software-Engineering/web-development/#recommendations","title":"Recommendations","text":"<p>Separation of Concerns:</p> <ul> <li>Keep business logic and data manipulation in the SQLAlchemy models and Flask routes.</li> <li>Ensure that templates are responsible for presentation and rendering, not for complex data processing.</li> </ul> <p>Template Context</p> <ul> <li>Pass only the necessary data to templates. Flask routes should prepare the data and send it to - templates in a ready-to-use format.</li> </ul> <p>Avoid Complex Logic in Templates</p> <ul> <li>Minimize the use of complex logic in templates. If you find yourself needing significant logic in - templates, consider moving that logic to the Flask routes or models.</li> </ul> <p>Reusability</p> <ul> <li>Encapsulate reusable logic (e.g., role-based rendering) in functions or macros within your templates.</li> <li>By following these recommendations, you can maintain a clean and maintainable separation of concerns in your Flask application, making it easier to understand, extend, and maintain over time.</li> </ul>"},{"location":"1-Software-Engineering/web-development/#business-logic-recommendations","title":"Business Logic Recommendations","text":"<p>Lean Toward Models for Data-Related Logic: Business logic directly related to the structure and integrity of your data is often better placed in models.</p> <p>Lean Toward Routes for Presentation Logic: Logic that specifically deals with handling requests, preparing data for presentation, or coordinating different parts of your application might be more suitable for routes.</p> <p>Keep Routes Thin: Aim for thin routes that primarily handle HTTP concerns, delegate to models for data manipulation, and delegate to templates for presentation.</p> <p>Consider a Service Layer: For more complex applications, you might introduce a service layer between routes and models to encapsulate business logic, providing a clean and modular design.</p> <p>Links</p> <ul> <li>Stack - business logic</li> <li>Large prod flask app</li> </ul>"},{"location":"1-Software-Engineering/web-development/#three-tier-architecture","title":"Three-Tier Architecture","text":"<p>The three-tier architecture consists of a single presentation tier, logic tier, and data tier. It is similar to MVC. Logic layer, business layer or application layer are all similar.</p> <ul> <li> learn architecture patterns and design patterns, like DDD, OOD, EDD.</li> </ul>"},{"location":"1-Software-Engineering/web-development/#backend-three-layer-architecture","title":"Backend Three-Layer Architecture","text":"<p>At backend, when the things get complex, then it is better to have 3 layer architecture</p> <ul> <li>API Layer - validates, formats, exposes end points</li> <li>Service Layer - processing logic, business logic, data compute</li> <li>Data Layer - data access and persistance</li> </ul> <p>This helps in having separation of concern but increases complexity and overhead to maintain.</p> <p>Link</p> <ul> <li>From Request to Database: Understanding the Three-Layer Architecture in API Development</li> <li>Full stack flask app</li> <li>TDD - Python</li> </ul>"},{"location":"1-Software-Engineering/web-development/#oauth","title":"OAuth","text":"<p>Oauth is used to provide login via third party providers like google, facebook etc.</p> <p>To setup google Oauth:</p> <ul> <li>Crate a google app in https://console.cloud.google.com/apis/credentials</li> </ul>"},{"location":"1-Software-Engineering/web-development/#designing-restful-apis-build-architecture","title":"Designing RESTful APIs -  Build &amp; Architecture","text":"<p>API is base for modern web development. Building a good API requires lot of thought process and methodologies to make it usable, upgradable and reliable.</p> <p>Some of the things to consider are:</p> <ul> <li>API can be built by REST framework using HTTP methods.</li> <li>Write down the model. If it is not written, it doesn't count.</li> <li>Tools don't matter, everthing should be in one place, be it conf, notepad, or notebook.</li> <li>Keep team involved.</li> </ul> <p>Successful API is Easy to use and Solves the purpose.</p>"},{"location":"1-Software-Engineering/web-development/#api-modeling-process-5-step-design-model","title":"API Modeling Process - 5 Step Design Model","text":"<p>Following steps can be followed to model and API</p> <ul> <li>Step 1: Identify Participants</li> <li>Step 2: Identify Activities</li> <li>Step 3: Break into Steps</li> <li>Step 4: Create API Definitions</li> <li>Step 5: Validate API</li> </ul> <p>Let's dive deeper into them</p> <p>Step 1: Identify Participants</p> <p>Participants are end users, humans or IoTs (entity). Know them, what they do, this defines the system that need to be built.</p> <p>Step 2: Identify Activities</p> <p>Outline the business</p> <ul> <li>what are the events. Eg: search, add to cart, add / remove items, payment, fulfil, ship, review.</li> <li>who is doint what</li> <li>who passes event to whom</li> <li>flow of events and queue</li> <li>start and end of process with all events in it</li> <li>boundaries, what is included and what is not.</li> </ul> <p>Step 3: Break into Steps</p> <p>Make a flowchart of events and business logic to complete a business process.</p> <p>Step 4: Create API Definitions</p> <p>API Resources - anything user interacts with, they are nouns. Items, you update or add them. Cart - you add or remove item from it. Order - you create and fulful it. Map these actions on resources to HTTP verbs, GET POST PUT DELETE. That defines your API endpoints.</p> <p>you do following, which is CRUD as:</p> <ul> <li>View items</li> <li>Edit items</li> <li>Create items</li> <li>List items</li> <li>Search items</li> <li>Create Cart</li> <li>Update Cart</li> <li>Checkout -&gt; Create Order</li> </ul> <p>Step 5: Validate API</p> <p>Now that you have API Definitions, do not code it. Instead validate in real scenarios. Document as if it already exists. Running on white board is okay too.</p>"},{"location":"1-Software-Engineering/web-development/#http-protocol-wrt-api","title":"HTTP Protocol wrt API","text":"<p>Headers come with request, Payload goes with response. Example:</p> <pre><code>curl -I https://api.github.com      # shows headers\n\nHTTP/1.1 200 Connection established\n\nHTTP/1.1 200 OK\nServer: GitHub.com\nDate: Wed, 15 May 2024 13:49:00 GMT\nContent-Type: application/json; charset=utf-8\n# and much more\n</code></pre> <p>HTTP Status Codes: 2xx 3xx 4xx all have different meaning, and based on this code, rest of the actions are taken. Like show data or error/info etc.</p> <p>Content-Type another important information, this tells what the payload is, for normal webpage it is <code>text/html</code> but for APIs, it is <code>application/json</code>.</p>"},{"location":"1-Software-Engineering/web-development/#api-constraints","title":"API Constraints","text":"<ul> <li>should be designed for client-server architecture</li> <li>stateless - every request should execute on its own and self-contained.</li> <li>cachable</li> <li>layered system - no not directly hook to db, have cahce in between, have auth, have logging, have audit trail, DNS lookup, load balancers in between.</li> </ul> <p>Code on demand - API can return code, so that browser can execute it. This lets you change the api and send changed code. ???</p> <p>Uniform interface - Each end point is unique and exists only once. You can have named api endpoints too.</p> <pre><code>curl https://api.github.com\n# get list of endoints\n</code></pre>"},{"location":"1-Software-Engineering/web-development/#api-authentication-and-authorization","title":"API Authentication and Authorization","text":"<ul> <li>by API Keys<ul> <li>pros - add to header, language agnostic</li> <li>cons - can be shared, not secret</li> </ul> </li> <li>OAuth<ul> <li>authorization protocol, map url with user</li> </ul> </li> <li>OAuth 2.0<ul> <li>standard, common solutions, not easy to use.</li> </ul> </li> </ul>"},{"location":"1-Software-Engineering/web-development/#api-versioning","title":"API Versioning","text":"<p>Versioning in url or header.</p> <p>URL is better.</p>"},{"location":"1-Software-Engineering/web-development/#api-media-type","title":"API Media-Type","text":"<p>Json and its structire, <code>_linka</code> and <code>items</code> are common structure.</p> <p>Links:</p> <ul> <li>Designing RESTful APIs - Linkedin Learning</li> </ul>"},{"location":"1-Software-Engineering/web-development/#email-server","title":"Email Server","text":"<p>You can use Google or SendGrid as email sever for you app.</p> <p>Send Grid</p> <ul> <li>Create an account.</li> <li>Click 'Integrate using our Web API or SMTP Relay'. Use, <code>MAIL_SERVER=smtp.sendgrid.net</code> for server.</li> <li>Add 'API Name' and get the Key. This will be <code>MAIL_USERNAME</code> in env file.</li> </ul>"},{"location":"2-Data-Engineering/apache-airflow/","title":"Apache Airflow","text":""},{"location":"2-Data-Engineering/apache-airflow/#airflow-installation-and-quick-start","title":"Airflow Installation and Quick Start","text":"<p>1. Build and activate venv</p> <pre><code>. /Users/username/code/repo/tutorials/airflow/venv/bin/activate\nexport AIRFLOW_HOME=~/code/airflow_home\n\n ```\n\n**0. Setup**\n\n```sh\nexport AIRFLOW_HOME=~/code/airflow_home\nAIRFLOW_VERSION=2.9.2\n\n# Extract the version of Python you have installed. If you're currently using a Python version that is not supported by Airflow, you may want to set this manually.\n# See above for supported versions.\nPYTHON_VERSION=\"$(python -c 'import sys; print(f\"{sys.version_info.major}.{sys.version_info.minor}\")')\"\n\nCONSTRAINT_URL=\"https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt\"\n# For example this would install 2.9.2 with python 3.8: https://raw.githubusercontent.com/apache/airflow/constraints-2.9.2/constraints-3.8.txt\n\npip install \"apache-airflow==${AIRFLOW_VERSION}\" --constraint \"${CONSTRAINT_URL}\"\n\n# builds and starts airflow\nairflow standalone\n\n# Outputs\n...\nwebserver  | [2024-06-12 11:51:03 +0100] [7815] [INFO] Starting gunicorn 22.0.0\nwebserver  | [2024-06-12 11:51:03 +0100] [7815] [INFO] Listening at: http://0.0.0.0:8080 (7815)\nwebserver  | [2024-06-12 11:51:03 +0100] [7815] [INFO] Using worker: sync\nwebserver  | [2024-06-12 11:51:03 +0100] [7867] [INFO] Booting worker with pid: 7867\nwebserver  | [2024-06-12 11:51:03 +0100] [7868] [INFO] Booting worker with pid: 7868\nstandalone | Airflow is ready\nstandalone | Login with username: admin  password: m7EF2S5pB6M9Hrqt\n...\n</code></pre> <p>Open UI http://localhost:8080</p> <p>You will see DAGs (54), these are examples/tutorial DAGs per built.</p> <p>Run and test example task</p> <pre><code># run your first task instance\nairflow tasks test example_bash_operator runme_0 2015-01-01\n# run a backfill over 2 days\nairflow dags backfill example_bash_operator \\\n    --start-date 2015-01-01 \\\n    --end-date 2015-01-02\n</code></pre>"},{"location":"2-Data-Engineering/apache-airflow/#tutorial-1-fundamental-concepts","title":"Tutorial 1 - Fundamental Concepts","text":"<p>Here you will</p> <ul> <li>Use bash commands like <code>date</code> and <code>sleep 5</code>, to create two tasks t1 and t2. date prints date time, sleep, sleeps for n seconds.</li> <li>the tasks can be created using, <code>BashOperator()</code>. It is used to execute some bash commands.</li> <li>Now that you have tasks created, you need to associate them with a DAG by creating a <code>DAG()</code> obj called <code>dag</code> and add tasks to it.</li> <li>Simple? operator in dag.</li> </ul> <ul> <li>This script is actually kept simple. It has just the DAG configuration. It does not have any logic for data processing.</li> <li>Script: <code>./examples/tutorial1.py</code></li> <li>Link: https://airflow.apache.org/docs/apache-airflow/stable/tutorial/fundamentals.html</li> </ul>"},{"location":"2-Data-Engineering/apache-airflow/#dag","title":"DAG","text":"<pre><code>with DAG(\n    \"tutorial1\",\n\n    # These are passed to all opertators, you can override them in operator\n    default_args={\n        \"depends_on_past\": False,\n        \"email\": [\"airflow@example.com\"],\n        \"email_on_failure\": False,\n        \"email_on_retry\": False,\n        \"retries\": 1,\n        \"retry_delay\": timedelta(minutes=5),\n    },\n\n    description=\"A simple tutorial DAG\",\n    schedule=timedelta(days=1),\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n\n) as dag:\n    ...\n</code></pre> <p>Adding Documentation</p> <p>You can add doc to DAG. It can be seen on web when you open a DAG. It can be added using following syntax:</p> <pre><code>dag.doc_md = \"\"\"\n    This is a documentation placed anywhere\n    \"\"\"\n</code></pre>"},{"location":"2-Data-Engineering/apache-airflow/#operators","title":"Operators","text":"<p>It defines unit work for Airflow to complete. All operators inherit <code>BaseOperator</code> and grow from there. Most common are <code>PythonOperator</code> and <code>BashOperator</code>. Airflow completes the work based on arguments passed to the operators.</p> <p>The precedence rules for a task are as follows:</p> <ul> <li>Explicitly passed arguments</li> <li>Values that exist in the default_args dictionary</li> <li>The operator\u2019s default value, if one exists</li> </ul> <p>Imp A task must include or inherit the arguments <code>task_id</code>.</p> <pre><code>t1 = BashOperator(\n    task_id=\"print_date\",\n    bash_command=\"date\",\n)\n\nt2 = BashOperator(\n    task_id=\"sleep\",\n    depends_on_past=False,\n    bash_command=\"sleep 5\",\n    retries=3,                   # default args overridden\n)\n</code></pre>"},{"location":"2-Data-Engineering/apache-airflow/#templating-using-jinja","title":"Templating using Jinja","text":"<p>Airflow lets do templating using Jinja. This is third command in example. If you run this it will execute the Jinja template.</p> <pre><code>templated_command = textwrap.dedent(\n    \"\"\"\n{% for i in range(5) %}\n    echo \"{{ ds }}\"\n    echo \"{{ macros.ds_add(ds, 7)}}\"\n{% endfor %}\n\"\"\"\n)\n\nt3 = BashOperator(\n    task_id=\"templated\",\n    depends_on_past=False,\n    bash_command=templated_command,\n)\n</code></pre> <p>This will loop 5 times and print the <code>ds</code> date which is logical date supplied after command.</p>"},{"location":"2-Data-Engineering/apache-airflow/#task-dependencies","title":"Task Dependencies","text":"<p>You can make the task dependents by defining the dependencies.</p> <pre><code># they are all same, t2 depends on t1\nt1.set_downstream(t2)\nt2.set_upstream(t1)\nt1 &gt;&gt; t2\nt2 &lt;&lt; t1\nt1 &gt;&gt; t2 &gt;&gt; t3\n\n# below are same\nt1.set_downstream([t2, t3])\nt1 &gt;&gt; [t2, t3]\n[t2, t3] &lt;&lt; t1\n</code></pre> <p>Dependency cannot be cyclic.</p>"},{"location":"2-Data-Engineering/apache-airflow/#running-the-script-testing","title":"Running the Script / Testing","text":"<p>You will see \"tutorial1\" in list.</p> <pre><code># initialize the database tables\nairflow db migrate\n\n# print the list of active DAGs\nairflow dags list\n# You will see \"tutorial1\" in list.\n\n# prints the list of tasks in the \"tutorial\" DAG\nairflow tasks list tutorial1\n# print_date\n# sleep\n# templated\n\n\n# prints the hierarchy of tasks in the \"tutorial\" DAG\nairflow tasks list tutorial --tree\n# &lt;Task(BashOperator): print_date&gt;\n#     &lt;Task(BashOperator): sleep&gt;\n#     &lt;Task(BashOperator): templated&gt;\n\n# command layout: command subcommand [dag_id] [task_id] [(optional) date]\n</code></pre> <p>When using <code>test</code>, no update is made in database, only local run with output on stdout is shown. This is for testing only.</p> <pre><code># testing print_date\nairflow tasks test tutorial1 print_date 2015-06-01\n\n# testing sleep\nairflow tasks test tutorial1 sleep 2015-06-01\n\n# testing templated\nairflow tasks test tutorial1 templated 2015-06-01\n</code></pre> <p>Here we pass an optional date, is called the logical date (also called execution date for historical reasons). It is just for simulation, actual run is now. DAG runs for a specific date, not at, eg, DAG runs task for today, but the task may be scheduled to run at midnight (or when condition is met). So there is a logical data and a physical date (actual run date).</p>"},{"location":"2-Data-Engineering/apache-airflow/#backfill","title":"Backfill","text":"<p>It will start the tasks (not test but actual run) on a specified logical start date and an optional end date. It populates the logs and db with running status. Eg</p> <pre><code># start your backfill on a date range\nairflow dags backfill tutorial1 \\\n    --start-date 2015-06-01 \\\n    --end-date 2015-06-07\n</code></pre>"},{"location":"2-Data-Engineering/apache-airflow/#conclusion-overall","title":"Conclusion Overall","text":"<p>You have built, tested and backfilled the Airflow pipeline. You have added your code to the repo, that has a scheduler running. It will pick the schedule and trigger jobs as needed.</p>"},{"location":"2-Data-Engineering/apache-airflow/#tutorial-2-working-with-taskflow","title":"Tutorial 2 - Working with TaskFlow","text":"<p>Link: https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html</p> <p>TaskFlow API is introduced in Airflow 2.0. This lets define DAG as a function with wrapper. The tasks in DAG are sub-functions with another wrapper. Eg, outline only</p> <pre><code>...\n\n@dag(\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n)\ndef tutorial_taskflow_api():\n\n    @task()\n    def extract():\n        pass\n\n    @task(multiple_outputs=True)\n    def transform(order_data_dict: dict):\n        pass\n\n    @task()\n    def load(total_order_value: float):\n        pass\n\n    order_data = extract()\n    order_summary = transform(order_data)\n    load(order_summary[\"total_order_value\"])\n\ntutorial_taskflow_api()\n</code></pre> <p>You can see, using decorators, how simply we have defined the dag and tasks. It separates the code.</p> <p>The decorated tasks and dag can be reused in another functions, or can imported from another file and reused.</p>"},{"location":"2-Data-Engineering/apache-airflow/#automation-scheduling-orchestrate","title":"Automation Scheduling Orchestrate","text":"<ul> <li> <p>DAG - Directed Acyclic Graph is used to represent collection of tasks in organized way to show dependencies and relationships. It has no cyclic link.</p> <pre><code>graph LR;\nA--&gt;B;\nB--&gt;D;\nB--&gt;C;\nC--&gt;E;</code></pre> </li> </ul> <ul> <li>Cron - Linux in build to schedule a job. Can't manage dependencies.</li> </ul> <ul> <li> <p>Apache Airflow</p> <ul> <li>Create DAGs in Python</li> <li>Define tasks of DAGs using Operators. Operators can operate various things like bash code, python code, StartCluster or SparkJob.</li> <li>Set up dependency of tasks - using <code>set_downstream()</code>. This will create relationships in jobs.</li> </ul> <ul> <li>configuration<ul> <li>make <code>mkdir airflow</code> dir</li> <li>export its location to variable <code>AIRFLOW_HOME</code></li> </ul> </li> <li>installation - <code>pip install airflow</code></li> <li>initiation<ul> <li><code>airflow db init</code> to generate airflow db, config and web-server files.</li> <li>make an admin user, code from docs.</li> </ul> </li> <li> <p>implementation</p> <ul> <li>define ETL tasks functions in <code>./airflow/dags/etl_tasks.py</code></li> </ul> <ul> <li>define <code>./airflow/dags/dags.py</code>, here<ul> <li>it will have airflow module implementation to schedule and execute tasks via DAG.</li> <li>import ETL tasks file as module.</li> <li>define execution function to run ETL tasks</li> <li>define DAG using DAG class.</li> <li>add config, like when to run, retries to try, gap in retries, email to send on failures, and many other configurations as dictionary object and pas that to <code>default_args</code> param of <code>DAG</code> class.</li> <li>define ETL Task using <code>operator</code>. this executes the execution function.</li> </ul> </li> </ul> </li> </ul> <ul> <li>schedule - <code>airflow scheduler</code> to add dag to server</li> <li>monitor<ul> <li><code>airflow webserver</code> this starts flask web-server where you can look the jobs.</li> <li>view DAGs, start/stop/pause jobs</li> </ul> </li> </ul> </li> </ul>"},{"location":"2-Data-Engineering/apache-airflow/#code-example-airflow-dag","title":"Code Example Airflow DAG","text":"<p>Following code shows snippet of basic DAG implementation</p> dags.py<pre><code># ``\nimport airflow\nfrom airflow.models import DAG # DAG class\nfrom airflow.operators.python_operator import PythonOperator # as we use Py\nfrom etl_tasks import *\n\ndef etl():\ndf_table1 = extract_table1_to_df()\ndf_score = transform_avg_score(df_table1)\nload_df_to_db(df_score)\n\n# define DAG with configs\ndag = DAG(dag_id=\"etl_ipeline\", \n        default_args=default_args, \n        schedule_interval=\"0 0 * * *\")\n\n# define ETL Task\netl_tasks = PythonOperator(task_id=\"etl_task\", python_callable=etl, dag=dag)\n\netl()\n</code></pre>"},{"location":"2-Data-Engineering/apache-airflow/#links","title":"Links","text":"<ul> <li>LinkedIn Learning - Data Engineering Foundations</li> </ul>"},{"location":"2-Data-Engineering/apache-iceberg/","title":"Apache Iceberg Notes","text":""},{"location":"2-Data-Engineering/apache-iceberg/#iceberg-overview","title":"Iceberg Overview","text":"<ul> <li>Apache Iceberg is a open-source lake formation solution for storing data for huge analytic tables.</li> </ul> <ul> <li>It is a open-source table format that uses Parquet open-data format to store the data in file. More.</li> </ul> <ul> <li>Enables use of SQL Queries as well as let use big data engines like <code>Spark</code>, <code>Flink</code>, <code>Hive</code> and others to safely work with the same tables, at the same time.</li> </ul> <p>Key Features:</p> <ul> <li>SQL-based table format: Iceberg uses a SQL-based table format, making it easier to manage and query large datasets.</li> <li>Transactionality: Iceberg supports transactional operations with ACID properties, ensuring data consistency and integrity.</li> <li>Schema evolution: Iceberg allows for schema evolution, making it easier to add or remove columns from a table without affecting existing data.</li> <li>Partition evolution: Iceberg supports partition evolution, making it easier to change the partitioning scheme of a table without affecting existing data.</li> <li>Time travel: Iceberg supports time travel, allowing you to query historical versions of a table.</li> <li>Hidden partitioning: Iceberg supports hidden partitioning, which simplifies query optimization and performance tuning.</li> <li>Multiple data processing engines: Iceberg is compatible with multiple data processing engines, including <code>Spark</code>, <code>Trino</code>, <code>Flink</code>, <code>Presto</code>, <code>Hive</code>, <code>Impala</code>, <code>StarRocks</code>, <code>Doris</code>, and <code>Pig</code>.</li> </ul> <p>Benefits:</p> <ul> <li>Improved data management: Iceberg provides a more efficient and reliable way to manage large datasets.</li> <li>Simplified query processing: Iceberg simplifies query processing by providing a SQL-based interface and optimizing query execution.</li> <li>Enhanced performance: Iceberg can improve query performance by using techniques such as hidden partitioning and columnar storage.</li> <li>Increased flexibility: Iceberg provides more flexibility for data engineering teams by allowing them to easily evolve their data models.</li> <li>Lower Costs - compared to use of other stacks like Glue, S3; Iceberg can reduce costs up to 4 times.</li> </ul>"},{"location":"2-Data-Engineering/apache-iceberg/#competitors-of-apache-iceberg","title":"Competitors of Apache Iceberg","text":"<ol> <li> <p>Delta Lake: Developed by Databricks, Delta Lake is another popular open-source format for structured data. It offers similar features to Iceberg, including transactionality, schema evolution, and time travel. However, Delta Lake is more tightly integrated with the Databricks platform, which can be a pro or a con depending on your specific use case.</p> </li> <li> <p>Apache Hudi: Originally developed by Uber, Hudi is another open-source format for streaming data. It offers features like upserts, deletes, and real-time updates, making it a good choice for streaming data pipelines. Hudi is particularly well-suited for data warehousing and analytics use cases.</p> </li> </ol>"},{"location":"2-Data-Engineering/apache-iceberg/#comparison-of-features","title":"Comparison of Features","text":"Feature Iceberg Delta Lake Apache Hudi Transactionality Yes Yes Yes Schema evolution Yes Yes Yes Time travel Yes Yes Yes Upserts Yes Yes Yes Deletes Yes Yes Yes Streaming updates Yes Yes Yes Integration with Databricks Good Excellent Good Performance Excellent Excellent Good <p>Note: The best choice for your specific use case will depend on factors such as your data volume, processing requirements, and preferred technology stack. It's often a good idea to evaluate multiple options and benchmark their performance in your environment before making a final decision.</p>"},{"location":"2-Data-Engineering/apache-iceberg/#iceberg-architecture","title":"Iceberg Architecture","text":"<ul> <li>Iceberg user parquet as open-data format, but along with this it stores metadata, catalog, and forms its own table-format that makes it different from other lake-house. This logic of storing table-format is open source.</li> <li>Eg, for interoperability Iceberg can use data stored in parquet format by Hive or DeltaLake, by not changing the data, yet building other things around data that enables its features like SQL querying, snapshots, time-travel, upserts etc.</li> </ul>"},{"location":"2-Data-Engineering/apache-iceberg/#aws-iceberg-integration","title":"AWS Iceberg Integration","text":"<ul> <li>AWS offers Apache Iceberg as a service with some modification on top of open source that makes it better than the open source version.</li> </ul>"},{"location":"2-Data-Engineering/apache-iceberg/#using-iceberg-ordered-data-format-instead-of-hive-parquet","title":"Using Iceberg Ordered data format instead of Hive Parquet","text":"<ul> <li>Cost and Time reduction<ul> <li>Using Iceberg Ordered data format instead of Hive Parquet can reduce cost and time.</li> <li>Reduces Athena cost by 34%. This is because athena cost depends on data scanned which is reduced when using Iceberg.</li> <li>Athena query time remains the same.</li> <li>Number of S3 get object API requests are reduced by 45%.</li> <li>S3 list object API requests are not needed, hence reduced by 100%.</li> </ul> </li> </ul> <p>Apache Iceberg runs and integrates with various AWS services, including:</p> <ul> <li>Amazon S3: Iceberg uses S3 as the underlying storage for tables.</li> <li>Amazon EMR: EMR clusters can be used to run Iceberg jobs, providing a managed Hadoop environment for data processing.</li> <li>AWS Glue: Glue can be used to create, manage, and query Iceberg tables, as well as to extract, transform, and load (ETL) data into Iceberg.</li> <li>Amazon Athena: Athena can be used to query Iceberg tables using standard SQL, providing a serverless, interactive query service.</li> <li>AWS Firehose You can use Firehose to load data into Iceberg tables in real-time</li> <li>AWS Redshift Spectrum (serverless query) You can query Iceberg tables stored in S3 directly (serverless)</li> <li>AWS Lambda: Lambda can be used to create custom functions that can interact with Iceberg tables, such as for data processing or ETL tasks.</li> </ul> <p>In addition to AWS services, Iceberg also integrates with other popular data processing frameworks:</p> <ul> <li>Apache Spark: Spark can be used to read, write, and query Iceberg tables, providing a powerful and scalable data processing engine.</li> <li>Trino: Trino is a distributed SQL query engine that can be used to query Iceberg tables</li> <li>Presto: Presto is another distributed SQL query engine that can be used to query Iceberg tables.</li> </ul>"},{"location":"2-Data-Engineering/apache-iceberg/#moj-ministry-of-justice-case-study","title":"MoJ (Ministry of Justice) Case Study","text":"<ul> <li>Data lands in S3 as bronze layer having Hive Table Format.</li> <li>Then to transform it and load as Silver layer, there are two approaches compared<ol> <li>Use AWS Glue then load into Iceberg Tables on S3 as Silver Layer</li> <li>Use AWS Athena to load into Iceberg Tables on S3 as Silver Layer</li> </ol> </li> <li>Using approach 2, athena instead of glue, has following benefits:<ul> <li>Athena cost reduction in bulk insert and merge (CDC)</li> <li>Reduced query time (faster results)</li> <li>Thus Athena is cheaper and faster compared to Glue.</li> <li>They reduced cost by 99%.</li> <li>It became 55% faster.</li> </ul> </li> </ul>"},{"location":"2-Data-Engineering/apache-iceberg/#cloudinary-case-study","title":"Cloudinary Case Study","text":"<ul> <li>10-20 TB per day</li> <li>Old Data pipeline<ul> <li>s3 -&gt; emr spark -&gt; s3(iceberg) -&gt; snowflake -&gt; reports/ML</li> <li>snowflake does everything as gold layer</li> </ul> </li> <li>New Pipeline, transition to Iceberg<ul> <li>Instead of using snowflake, athena is used directly on Iceberg tables for analytics and ML.</li> <li>This resulted in:<ul> <li>reduces costs</li> <li>better query response time</li> </ul> </li> </ul> </li> </ul> <ul> <li>migration challenges<ul> <li>compaction<ul> <li>iceberg provides it out of box</li> <li>bin-pack, sort key</li> <li>on getting right sort-key, the time and cost to run the query is less.</li> </ul> </li> <li>maintenance<ul> <li>remove orphan</li> <li>expire snapshots - each txn makes snapshot, this can be deleted in some time.</li> <li>it reduced data by 112TB, that is, $2700 per month.</li> </ul> </li> <li>monitoring<ul> <li>iceberg exposes lot of metadata, this can be helpful to look at using dashboard.</li> </ul> </li> </ul> </li> </ul> <ul> <li>blog link</li> <li>presentation link PDF</li> </ul> <ul> <li>What is trino?<ul> <li>Trino is an open-source SQL query engine that enables users to perform fast analytic queries on large data sets. It's designed for interactive data analytics and can scale to meet the needs of the world's largest organizations.</li> </ul> </li> </ul> <ul> <li> to be finalized</li> </ul>"},{"location":"2-Data-Engineering/apache-iceberg/#technical-deep-dive-in-apache-iceberg","title":"Technical Deep dive in Apache Iceberg","text":"<ul> <li>Apache Iceberg Overview<ul> <li>benefits<ul> <li>acid transactions on data lake, commit ot object, do inset and update on object,</li> <li>fast and quick using metadata</li> <li>expressive sql, use sql on data lake. or you can use flink, spark as well. Query with redshift needs redshift spectrum, then you can query iceberg table.</li> <li>partition - less table scan, flexible schema</li> <li>agnostic processing engine - no lock-in, works with any vendor.<ul> <li>create table with athena, query with spark, use with glue, you can do time-travel, go back with snapshots, do incremental querying.</li> </ul> </li> </ul> </li> </ul> </li> <li>AWS Native Support<ul> <li>11:25</li> </ul> </li> </ul> <ul> <li>Iceberg Table anatomy<ul> <li>not just one parquet file</li> <li>It has catalog, metadata file and manifest file, and data file, so 3 different files.</li> <li>Here catalog is on Glue, then other files are no S3.</li> </ul> </li> </ul> <ul> <li>Catalog<ul> <li>points to latest metadata?</li> </ul> </li> </ul> <ul> <li>Metadata<ul> <li>three parts<ul> <li>metadata</li> <li>manifest list</li> <li>manifest file</li> </ul> </li> <li>Metadata file<ul> <li>table level, schema, partition info, snapshot info.</li> <li>it can point to multiple snapshots, you are not re writing all data on modification but create new snapshot</li> </ul> </li> <li>Manifest<ul> <li>they are indexes in iceberg table</li> <li>it has high level query stat for query optimization</li> <li>has pointer for actual data file.</li> <li>it has data stats per column, value and ranges. for improving query performance.</li> </ul> </li> </ul> </li> </ul> <ul> <li> half an hour miss</li> </ul> <ul> <li>Table Maintaining<ul> <li>binpack<ul> <li>it merges or separates files to a set size</li> </ul> </li> <li>sort<ul> <li>it sorts and shuffles</li> </ul> </li> </ul> </li> <li>Managed Compasion<ul> <li>easy to do using Glue</li> </ul> </li> </ul> <ul> <li>Migration Strategies<ul> <li>in place migration<ul> <li>do not rewrite whole data file</li> <li>iceberg can use hive data file</li> <li>iceberg builds its metadata and catalog.</li> </ul> </li> <li>Fill data migration<ul> <li>new table in new location</li> <li>it can be expensive and time consuming if data is huge</li> </ul> </li> </ul> </li> </ul>"},{"location":"2-Data-Engineering/apache-iceberg/#streaming-with-iceberg","title":"Streaming with Iceberg","text":"<ul> <li>data has short self life, do analytics in real time.</li> <li>iceberg benefits are snapshots, time-travel, schema-evolution</li> </ul> <ul> <li>CDC with Debezium<ul> <li>Kafka Source Connector</li> <li>Flink cna also run Debezium</li> </ul> </li> </ul> <ul> <li>kafka Connect<ul> <li>kafka is streaming source</li> <li>KC is additional part, lets connect and send data to ext sources.<ul> <li>KC lets connect to sink and sources, then in between is Kafka.</li> </ul> </li> </ul> </li> <li>Deb Kaf Connect - 12:20</li> <li>Amazon MSK - managed akfka</li> </ul> <ul> <li>Append vs Upsert - Trade Off<ul> <li>append keeps history, upsert looses history</li> </ul> </li> </ul> <ul> <li>Low code streaming ingestion<ul> <li>firehose and kafka connect.</li> <li>KC uses apache iceberg sink connector</li> </ul> </li> </ul> <ul> <li> <p>Streaming Ingestions to Iceberg tables with Streaming Frameworks</p> <ul> <li>using spark/flink you can read iceberg table in real time.</li> <li>Flink<ul> <li>lets use data stream api of iceberg</li> <li>use connector of ice berg</li> <li>you can read write to iceberg in real time</li> <li>Issues<ul> <li>enable checkpoint is required</li> <li>flink commits when checkpoint is enables</li> <li>no support for watermark, schema evolution, hidden partitioning</li> </ul> </li> <li>Spark<ul> <li>structured stream</li> <li>read write</li> <li>micro batches in use</li> <li>writes commits in smaller intervals</li> </ul> </li> <li>For iceberg, spark is most feature rich compared to flink. 12:42.</li> </ul> </li> </ul> <ul> <li>Iceberg streaming problem<ul> <li>duplicates, due to double updates problem.</li> <li>on upsert, you have to do de-duplicates.</li> <li>firehose does this automatically</li> <li>small files are problem due to high rate of commits, merge on read updates.</li> <li>maintenance is a solution to this.</li> </ul> </li> </ul> </li> </ul> <ul> <li> what is compaction, how does it effect maintenance</li> </ul>"},{"location":"2-Data-Engineering/apache-iceberg/#what-is-new-in-iceberg-on-aws","title":"What is new in Iceberg on AWS?","text":"<ul> <li>iceberg iterates perfectly with AWS.</li> <li>AWS managed iceberg cluster is 4x faster compared to OpenSource Iceberg.</li> <li>AWS EXpress One Zone S3 Buckets lets do up to 4x faster reads. 13:01</li> </ul> <ul> <li>fine grained access control with lake formation</li> <li>may features are AWS restricted and are not on open source, which make it fast and integrated with AWS.</li> </ul> <ul> <li>Blog AWS Iceberg Tutorial</li> </ul> <ul> <li>Hands on Lab - Apache Iceberg Roadshow - London</li> </ul>"},{"location":"2-Data-Engineering/apache-kafka/","title":"Apache Kafka","text":"<p>No processing, only storing</p>"},{"location":"2-Data-Engineering/apache-kafka/#event-driven-architecture","title":"Event Driven Architecture","text":"<p>Is a modern application design pattern that is based on concept of loose-coupling and message-driven interaction. It is useful in large real-time datasets.</p> <p>What is an Event?</p> <p>An event is any change in state for system hardware or software. It can be generated from web-UI, IoT sensors, or system jobs monitoring. Eg, new GPS coordinates of moving vehicle. A user clicking a button. The leading technologies for event-driven architecture are built to support trillions of events per day.</p> <p>Event Driven Architecture</p> <pre><code>flowchart LR;\n\np[Producer or\\n Emitter / Agent] --Event--&gt; b[Event Channel\\n or Brokers] --Event--&gt; c[Consumer\\n or Sink]\n</code></pre> <p>Here, loosely coupled or decoupled means that producer just sends event to sink and does not care if there is any consumer at all or how many consumers, or what happens with an event.</p> <p>Sinks are responsible for transforming event as needed.</p> <p>Link: scylladb EDA</p> <p>Streaming architecture is form of EDA. It is built on technology like Apache Kafka.</p> <p>It is based on pub-sub model. Where publisher produces events, and consumers consume events.</p> <p>Event streaming services like kafka publish stream of events to broker.</p> <p>Types of EDA</p> <p>There are two main kinds of event-driven architectures</p> <ul> <li>Message Queues</li> <li>Stream Processing</li> </ul>"},{"location":"2-Data-Engineering/apache-kafka/#apache-nifi-with-kafka","title":"Apache Nifi with Kafka","text":"<ul> <li>distributed event store and stream-processing platform</li> <li>stream processing compared to batch processing</li> <li>architecture it supports is scalable and makes data distributed, replicated and fault-tolerant, hence, allowing stream processing in real-time.</li> <li>it is based on pub-sub (publishing and subscribing) messaging system.</li> <li>kafka sends data in real-time to <code>topics</code>. data may be infinite and incomplete at time of query.</li> <li>consumers who process data can read topics.</li> <li>works on 3-nodes cluster.<ul> <li>use IP of three servers in configuration.</li> </ul> </li> </ul> <ul> <li>how it works<ul> <li>kafka uses logs to store data and calls it topics. it is saved to disk as log file. they are horizontally scaled and partitioned.</li> <li>producer can write to partitions as fire-and-forget, or synchronous, or asynchronous</li> <li>consumers can read and be part of a consumer group, so that they consume from different partitions at fast rate.</li> </ul> </li> </ul> <ul> <li>configuration<ul> <li>configuration connects zookeeper and kafka together, it is where you define the server and port to connect and data and log directories.</li> <li><code>zookeeper.properties</code> file has info on configs for zookeeper <code>dataDir</code>, <code>servers</code>, <code>clientPort</code></li> <li>kafka configs are in <code>server.properties</code> file, like, <code>log.dirs=</code>, <code>zookeeper.connect=</code>.</li> </ul> </li> </ul> <ul> <li> <p>hello test</p> <ul> <li>create a topic, a producer, some messages, a consumer to read them.</li> </ul> <ul> <li>create topic called 'dataengineering'<ul> <li><code>bin/kafka-topics.sh --create --zookeeper localhost:2181,localhost:2182,localhost:2183 --replicationfactor 2 --partitions 1 --topic dataengineering</code></li> </ul> </li> </ul> <ul> <li>list all topics<ul> <li><code>bin/kafka-topics.sh \u2013list --zookeeper localhost:2181,localhost:2182,localhost:2183</code></li> </ul> </li> </ul> <ul> <li>write messages to topic<ul> <li>you can use console to add messages to a topic</li> <li><code>bin/kafka-console-producer.sh --broker-list localhost:9092,localhost:9093,localhost:9094 -topic dataengineering</code></li> </ul> </li> </ul> <ul> <li>read messages from topic<ul> <li>you can read from beginning or define an offset if already read.</li> <li><code>bin/kafka-console-consumer.sh --zookeeper localhost:2181,localhost:2182,localhost:2183 --topic dataengineering \u2013from-beginning</code></li> </ul> </li> </ul> <ul> <li>whatever you write in producer appears on consumer after a lag. this shows the connectivity between two no you can use Python, Airflow/NiFi to build a pipeline.</li> </ul> </li> </ul> <ul> <li>Kafka data pipeline using NiFi<ul> <li>use NiFi to build processors that act as producer and consumer.</li> <li>Consumer can have multiple consumers in consumer-group.</li> <li>later you can add it to prod pipeline as normal that is, read kafka -&gt; staging, transformation, validation, loading, etc.</li> </ul> </li> </ul> <ul> <li>Batch vs Streaming<ul> <li>if streaming data is unbounded (infinite), then you need to rethink of validating it for completeness, recalculate min, max and avg.</li> <li>you can use <code>time-window</code> to make unbounded data bounded, that is, if 2022 records are fetched then avg for that year is calculated and will not change, however, new data for 2023 can still be unbounded and keep coming.<ul> <li><code>fixed</code> - like 1 min, no overlapping</li> <li><code>sliding</code> - of 1 min, slides 10s, has overlapping</li> <li><code>session</code> - no time bound but event based, like log in to log out activity.</li> <li>also the time can be <code>event-time</code>, <code>ingest-time</code> or <code>processing-time</code></li> </ul> </li> </ul> </li> </ul> <ul> <li>Producing and consuming with Python<ul> <li>use library</li> <li>import producer and consumer</li> <li>add servers and topics, collect receipt as callback.</li> <li> <p><code>from confluent_kafka import Producer</code></p> <pre><code>from confluent_kafka import Producer\ndef receipt(err,msg):\n    ...\n\np=Producer(..)\np.produce('users',m.encode('utf-8'),callback=receipt)\n\nfrom confluent_kafka import Consumer\nc=Consumer({... : ...})\nc.list_topics().topics\nt.topics['users'].partitions\nc.subscribe(['users'])\nwhile True:\n  msg=c.poll(1.0)\n  ...\n\nc.close()\n</code></pre> </li> </ul> </li> </ul>"},{"location":"2-Data-Engineering/apache-spark/","title":"Apache Spark","text":"<p>No storing, only processing</p> <ul> <li>Apache Spark<ul> <li>allows distributed parallel execution.</li> <li>external data -&gt; loaded to -&gt; data frame -&gt; which is -&gt; RDD -&gt; runs on -&gt; different nodes within the cluster.</li> <li>large-scale data processing as pandas.</li> <li>InMemory to avoid disk-writes slowness  of map reduce</li> <li>Data Structure is RDDs</li> <li>Interactive analytics are faster, just like we do in Jupyter where next step is based on prev.</li> <li>Transformations - <code>filter()</code> map groupByKey union - give RDD</li> <li>Actions - count first collect reduce - give single result</li> <li>PySpark - Python API for spark, RDD as DataFrame so makes similar to Pandas.</li> </ul> </li> </ul>"},{"location":"2-Data-Engineering/apache-spark/#spark-architecture-and-execution","title":"Spark Architecture and Execution","text":"<ul> <li>Driver Node connects to DB-Source, reads Data-1</li> <li>Data-1 is broken into partitions (RDDs) and distributed to executors nodes for parallel execution.</li> <li>In each Executor, Transform operation happens locally, that is, no movement.</li> <li>Action operation, crates shuffle and data has to move. This may cause bottleneck.</li> <li>Drive Node collects back results.</li> </ul> <pre><code>flowchart TB\n\nsubgraph Driver-Node\n  Data-1\nend\n\nsubgraph Driver-Node-\n  Data-2\nend\n\nsubgraph Executor-node-1\n  RDD-11 --Transform--&gt; RDD-12 --Action--&gt; RDD-13 --Collect--&gt;Data-2\nend\n\nsubgraph Executor-node-2\n  RDD-21 --Transform--&gt; RDD-22 --Action--&gt; RDD-23 --Collect--&gt;Data-2\nend\n\na1[(Source DB)] --&gt; Data-1\nData-1 --Partition--&gt; RDD-11\nData-1 --Partition--&gt; RDD-21\n\nRDD-12 --Shuffle--&gt; RDD-23\nRDD-22 --Shuffle--&gt; RDD-13\n\nData-2 --&gt; a2[(Destination DB)]\n</code></pre>"},{"location":"2-Data-Engineering/apache-spark/#parallelism","title":"Parallelism","text":"<ul> <li>Parallelism is required in all activities, else it becomes bottleneck.</li> <li>Spark supports parallelism out of the box</li> <li>Reads<ul> <li>JDBC - it lets read partitioned column values simultaneously</li> <li>Kafka - each spark executor reads from subset of Kafka partitions</li> </ul> </li> <li>Processing<ul> <li>Spark can do transformation, parallelly on different executors.</li> <li>Spark alo implements predicate push down which understands any filter during execution plan, and filter that data at the read itself, that is, it does not even read the data that is to be filtered out.</li> </ul> </li> <li>Writes<ul> <li>Writing data to sinks is parallel out-of-the-box, each Spark executor can write to JDBC sinks in parallel. Similarly, Spark executors can write to Kafka topics in parallel too.</li> </ul> </li> </ul>"},{"location":"2-Data-Engineering/apache-spark/#execution-plan","title":"Execution Plan","text":"<ul> <li>Spark has Lazy Execution - only an action triggers execution.</li> <li>Spark Optimizer comes up with a physical plan. This plan optimizes for:<ul> <li>Reduced I/O</li> <li>Reduced Shuffling</li> <li>Reduced Memory Usage</li> </ul> </li> </ul>"},{"location":"2-Data-Engineering/apache-spark/#spark-core","title":"Spark Core","text":"<ul> <li>The foundation of the overall project, it provides basic I/O functionalities, distributed task dispatching, scheduling, and fault recovery.</li> <li>It supports a wide range of data sources including HDFS, HBase, Cassandra, and S3.</li> </ul>"},{"location":"2-Data-Engineering/apache-spark/#spark-streaming","title":"Spark Streaming","text":"<p>Enables scalable and fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, and HDFS, and can be processed using complex algorithms.</p> <p>In stream processing it is important to maintain the current state. It can help to resume a halted pipeline or restart on failure.</p> <ul> <li>Checkpoints<ul> <li>Saves the job-state to persisted location (HDFS/S3)</li> <li>recover job state on failure and restart</li> <li>save metadata and RDDs, like Kafka Offsets, state tracking by Keys, RDDs requiring transformation across multiple batches.</li> </ul> </li> </ul> <ul> <li>Watermarking - It lets manage late data arrival in even-time window operations. Spark waits for data to arrive, keeps track of events and ordering</li> </ul> <ul> <li>Keys Tracking - Keys can let track current state, key is modified if state changes.</li> </ul>"},{"location":"2-Data-Engineering/apache-spark/#spark-sql-spark-analytics-with-spark-sql","title":"Spark SQL - Spark Analytics with Spark SQL","text":"<ul> <li>Spark SQL is part of Spark which lets use SQL interface to interact with data.</li> <li>It supports batch and streaming.</li> <li>It lets analyse data in same pipeline.</li> </ul>"},{"location":"2-Data-Engineering/apache-spark/#spark-ml-lib-machine-learning-with-spark","title":"Spark ML Lib - Machine Learning with Spark","text":"<ul> <li>it make easy to do, feature extraction, transformation, dimensionality reduction.</li> </ul>"},{"location":"2-Data-Engineering/apache-spark/#spark-graphx","title":"Spark GraphX","text":"<ul> <li>A distributed graph processing framework.</li> <li>It supports graph computation and provides an API for expressing graph analytics.</li> </ul>"},{"location":"2-Data-Engineering/apache-spark/#processing","title":"Processing","text":"<p>Batch Processing</p> <ul> <li>Process data in batches with defined sizes and windows</li> <li>Source data does not change during processing</li> <li>Dataset is bounded</li> <li>High latency (data lags), creating output at the end of processing</li> <li>Easy to reprocess and get outputs</li> </ul> <p>Real Time Processing</p> <ul> <li>Process data as they are created at source</li> <li>Source data may change during processing - it can be added, modified or deleted during processing</li> <li>Unbounded datasets</li> <li>Low-latency requirements (data is processed as it is generated)</li> <li>State Management - Need to keep track of things.</li> </ul>"},{"location":"2-Data-Engineering/apache-spark/#spark-installations","title":"Spark Installations","text":"<ul> <li>Standalone Mode<ul> <li>download the spark, install it. This makes <code>bin/pyspark</code> available locally and you can do pyspark things.</li> <li>simple and lightweight deployment mode for Apache Spark.</li> <li>you set up spark-home, start master-node and worker-nodes.</li> <li>They contact on TCP-IP, and then you can use spark.</li> </ul> </li> </ul> <ul> <li>Cluster Mode</li> </ul> <ul> <li>Local Mode<ul> <li>local on single machine. without a cluster, is on a single thread.</li> <li>eg, just install py package pyspark and you can build a <code>SparkSession</code> and play with it. eg https://gist.github.com/iYadavVaibhav/2f282e8fc34488ba150542033c9f2c82</li> </ul> </li> </ul>"},{"location":"2-Data-Engineering/apache-spark/#databricks","title":"Databricks","text":"<ul> <li>Lake formation tool. More on Databricks Notes</li> </ul>"},{"location":"2-Data-Engineering/apache-spark/#ll-apache-spark-essential-training-big-data-engineering","title":"LL - Apache Spark Essential Training: Big Data Engineering","text":"<p>Link: Apache Spark Essential Training: Big Data Engineering</p> <p>Learnings</p> <ul> <li>Spark jobs are java classes with class-methods doing specific work.</li> <li>Spark splits data into RDDs based on key, they do single operation in one executor, multi-record-operation by swapping the data in between executors.</li> <li>Spark can read and write to multiple databases like rdbms, file, kafka etc.</li> <li>Spark comes up with execution plan by analysing the whole code.</li> <li>Spark lets maintain the state of job by IDs.</li> <li>You can build hybrid of real-time and batch job using spark.</li> <li>The code is java jobs, docker containers for source and sinks.</li> <li>Spark can consume from Kafka in real time.</li> <li>Spark can publish a Kafka queue in real time.</li> <li>Spark can read write to S3, parallelly</li> <li>Spark can read write to RDBMS, parallelly</li> </ul> <p>Project Architecture</p> <ul> <li>RDBMS Maria DB<ul> <li>Runs in docker as MariaDB. Change it to Postgres. <code>image: mariadb</code>, <code>ports: 3306</code></li> <li>has different data warehouse as databases</li> <li>Three different databases, represent different purposes<ul> <li><code>warehouse_stock</code> DB - has different schemas which have data from different regional warehouse data.</li> <li><code>global_stock</code> DB - has combined worldwide processed stock numbers</li> <li><code>website_stats</code> DB - has processed stats</li> </ul> </li> <li>Schemas in <code>warehouse_stock</code> DB, represent warehouses<ul> <li><code>UK-warehouse</code></li> <li><code>US-warehouse</code></li> <li><code>IN-warehouse</code></li> </ul> </li> </ul> </li> </ul> <ul> <li>Kafka<ul> <li>Runs in docker. <code>image: 'bitnami/kafka</code>, <code>ports: 9092</code>.</li> </ul> </li> </ul> <ul> <li>Spark</li> </ul> <ul> <li>Redis</li> </ul> <ul> <li>Zoo Keeper</li> </ul> <ul> <li>Docker<ul> <li>runs 4 container for <code>mariadb</code>, <code>kafka</code>, <code>redis</code> and <code>zookeeper</code>.</li> </ul> </li> </ul> <ul> <li>Java<ul> <li>need to be installed on system to run the project. Also needs build tools.</li> </ul> </li> </ul> <ul> <li>Ch 3 - batch processing<ul> <li>An enterprise has warehouse across the globe. Each has local data center.</li> <li>Stock data for each each item, each day<ul> <li>Opening Stock - number of items in warehouse</li> <li>Receipts</li> <li>Issues</li> </ul> </li> <li>Do Batch Processing to<ul> <li>Build Global Database with information from each country warehouse.</li> <li>Gather and upload data to central cloud storage like S3.</li> <li>Scalable to hundreds of warehouse.</li> </ul> </li> </ul> </li> </ul> <pre><code>flowchart LR\n\ndb1[(UK RDBMS \\n\\n warehouse_stock\\n.london )]\ndb2[(US RDBMS \\n\\n warehouse_stock\\n.bostob )]\ndb3[(India RDBMS \\n\\n warehouse_stock\\n.delhi )]\n\nsubgraph sub1[Spark Batch Processing \\n Stock Uploader Job]\n  sp1[Read MariaDB \\n\n  SparkSession\n  .. .read\n  .option: dbtable\n  .. .load =&gt; DF\n\n\n  DF.write\n  .. .parquet\n\n  Write to Parquet\n  ]\nend\n\ndb1 --&gt; sp1\ndb2 --&gt; sp1\ndb3 --&gt; sp1\n\ndfs1[(Distributed \\n File System \\n\\n raw_data \\n Parquet Files)]\n\nsp1 --&gt; dfs1\n\nsubgraph sub2[Spark Batch Aggregator Job]\n  sp2[\n    Read Parquet\n    SparkSession.\n    .read.parquet: dir\n    =&gt; df\n\n    DF =&gt; .sql\n    .write: MariaDB JDBC.\n    .. .save\n    Write to MariaDB\n  ]\nend\n\ndfs1 --&gt; sp2\n\ndb4[(gloabal_stock \\n RDBMS)]\n\nsp2 --&gt; db4\n</code></pre> <ul> <li>Ch 4 - Real time processing<ul> <li>eCom website like Amazon</li> <li>Publisher publishes stream of website events like<ul> <li>visit date</li> <li>last event - Catalog/FAQ/ShoppingCart/Order</li> <li>duration</li> <li>country</li> </ul> </li> <li>Do real time processing to<ul> <li>Aggregate Five Second summary by last action - each 5s, compute summary and store in RDBMS</li> <li>Count number of visits by country - Running Counter</li> <li>Store data of event Abandoned Shopping Cart for further downstream processing - like send email that we have save it for you and it is awaiting checkout.</li> </ul> </li> </ul> </li> </ul> <pre><code>flowchart LR\n\na1&gt;eCom \\n Website]\na2[[ Kafka \\n Topic \\n\\n spark.\\nstreaming.\\nwebsite.\\nvisits ]]\n\na1 --&gt; a2\n\na2 --&gt; sp0\n\nsubgraph sub1[Spark Stream Processing]\n  sp0[\n    Website \\n Analytics Job:\n\n  SparkSession\n  .readStream\n  .selectExpr\n  .load\n  =&gt; visitsDf\n  ]\n\n  sp1[.filter \\n .selectExpr \\n .format: Kafka \\n .option: Topic \\n .writeStream]\n\n  sp2[select \\n writeStream \\n forEach: Redis]\n\n  sp3[.withColumn \\n .withWaterMark \\n .groupby..agg \\n .writeStream \\n .forEach: Maria]\n\nend\n\nsp0 --&gt; sp1\nsp0 --&gt; sp2\nsp0 --&gt; sp3\n\n\nb2[(Redis\\n\\n country-stats)]\nb3[(Maria DB\\n\\nwebsite.\\nvisit_stats)]\nb1[[Kafka \\n Topic\\n\\nspark.\\nstreaming.\\ncarts.\\nabandoned]]\n\n\n\nsp1 --&gt; b1\nsp2 --&gt; b2\nsp3 --&gt; b3</code></pre> <ul> <li>Ch 6 - End to End project - Hybrid Pipeline</li> </ul> <pre><code>flowchart LR\n\nb3[(Maria DB\\n\\nwebsite.\\nvisit_stats)]\n\nb3 --&gt; sp1\n\nsubgraph sub1[Spark Batch Job]\n  sp1[\n    Read MariaDB\n\n    SoarkSession\n    .read.option: mariadb\n    .load\n    =&gt; DF\n\n    DF =&gt; selectExpr \\n format: Kafka \\n option: Topic \\n save\n\n    Write to Topic\n  ]\nend\n\nsp1 --&gt; a2\n\na2[[ Kafka \\n Topic \\n\\n spark.\\n exercise.\\n lastaction. \\n long ]]\n\na2 --&gt; sp2\n\nsubgraph sub2[Spark Streaming Job]\n  sp2[\n    Read Topic\n\n    SparkSession\n    .readStream\n    .selectExpr.load\n    =&gt; visitsDf\n\n    visitsDf =&gt; .selectExpr \\n .writeStream \\n .forEach: Redis \\n .start\n\n    Write to Redis\n  ]\nend\n\nsp2 --&gt; b2\n\nb2[(Redis\\n\\n last-action \\n -stats)]\n</code></pre> <p>Project Hands On</p> <ul> <li>run docker compose to start containers<ul> <li>Run using <code>docker compose -f spark-docker.yml up</code></li> <li>It runs following containers<ul> <li><code>maria db</code> - Just a MySQL RDBMS to store simple tables, exposing 3306</li> <li><code>redis</code> - For log?, exposing 6379</li> <li><code>kafka</code> - container running kafka on single node and exposing ports 9092, 29092</li> <li><code>zookeeper</code>, exposing 2181</li> </ul> </li> </ul> </li> </ul> <ul> <li>Java Installation<ul> <li>Install <code>Jdk 17</code> on ubuntu, setup <code>JAVA_HOME</code> to <code>export JAVA_HOME=/usr/bin/java</code>.</li> <li>Install Java Extension on vscode, it builds all packages using <code>Maven</code>.</li> </ul> </li> </ul> <ul> <li>run project setup <code>SetupPrerequisites.java</code><ul> <li>Build Database Structure, DDL Only<ul> <li>For each warehouse database<ul> <li>Check databases, create if not created to avoid dupes</li> <li>create databases using schema</li> <li>create table using schema</li> <li>grand privileges</li> <li>flush privileges</li> </ul> </li> </ul> </li> <li>creates kafka topics<ul> <li>Create 3 new kafka topics<ul> <li><code>spark.streaming.website.visits</code></li> <li><code>spark.streaming.carts.abandoned</code></li> <li><code>spark.exercise.lastaction.long</code></li> <li>For each topic, check present topic list, if not exists, create it.</li> <li> read more on what/how to create kafka topics.</li> </ul> </li> </ul> </li> </ul> </li> </ul> <ul> <li>Ch 3 Setting local db, run <code>rawStockDataGen.java</code><ul> <li>It generates data for three warehouse, it puts in three database instances in Maria DB.</li> <li><code>generateData(warehosue=\"\")</code> function, for each warehouse, it inserts fake data.<ul> <li>executes insert statement to generate 3 days data.</li> </ul> </li> </ul> </li> </ul> <ul> <li>Ch 3 - Spark Read then Upload - using <code>DailyStockUploaderJob.java</code><ul> <li>Read Daily Stock from RDBMS to FileStorage</li> <li>The function <code>uploadStock(String startDate, String endDate, String warehouseId)</code><ul> <li>Runs query to find min and max id, in between dates in a warehouse. This gives binds.</li> <li>Create SparkSession<ul> <li>define host, partition, parallelism, job-name etc.</li> <li>everything is done using this session object, <code>spark</code></li> </ul> </li> <li>Read from RDBMS using Spark Session Object into Spark-DataFrame<ul> <li>create a query to read from RDBMS</li> <li>pass this query to <code>spark.read().option()</code></li> <li>it returns Spark DataFrame, <code>stockDF</code></li> </ul> </li> <li>Write Spark-DF to File-Storage (normal file/HDFS/S3 logic is same, config change)<ul> <li><code>stockDF.write()</code> add <code>partitionBy()</code> and <code>parquet(path:)</code></li> <li>a folder for each date, then folder for each warehouse, this has the parquet file. This is due to partition by stock-date and partition-id.</li> </ul> </li> <li>Close Spark Session<ul> <li><code>spark.close()</code></li> </ul> </li> </ul> </li> </ul> </li> </ul> <ul> <li>Ch 3 - Spark Processing - using <code>StockAggregatorJob.java</code><ul> <li>Aggregate uploaded Stocks Data by reading from file-storage</li> <li>This is computed on raw parquet files, using spark session, the spark-df is written back to RDBMS Maria-DB using multiple JDBC connectors for parallelism.</li> <li>In function, <code>aggregateStock(String sourceDir)</code><ul> <li>Create spark session</li> <li>Read parquet files at sourceDir to get SparkDF</li> <li>Load from spark-df to view using <code>stockDF.createOrReplaceTempView(\"GLOBAL_STOCK\");</code> Adds view <code>GLOBAL_STOCK</code> to RDBMS.</li> <li>Read loaded data using <code>spark.sql(\"query\").show()</code>.<ul> <li>Group by items, date and aggregate all measure values. You loose, country information.</li> <li>This is only to print grouped results.</li> </ul> </li> </ul> </li> </ul> </li> </ul> <ul> <li>Ch3 - Browse RDBMS data, using <code>GlobalStockDBBrowser.java</code><ul> <li>lastly, we simply run a java code, to connect to MariaDb and display total count of group-summary table (no spark code here).</li> <li>alternatively, you can directly run query on RDBMS using various methods.</li> <li>To use mysql client<ul> <li>do <code>docker exec -it mariadb bash</code> where 'maria-db' is the container name.</li> <li>once in container, do <code>mysql -u spark -p</code>, where spark is username and then enter password, which is spark as can be seen in <code>docker-compose.yml</code> file.</li> </ul> </li> </ul> </li> </ul> <ul> <li>Ch 4 - Solution<ul> <li><code>spark.streaming.websites.visit</code> is publisher that has events data pushed from website whenever user exists the website.</li> <li>Spark Job 'website analytics job' is consumer for this queue. Now it runs 3 actions on the data received<ul> <li>Compute 5 sec summary and insert in RDBMS Maria DB, <code>website.visit_stats</code></li> <li>Running Counter in Redis Socket Set <code>country-stats</code></li> <li>Filters event having abandoned shopping cart and publishes them to Kafka Queue <code>spark.streaming.carts.abandoned</code></li> </ul> </li> </ul> </li> </ul> <ul> <li>CH 4 - Generate Kafka Stream<ul> <li>Code generates Kafka events, this is JSON data that tells lastAction, duration, country, datetime.</li> <li> <p>Loop 100 times, to generate 100 events, in each loop</p> <ul> <li>Using <code>KafkaProducer</code> you can create producer.</li> <li>Using <code>ProducerRecord</code> you can make a JSON record in a topic, to add to producer.</li> </ul> <pre><code>import org.apache.kafka.clients.producer.KafkaProducer;\nimport org.apache.kafka.clients.producer.ProducerRecord;\n\n...\nfor 1...100:\n    Producer&lt;String,String&gt; myProducer\n                    = new KafkaProducer&lt;String, String&gt;(kafkaProps);\n    ...\n    ProducerRecord&lt;String, String&gt; record =\n                                new ProducerRecord&lt;String, String&gt;(\n                                        topic,\n                                        recKey,\n                                        value );\n\n    RecordMetadata rmd = myProducer.send(record).get();\n    sleep(1000);\nend For;\n</code></pre> </li> </ul> </li> </ul> <ul> <li> <p>Ch 4 - Spark Stream Processing</p> <ul> <li>Now that we have kafka events in a topic, we can consume them using spark streaming.</li> <li>reads a real time website visits stream from kafka<ul> <li>Updates 5 second summaries in MariaDB</li> <li>Updates a Country scoreboard in Redis</li> <li>Updates a Kafka topic, when shopping cart is the last action.</li> </ul> </li> </ul> <ul> <li>You create a spark session <code>spark = SparkSession.....getOrCreate();</code></li> <li>Read into data frame using, <code>df = spark.readStream().format(\"kafka\")......load();</code>. you subscribe to topic here.</li> <li>Next, use query to read data as required from this broker, into DF</li> <li>Use DF.writeStream().foreach(ForeachWriter( open.. process.. close.. )) to write. open, process, close - are implemented in <code>RedisWriter.java</code> and <code>MariaDBWriter.java</code>.<ul> <li>Filter from df to find <code>action: Shopping Cart</code> and then publish this to a kafka topic, <code>spark.streaming.carts.abandoned</code>.</li> <li>Filter, from df, to filter country info and write it to redis counter.</li> <li>Use, WaterMark feature of spark to group by and aggregate the data every 5 seconds, and write to RDBMS MariaDB.</li> </ul> </li> </ul> <ul> <li>Now our Spark-Streaming Pipeline is writing to 3 different places, RDBMS, Redis, Kafka-Topic.</li> </ul> <ul> <li>Browse the written data<ul> <li>Display the published abandoned shopping cart event using,<code>ShoppingCartTopicBrowser.java</code>. Reads by subscribing to topic using Spark Session.</li> <li>Display Redis counter data using, <code>CountryStatsBrowser.java</code> every 5 seconds. No spark, connect to Redis and print.</li> <li>Browse Stats DB using, <code>VisitStatsDBBrowser.java</code> every 5 seconds. No spark, connect to MariaDB and print.</li> </ul> </li> </ul> </li> </ul> <ul> <li> <p>Ch 6 - Exercise to make Hybrid pipeline having Spark Batch Processing and Stream Processing</p> <ul> <li>Spark batch Processing<ul> <li>Read from MariaDB every night and save to Kafka Topic, use <code>LongLastActionExtractorJob.java</code><ul> <li>Create session</li> <li>connect to mariaDB</li> <li>execute Query</li> <li>get DF</li> </ul> </li> <li>Write to kafka Topic<ul> <li>DF, selectExpr</li> <li>create topic, <code>spark.exercise.lastaction.long</code></li> <li>save to topic</li> </ul> </li> </ul> </li> <li>Spark Streaming Processing<ul> <li>Subscribe to Topic<ul> <li>create sparkSession</li> <li>readStream to read from topic, <code>spark.exercise.lastaction.long</code></li> <li>load it to DF</li> </ul> </li> <li>Write to redis counter<ul> <li>DF, selectExpr</li> <li>writeStream using foreach and process data to write to redis.</li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>Java Classes</p> <ul> <li> <p>Kafka</p> <pre><code>import org.apache.kafka.clients.admin.AdminClient;\nimport org.apache.kafka.clients.admin.NewTopic;\n</code></pre> </li> </ul> <ul> <li> <p><code>Spark</code></p> <ul> <li>framework, here is a local JVM code, this is only for demo and testing purpose. Master/node/executors all are running on single JVM code. All you need to do is import the package and create a session with local master and node.</li> </ul> <pre><code>import org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SaveMode;\nimport org.apache.spark.sql.SparkSession;\n</code></pre> </li> </ul>"},{"location":"2-Data-Engineering/apache-spark/#links","title":"Links","text":"<ul> <li>LL - Apache Spark Essential Training</li> </ul>"},{"location":"2-Data-Engineering/big-data/","title":"Big Data","text":""},{"location":"2-Data-Engineering/big-data/#data-lake-vs-data-lakehouse-vs-data-warehouse","title":"Data Lake vs. Data Lakehouse vs. Data Warehouse","text":"<p>Data Lake:</p> <ul> <li>Purpose: Stores raw data in its original format without predefined structure.</li> <li>Structure: Unstructured or semi-structured.</li> <li>Schema: Schema is defined on-the-fly during query execution.</li> <li>Use Cases: Big data analytics, machine learning, data exploration.</li> <li>Flexibility: Highly flexible, allowing for various use cases and data types.</li> <li>Performance: May have performance limitations due to the unstructured nature of the data.</li> </ul> <p>Data Lakehouse:</p> <ul> <li>Purpose: Combines the flexibility of a data lake with the structured nature of a data warehouse.</li> <li>Structure: Structured, semi-structured, or unstructured.</li> <li>Schema: Supports schema evolution, allowing for changes to the data structure.</li> <li>Use Cases: Data warehousing, analytics, machine learning.</li> <li>Flexibility: Offers a balance of flexibility and structure.</li> <li>Performance: Generally better performance than data lakes due to the structured nature of the data and optimization techniques.</li> </ul> <p>Data Warehouse:</p> <ul> <li>Purpose: Stores structured data for reporting and analysis.</li> <li>Structure: Highly structured, with predefined schemas.</li> <li>Schema: Schema is defined upfront and is relatively static.</li> <li>Use Cases: Business intelligence, reporting, data warehousing.</li> <li>Flexibility: Less flexible than data lakes or data lakehouses, as data must conform to a predefined schema.</li> <li>Performance: Optimized for querying and reporting, with good performance for structured data.</li> </ul> <p>Key Differences:</p> Feature Data Lake Data Lakehouse Data Warehouse Structure Unstructured/semi-structured Structured/semi-structured/unstructured Structured Schema Defined on-the-fly Supports schema evolution Predefined and static Use Cases Big data analytics, machine learning Data warehousing, analytics, machine learning Business intelligence, reporting Flexibility Highly flexible Balanced Less flexible Performance May have performance limitations Generally better performance Optimized for querying and reporting <p>Choosing the Right Approach:</p> <p>The best choice depends on your specific needs and use cases. If you require high flexibility and the ability to store various data types, a data lake might be suitable. For a balance of flexibility and structure, a data lakehouse is a good option. If you primarily need a structured data repository for reporting and analysis, a data warehouse is the best choice.</p>"},{"location":"2-Data-Engineering/big-data/#open-data-formats","title":"Open Data Formats","text":"<ul> <li>Includes Parquet, Avro, JSON, CSV.</li> <li>It defines standardized data format that is publicly accessible.</li> <li>These formats are designed to promote interoperability, data sharing, and collaboration. So data once stored can be used by different processing engines/solutions.</li> </ul>"},{"location":"2-Data-Engineering/big-data/#key-characteristics-of-open-data-formats","title":"Key Characteristics of Open-Data Formats","text":"<ul> <li>Publicly Available: The format specifications are freely accessible to anyone, without any licensing restrictions.</li> <li>Standardized: The format follows a well-defined structure, ensuring consistency and compatibility.</li> <li>Interoperable: Data stored in the format can be easily exchanged and processed by different software tools and platforms.</li> <li>Community-Driven: Open-data formats are often developed and maintained by communities of developers and users, ensuring their continued evolution and improvement.</li> </ul>"},{"location":"2-Data-Engineering/big-data/#popular-open-data-formats-for-data-lakes","title":"Popular Open-Data Formats for Data Lakes","text":"<ol> <li>Apache Parquet: A columnar storage format that is highly efficient for analytical workloads, especially on large datasets. It supports a variety of data types and can be used with different data processing frameworks.</li> <li>Apache ORC: Another columnar storage format designed for analytical workloads. ORC offers similar performance benefits to Parquet but has a slightly different structure.</li> <li>CSV (Comma-Separated Values): A simple text-based format that is widely used for data exchange. While not as efficient as columnar formats for analytical workloads, CSV is easy to read and write.</li> <li>JSON (JavaScript Object Notation): A flexible format for representing structured data. JSON is often used for semi-structured data and can be easily parsed by various programming languages.</li> <li>Avro: A binary data serialization system that is designed for efficiency and flexibility. Avro can be used to store both structured and semi-structured data.</li> </ol>"},{"location":"2-Data-Engineering/big-data/#benefits-of-using-open-data-formats","title":"Benefits of Using Open-Data Formats","text":"<ul> <li>Interoperability: Data stored in open-data formats can be easily shared and processed by different tools and platforms.</li> <li>Flexibility: Open-data formats can be used to store a variety of data types and structures.</li> <li>Community Support: Open-data formats are often supported by large communities of developers, ensuring their continued development and maintenance.</li> <li>Cost-Effectiveness: Using open-data formats can help reduce costs by avoiding vendor lock-in and promoting interoperability.</li> </ul> <p>By adopting open-data formats, organizations can create more accessible and interoperable data lakes, enabling better data sharing, collaboration, and analytics.</p>"},{"location":"2-Data-Engineering/big-data/#sequencefile-data-format-hive","title":"SequenceFile Data Format - Hive","text":"<ul> <li>SequenceFile is a Hadoop-specific file format designed for storing key-value pairs.</li> <li>It is efficient for storing large amounts of data</li> <li>It can be less performant for analytical workloads compared to columnar formats like Parquet or ORC.</li> <li>It is also open-source but not widely adopted and used.</li> <li>Hive can use Parquet or ORC, but what to use depends on use case</li> </ul>"},{"location":"2-Data-Engineering/big-data/#open-source-table-formats-data-lake-table-formats","title":"Open Source Table Formats (Data Lake Table Formats)","text":"<ul> <li>Includes Apache Iceberg, DataBricks DeltaLake, Apache Hudi. They are all data lake table-formats.</li> <li>An open table format is a standardized way to organize and manage data files within a data lake.</li> <li>They all can use different open-data formats, but parquet is default.</li> <li>Its purpose is to bring database-like features to data lakes, which are crucial in enabling the data lakehouse architecture (combining the best features of data lakes and data warehouses).</li> <li>They are not limited to just table-format but also offer warehousing features like transactionality, schema evolution, and time travel.</li> </ul> <ul> <li>Link: Comparison of Data Lake Table Formats (Apache Iceberg, Apache Hudi and Delta Lake) </li> </ul>"},{"location":"2-Data-Engineering/big-data/#hdfs","title":"HDFS","text":"<p>Cheap large scale distributed data storage, that can scale horizontally. Alternatives are S3, Azure Blob. It is only storage, not a processing thing.</p>"},{"location":"2-Data-Engineering/big-data/#map-reduce","title":"Map Reduce","text":"<p>Processing algorithm.</p> <p>As of 2022, it has better alternatives, like Apache Spark and Flink. MR is slow as it keeps cache in disk while new technologies keep cache and processing in memory. New technologies also have better connectors and more libraries.</p>"},{"location":"2-Data-Engineering/big-data/#big-data-applications-architecture","title":"Big Data Applications Architecture","text":"<p>Parallel Data Processing</p> <ul> <li>Python multiprocessing Pool - low level native python code, explicitly implement parallel processing</li> <li>Python dask - library having multiprocessing out of box</li> <li>Hive - framework that lets extract data using SQL. Behind it converts to MapReduce job.</li> <li>Spark - Apache Spark is InMemory to avoid disk writes slowness  of map-reduce</li> <li>Map Reduce - technique/algorithm</li> <li>Hadoop - framework</li> </ul> <p>Single vs Multi Record Operations for Parallelism</p> Records involved Operation Method to allocate in cluster Single record processing Filter, transform, Map Round robin to available nodes Multi record in group processing Group based processing Using a key to distribute across nodes Multi record in group processing Aggregation by key Distribution based on aggregation key All record processing Global Aggregation Multistep aggregation to reduce load on single node <p>Here, Filter map and transform can be on a record, hence on different node but aggregation needs all records, hence would need to be done on multiple nodes, so should be done after above operations.</p> <p>These should be individual jobs, like job 1 does filter, job 2 enrich, job 3 aggregate. So each job can be tweaked to run on single vs multiple nodes and hence can be provided different compute power. If your all process in etl need same scalability then all of them can be in one job.</p> <p>Key based Parallelism for Concurrency</p> <p>Key based distribution ensures, all records of one key is executed on single node, but different keys can work on different nodes. Eg, total sales by customer. This can be distributed by customer-key, one customer-key is executed on one node, so that it gets all sale records of that customer, and then other customer-keys can be processed by other nodes on cluster, this ensures parallel execution in multi-records processing.</p> <p>Note that, Selecting key is critical to ensure all nodes are used, and all data is evenly distributed on all nodes.</p> <p>Batch Processing</p> <ul> <li>Job should be scheduled such that there is gap if job overruns. There is no peak on one job, jobs should divide data in a good way. Job have balance between latency and resource usage.</li> <li>Minimize the data at source. Do not use same data for multi pipelines, minimize the data. Use, where clause, select less, use partition in parquet file.</li> </ul> <p>Clustering in Big Data Technologies</p> <p>Big Data technologies are deployed in Cluster, be it</p> <ul> <li><code>Spark</code> for processing</li> <li><code>HDFS</code> for storage, <code>Impala</code> to query this stored data.</li> <li><code>Kafka</code> for queuing</li> </ul> <p>As the load increases, we simply add more nodes to the cluster to scale horizontally.</p> <p>Data Caching</p> <ul> <li>Can be done on static or near static data, eg gender, demography; these data are static and rarely change.</li> <li>Is much faster than database query</li> <li>Eg, get category name for category code, it is much faster from cache, like Redis cache.</li> </ul> <p>Reprocessing</p> <ul> <li> <p>You may need to reprocess when a batch fails, or new data comes while processing or other circumstances. In any of it, the pipeline should ensure:</p> <ul> <li>batch is repeatable, based on time or key.</li> <li>processing logic should not double count the records</li> </ul> </li> </ul>"},{"location":"2-Data-Engineering/big-data/#use-case-audit-trail-data","title":"Use Case: Audit Trail Data","text":"<p>Consider a use case, where you an eCommerce website generates user activity log data, which is about 3GB per day. It is stored in Oracle DB. In 15 days the size reaches about 30GB. Oracle DB only keeps last 15 days audit data to keep size about 30GB.</p> <p>The analytics team, need at least 3 years of audit trail data, so that they can find patterns and take decisions. This will need 10 terabyte scalability.</p> <p>Evaluate the functional / non-functional requirements.</p> <p>You need to create an ETL job that can read new data from Oracle DB in hourly batches, do the transformation to remove customer name and load it.</p> <p>The hourly batch ETL job should be designed so that it can concurrently read, transform parallelly and concurrently and load it parallelly, so that the ETL is fast.</p> <p>The load db technology should be such that is can scale, is cheap, is mature, can be queries by SQL.</p> <p>Evaluate load technologies like, MySQL / MongoDB / HDFS+Impala. MySQL might get difficulty to scale, MongoDB is preferred for documents. HDFS+Impala seems ideal choice for this.</p> <p>Evaluate ETL technologies like, Apache Pig / Apache Spark. Both can let read from RDBMS using ODBC, Both can let write to HDFS, both allow concurrent processing. However, in performance Pig is slow as it uses map-reduce on disk, while spark is fast as it uses in-memory processing. Hence, we can pick spark for ETL job.</p> <p>Considerations</p> <ul> <li>Spark lets concurrent processing, the ETL task is divided into multiple spark-tasks in spark cluster.</li> <li>Use hashing-algorithm on audit-ids, so that each tasks can pick certain records.</li> <li>Each thread can write to different HDFS partitions concurrently based on hashing algo.</li> <li>Organise HDFS folder by dates, this gives natural indexing for querying.</li> <li>If required, you can add more nodes for horizontal scaling of ETL/Load, as Spark/HDFS support horizontal scaling.</li> </ul> <p>The scaling also depends on how often Orcale DB (source), allows to read. Also, how much load can you put on DB to read, as it would be used by other applications. Oracle does allow concurrent reading with multiple connections and hence spark can make use of spark tasks.</p>"},{"location":"2-Data-Engineering/big-data/#use-case-advertising-analytics","title":"Use Case: Advertising Analytics","text":"<p>Consider an eCommerce website using email and ads for marketing. They want to know the effectiveness of both the channels. They send 1 million email each day using an enterprise tool. Ads generate revenue of 0.75 million per day.</p> <p>The data for both these channels is not at one place. The analytics team need both data sets for last 3 years for proper analysis.</p> <p>The data channel offer API based access, which has limitation like can call api in 15 mins, API only gives JSON dumps or CSV downloads.</p> <p>Functional Requirements</p> <ul> <li>acquire data daily, build temp-cache</li> <li>aggregate data and compute summaries</li> <li>store summary to analytical db</li> <li>provide sql access to db</li> </ul> <p>Non-functional Requirements</p> <ul> <li>Scalability - 1m email, .75m revenue, 3 year data</li> <li>Availability - downtime of 1 hr is okay</li> <li>Latency - day summary within 24 hours</li> <li>Security - authenticated access only</li> </ul> <p>There will be two pipelines, each dump the data in local file-temp-catch, then this is aggregated and loaded to analytics db.</p> <p>Imp: The ETL jobs for big data are different in a way that they should scale horizontally. This has to be thought when designing the job, is the technology you are using ready scale horizontally, can it read, process and load concurrently.</p> <pre><code>flowchart LR\n\na1[Ecommerce \\n Platform API]\na2[Fetch Ad \\n Data Job]\na3[(Ad \\n temp \\n cache)]\na4[Ad Daily \\n Summarizer Job]\n\nc[(Analytics \\n DB)]\n\nb1[Campaign \\n Management API]\nb2[Fetch Email \\n Data Job]\nb3[(Email \\n temp \\n cache)]\nb4[Email Daily \\n Summarizer Job]\n\na1 --&gt; a2 --&gt; a3 --&gt; a4 --&gt; c\nb1 --&gt; b2 --&gt; b3 --&gt; b4 --&gt; c\n</code></pre> <p>Scaling Opportunities</p> <ul> <li>Work can be divided among nodes using composite key on name-age-gender-etc.</li> <li>summarizers can run parallel task based on this key.</li> <li>analytical db can provide concurrent write based on this key.</li> </ul> <p>Scaling Limitation</p> <ul> <li>API rate limiting, you cannot make concurrent calls to API at unlimited rate</li> <li>temp-file-caching has write limit, choose this tech as required.</li> </ul> <p>Tech Stack</p> <ul> <li>For API extraction you can use SDK by API provider but this may not scale. You can build API call, by making con-current threads or process using Java/Python.</li> <li>For Cache, since it will be granular, it will be huge, use HDFS for scaling.</li> <li>Summarization, use apache spark.</li> <li>Analytics DB, need concurrent writes, good SQL analytics capability, here MySQL wins over Mongo and HDFS+Impala.</li> </ul>"},{"location":"2-Data-Engineering/big-data/#use-case-product-recommendations","title":"Use Case: Product Recommendations","text":"<p>Consider e-com company XYZ with 20m users and 200k transactions a day. XYZ wants to scale by recommending products based on user behaviour.</p> <ul> <li>create a pipeline that recommends products based on user transactions. Pipeline has a ML model that can give recommendation via API.</li> </ul> <ul> <li>Goals<ul> <li>scale to user base</li> <li>consider user recent history</li> <li>daily updates</li> <li>recommend in real time with low latency</li> </ul> </li> </ul> <ul> <li>Inputs<ul> <li>user age, demographics via RDBMS SQL Query</li> <li>Transactions via Kafka Topic</li> </ul> </li> </ul> <ul> <li>Output<ul> <li>recommended items available in db, with ranking by customer id.</li> <li>real time query</li> <li>scale beyond 10m user base.</li> </ul> </li> </ul> <pre><code>flowchart LR\na1[Transaction \\n Topic] --&gt; \na2[Archive \\n Transactions] --&gt;\na3[(Txn History)] --&gt; a4\nb1[(Customer \\n Demographics)] --&gt;\na4[Merge Data] --&gt;\na6[Create \\n Recommendations] --&gt; \na7[Expose API]\na6 --&gt; a8[(Recommendations \\n DB)]</code></pre> <p>Scaling should support to do these concurrently.</p> <p>Scaling Limitations</p> <ul> <li>Recommendation model API capacity may limit concurrency.</li> <li>RDBMS read for demographics may limit, but can be cached.</li> <li>Recommendation DB should be update friendly.</li> </ul> <p>Tech Stack</p> <ul> <li>Txn History DB - we pick MongoDB as it support concurrent IO/ nested docs / scaling and high availability. HDFS has no out of the box support for nested / m-m relationship requirement.</li> <li>Recommendations DB - Cassandra as it supports update, fast query by id, scalability and availability. HDFS has no fetch by ID, RDBMS has no high  scaling and availability.</li> <li>Processing Jobs - Spark.</li> </ul> <p>Link: LL - Architecting Big Data Applications: Batch Mode Application Engineering</p>"},{"location":"2-Data-Engineering/data-engineering/","title":"Data Engineering Roadmap","text":"<p>how big data engineering projects work</p> <ul> <li>'Terraform' to build infra on AWS and tear it down.</li> <li>Github Actions for CI CD to build, test and deploy changes.</li> <li>AWS ECR to host docker images.</li> <li>AWS Lambda to host code.</li> <li>AWS EventBridge to trigger something on event.</li> <li>AWS S3 to store data dumps.</li> <li>Docker to run in container, infra independent.</li> <li>AWS IAM roles to give permission to resources to talk to each other.</li> <li>DBT is used to do ETL.</li> <li> how to read from api using lambda, load to S3?</li> <li> what is <code>Makefile</code>? is shell command file, it is used to put all infra build and docker compose commands in a file.</li> </ul> <p>Links</p> <ul> <li>Read all blogs to get overview: https://www.startdataengineering.com/</li> <li>DE Projects on Youtube and medium article https://medium.com/@yusuf.ganiyu/7-end-to-end-data-engineering-projects-that-sets-you-apart-from-the-rest-bd809fe5aa95</li> </ul> <ul> <li>Project 1 - The Ultimate Data Engineering Project To Land Your Dream Job In 2024 62815432c682</li> <li>Project 2 - Data Engineering Ops Project With Ci Cd And Iac Af3ec23548d4</li> <li>project 3 - Create Serverless Data Pipeline Using Aws Cdk Python 5cg2</li> <li>Multi Project - Overview</li> <li>All DE Projects from Community</li> <li>Best Books - Awesome Data Engineering</li> </ul> <ul> <li>Dataengineering - Community Projects</li> <li>Startdataengineering - Post Data Engineering Project To Impress Hiring Managers</li> </ul>"},{"location":"2-Data-Engineering/data-engineering/#terraform-iac","title":"Terraform - IaC","text":"<p>It is a generic IaaC tool that can let you build infra using JSON on AWS/Azure and other platforms. Eg</p> <ul> <li>You need not do things on AWS manually, like creating an IAM role.</li> <li>You can create IAM Role on AWS.</li> <li>You can create Rule on AWS EventBridge, like trigger lambda function daily.</li> <li>You can create AWS ECR repository, where you can push the docker image.</li> <li>You can create policy to save the logs to Cloudwatch event.</li> <li>basically it is code the lets you create resources on AWS.</li> <li>It is Infrastructure as Code IaC.</li> </ul> <p>Terraform is container form image, where it runs code on a machine.</p> <p>Alternatives: AWS CDK (Cloud Development Kit)</p>"},{"location":"2-Data-Engineering/data-engineering/#github-actions-cicd","title":"Github Actions - CI/CD","text":"<p>Action has jobs. A job has steps. Steps are commands.</p> <p>Action is set of commands called jobs (a command is called step and set of jobs is action). It runs the commands on a machine (eg, ubuntu server). The jobs can be dependent on each other. The action (set of jobs) is triggered <code>on</code> something (eg, on repo push do build and deploy job).</p>"},{"location":"2-Data-Engineering/data-engineering/#makefile","title":"Makefile","text":"<p>It specifies shell commands for docker-compose like run up down etc. Example</p> <p>Commands to use it:</p> <pre><code># compose up\nmake up\n\n# run build format test etc\nmake ci\n\n# compose down\nmake down\n</code></pre>"},{"location":"2-Data-Engineering/data-frameworks-tools/","title":"Data Tools &amp; Frameworks","text":"<p>data processing tools, libraries and frameworks</p> <ul> <li>Processing Engines - lets do the transformations. Eg, Spark</li> <li>Workflow Managers - Airflow and Nifi</li> <li>Administrative Tools - <code>pgAdmin</code> for PostgreSQL and <code>Kibana</code> for Elasticsearch.</li> <li>Cluster is a group of server, where each server is called node.</li> </ul> <ul> <li>Connection pooling is a technique of creating and managing a <code>pool of connections</code> that are ready for use by any thread that needs them. It is a cache of database connections maintained so that the connections can be reused when future requests to the database are required. Connection pools are used to enhance the performance of executing commands on a database. Pool means a particular thing collected together for shared use by several people. <code>JDBC</code> does this. JDBC (Java Database Connectivity) is a standard interface that enables communication between database management systems and applications written in Oracle Java. Most database build this as driver eg, <code>postgresql-42.2.10.jar</code></li> </ul> <ul> <li> <p>Data Pipelines</p> <ul> <li>Combine database, a programming language, a processing engine and a data warehouse to build a pipeline. Here, database can be source, programming language can be used to control the processing engine to transform the data and load into data warehouse.</li> <li>pipeline can be scheduled using crontab, a better workflow manager is <code>Apache Airflow</code> or NiFi.</li> <li>for dev you can install all above tools on same machine, but in prod they are network of machines.</li> </ul> <ul> <li>In Prod<ul> <li>production data pipeline \u2013 idempotence and atomicity</li> <li>you need to stage the data as file or in database</li> <li>you need to validate the data, use <code>great-expectations</code></li> <li>idempotence - if you accidentally click run on your pipeline three times in a row by mistake, there are not duplicate records \u2013 even if you accidentally click run multiple times in a row. use ID for this or date.</li> <li>Atomicity means that if a single operation in a transaction fails, then all of the operations fail. If you are inserting 1,000 records into the database, as you did in Chapter 3, Reading and Writing Files, if one record fails, then all 1,000 fail.</li> <li>version controlling</li> <li>logging and monitoring</li> </ul> </li> </ul> </li> </ul>"},{"location":"2-Data-Engineering/data-frameworks-tools/#apache-beam","title":"Apache Beam","text":"<p>It is unified programming model to define and execute data processing pipelines, including ETL, batch and stream-processing.</p>"},{"location":"2-Data-Engineering/data-frameworks-tools/#apache-spark","title":"Apache Spark","text":"<p>It is a framework for distributed parallel execution. More on Apache Spark Notes</p>"},{"location":"2-Data-Engineering/data-frameworks-tools/#apache-spark-streaming","title":"Apache Spark Streaming","text":"<p>distributed stream-processing. Extension of core framework.</p>"},{"location":"2-Data-Engineering/data-frameworks-tools/#apache-kafka","title":"Apache Kafka","text":"<p>It is distributed event store and stream-processing platform. More on Apache Kafka Notes</p>"},{"location":"2-Data-Engineering/data-frameworks-tools/#apache-airflow","title":"Apache Airflow","text":"<p>It is workflow management platform for data engineering pipelines. It lets you create your data flows using pure Python. More on Apache Airflow Notes</p>"},{"location":"2-Data-Engineering/data-frameworks-tools/#apache-nifi","title":"Apache NiFi","text":"<ul> <li>a framework for building data pipelines, used DAGs.</li> <li>looks like Informatica on the web.</li> <li>NiFi allows you to build data pipelines using prebuilt <code>processors</code> that you can configure.</li> </ul> <ul> <li>processors are -<ul> <li><code>GenerateFlowFile</code> - generates file</li> <li><code>PutFile</code> - saves file</li> <li><code>ExecuteSQL</code> - executes sql connecting to JDBC.</li> <li>you can configure properties of processor</li> <li>create a connection</li> <li>specify a relationship</li> </ul> </li> </ul> <ul> <li>clustering and the remote execution of pipelines?</li> </ul>"},{"location":"2-Data-Engineering/data-frameworks-tools/#apache-zookeeper","title":"Apache Zookeeper","text":"<ul> <li>manages information about the clusters of kafka.</li> <li>elects leaders</li> <li>zookeeper is also installed in clusters.</li> </ul>"},{"location":"2-Data-Engineering/data-frameworks-tools/#apache-flink","title":"Apache Flink","text":"<p>stream-processing and batch-processing framework</p>"},{"location":"2-Data-Engineering/data-frameworks-tools/#apache-storm","title":"Apache Storm","text":"<p>distributed stream-processing</p>"},{"location":"2-Data-Engineering/data-frameworks-tools/#dbt-data-build-tool","title":"DBT - Data Build Tool","text":"<p>It helps manage 100s of scripts and procedures. It is applicable after staging area, when you are working within a warehouse and have hundreds of transformation script to be managed, tested and deployed.</p>"},{"location":"2-Data-Engineering/data-frameworks-tools/#links","title":"Links","text":"<ul> <li>Medium - Stream Processing Frameworks and differences</li> </ul>"},{"location":"2-Data-Engineering/data-python/","title":"SQL Operations in Python","text":""},{"location":"2-Data-Engineering/data-python/#database-connections-101","title":"Database Connections 101","text":"<ul> <li>Connection should not be left open, it should be open right after write and disposed once written. It can be open and closed multiple times but never left open</li> <li>Connection object should be completely separate from business logic.</li> <li><code>with</code> performs the cleanup activity automatically. \"Basically, if you have an object that you want to make sure it is cleaned once you are done with it or some kind of errors occur, you can define it as a context manager and <code>with</code> statement will call its <code>__enter__()</code> and <code>__exit__()</code> methods on entry to and exit from the with block.\"</li> <li>more connections here  (SQLAlchemy).</li> </ul>"},{"location":"2-Data-Engineering/data-python/#pyodbc","title":"PyODBC","text":"<p>Works with most databases but not well with MSSQL+Pandas. For MSSQL+Pandas use sqlalchemy MSSQL engine.</p> <p>It requires ODBC driver to be installed on system on which the app is running. In the connection string, you need either driver or DSN.</p> <pre><code>import pyodbc\n\n# MS SQL Server\nconnection_url = \"driver={SQL Server};server=000Server.somedomain.com/abcinc;database=SAMPLE_DB;Trusted_Connection=yes\"\n\n# MYSQL \nconnection_url = \"DRIVER={MySQL ODBC 3.51 Driver};SERVER=localhost;DATABASE=test;USER=venu;PASSWORD=venu;OPTION=3;\"\n\n# DSN\nconnection_url = \"dsn=\" + \"Your DSN Name\"\n\n## Teradata\nconnection_url = \"DRIVER={DRIVERNAME};DBCNAME={hostname};;UID={uid};PWD={pwd}\"\n\nconnection = pyodbc.connect(connection_url)\n\nsql = \"select top 10 * from [db].[schema].[table]\"\n\ncursor = connection.cursor().execute(sql)\n\n# list of column names\ncolumns = [column[0] for column in cursor.description]\n\nfor row in cursor.fetchall():\n    print(row) # row is object of class row\n    results.append(dict(zip(columns, row))) # builds list of dictionary\n\nconnection.close()\n\n# Using connection in pandas\nfx_df = pd.read_sql(query, connection)\n</code></pre>"},{"location":"2-Data-Engineering/data-python/#sqlalchemy-connection","title":"SQLAlchemy Connection","text":"<p>Works as connection engine as well as ORM.</p> <p>For DB-API, it needs on of below:</p> <ul> <li>\"ODBC driver and pyodbc\", or</li> <li>python-driver package like psycopg2 for postgres, pymssql for MsSql.</li> </ul> <p>Connection Strings</p> <pre><code>import sqlalchemy\n\n## Microsoft SQL Server, using ODBC Driver and Server Name\nconnection_url = \"mssql+pyodbc://server_name\\schema_name/database_name?driver=SQL+Server\"\n\n## Microsoft SQL with ODBC and Server Name\nconnection_url = \"mssql+pyodbc:///?odbc_connect=\"+urllib.parse.quote('driver={%s};server=%s;database=%s;Trusted_Connection=yes')\n\n## Postgres with Server Name, psycopg2 should be installed\nconnection_url = \"postgresql+psycopg2://user:pass@server:port/database\"\nconnection_url = \"postgresql+psycopg2://scott:tiger@localhost/mydatabase\"\n</code></pre> <p>Most connection strings can be found here: https://docs.sqlalchemy.org/en/20/core/engines.html</p> <p>Create Engine and Connection</p> <pre><code>engine = sqlalchemy.create_engine(connection_url, echo=False)\nconnection = engine.connect()\n</code></pre> <p>Here, <code>echo=True</code> will log statements to default log handler. More engine configs can be found here: https://docs.sqlalchemy.org/en/20/core/engines.html#engine-creation-api</p> <p>Querying and Reading</p> <pre><code>sql = \"select top 10 * from [db].[schema].[table]\"\ncursor = connection.execute(sql)\nres = cursor.fetchall()    # list of rows \nconnection.close()\n\n# OR, using with you do not need to close connection\nwith engine.connect() as connection:\n    connection.execute(\"UPDATE emp set flag=1\")\n\n# With Pandas\ndf.to_sql('table_name', con=engine, schema='dbo', if_exists='append', index=False)\n</code></pre> <p>Transactions in v1.45</p> <pre><code>engine = sqlalchemy.create_engine(con_mssql)\nconnection = engine.connect()\ntrans = connection.begin()\n\ntry:\n    connection.execute(sql_mark_unpivoted)\n    df.to_sql(name='dv_flows_big', con=connection, if_exists='append', index=False)\n    # connection.execute('err') # &lt;-- triggers error\n    trans.commit()\n    logger.info(f\"Transaction of update and load completed successfully.\")\n    logger.info(f\"Data loaded to dv_bigflows, shape: {df.shape}\")\nexcept Exception as e:\n    logger.error(f\"Update and load failed! Rolling back. Error: {e}\")\n    trans.rollback()\n    logger.error(f\"Rolled back!\")\nfinally:\n    trans.close()\n</code></pre>"},{"location":"2-Data-Engineering/data-python/#flask_sqlalchemy","title":"Flask_sqlalchemy","text":"<p>Flask wrapper for sqlalchemy</p> <pre><code>from flask_sqlalchemy import SQLAlchemy\n\nconnection_url = \"mssql+pyodbc://server_name\\schema_name/database_name?driver=SQL+Server\"\n\napp.config['SQLALCHEMY_DATABASE_URI'] = connection_url\n\ndb = SQLAlchemy(app)\ndb.session.execute(sql).all() # list of rows\n</code></pre>"},{"location":"2-Data-Engineering/data-python/#sqlite-sqlalchemy-pandas","title":"SQLite SQLAlchemy Pandas","text":"<pre><code>import sqlalchemy\n\n# SQLite path - If same directory as code\nimport os\nbasedir = os.path.abspath(os.path.dirname(__file__))\nconnection_url = 'sqlite:///' + os.path.join(basedir, 'db.sqlite3')\n\n# SQLite path - Absolute path\nconnection_url = 'sqlite:///' + r'C:\\code\\db.sqlite3'\n\n# SQLite Connection\nengine = sqlalchemy.create_engine(connection_url, echo=False)\nconnection = engine.connect()\n\n# Pandas Read and Write\nimport pandas as pd\ndf = pd.read_csv('my.csv')\ndf.to_sql(name='my_table', con=connection, if_exists='append', index=False)\n</code></pre> <ul> <li>Execution - from flask shell do <code>db.create_all()</code> - creates table with schema.</li> </ul>"},{"location":"2-Data-Engineering/data-python/#pandas","title":"Pandas","text":"<ul> <li>needs a connector to database like sqlalchemy or pyodbc</li> <li><code>df_txns = pd.read_sql(sql=sql, con=conn_dsn)</code></li> <li><code>df.to_sql('table_name', con=engine)</code> - sqlalchemy</li> </ul> <ul> <li>pd write has issues<ul> <li>pyodbc lets read but not write, <code>pyodbc==4.0.35</code></li> <li>sqlalchemy lets read and write but with version <code>SQLAlchemy==1.4.46</code> with <code>pandas==1.3.5</code> as on Feb-2023.</li> </ul> </li> </ul>"},{"location":"2-Data-Engineering/data-python/#sqlite-and-pandas","title":"SQLite and Pandas","text":"<pre><code>import sqlite3\n\n# Data Load\nconn = sqlite3.connect(app_path + '\\db.sqlite')\ndf.to_sql(name='table_name', con=conn, if_exists='append', index=False)\nconn.close()\n</code></pre>"},{"location":"2-Data-Engineering/data-python/#sqlite","title":"SQLite","text":"<pre><code>import sqlite3\nconnection = sqlite3.connect('database_name')\ncursor = connection.cursor()\ncursor.execute(query)\nrows = cursor.fetchall()\nconnection.commit() # for non read tasks\nconnection.close()\n\n\n\"\"\"\n    Transactions in SQLite\n    Works file for sql statements, \n    but not for Pandas df.to_sql() as it commits as another transaction.\n\"\"\"\nconnection = sqlite3.connect(sqlite_path)\nconnection.isolation_level = None\ntry:\n    c = connection.cursor()\n    c.execute(\"begin\")\n    c.execute(\"some update\")\n    c.execute(\"some insert\")\n    #c.execute(\"fnord\") # &lt;-- trigger error, above update and insert will rollback\n    df.to_sql(name='orders', con=connection, if_exists='append', index=False)\n    #c.execute(\"fnord\") # &lt;-- trigger error, now, it raises exception, but pandas did the commit.\n    connection.commit()\n    logger.info(f\"Transaction of update and load completed successfully.\")\nexcept connection.Error as e:\n    logger.error(f\"Update and load failed! Rolling back. Error: {e}\")\n    connection.rollback()\nfinally:\n    connection.close()\n</code></pre>"},{"location":"2-Data-Engineering/data-python/#ms-sql-connector","title":"MS SQL Connector","text":"<pre><code>brew install freetds\npip install pymssql --no-binary=pymssql\n</code></pre> <pre><code># Connections\nconn = pymssql.connect(server='', database='', user='', password='')\n\n# one line pandas\npd.read_sql(sql=q, con=conn)\n\n# Querying\ncursor = conn.cursor(as_dict=True)\ncursor.execute(\"select top 10 from employee\")\nfor row in cursor:\n    print(\"ID=%d, Name=%s\" % (row['id'], row['name']))\nconn.close()\n</code></pre> <p>Here, <code>as_dict=True</code> makes each row as dictionary, otherwise it is Tuple of values (you have no information of column name, but is less is size).</p> <p>The dictionary has proper data-types as in database. So, name as str, age as int, value as float, created as datetime etc.</p> <p>You can then use <code>json.dumps()</code> to convert dict to str.</p> <pre><code>&lt;class 'tuple'&gt;\n(1, 'John', 22, 230.54, datetime.datetime(2023, 8, 2, 16, 38, 43, 47000))\n\n&lt;class 'dict'&gt;\n{'id': 1, 'name': 'John', 'age': 22, 'value': 230.54, 'load_datetime': datetime.datetime(2023, 8, 2, 16, 38, 43, 47000)}\n\n&gt;&gt;&gt; json.dumps(obj=row, indent=4, sort_keys=True, default=str)\n{\n    \"age\": 22,\n    \"id\": 1,\n    \"load_datetime\": \"2023-08-02 16:38:43.047000\",\n    \"name\": \"John\",\n    \"value\": 230.54\n}\n</code></pre> <p>With SQLAlchemy</p> <pre><code>connection_url = \"mssql+pymssql://host/database?charset=utf8\"\nengine = sqlalchemy.create_engine(url=connection_url)\nconn = engine.connect()\n\ncursor = conn.execute(q)\nfor row in cursor:\n    print(\"ID=%d, Name=%s\" % (row['id'], row['banker_name']))\nconn.close()\n\n# or one line pandas\npd.read_sql(sql=q, con=conn)\n</code></pre> <p>Now you can use conn in pandas, or sql-alchemy orm, or create a cursor and execute queries.</p> <p>Link: Examples</p>"},{"location":"2-Data-Engineering/data-python/#mysql-connector-python","title":"MySql-connector-python","text":"<pre><code>import mysql.connector\nconnection = mysql.connector.connect(host=host_name,user=user_name,passwd=user_password)\ncursor = connection.cursor()\ncursor.execute(query)\nconnection.commit() # for non read tasks\n</code></pre>"},{"location":"2-Data-Engineering/data-python/#teradatasql","title":"TeradataSQL","text":"<p>Teradata SQL Driver for Python. This package enables Python applications to connect to the Teradata Database. There is no need for ODBC driver to be installed on the system for this to work.</p> <pre><code>conn = teradatasql.connect(\n    host='dwh.brand.com',\n    user='',\n    password=''\n)\n</code></pre> <p>You may enable logging, pass http-proxy or set other config params. More on https://pypi.org/project/teradatasql/.</p>"},{"location":"2-Data-Engineering/data-python/#sqlalchemy-core","title":"SQLAlchemy Core","text":"<p>This package has following uses:</p> <ul> <li>Use pythonic way to build sql statements. You can define metadata and build INSERT, UPDATE, DELETE statements using methods. This does not use object, but uses python methods to do sql operations.</li> <li>Lets you change the db-engine, without modifying the code. It will automatically build query using dialect for new engine.</li> </ul>"},{"location":"2-Data-Engineering/data-python/#sqlalchemy-orm","title":"SQLAlchemy ORM","text":"<p>This package lets you do DB operations in Object Oriented way. It is high level abastraction.</p> <p>What? - The ORM provided by SQLAlchemy sits between the database and Python program and transforms the data flow between the database engine and Python objects. SQLAlchemy allows you to think in terms of objects and still retain the powerful features of a database engine. It is ORM for Python, has two parts</p> <ul> <li>CORE - can be used to manage SQL from python,</li> <li>ORM - can be used in Object oriented way to access SQL from python.</li> </ul> <p>Why use SQLAlchemy? - When you\u2019re working in an object-oriented language like Python, it\u2019s often useful to think in terms of objects. It\u2019s possible to map the results returned by SQL queries to objects, but doing so works against the grain of how the database works. Sticking with the scalar results provided by SQL works against the grain of how Python developers work. This problem is known as object-relational impedance mismatch.</p> <p>ORMs allow applications to manage a database using high-level entities such as classes, objects and methods instead of tables and SQL. The job of the ORM is to translate the high-level operations into database commands.</p> <ul> <li>It is an ORM not for one, but for many relational databases. SQLAlchemy supports a long list of database engines, including the popular MySQL, PostgreSQL and SQLite.</li> <li>The ORM translates Python classes to tables for relational databases and automatically converts Pythonic SQLAlchemy Expression Language to SQL statements</li> </ul> <p>Mappings - There are two types of mapping</p> <ul> <li>Declarative - new - more like oops</li> <li>Imperative - old - less like oops</li> </ul> <p>Flask SQLAlchemy</p> <p>It lets easy use of SQLAlchemy in Flask. For eg, to connect to db, you can just define the config param <code>SQLALCHEMY_DATABASE_URI</code> in flask-config object, and then when you instantiate the db object, <code>db.init_app(app)</code>, it makes use of this config param to establish the connection.</p> <p>More config details can be found on: https://flask-sqlalchemy.palletsprojects.com/en/latest/config/</p> <p>Link: Flask Notes - SQLAlchemy</p> <p>Links</p> <ul> <li>https://realpython.com/python-sqlite-sqlalchemy/#working-with-sqlalchemy-and-python-objects</li> </ul>"},{"location":"2-Data-Engineering/data-python/#pyodbc-manual-orm","title":"PyODBC Manual ORM","text":"<pre><code># Class of table\nclass Book:\n\n    def __init__(self, row):\n        # row is db record\n        self.id = row.id\n        self.title = getattr(row, 'title', None)\n        self.author = getattr(row, 'author', None)\n        self.year = getattr(row, 'year', 1880) # default value\n\n    # calculated column, instance method\n    def age(self):\n        # return age of book\n        return (2022 - self.year)\n\n    @classmethod\n    def get_book_by_id(cls, id):\n        sql = f'select * from table_name where id = {str(id)}'\n        cursor = conn.cursor().execute(sql)\n\n        objs = []\n        for row in cursor.fetchall():\n            objs.append(Book(row))\n\n        if len(objs) &gt; 0:\n            return objs[0] # as sending 1\n\n        return None\n\n    @classmethod\n    def get_books(cls):\n        sql = select_flows_sql + f' where email = \"?\"'        # optional where clause\n        cursor = conn.cursor().execute(sql, session['email']) # where clause placeholder\n\n        objs = []\n        for row in cursor.fetchall():\n            objs.append(Book(row))      # instantiate obj\n\n        if len(objs) &gt; 0:\n            return objs                 # list of objects\n\n        return None\n\nbook_obj = Book.get_book_by_id(21) # class method\nbook_obj.age() # instance method\n\nbook_objects = Book.get_books() # static method, no auto arg passed\n\n# class of table simple code to auto set to dictionary items\nclass Post:\n\n    def __init__(self, row):\n        for k, v in dictionary.items():\n            setattr(self, k, v)\n</code></pre>"},{"location":"2-Data-Engineering/data-python/#sqlite-etl","title":"SQLite ETL","text":"<ul> <li>when reading CSV you can read data in correct data-type by specifying <code>dtype</code> and <code>thousands</code> and <code>parse_dates</code>.</li> <li>when adding a column use proper data-type to new column has required format. eg, use <code>pd.to_datetime()</code> to add date column.</li> <li>when saving to SQL, pandas creates tabes with data-type similar to pandas columns type.</li> <li>when reading a SQL, pandas might not read in proper date format. Again use <code>parse_dates</code> to fix it.</li> </ul> <pre><code># Data Types\ndtype_ = {\n    'Quantity' : 'int64',\n    'Amount' : 'float64',\n}\ndv_date_parser = lambda x: pd.to_datetime(x, format=\"%d/%m/%Y\", errors='coerce')\n\ndf = pd.read_csv(file, low_memory=False, dtype=dtype_, thousands=',',\n                 parse_dates=['Date'], date_parser=dv_date_parser)\n\n# Add datetime type column\ndf['load_datetime'] = pd.to_datetime(arg='now', utc=True)\n\n# To SQL\ndf.to_sql(name='orders_staging', con=conn_target, if_exists='append', index=False)\n\n# Read SQL\ndf = pd.read_sql(sql=sql_read, con=conn_target, parse_dates=['Sale Date', 'load_datetime'])\n</code></pre>"},{"location":"2-Data-Engineering/data-python/#pyodbc-etl","title":"PyODBC ETL","text":"<pre><code>import pandas as pd\nimport sqlite3\n\n# sources\nconn_source = pyodbc.connect('dsn=' + \"Your_DSN\")\nconn_target = sqlite3.connect('../app_v2/data-dev.sqlite')\n\n# Incremental Load: Find IDs already in target\ndf_target_ids = pd.read_sql(sql='select id from target_table', con=conn_target)\nids_before = df_target_ids[\"id\"].values\nrecords_before = len(df_target_ids)\nprint(f\"Records present: \"+str(records_before))\n\n# Build where clause tuple, eg, 'not in (a,..)'\nif len(ids_before) == 0:\n    ids_before_tuple = '(-1)'\nelif len(ids_before_tuple) == 1:\n    ids_before_tuple = \"(\"+ str(df_target_ids[\"id\"].values[0]) + \")\"\nelse:\n    ids_before_tuple = tuple(df_target_ids[\"id\"].values)\n\n\n# Extract new IDs from source\nsql = f'SELECT distinct id, email FROM source_table where id not in {ids_before_tuple}'\ndf_new_data = pd.read_sql(sql=sql, con=conn_source)\nprint(f\"Records to be added: \"+str(len(df_new_data)))\n\n# Load new Data to target\ndf_new_data.to_sql(name='target_table', con=conn_target, if_exists='append', index=False)\n\nfinal_responses = pd.read_sql(sql='select count(id) from target_table', con=conn_target).iloc[0,0]\nprint(f\"Records added: \"+str(final_responses-records_before))\n\n#assert len(df_new_data) == final_responses-records_before\n</code></pre>"},{"location":"2-Data-Engineering/data-python/#fail-proof-data-read","title":"Fail proof data read","text":"<pre><code>import pyodbc\nimport logging\nimport urllib\nimport pandas as pd\nimport time\nfrom config import config # has all variables defined\n\ndef get_connection_string(service):\n    \"\"\"\n    Returns connection string for a service. All variables are picked from environment config file\n    :param service: [teradata, mssql]\n    :return: connection string\n    \"\"\"\n\n    if service == 'teradata':\n        return f\"DRIVER=Teradata;DBCNAME={config['database_host']};;UID={config['database_user']};PWD={urllib.parse.quote(config['database_password'])}\"\n    elif service == 'mssql':\n        return f\"driver={config['driver']};server={config['server']};database={config['database']};Trusted_Connection=yes\"\n    else:\n        raise Exception('DB: No such connection available')\n\n\ndef sql_select_df(query, service):\n    \"\"\"\n    Runs query and returns results as a Pandas DataFrame\n    :param query: SQL Query\n    :param service: [teradata, mssql]\n    :return: DataFrame\n    \"\"\"\n\n    df = None\n\n    try:\n        connection = pyodbc.connect(get_connection_string(service))\n        try:\n            start_time = time.time()\n            df = pd.read_sql(query, connection)\n            time_taken = time.time() - start_time\n            logger.info(f'sql_select_df - Records fetched: {len(df):,} ;  Time taken: {time_taken:,.5f} seconds.')\n        except Exception as e:\n            logger.error(f'sql_select_df - Query failed!. Error \"{str(e)}\".')\n        finally:\n            connection.close()\n    except Exception as e:\n        logger.error(f'sql_select_df - No connection to \"{service}\". Message: \"{str(e)}\"')\n\n    return df\n\n\ndef sql_run_file(file, service):\n    \"\"\"\n    Runs SQL Script stored in a file and returns the number of rows processed\n    :param file: file path\n    :param service: [teradata, mssql]\n    :return: number of rows processed\n    \"\"\"\n\n    n_rows = 0\n\n    try:\n        with open(file, 'r') as f:\n            query = f.read()\n\n        n_rows = sql_execute(query, service)\n    except Exception as e:\n        logger.error(f'sql_run_file - Cannot read file at \"{file}\". Error: \"{str(e)}\"')\n\n    return n_rows\n\n\ndef sql_execute(query, service, log_info=False, fail=True):\n    \"\"\"\n    Runs SQL Script and returns the number of rows processed\n    :param service: teradata or mssql\n    :param query: sql query\n    :param log_info: log successful execution?\n    :param fail: exit execution?\n    :return: number of rows processed\n    \"\"\"\n\n    n_rows = 0\n\n    try:\n        connection = pyodbc.connect(get_connection_string(service))\n        cursor = connection.cursor()\n\n        try:\n            start_time = time.time()\n            cursor.execute(query)\n            n_rows = cursor.rowcount\n            time_taken = time.time() - start_time\n            if log_info:\n                logger.info(f'sql_execute - Query executed in {time_taken:,.5f} seconds. Records processed: {n_rows:,}')\n            cursor.commit()\n            cursor.close()\n        except Exception as e:\n            cursor.rollback()\n            logger.error(f'sql_execute - Query failed!. Error \"{str(e)}\".')\n            if fail:\n                sys.exit(1)\n        finally:\n            connection.close()\n    except pyodbc.OperationalError as e:\n        logger.error(f'sql_execute - No connection to \"{service}\". Message: \"{str(e)}\"')\n        print(f'sql_execute - Please check if server is running. No connection to \"{service}\".')\n        if fail:\n            sys.exit(1)\n    except Exception as e:\n        logger.error(f'sql_execute - No connection to \"{service}\". Message: \"{str(e)}\"')\n\n    return n_rows\n</code></pre>"},{"location":"2-Data-Engineering/data-science/","title":"Data Science","text":"<ul> <li>BERT<ul> <li>ML Framework for NLP. It helps computer to understand a language and context of text by using surrounding text.</li> <li>Bidirectional Encoder Representations from Transformers</li> </ul> </li> </ul> <ul> <li>Generative design is a technology in which 3D models are created and optimized by cloud computing and AI. A user sets up requirements for the model, such as manufacturing processes, loads, and constraints, and then the software offers designs that meet those requirements.</li> </ul>"},{"location":"2-Data-Engineering/data-science/#table-of-index-kaggle","title":"Table of Index Kaggle","text":"<p>Maths:</p> <ul> <li>Probability</li> <li>Statistics</li> <li>Statistics Advance - Distributions</li> </ul> <p>Data ETL EDA Wrangling</p> <ul> <li>Python Notes - Data Structures</li> <li>Pandas Notes - Series, DataFrame, Heirarchy</li> <li>Data Handling - Pre-Processing, EDA, Transformation</li> <li>Dimentionality Reduction - PCA, LDA, Kernel PCA</li> </ul> <p>Regression - supervised:</p> <ul> <li>Linear Regression</li> <li>Simple Linear Regression</li> <li>Multiple Linear Regression</li> <li>Polynomial Linear Regression</li> <li>Support Vector Regression</li> <li>Decision Tree Regression</li> <li>Random Forest Regression</li> </ul> <p>Classification - supervised:</p> <ul> <li>Logistic Regression - confusion matrix, accuracy, sigmoid, CAP Curve</li> <li>KNN - K Nearest Neighbour Classifier - Euclidean distance</li> <li>SVM - Support Vector Machines - Maximum Margin Hyperplane</li> <li>Kernel SVM - Map to Higher Dimention, Kernel Trick, Gaussian RBF Kernel</li> <li>Naive Bayes - Bayes Theorem</li> <li>Decision Tree Classifier</li> <li>Random Forest Classifier - entropy, ensemble learning</li> </ul> <p>Clustering - unsupervised:</p> <ul> <li>K-Means Clustering - wcss, choosing k-value, k-means++</li> <li>Hierarchical Clustering - Agglomerative, Dendrogram</li> </ul> <p>Association Rule - Unsupervised:</p> <ul> <li>Apriori Algorithm - market basket analysis, lift, support, confidence, todo</li> <li>Eclat (Part 5 todo)</li> <li>FP Growth</li> </ul> <p>Other Bonus Extra:</p> <ul> <li>Model Select</li> <li>Reinforcement Learning (Part 6 todo)<ul> <li>Upper Confidence Bound</li> <li>Thompson Sampling</li> </ul> </li> </ul> <p>NLP:</p> <ul> <li>Cleaning, stemming, nltk, bag of words, tokenization</li> </ul> <p>Deep Learning: (Part 8 todo)</p> <p>Dimentionality Reduction:</p> <ul> <li>PCA - max variance, unsupervised</li> <li>LDA - max class separation, supervised</li> <li>Kernel PCA - kernel trick on non-linearly separable dataset</li> <li>SVD</li> <li>GDA - Generalized Discriminant Analysis</li> </ul>"},{"location":"2-Data-Engineering/data-solutions/","title":"Data Solutions","text":"<p>Here are all the \"conceptual\" notes related to data soulutions, archirecture and engineering. It can have links to practical notes.</p> <p>Data engineering is the development, operation, and maintenance of data infrastructure, either on-premises or in the cloud (or hybrid or multi-cloud), comprising databases and pipelines to extract, transform, and load data.</p> <p>Data engineers need to be knowledgeable in many areas \u2013 programming, operations, data modeling, databases, and operating systems. The breadth of the field is part of what makes it fun, exciting, and challenging.</p>"},{"location":"2-Data-Engineering/data-solutions/#data-strategy","title":"Data Strategy","text":"<p>To meet medium or long term business objectives, many aspect of organisation need to work in harmony - all in same direction. Data Strategy underpins business strategy and sets agenda for IT delivery roadmap. Basically, it defines where and how data supports orgs critical business process. It includes data challenges and unlocks opportunities by using right strategy and solution in place, thus achieve the business objectives. Eg, if all tables have key to trace back?; all row have identifier; data is being captured; data can be tied up at all hierarchies and dimensions.</p> <p>Steps to build a Data Strategy</p> <ul> <li>understand the business objectives. Eg, how often are email responded. why the money is going?</li> <li>assess how data is stored and consumed in org.</li> <li>understand current data challenges. Eg, not being captured. isolated availability with no link up or down. non traceble data. stale data. not connected to pipeline or lake.<ul> <li>how can you collect data, apply data</li> </ul> </li> <li>work with business to define optimum target state to meet business objectives, incorporating<ul> <li>data architecture and engineering</li> <li>data management and operating model</li> <li>data analytics, reporting and visualization - or business intelligence</li> </ul> </li> <li>build a road map for data journey, define actionable data strategy.</li> </ul> <pre><code>flowchart LR\nA[(Current\\n Data State)] --&gt; C{Find Data\\n Challenges}\nB(Business Objectives) --&gt; C\nC --&gt; D{Data Strategy}\nD --&gt;|Roadmap| E[(Target\\nData State)] --&gt; F(Data-driven\\ndecision making)</code></pre>"},{"location":"2-Data-Engineering/data-solutions/#data-lifecycle","title":"Data Lifecycle","text":"<ul> <li>Generation</li> <li>Collection</li> <li>Storage</li> <li>Processing - integration, cleaning, reduction, transformation</li> <li>Management</li> <li>Analysis - Clustering, Regression, Forecasting, Prediction</li> <li>Visualization - Interpretation</li> <li>Decision Making</li> <li>Destruction</li> </ul> <pre><code>graph LR;\n\na[data collection \\n or generation] --&gt; b[data storage] --&gt; c[data processing] --&gt; d1[data analysis \\n or visualization] --&gt; e[decision making]\n\nsubgraph governance\nb\nc\nend</code></pre>"},{"location":"2-Data-Engineering/data-solutions/#data-architecture","title":"Data Architecture","text":"<p>Now that you have a strategy with known challenges and a roadmap to target state, it is time to build the architecture and do the engineering work aligned to roadmap to rach the target state.</p> <p>Data Architecture defines the blueprint for managing data from collection to storage and transformation to consumption. It is base foundation to support business objectives. It is essential to determine the sharp and quick tools that solve the purpose.</p> <pre><code>flowchart LR\na[Storage / Warehousing] --&gt; b[Movement / ETL] --&gt; c[Analytics / Reporting]</code></pre> <p>What to Architect</p> <ul> <li>determine cloud architecture or on premise.</li> <li>if required how can data be scalable, avilable and fault tolerant</li> <li>big data architecture</li> </ul> <p>Parts of Data Architecture - Aim is to achieve below safely</p> <ul> <li>E - extract / connect - have automated connectors and access permissions, to sharepoint, salesforce, sharedrive, etc. Int and ext.</li> <li>L - load / store - all in one place, MSSQL, DVS, Hive, so you can build mart and combine.</li> <li>T - transform / transport - integrate, transform, clean, aggregate, filter. Determine the best tool to do the job. Python, SQL, Prep, Alteryx?</li> <li>P - present - right viz tool, dash, tableau. Keeping the end user in mind.</li> <li>Analogy - Load:HTML :: Transform:JavaScript :: Present:CSS</li> </ul>"},{"location":"2-Data-Engineering/data-solutions/#data-transformation","title":"Data Transformation","text":""},{"location":"2-Data-Engineering/data-solutions/#etl-data-pipeline","title":"ETL &amp; Data Pipeline","text":"<p>ETL - Data Pipeline - Help move data from source to target with transformations in between. Challenge and skill is to build an efficient, reliable and automate pipeline that can help connect sources to lake/warehouse/mart. Big Data pipelines. Batch and Event Driven or real-time.</p> <ul> <li>Big part of design of warehouse.</li> <li>usually a weekly or nightly batch job that updates data warehouse.</li> </ul>"},{"location":"2-Data-Engineering/data-solutions/#data-virtualization","title":"Data Virtualization","text":"<ul> <li>It is used to connect and query different data sources, transform it. It does not store or move the data. Query goes down to source systems.</li> <li>Eg, Tibco Data Virtualization.</li> <li>Link - Difference in ETL &amp; Virtualization</li> </ul>"},{"location":"2-Data-Engineering/data-solutions/#data-analytics-reporting-visualization","title":"Data Analytics, Reporting &amp; Visualization","text":"<p>Flat data, denormalized is best to query for visualization.</p> <p>Steps to follow</p> <ul> <li>understand<ul> <li>requirement gathering,</li> <li>data dicovery</li> </ul> </li> <li>design and develop<ul> <li>tool selection - correct tool for need, right tool for viz/reporting, tableau, Plotly, D3, OBIEE, self-serve;</li> <li>data modelling - reporting view prep, what needs to be shown should be a row of data, add hierarchy to roll up and down.</li> <li>story telling - art of making data easy to understand, animations, live?</li> <li>test - numbers help make decision</li> <li>distribution - mobile, pdf, interactive, embed (portal),</li> <li>actionable insight - (optional) let user do actions right from report. (write-back)</li> <li>usage analytics - (optional) but really useful in determining ROI</li> </ul> </li> </ul>"},{"location":"2-Data-Engineering/data-solutions/#links","title":"Links","text":"<ul> <li>Oracle - Data Warehousing Concepts</li> <li>Data Company - Dufrain</li> <li>Ralph Kimball - Data Warehousing and BI Author</li> </ul>"},{"location":"2-Data-Engineering/data-solutions/#todo","title":"ToDo","text":"<ul> <li> - link this with project management notes to have a road map to follow when starting a new data solutions project.</li> <li> - align https://careers.dufrain.co.uk/jobs/2225356-senior-data-engineer</li> <li> - Experience working with one or more of - Spark, Hadoop, Kafka, Snowflake, airflows</li> <li> - Experience building Data Modelling, ETL / ELT pipelines, Data Lakes, Data Warehousing, Master Data</li> <li> - A solid understanding of key processes in the engineering delivery cycle including Agile and DevOps, Git, APIs, and Data Pipelines.</li> </ul>"},{"location":"2-Data-Engineering/data-testing/","title":"Data Testing","text":"<p>With incresed data focused apps, it is improtant to have data fully tested</p> <ul> <li>great expectations<ul> <li>is python library to test data</li> </ul> </li> </ul>"},{"location":"2-Data-Engineering/data-warehousing/","title":"Data Architecture","text":"<p>how to architect, where to architect, stages of storage, storage solutions</p> <ul> <li> add GPT notes</li> </ul> <p>Aim to architect database is to collect and store data in a way that it is optimised for reading to enable analytics and BI.</p>"},{"location":"2-Data-Engineering/data-warehousing/#overview-concepts","title":"Overview / Concepts","text":"<ul> <li>Database Modelling<ul> <li>Logical Model is modelling on paper/ppt</li> <li>Physical Model is modelling on database with the data.</li> </ul> </li> </ul> <ul> <li>Data Storage Stages can follow this journey<ul> <li>Staging area where it is dump from feeds, can be OLTP dumps</li> <li>Data Warehouse - where it is stored in star schema, is clean, easy to understand and update and ready to use.</li> <li>Data Mart - then denormalised simple query for consumers, focusing on small business areas / purpose.</li> </ul> </li> </ul> <ul> <li>Database Schema is collection of database objects (tables, views and indexes).</li> </ul> <ul> <li>3NF Schema minimizes redundancy by splitting data in multiple tables and linking them with relationships. Adding new entity is easy without effecting current application. But, this makes reading data slow as the query joins multiple tables.</li> </ul> <ul> <li>Data Warehousing, data mart build, database modelling, dimensional modelling, data modelling,  - they all have a common goal to improve data retrieval (select query optimized).</li> </ul>"},{"location":"2-Data-Engineering/data-warehousing/#data-storage-solutions","title":"Data Storage Solutions","text":"<ul> <li>Data Lake - is dumped data with no purpose</li> <li>Staging Area is a dump from feeds. It simplifies cleaning and consolidation.</li> <li>Data Warehouse - data from different sources into central store to provide single source of truth on which BI and Analysts can rely.<ul> <li>OLAP vs OLTP - Compared to OLTP (transaction processing), warehousing is read oriented, for analytics workload OLAP.<ul> <li>read oriented, vs insert/update/delete</li> <li>denormalized for reads, fully normalized for consistency</li> <li>ETL batch updates, always up to date.</li> </ul> </li> <li>Big data warehousing handling petabytes in an distributed environment. Handle 3Vs, real time, no sql, petabytes? It is ETL but at industry level,</li> </ul> </li> <li>Data Mart - usually build for single purpose, for particular LOBs, can be physically designed or implemented logically by creating views, materialized view or summary data in warehouse (they have an overlap). It mainly focuses on a subset of data instead of complete enterprise data. They can exist as<ul> <li>Island is right from source, can be inconsistent.</li> <li>Dependent is fed from warehouse, mostly consistent.</li> </ul> </li> <li>Operation Data Store - ODS gives data warehouses a place to get access to the most current data, which has not yet been loaded into the data warehouse. Usually current day data.</li> <li>Usually - Data Lake &gt; Data Warehouse &gt; Data Mart</li> <li>Data Warehousing, data mart build, database modelling, dimensional modelling, data modelling,  - they all have a common goal to improve data retrieval (select query optimized).</li> </ul>"},{"location":"2-Data-Engineering/data-warehousing/#data-warehousing-concepts","title":"Data Warehousing Concepts","text":"<ul> <li>What is Data Warehouse<ul> <li>simply it is a database.</li> <li>designed in a way to facilitate easy reads and accommodates change in model like adding a new dimension.</li> <li>lets slice and dice data from different dimensions and by time.</li> <li>lets view highly-aggregated data and same time lets drill-down to lowest granularity.</li> <li>the data is non-volatile (does not change) and lets analyze what occurred over time.</li> <li>it includes ETL process, multi-dimensional modelling, backups, availability</li> <li>Big data warehousing handling petabytes in an distributed environment. Handle 3Vs, real time, no sql, petabytes? It is ETL but at industry level.</li> </ul> </li> </ul> <ul> <li>Why is Data Warehouse required<ul> <li>to combine data from different sources into single source of truth on which BI and Analysts can rely.</li> <li>to enhance organization's performance by analysing data.</li> <li>to maintain historical records to look over years.  </li> </ul> </li> </ul> <ul> <li>How Data Warehouse works<ul> <li>Read Optimized - they are designed to query and analyze rather than transaction processing.</li> </ul> </li> </ul> <ul> <li>Characteristics of a Data Warehouse<ul> <li>simplicity of access and high-speed query performance.</li> </ul> </li> </ul> <ul> <li>OLAP vs OLTP - OLTP (Online Transaction Processing), OLAP (Online Analytical Processing)<ul> <li>OLAP is optimized for quick reads and analysis, OLTP is optimized for insert/update/delete</li> <li>OLAP is denormalized for reads, OLTP is fully normalized for consistency</li> <li>OLAP is populated with ETL batch updates, OLTP is always up to date with transactional writes.</li> </ul> </li> </ul> <ul> <li> <p>Data Mart - similar to warehouse but is usually build for single purpose, for particular LOBs.</p> <ul> <li>It can be physically designed or implemented logically by creating views, materialized view or summary data in warehouse (they have an overlap).</li> <li>It mainly focuses on a subset of data instead of complete enterprise data.</li> </ul> <ul> <li>They can exist as<ul> <li>Island Data Marts - it is right from source (OLTP), can be inconsistent. Quick workaround if there is no data warehouse.</li> <li>Dependent Data Marts - it is fed from warehouse, mostly consistent. Lengthy as it needs data warehouse to be built.</li> </ul> </li> </ul> </li> </ul> <ul> <li>Operation Data Store - ODS gives data warehouses a place to get access to the most current data, which has not yet been loaded into the data warehouse. Usually current day data. It not historic.</li> </ul> <ul> <li>Data Warehouse Architectures<ul> <li>Basic - Source-data to warehouse to users, no data-marts, no staging-area.</li> <li>Staging and warehouse - from source data is landed to staging area then to warehouse.</li> <li>Staging, warehouse and data marts - data lands from source to staging area, then to warehouse, then individual LOBs can have data-marts for more refined use cases. Also called EDW (Enterprise Data Warehousing)</li> </ul> </li> </ul> <p>Figure: Architecture of a Data Model (with optional \"Staging Area\" and \"Data Marts\")</p> <pre><code>flowchart LR\nds1[(Ops Sys 1)] --&gt; sa[(Staging\\nArea)]\nds2[(Ops Sys 2)] --&gt; sa\nds3[Flat Files] --&gt; sa\nsa --&gt; wh[(Warehouse\\n\\nMeta Data\\nSummary Data\\nRaw Data)]\nwh --&gt; dm1[(Purchasing\\nData Mart)]\nwh --&gt; dm2[(Sales\\nData Mart)]\nwh --&gt; dm3[(Inventory\\nData Mart)]\nu1(Analysis Users)\nu2(Reporting Users)\nu3(Mining Users)\ndm1 --&gt; u1\ndm2 --&gt; u1\ndm1 --&gt; u2\ndm2 --&gt; u2\ndm2 --&gt; u3\ndm3 --&gt; u3</code></pre> <p>As of 2022, this is traditional data warehousing. It works fine for most of the needs specially internal work. However, with the big data shift, some things have changed and this may not be an ideal solution.</p> <p>Changes in modern big data landscape:</p> <ul> <li>Sources - variety of data sources has increased, eg, APIs, File</li> <li>Triggers - Batch / Event / Realtime - along with batch, now we have event based data pipeline triggers and realtime data pipeline.</li> <li>Process - ETL to ELT - Now that we have more compute and more data, it makes sense to first load the data as it is huge, and then do transformations as we have more compute.</li> <li>Structure - Now we have JSON, Blog, Images, GeoSpacial, IoT data and more, this is a shift from structured datasets.</li> </ul> <p>To cater these changes you may have to include:</p> <ul> <li>Real time data pipeline using Kafka and Kinesis</li> <li>Event based pipeline using AWS Lambda</li> <li>ETL to Generic ETL where same ETL job can be used to connect to multiple data sources with some param modifications.</li> <li>Orchestration tools to better manage 100s of ETL scripts.</li> </ul>"},{"location":"2-Data-Engineering/data-warehousing/#logical-design-in-data-warehousing","title":"Logical Design in Data Warehousing","text":"<ul> <li>What is Logical Modelling<ul> <li>Logical Model is conceptual (pen &amp; paper), focus on business needs and build subject-oriented <code>schema</code>. It more to understand use case, end user and the information you need.</li> </ul> </li> </ul> <ul> <li>How to build Logical Model<ul> <li>Identify the things of importance, entity (data item, like user, book) and its properties attributes (columns; like name, dob).</li> <li>Determine data granularity, week, day, month.</li> <li>Determine how entities are related to each other, relationships. Also called entity relationship modelling.</li> <li>Determine the unique identifier for each entity record, which is <code>primary_key</code> in physical model. It applies to OLAP, OLTP, 3NF EDW, star and snowflake.</li> <li>Next, divide data into facts and dimensions. Write down all dimension and facts required. Several distinct dimensions, combined with facts, enable you to answer business questions.</li> <li>Identify the source data that will feed the data mart model, that is, populate the facts and dimensions.</li> <li>Design your schema for data mart, star-schema, snow-flake schema or other.</li> <li>Lastly you need a routine/pipeline to move data from sources to mart as facts and dimensions. Determine the frequency at which the data is refreshed.</li> </ul> </li> </ul> <ul> <li> <p>Facts</p> <ul> <li>It is numeric, transactional data, fast changing. Mostly tall table with numeric data, datetime and contains foreign keys  of dimension table which combined make composite key as its primary key.</li> <li>Fact table with aggregated facts is called summary table.</li> <li>A fact table has a composite key made up of the primary keys of the dimension tables of the schema.</li> </ul> <ul> <li>Adding rows to fact table, there are three ways<ul> <li>Transaction-based: row shows a lowest grain transaction for a combination of dimension.</li> <li>Periodic Snapshot: each row is related to a period, like daily or weekly.</li> <li>Accumulating Snapshot: each row shows occurrence of process, that is, multiple rows for one process but each row tracks a movement.</li> </ul> </li> </ul> </li> </ul> <ul> <li>Dimensions<ul> <li>It is descriptive, slow changing, known as lookup tables or reference tables. Mostly wide. It may contain hierarchies. Eg, product, customer, time.</li> <li>Data is kept at lowest level of detail, it can be rolled up higher level of hierarchy.</li> </ul> </li> </ul> <ul> <li>Star Schema<ul> <li>It is simple, having fact in centre and dimensions around it, just like a star, where only one join establishes the relationship between the fact table and any one of the dimension tables.</li> <li>Star-schema have fraction of table compared to 3NF. 15-20 star-schema can cover all LOBs of enterprise. BI users can easily query and join multiple star-schemas as they have few tables.</li> <li>Star schemas can have denormalized dimensions for easy understanding and faster data retrieval and less complex queries.</li> <li>Most important is to consider the level of detail, grain of data.</li> <li>Both 3NF and Star-schema don't contradict but can work in layers with 3NF as foundation (OLTP) and star-schema as access and optimized layer.</li> <li>Data Warehouse can have multiple star-schema, each based on a business-process such as sales / tracking / shipments. Each star-schema represents a data-mart, this can serve the BI needs.</li> </ul> </li> </ul>"},{"location":"2-Data-Engineering/data-warehousing/#physical-design-in-data-warehousing","title":"Physical Design in Data Warehousing","text":"<p>This defines process to turn architecture to system deliverable. From - Oracle\u00ae Database - Data Warehousing Guide 21c It implements logical model, with variations based on system parameters like memory, disk, network and software type.</p>"},{"location":"2-Data-Engineering/data-warehousing/#multi-dimensional-modelling","title":"Multi Dimensional Modelling","text":"<p>BI developers create cubes to support fast response times, and to provide a single data source for business reporting.</p>"},{"location":"2-Data-Engineering/data-warehousing/#other-tasks-in-data-warehousing","title":"Other Tasks in Data Warehousing","text":"<ul> <li>Configuring database to be used as a warehouse.</li> <li>Performing upgrades of new release.</li> <li>Managing users, security and objects.</li> <li>Backing up and performing recoveries.</li> <li>Monitoring performance and taking preventive actions.</li> </ul>"},{"location":"2-Data-Engineering/data-warehousing/#data-strategy-modelling-warehousing-mart-analysis-example","title":"Data Strategy Modelling Warehousing Mart Analysis Example","text":"<p>Define operational database, this can be used for data collection.</p> <p>Then load all this data into staging area in a DWH, from all operational/source data.</p> <p>Then define transformation to clean, modify and quality checks.</p> <p>Finally load it into dim, facts table, which would be denormalized. This supports analytics queries.</p> <p>data marts are data tailored to specific business areas or user groups.</p>"},{"location":"2-Data-Engineering/data-warehousing/#position-of-data-marts-in-the-data-warehouse-architecture","title":"Position of Data Marts in the Data Warehouse Architecture","text":"<ol> <li> <p>Data Sources:</p> <ul> <li>Operational databases (e.g., CRM, ERP, marketing tools) provide raw data.</li> <li>Data from these sources is extracted and loaded into the staging area.</li> </ul> </li> <li> <p>Staging Area:</p> <ul> <li>Temporary storage where raw data is cleaned and transformed.</li> <li>Staging tables mirror the source tables.</li> </ul> </li> <li> <p>Data Warehouse:</p> <ul> <li>Centralized repository storing integrated, cleansed, and transformed data.</li> <li>Contains dimension tables and fact tables designed for analytical querying.</li> </ul> </li> <li> <p>Data Marts:</p> <ul> <li>Subsets of the data warehouse optimized for specific business needs.</li> <li>Provide focused, departmental access to data for faster and easier querying.</li> </ul> </li> </ol> <p>Next questions:</p> <ul> <li>ask gpt to build loader app in flask.</li> <li>define ODS data structure.</li> </ul>"},{"location":"2-Data-Engineering/databases/","title":"Databases &amp; SQL","text":"<p>all about databases, SQL only</p> <ul> <li> add query examples for each.</li> </ul> <p>Database lets store data usually in tabular format.</p>"},{"location":"2-Data-Engineering/databases/#-ddl-","title":"----- DDL -----","text":"<p>Data Definition Commands, they let define the data table structure and things around them.</p>"},{"location":"2-Data-Engineering/databases/#cascade","title":"CASCADE","text":"<p>can be used when defining tables, it helps do action on the child table based on a action on parent table.</p>"},{"location":"2-Data-Engineering/databases/#triggers","title":"TRIGGERS","text":"<p>Triggers let do action on another table based on activity on one table.</p>"},{"location":"2-Data-Engineering/databases/#-dml-","title":"----- DML -----","text":"<p>Data Manipulation Language, this lets manipulate the data.</p>"},{"location":"2-Data-Engineering/databases/#date-operations","title":"Date Operations","text":"<pre><code>-- get month from timestamp\nextract(month from activity_date)\n</code></pre> <p>Link https://www.postgresql.org/docs/8.1/functions-datetime.html</p>"},{"location":"2-Data-Engineering/databases/#distinct","title":"Distinct","text":"<p>Can bes used after select to fetch only distinct row in result set. It works on multiple column as a set of row, whole row would be unique.</p>"},{"location":"2-Data-Engineering/databases/#nvl-coalesce","title":"NVL COALESCE","text":"<p>To change null values cell</p> <pre><code>NVL (col, \"val-when-null\")\nNVL2 (col, \"val-not-null\", \"val-when-null\")\n</code></pre> <p>COALESCE</p> <p>returns first not-null value, or null if all are null, <code>COALESCE()</code></p> <pre><code>SELECT column(s), CAOLESCE(expression_1,\u2026.,expression_n)FROM table_name;\n</code></pre>"},{"location":"2-Data-Engineering/databases/#case-when-then","title":"CASE WHEN THEN","text":"<p>END ELSE is an optional component but WHEN THEN these cases must be included in the CASE statement.</p> <pre><code>SELECT ...,\nCASE\n  WHEN col_a = \"val_1\" THEN 'res_1'\n  WHEN col_a = \"val_2\" THEN 'res_2'\n  WHEN col_b = \"val_4\" THEN 'res_4'\n  ELSE 'val_0'\nEND as col_my\nFROM ...\n\n\n-- syntax 2\nCASE col_a\n  WHEN val_1 THEN res_1\n  WHEN val_1 THEN res_1\n  ELSE res_0\nEND CASE\n\n\n-- Use in Order by\nSELECT CustomerName, Country\nFROM Customer\nORDER BY\n(CASE\n    WHEN Country  IS 'India' THEN Country\n    ELSE Age\nEND);\n</code></pre> <p>DECODE</p> <p>same as case</p> <p>Use AVG to find Percentage</p> <p>Percentage is <code>flag/total</code>, say flag is active uses as 1 and total is total users.</p> <p>AVG age is (sum of age) / (number of students), that is, if age is 1 and 0, like flag, then avg(flag) is percentage. Because avg will be sum(flag)/total row. So use:</p> <pre><code>select\navg(case when is_active_flg = 1 then 1.0 else 0 end) as pct_low_fat_and_recyclable\nfrom products p\n</code></pre>"},{"location":"2-Data-Engineering/databases/#group-rollup-cube","title":"GROUP ROLLUP CUBE","text":"<p>Use <code>GROUP BY</code> to group data. the columns in select should be same as in group by clause, or you can have aggregated columns.</p> <p>use <code>ROLLUP</code>, to add sub-totals and totals in grouped columns.</p> <pre><code>GROUP BY ROLLUP( col_a, col_b ... )\n</code></pre> <p>Use <code>CUBE</code> similar to rollup to get subtotals by combination of dimensions.</p> <p>Eg, lets say database having, country, state, department and Sales. You can make a cube to get sub totals by combination of dimensions, that is, country-department totals, country-state totals, state-department totals, and total by all 3dims.</p>"},{"location":"2-Data-Engineering/databases/#having","title":"HAVING","text":"<p>HAVING is used to specify a condition for a group or an aggregate function.</p> <p>WHERE filters before grouping, then HAVING filters after grouping.</p>"},{"location":"2-Data-Engineering/databases/#exists","title":"EXISTS","text":"<p>The EXISTS operator tests for the existence of rows in the results set of the subquery. If a subquery row value is found the condition is flagged TRUE and the search does not continue in the inner query, and if it is not found then the condition is flagged FALSE and the search continues in the inner query.</p> <pre><code>SELECT employee_id, last_name, job_id, department_id\nFROM employees outer\nWHERE EXISTS ( SELECT \u2019X\u2019\nFROM employees\nWHERE manager_id =\nouter.employee_id);\n</code></pre>"},{"location":"2-Data-Engineering/databases/#sub-queries","title":"Sub Queries","text":"<ul> <li>A subquery is a query within a query.</li> <li>We can use it in multiple ways:<ul> <li>in <code>from</code> as another table</li> <li>in <code>where</code> as a set to filter in main query</li> <li>as a <code>column</code></li> </ul> </li> <li>example</li> </ul> <pre><code>SELECT name, cost\nFROM items\nWHERE cost &gt; (        --sub query\n    SELECT AVG(cost)\n    FROM items\n);\n</code></pre> <p>Correlated Sub-Queries</p> <p>The Correlated Subqueries are dependent on the outer query and are executed for each row of the main query.</p> <p>Whereas, regular subqueries (non-correlated) are independent of the outer query and are evaluated only once before the main query runs.</p> <p>Correlated subqueries are useful when you need to filter the result of the outer query based on some condition that requires referencing values from the outer query itself. They are commonly used in scenarios where the condition relies on data from related tables.</p> <pre><code>SELECT last_name, salary, department_id\n FROM employees outer\n WHERE salary &gt;\n                (SELECT AVG(salary)\n                 FROM employees\n                 WHERE department_id =\n                        outer.department_id group by department_id);\n</code></pre>"},{"location":"2-Data-Engineering/databases/#joins","title":"JOINS","text":"<p>A <code>CROSS JOIN</code> produces a cartesian product between the two tables, returning all possible combinations of all rows. It has no ON clause because you're just joining everything to everything.</p> <p>A <code>FULL JOIN</code> / <code>OUTER JOIN</code> / <code>JOIN</code> are same and have <code>ON</code> clause that has matching condition in between tables.</p> <p>Problem: Rank with Self Join</p> <p>You can self join a table:</p> <ul> <li>left side is your table</li> <li>right side is again your table but only rows that have sales more than the current row, so<ul> <li>sales 5 in left, will have sales 6, 7, 8 joined to it</li> <li>sales 10 will have only 11 joined to it.</li> </ul> </li> <li>then you can count the rows that got joined from right table. It will give you the rank.</li> </ul> <pre><code>SELECT a1.Name, a1.Sales, COUNT(a2.sales) Sales_Rank\nFROM Total_Sales a1, Total_Sales a2\nWHERE a1.Sales &lt;= a2.Sales or (a1.Sales=a2.Sales and a1.Name = a2.Name)\nGROUP BY a1.Name, a1.Sales\nORDER BY a1.Sales DESC, a1.Name DESC;\n</code></pre> <p>Link: https://www.experts-exchange.com/questions/24783940/RANK-Using-a-SELF-JOIN-or-other.html</p>"},{"location":"2-Data-Engineering/databases/#union","title":"UNION","text":"<p>In Union the order is not guaranteed to be preserved. As a result, you can add <code>row_number()</code> or <code>rank()</code> and table_number hardcoded as table1 and table2 to reselect from union.</p> <p><code>UNION</code> only returns a unique record, while <code>UNION ALL</code> returns all the records (including duplicates)</p>"},{"location":"2-Data-Engineering/databases/#cte-common-table-expression","title":"CTE - Common Table Expression","text":"<ul> <li>It is a query that we write before the main query.</li> <li>use <code>WITH</code> to implement create CBT and use it in main query</li> <li>Example</li> </ul> <pre><code>WITH names_cte AS (\n  SELECT name\n  FROM students\n)\nSELECT * FROM names_cte;\n</code></pre> <ul> <li>CTEs can be recursive</li> <li>CTEs are reusable</li> <li>CTE are more readable and help manage long and complex queries</li> <li>CTE are similar to Subqueries, however, CTE can't be used in WHERE clause</li> </ul> <p>The WITH clause lets define a table. The definition is available only to the query in which the with clause occurs.</p> <p>Recursion</p> <p>Imagine you have a tree structure:</p> <pre><code>drop table if exists data_sci.org_structure;\ncreate table data_sci.org_structure (\n    id integer,\n    department_name text,\n    parent_department_id integer);\n\n\ninsert into data_sci.org_structure values\n  (1, 'CEO Office', null),\n  (2, 'VP Sales', 1),\n  (3, 'VP Operations', 1),\n  (4, 'Northeast Sales',2),\n  (5, 'Northwest Sales',2),\n  (6, 'Infrastructure Operations', 3),\n  (7, 'Management Operations', 3);\n</code></pre> <p>and you want to find complete path from one node to root (top parent), you know the algo, find parent of child, then find grand-parent, ... until you reach the root.</p> <p>This is where recursion can help.</p> <pre><code>WITH RECURSIVE &lt;table_name_recursive_cte&gt; (&lt;columns&gt;) AS\n  &lt;terminal select statement&gt;\n  UNION\n  &lt;recursive select statement&gt;\nSELECT\n  ...\n</code></pre> <p>Eg</p> <pre><code>with recursive report_structure(id, department_name,parent_department_id) as\n (select id,  department_name, parent_department_id \n  from data_sci.org_structure where id = 7\n union\n   select os.id, os.department_name, os.parent_department_id \n    from data_sci.org_structure os\n    join report_structure rs on rs.parent_department_id = os.id\n )\n select\n  *\n from report_structure\n</code></pre> <p>This will find all parent for child id 7, you can similarly pass any id and find all its parents using recursion.</p>"},{"location":"2-Data-Engineering/databases/#window-functions","title":"Window Functions","text":"<p>You can partition the data over a column, and then apply some function over the rows in each partition. The value of function is related to items in a partition, but resets in each partition. Some of the functions that you can apply are</p> <ul> <li><code>RANK()</code> gives rank based on value, if two item get same rank, next one would be same as row number, that is, 1,2,2,4 and so on, where 2, 3 have same rank 2, hence 4th item has rank 4, 3 is skipped.</li> <li><code>DENSE_RANK()</code> gives rank, but it doesn't skip rank, 1,2,2,3 and so on.</li> <li><code>ROW_NUMBER()</code> give row number like 1,2,3,4 and so no</li> <li><code>SUM(col_a)</code> - like sum (cost) over months, this will give running total by months</li> <li><code>NTILE(n)</code> - over a partition, it gives a number to each data item by dividing into n quarters, eg, 4 will break into 4 quartiles based on value of data item in window.</li> <li><code>LEAD(sales)</code> will show 2nd row in 1st row and so on..</li> <li><code>LAG(sales)</code> will show 1st row in 2nd row and so on..; <code>LAG(sales,2)</code> will show 1st row in 3rd row and so on.. you can specify the offset, by default it is 1.</li> </ul> <p>Syntax: <code>RANK() OVER (PARTITION BY col_b ORDER BY col_a)</code></p> <ul> <li>Imp In PARTITION BY... , the ORDER BY .... makes huge difference as order changes the way function is applied, eg:</li> </ul> <pre><code>-- this gives running total by each month and resets on year\nsum(total_sales) over (partition by year_sold order by year_sold, month_sold) as running_total\n\n-- this gives running total by year and resets on year, it skips months, that is one total for year and is same in each month\nsum(total_sales) over (partition by year_sold) as running_total\n</code></pre> <p>The partition that you create is called window. You can use as above, or have to use same partition again and again, you can define the partition as window and re-use it. syntax <code>WINDOW win_name AS (PARTITION by col_a ORDER by col_b)</code></p> <p>Example query:</p> <pre><code>-- Window Function\n\nSELECT emp.firstName, mdl.model, count(sls.salesId) as num_sold,\n  rank() over (PARTITION by emp.firstName ORDER by count(sls.salesId) desc) as ranks, -- this could also be fname, it is just to show\n  dense_rank() over (fname) as dense_ranks,\n  row_number() over (fname) as row_numbers,\n  sum(sls.salesAmount) over (fname) as running_sales\nFROM sales sls\nINNER JOIN employee emp\n  ON sls.employeeId = emp.employeeId\nINNER JOIN inventory inv\n  ON inv.inventoryId = sls.inventoryId\nINNER JOIN model mdl\n  ON mdl.modelId = inv.modelId\nGROUP BY emp.firstName, emp.lastName, mdl.model\nWINDOW fname AS (PARTITION by emp.firstName ORDER by count(sls.salesId) desc)\n\n-- Running Total using Window fn with CTE\n\nwith total_sales_cte as (\nselect\n  soldDate\n  , strftime('%Y', soldDate) as year_sold\n  , strftime('%m', soldDate) as month_sold\n  , sum(salesAmount) as total_sales\nfrom sales\nGROUP by year_sold, month_sold\nORDER by soldDate\n--LIMIT 5\n)\n\nselect * \n  , sum(total_sales) over (partition by year_sold order by year_sold, month_sold) as running_total\nfrom total_sales_cte\n</code></pre> <p>Links:</p> <ul> <li>All Cars DB SQLite, LL Course Adv SQL</li> </ul>"},{"location":"2-Data-Engineering/databases/#pivot","title":"Pivot","text":"<p>Lets say you have table as below</p> Name Occupation Ashley Professor Samantha Actor Julia Doctor Britney Professor Maria Professor Meera Professor <p>You can pivot by hardcoding the column names and using case statement.</p> <p>Lets first try to give row numbers over occupations</p> Name Occupation row_id Actor Eve 1 Actor Jennifer 2 Actor Kate 3 Actor Samantha 4 Doctor Amanda 1 Doctor Julia 2 Doctor Emma 3 Professor Ashley 1 Professor Jason 2 Professor Britney 3 <p>Then select from this table and hardcode the column names</p> <pre><code>-- sql server\nselect \n    t1.row_id\n    , max(case when o='Doctor' then name else NULL END) as 'Doctors'\n    , max(case when o='Professor' then name else NULL END) as 'Professors'\n    , max(case when o='Singer' then name else NULL END) as 'Singers'\n    , max(case when o='Actor' then name else NULL END) as 'Actors'\nFROM\n\n(select Occupation as o\n    , Name\n    , Row_number() over (partition by occupation order by name) as row_id\nfrom occupations) as t1\n\nGROUP BY t1.row_id\nORDER by t1.row_id\n</code></pre> <p>Output:</p> row_id Doctors Professors Singers Actors 1 Amanda Ashley Chris Eve 2 Julia Boris Jane Jennifer 3 Jane Britney Jenny Emma 4 NULL Maria Kris Samantha 5 NULL Meera NULL NULL 6 NULL Naomi NULL NULL 7 NULL Priyanka NULL NULL <p>That's how you can pivot tables.</p>"},{"location":"2-Data-Engineering/databases/#-dbms-","title":"----- DBMS -----","text":""},{"location":"2-Data-Engineering/databases/#normalization","title":"Normalization","text":"<p>Normalization is done to avoid data duplication and easily manage relational data, specially 1-many and many-many relationships</p> <p>OLTP</p> <ul> <li>It is mostly normalized, as there are many reads and writes but on specific data, eg, add new book, update an author</li> <li>you work on row level, eg, update address of a customer, you need all detail but only of one customer, so one row</li> </ul> <p>Snowflake schemas are normalized, avoiding data redundancy by storing dimensional data in separate tables. This improves data quality and reduces the required storage space.</p> <p>LINK: NORM VS DE-NORM</p>"},{"location":"2-Data-Engineering/databases/#denormalisation","title":"Denormalisation","text":"<ul> <li>it improves read performance.</li> <li>reduced risk of anomalies as we are mostly analysing and not updating / writing / deleting data at atomic level.</li> <li>streaming inserts, like from IoT can use denormalized data model as it has simple structure of timestamp and value.</li> <li>Eliminates complex joins and give performance gain</li> <li>Common way is star-schema where you have facts and dimensions.</li> <li>Wide table is also good but when data is petabyte scale then use specific databases like google big query.</li> </ul> <p>OLAP</p> <ul> <li>It is for data analysis, many read by many process on many data points. hence denormalized is preferred.</li> <li>you work on column level, eg average of temperature.</li> </ul> <p>Why star schema is not normalized?</p> <ul> <li>In a star schema, there is only one level of dimension tables, and all foreign key relationships are defined between the fact table and the dimensions. A query never needs to join tables beyond the single layer of dimension tables, resulting in better performance than if the dimension tables were normalized.</li> </ul>"},{"location":"2-Data-Engineering/databases/#partitioning","title":"Partitioning","text":"<ul> <li>large table can be inefficient, as it may have large indexes to manage. It has to scan large index.</li> <li>Horizontal Partitioning divides table into parts, it limits scans to subset of partitions. So if you break into 3 horizontal partition, there will be smaller index to scan/maintain and limited rows to read.</li> <li>efficient deleting, lets say you can drop a old timed partition, eg if older than 5 year, drop whole partition.</li> </ul> <p>Vertical Partitions</p> <ul> <li>partitions into set of columns, good when using columnar storage</li> <li>retains all rows in one partition</li> <li>cam have global indexes, but reduced I/O as reading less</li> </ul> <p>Range Partitioning</p> <ul> <li>similar to horizontal, uses time / numeric range / alphabetical range for partitioning</li> <li>it partitions on non-overlapping keys.</li> </ul> <p>Hash Partitioning</p> <ul> <li>identify a partitioning key, apply a hash and take modulus of hash (remainder) to identify which partition it should go into. Eg, you determine the number of partitions you need, then can do hash partitioning.</li> <li>useful when range partition does not make sense, or you need more balanced partitions.</li> </ul> <p>Code in Postgres</p> <ul> <li>define partition when creating,</li> <li>you can create partition tables by creating table as <code>partition of for values from ... to ...</code></li> </ul> <pre><code>CREATE TABLE (\n  ...\n  msmt_date date not null,\n  ...\n)\nPARTITION BY RANGE (msmt_date); \n\nCREATE TABLE tab_part_a PARTITION OF table_x\n  FOR VALUES FROM ('2021-01-01') TO ('2021-01-31') ; -- monthly\n</code></pre>"},{"location":"2-Data-Engineering/databases/#materialized-views","title":"Materialized Views","text":"<ul> <li>Instead of executing query again and again through execution plan, you can save results using materialized view.</li> <li>Persist the results, form of caching</li> <li>trade space for time.</li> </ul> <p>When to use</p> <ul> <li>long running complex queries with joins</li> <li>computing aggregate data</li> </ul> <p>When NOT to use</p> <ul> <li>can get out of sync, so depends how consistent data is required in view, eg 1hr refresh of materialized view will not have data generated in last 59 mins.</li> </ul> <pre><code>CREATE MATERIALIZED VIEW mv_myview AS (\n  -- your sql query\n)\n\nSELECT * FROM mv_myview;    -- use as normal table\n</code></pre>"},{"location":"2-Data-Engineering/databases/#read-replicas","title":"Read Replicas","text":"<p>There is a primary server which takes all read and write operation, you can add a read replica fo this server where all the writes also go in, now you can query this.</p> <p>Data is up to date with write ahead log.</p> <p>this makes primary to focus on writes, other replica(s) can help heavy reads.</p> <p>there are things to consider, like consistency requirements, transaction completion, all depends on trade offs.</p> <p>Challenge Sensor Data - Write Heady example</p> <p>You have sensor data coming in from IoT device, there should not be latency or lag in writes.</p> <p>System Design</p> <ul> <li>You should model the table to be same as data coming in, eg (device_id, timestamp, measure1, measure 2), exactly these 4.</li> <li>then can have materialized view on top of these to have aggregated hourly / daily results</li> <li>also consider partitioning which would ideally be on the time column.</li> <li>consider when do you need to purge the materialised view, old hourly data need not to be kept</li> <li>latest hour data materialized view need to be refreshed more frequently if required</li> <li>all these questions should be asked from domain expert, interrogate.</li> <li>if low level raw data is required, then avoid doing reads on primary server, rather create read-replica for this.</li> </ul>"},{"location":"2-Data-Engineering/databases/#indexes","title":"Indexes","text":"<p>This is indexing strategy. Indexes help reduce the data blocks. But there is cost to it. more space, more writes on each update/write. more cardinality (more distinct values) then indexing is more useful.</p> <p>Indexes are ordered, whereas tables might be unordered.</p> <p>Indexing Attributes</p> <ul> <li>Access Types: does it help to access range or a specific value</li> <li>Access Time: time to find the result</li> <li>Insertion Time: time to find the appropriate space to insert data</li> <li>Deletion Time: time to find data, delete it and update the index.</li> <li>Space Overhead: additional space required by the index.</li> </ul> <p>Index is not used in Google Big query or aws red shift. They are columnar storage databases and have different strategy.</p> <p>Type</p> <ul> <li>B-tree</li> <li>bit map</li> <li>hash index</li> </ul> <p>B-tree</p> <ul> <li>balanced tree index</li> <li>look-up in logarithmic time, same as binary search.</li> <li>its like binary tree, small value go to left, large go to right, and so on. it halves and halves and halves, hence, \\(O(\\log n)\\)</li> </ul> <p>Bitmap index</p> <ul> <li>small possible values in column, encode to bit string. eg, yes 1, no is 0, null is 00; Any enums can be represented by bit map.</li> <li>read intensive and few writes, because can be expensive to create bit maps for write</li> <li>PostgreSql does not support natively</li> </ul> <p>Hash</p> <ul> <li>Function for mapping arbitrary length data to a fixed-size string</li> <li>unique values output, eg <code>hello</code> is <code>5d41402abc4b2a76b9719d911017c592</code> in MD5 hash. and is always same for a string.</li> <li>any change in input produces new hash.</li> <li>You cannot reverse a hash, simple it is way math algo have done it, as eg, consider 912 is hash, 912 = 900 + 12 or 400 + 512 and so on. You will have to try all possible outputs to match, but can't exactly know what it is.</li> </ul> <ul> <li>Use case: It is useful when there are usually unique value in column, like email. Rather than fetch by email, you create hash index on it, and then you can fetch using hash index scan.</li> </ul> <p>GiST and SP-GiST</p> <ul> <li>Generalized search tree.</li> <li>for specialized data types, like <code>ltree</code> and <code>htree</code>. special type like circle, box, latitudes, etc.</li> <li>SP - partitioned trees.</li> </ul> <p><code>GIN</code> and <code>BRIN</code></p> <ul> <li>Generalized inverted index. values that are in composite item.</li> <li>slow insertion.</li> <li>either you can make insert fast or fetching.</li> <li><code>BRIN</code> - block range index,<ul> <li>used with very large index, ike post-code or dates.</li> <li>pages adjacent in table.</li> <li>stores summary information in block ranges.</li> </ul> </li> </ul> <p>Challenge</p> <p>Claims data, large size, 12 col data. how would you index to optimize join on claim id.</p> <p>Solution</p> <ul> <li>claim id is for join. by default, Postgres has b-tree. but claim_id is unique in dataset, so b-tree is not useful, rather we can use hash-index. So that would be one-one lookup.</li> </ul>"},{"location":"2-Data-Engineering/databases/#sql-execution-plan","title":"SQL Execution Plan","text":"<p>how database engine executes the SQL statements</p> <ul> <li>In SQL we tell what we want, not how we want, in java/python we do tell how we want to do.</li> <li>The cost of execution mostly depends on the number of rows.</li> <li>Few scans on large table is okay, otherwise, prefer indexes.</li> <li>Indexes help filter data on index, rather than on rows. And this may help in fetching just a partition.</li> </ul> <p>Table Join Algo</p> <ul> <li>tables join on keys, the internal algo is one of these</li> <li>Nested Loop Join - compare all rows in both tables. n^2 ?</li> <li>Hash Join - Calculate hash value of keys and join based on matching hash values.</li> <li>Sort Merge Join - Sort both tables and then join rows with advantage of order. n log n ?</li> </ul> <p>Plan Builder</p> <ul> <li>It builds the plan of execution based on the meta data statistics.</li> <li>You can look at query plan for a query</li> <li>If that query plan is inefficient, you can run <code>ANALYZE</code> command to update the statistics on meta data.</li> </ul> <ul> <li>SQL is declarative language, where you declare what you need. Whereas, programming is procedural language where you tell how to do things.</li> </ul> <p>Explain</p> <ul> <li>Add <code>EXPLAIN</code> before any query to see the query plan.</li> <li>It shows operations, like Seq Scan.. or Append. It can be something happening in parallel and others.</li> <li>With indexed in table, the plan changes, instead of sequence scan, you can see index scan, which takes less time.</li> <li>This way you can see what going on under the hood, are the indexes useful?</li> </ul> <ul> <li>Explain Join - here you can see the algo used for join. If the execution plan is expensive, you can consider adding indexes on key used for join.</li> </ul> <ul> <li>Eg, <code>Seq Scan on employee(cost=0.00..26.50 rows=323 width=65)</code> means it is a full table scan.</li> </ul> <p>Analyze</p> <p>If you add <code>Explain Analyze</code> before a query you can see the time taken by query to get completed.</p> <p>Challenge</p> <ul> <li>You have to optimise a long running query. You see that it has<ul> <li>3 tables</li> <li>2 left joins</li> <li>2 tables with ~500k rows and one with 200 rows</li> </ul> </li> </ul> <p>Solution</p> <ul> <li>Run EXPLAIN</li> <li>look for full-table scan, if so create index, on col used in join.</li> <li>look to filter dataset if not required.</li> <li>Run ANALYZE to ensure stats are up to date.</li> </ul> <p>Tip You can generate fake data in Postgres using <code>generate_series()</code> function. It is like range() in Python. It can generate numbers, timestamps etc. And use <code>random()</code> for random data.</p> <ul> <li>select is executed before group hence you can use select aliases in group by.</li> </ul> <ul> <li>examples<ul> <li>you can calculate month, year from order_date and use that in group by.</li> <li>use sum over month to find running total</li> <li>make this CTE, then reuse it.</li> </ul> </li> </ul>"},{"location":"2-Data-Engineering/databases/#user-defined-functions","title":"User Defined Functions","text":"<ul> <li>you can define functions to do some steps, it can be overloaded</li> <li>Postgres supports using python in functions.</li> </ul>"},{"location":"2-Data-Engineering/databases/#hstore","title":"HStore","text":"<p>You can use HStore with postgres to support storage of key-value pair in efficient way.</p> <ul> <li>Useful when you have large number of attributes not being used, this creates sparse columns.</li> <li>GIN and GiST indexes can be used efficiently.</li> <li>Eg: Catalog for store items.</li> <li>you can have a normal table. with columns as defined and have another column defined as HBase, which can have key value pair to store attributes.</li> <li>Useful when you do not have fixed structure to attributes. It is combination of sql with no-sql.</li> </ul>"},{"location":"2-Data-Engineering/databases/#json-and-semi-structured-data","title":"JSON and Semi-structured data","text":"<p>Postgres 9+ lets you store both kind of data in relational database with flexibility of semi-structured data. You can have a column data type as JSON and store JSON in it. It will have JSON as text, but JSON key can be queries and filtered using <code>-&gt;</code> notation.</p> <pre><code>CREATE TABLE customer_summary  \n  (id serial primary key,  \n  customer_doc jsonb);\n\ninsert into customer_summary (customer_doc) values \n  ('{\n  \"customer_name\": { \"first_name\": \"Alice\", \"last_name\": \"Johnson\" },\n  \"address\": { \"city\": \"Boston\", \"state\": \"MA\" }\n}');\n\nselect customer_doc-&gt;'customer_name'-&gt;&gt;'first_name' from customer_summary;\n\n-- Alice\n</code></pre>"},{"location":"2-Data-Engineering/databases/#-databases-","title":"----- Databases -----","text":"<ul> <li>Relational Databases<ul> <li>store in rows. Eg, MySQL, <code>PostgreSQl</code></li> <li>good to store transactions and build relationships.</li> <li>Eg, PostgreSQL is relational db.</li> </ul> </li> </ul> <ul> <li>Columnar Databases<ul> <li>used commonly in warehousing. eg, Amazon Redshift, Google BigQuery, Apache Cassandra.</li> </ul> </li> </ul> <ul> <li>NoSQL Databases<ul> <li>They do not have defined structure and are document based.</li> <li>Elasticsearch - a search engine based on <code>Apache Lucene</code> and can be used as NoSql db.</li> <li>Apache Kibana - adds GUI to ElasticSearch.</li> </ul> </li> </ul>"},{"location":"2-Data-Engineering/databases/#postgresql","title":"PostgreSQL","text":"<ul> <li>PostgreSQL is relational db.<ul> <li><code>pgadmin4</code> is GUI to it. It is available on web and can run on localhost.</li> <li><code>pgcli</code> is eMac CLI to client for database access. It can be installed within PIP as a package.</li> <li>login with linux user on the server as <code>username@server</code></li> <li>then add server, expand it, see database, schemas, public.</li> </ul> </li> </ul>"},{"location":"2-Data-Engineering/databases/#sqlite","title":"SQLite","text":"<ul> <li>It is a micro database that can work in memory or a saved in file, eg, <code>store.db</code> .</li> <li>Queries are same as any other SQL.</li> <li>It can be used in many ways, some are:<ul> <li>Python Program and DB Engine In memory</li> <li>Python Program and DB Engine as File</li> <li>SQLite installed as utility and access via shell, this is <code>sqlite3.exe</code> program.</li> <li>SQLite ODBC driver.</li> </ul> </li> </ul> <p>Interaction</p> <ul> <li>shell<ul> <li><code>sqlite3</code> opens a shell in command line, just like mysql shell. DB is in memory<ul> <li>to open a file, use <code>.open FILENAME</code> to open an existing database.</li> </ul> </li> <li><code>sqlite3 data.sqlite</code> to work on this file</li> <li><code>ctrl + z</code> enter to exit</li> </ul> </li> </ul> <ul> <li>DDL<ul> <li><code>.tables</code> to show all tables</li> <li><code>.schema orders</code> to check create statement</li> </ul> </li> </ul> <ul> <li>load CSV<ul> <li><code>.mode csv</code> and then <code>.import data.csv orders</code> loads csv to db, creates if not exists.</li> </ul> </li> </ul> <ul> <li>GUI SQLite Browser</li> </ul>"},{"location":"2-Data-Engineering/databases/#mysql","title":"MySQL","text":"<p>Installation:</p> <ul> <li><code>brew install mysql</code></li> </ul> <p>Start Server:</p> <ul> <li><code>brew services start mysql</code> - background mysql start</li> <li><code>mysql.server start</code> - no background</li> <li><code>mysql_secure_installation</code> - run this and set root pwd, etc.</li> <li><code>ps -ef | grep mysqld</code> process</li> </ul> <p>Login:</p> <ul> <li><code>mysql -u root -p</code> then enter password</li> </ul> <p>Queries:</p> <pre><code>show DATABASES;\n\ncreate user 'bob_me'@'localhost' identified with mysql_native_password by 'bob_pwd';\n\ncreate database bob_db;\n\ngrant all privileges on bob_db.* to 'bob_me'@'localhost';\n</code></pre> <p>Client - SequelPro:</p> <ul> <li>Connecting:<ul> <li>Standard</li> <li>Host: 127.0.0.1</li> <li>enter username and password and connect.</li> </ul> </li> </ul> <p>Shutdown Server:</p> <ul> <li><code>mysql.server stop</code> - stops server</li> </ul> <p>Other Notes:</p> <ul> <li>Column and Table names are case-sensitive.</li> <li><code>mysqladmin</code> is also installed</li> </ul> <p>Trouble Shooting:</p> <ul> <li>If you see error in clients, eg Sequel Pro, it might not be ready yet for a new kind of user login, link.</li> <li>do <code>ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'newrootpassword';</code> then try logging in.</li> </ul>"},{"location":"2-Data-Engineering/databases/#microsoft-sql-server","title":"Microsoft SQL Server","text":"<p>Snippets</p> <pre><code>-- Create and Insert\nSELECT FirstName, LastName\nINTO TestTable\nFROM Person.Contact\nWHERE EmailPromotion = 2\n\n-- Existing Table Append\nINSERT INTO TestTable\nSELECT FirstName, LastName\nFROM Person.Contact\nWHERE EmailPromotion = 2\n</code></pre>"},{"location":"2-Data-Engineering/databases/#redis","title":"Redis","text":"<ul> <li><code>redis-server</code> to start the server.</li> </ul>"},{"location":"2-Data-Engineering/databases/#elasticsearch","title":"Elasticsearch","text":"<ul> <li>Elastic Search is a search engine based on <code>Apache Lucene</code> and can be used as NoSql db.</li> <li>you can set cluster and node in elastic search.</li> <li>once installed it doesn't has GUI but is rather an API. at <code>localhost:9200</code> you can see JSON out from API</li> <li>uses JSON Query called Elastic Query DSL (Domain Specific Language)</li> </ul> <ul> <li>Apache Kibana<ul> <li>adds GUI to ElasticSearch.</li> <li>you can use it to build visualizations and dashboards of your data held in Elasticsearch.</li> <li>after installation it can be accessed on <code>http://localhost:5601</code></li> <li>you can create viz from index in elasticsearch. viz is widget that can be added to dashboard. dashboard filter filter all widget if field is present.</li> <li><code>developer tools</code> is scratch-pad area where you can crate and query data/indices.</li> </ul> </li> </ul>"},{"location":"2-Data-Engineering/databricks/","title":"Databricks Notes","text":"<p>Category: Data Lake Formation, Suite of tools for Data Storage, Processing, Analytics and more.</p>"},{"location":"2-Data-Engineering/databricks/#databricks-overview","title":"Databricks Overview","text":"<ul> <li>It is managed spark service on cloud. You do not have to do cluster management, server setup, tuning and backup.</li> <li>You can spin up a new cluster from easy UI.</li> <li>Community edition is free with 6gb size.</li> <li>It lets you use cluster, execute notebook, lets persist data in files/tables.</li> </ul>"},{"location":"2-Data-Engineering/etl_pipelines/","title":"ETL Pipelines","text":"<p>all about ETL pipelines scheduling</p> <ul> <li>ETL Pipelines<ul> <li>Data Pipeline Design Patterns<ul> <li>Pipeline Terms</li> <li>Extraction Patterns</li> <li>Fail Recovery Patterns</li> </ul> </li> <li>Coding Data Pipeline Patters<ul> <li>Functional Design</li> <li>Factory Pattern</li> <li>Strategy Pattern</li> <li>Singleton, \\&amp; Object pool patterns</li> <li>Python Helpers<ul> <li>DataClass</li> </ul> </li> <li>Python Best Practice</li> </ul> </li> <li>ETL Pipeline</li> <li>Links</li> </ul> </li> </ul>"},{"location":"2-Data-Engineering/etl_pipelines/#data-pipeline-design-patterns","title":"Data Pipeline Design Patterns","text":"<p>all about design patterns and implementation</p>"},{"location":"2-Data-Engineering/etl_pipelines/#pipeline-terms","title":"Pipeline Terms","text":"<p>Idempotent</p> <ul> <li>One can run a data pipeline multiple times with the same input, and the output will not change</li> </ul> <p>Backfilling</p> <ul> <li>add/modifying data to already existing dataset. Eg, apply change in logic to processed dataset, adding a new column from source to processed dataset. Most orchestration tools support backfilling.</li> </ul> <p>Source Replayability</p> <ul> <li>ability of source to provide historical data. This is a requirement for backfilling. You can make non-replayable source replayable by dumping the incoming data into raw/loading zone at a certain frequency, say, daily. Then data is replayable to daily degree and not hourly.</li> <li>Replayable sources: Event stream, web server logs, a dump of database WAL (write ahead log) showing all the create/update/delete statements (CDC - Change Data Capture), etc</li> <li>Nonreplayable sources: Application tables that are constantly modified, APIs, etc., that only provide the current state of the data.</li> </ul> <p>Source Ordering</p> <ul> <li>Is the data coming from source in order? Eg, event of checkout happens after click, log-out happens after log-in.</li> </ul> <p>Sink Overwritability</p> <ul> <li>ability to modify based on unique key or run ID. Overwritability is crucial to prevent duplicates or partial records when a pipeline fails.</li> <li>Kafka queue is non overwritable, table entries without key.</li> </ul> <p>Multi-hop pipeline</p> <ul> <li>After each transformation the data state is saved, this lets easy debug and lets start the pipeline from failed state, and hence only failed transformations are reprocessed.</li> </ul>"},{"location":"2-Data-Engineering/etl_pipelines/#extraction-patterns","title":"Extraction Patterns","text":"<p>Time Ranged</p> <p>Extract the data from source based on time frame. Eg, pull all data from yesterday.</p> <p>Full Snapshot</p> <ul> <li>Entire data is fetched.</li> <li>Add run_id to build historic snapshot using which you can go back in time.</li> <li>Replace full data if history is not required.</li> </ul> <p>Lookback</p> <ul> <li>This pull is used to build aggregated metric. Eg, active users in a month. This is used when source data is constantly updated and end user only care about current state. Usually required for dashboards needs.</li> </ul> <p>Streaming</p> <ul> <li>Each record flows through pipeline.</li> </ul>"},{"location":"2-Data-Engineering/etl_pipelines/#fail-recovery-patterns","title":"Fail Recovery Patterns","text":"<p>Idempotent</p> <p>Ability of a code to produce same result despite being executed multiple times. Eg, if ETL job is run multiple times, it should not load duplicate data. Eg, Pull daily data and dump into a dated folder. If you re-run this pipe, the folder content will get overwritten and hence the output remains same.</p> <p>Re-run should not produce duplicates or incorrect values.</p> <p>Eg, you need to re-run the ETL as you have to apply new transformation to filter out the records with, say, customers not having DOB. Now you will rerun the pipeline (also called Backfill) and this will re populate the table by overwriting the customer having dbo. However, you may have still have stale records having no dob from previous run. In this case you should do Delete-Write and can add delete all data from loading area before reloading after new transformation. Most libraries and frameworks offer an overwrite option (e.g. Spark overwrite , Snowflake overwrite ) which is safer than deleting and writing.</p> <p>Link: SDE - How to make data pipelines idempotent</p> <p>Self Healing</p> <p>The pipeline will re-catchup the data on failure. Eg, if the data is inserted based on ID difference between source and sink. Then the re-run will fetch the IDs that were failed to load in previous run.</p>"},{"location":"2-Data-Engineering/etl_pipelines/#coding-data-pipeline-patters","title":"Coding Data Pipeline Patters","text":"<p>how to code in different patters</p> <p>Coding in pattern can make pipeline robust, easy to test and maintain. Different patterns are required to cater different use cases and needs.</p>"},{"location":"2-Data-Engineering/etl_pipelines/#functional-design","title":"Functional Design","text":"<p>This is simple, where you define three functions <code>extract()</code>, <code>transform()</code> and <code>load()</code> and then define a <code>run()</code> function which calls all three.</p> <p>This is good for simple use cases, however, at least try that they follow following standards:</p> <ul> <li>Atomicity - one function does only one work. eg, <code>load()</code> should not create <code>db_conn</code></li> <li>Idempotency - rerun should not produce duplicates or incorrect data. Eg, <code>load()</code> should <code>delete</code> then <code>write</code>.</li> <li>No Side Effects - one function should not affect another function. eg, <code>load()</code> should not close <code>db_conn</code> as it is passed to it and is created in another function.</li> </ul>"},{"location":"2-Data-Engineering/etl_pipelines/#factory-pattern","title":"Factory Pattern","text":"<p>If you have similar ETL jobs, eg pulling data from Twitter/Reddit/Quora using APIs then you can code using factory pattern. If you have non-similar ETL jobs then this may not be good implementation.</p> <p>In factory pattern:</p> <ul> <li>You define abstract class - this is structure with no logic</li> <li>You define concrete classes - these use abstract class and implement them with logic and ensure that the pattern is same as defined in abstract class</li> <li>You define the factory - which is nothing but a function, that takes in a param which tells which concrete class to return.</li> </ul> <p>Eg:</p> <pre><code>import os\nfrom abc import ABC, abstractmethod # python module to define abstract interfaces\n\n# Abstract class with abstract methods\nclass SocialETL(ABC):\n    @abstractmethod\n    def extract(self, id, num_records, client):\n        pass\n\n    @abstractmethod\n    def transform(self, social_data):\n        pass\n\n    @abstractmethod\n    def load(self, social_data, db_conn):\n        pass\n\n    @abstractmethod\n    def run(self, db_conn, client, id, num_records):\n        pass\n\n# Concrete implementation of the abstract Class\nclass RedditETL(SocialETL):\n    def extract(self, id, num_records, client):\n        # code to extract reddit data\n\n    def transform(self, social_data):\n        # code to transform reddit data\n\n    def load(self, social_data, db_conn):\n        # code to load reddit data into the final table\n\n    def run(self, db_conn, client, id, num_records):\n        # code to run extract, transform and load\n\n# Concrete implementation of the abstract Class\nclass TwitterETL(SocialETL):\n    def extract(self, id, num_records, client):\n        # code to extract reddit data\n\n    def transform(self, social_data):\n        # code to transform reddit data\n\n    def load(self, social_data, db_conn):\n        # code to load reddit data into the final table\n\n    def run(self, db_conn, client, id, num_records):\n        # code to run extract, transform and load\n\n# This \"factory\" will acccept an input and give you the appropriate object that you can use to perform ETL\ndef etl_factory(source):\n    factory = {\n        'Reddit': (\n            praw.Reddit(\n                client_id=os.environ['REDDIT_CLIENT_ID'],\n                client_secret=os.environ['REDDIT_CLIENT_SECRET'],\n                user_agent=os.environ['REDDIT_USER_AGENT'],\n            ),\n            RedditETL(),\n        ),\n        'Twitter': (\n            tweepy.Client(bearer_token=os.environ['BEARER_TOKEN']),\n            TwitterETL(),\n        ),\n    }\n    if source in factory:\n        return factory[source]\n    else:\n        raise ValueError(\n            f\"source {source} is not supported. Please pass a valid source.\"\n        )\n\n# calling code\nclient, social_etl = etl_factory(source)\nsocial_etl.run(db_conn, client, ...)\n\n# code credit goes to link below\n</code></pre> <p>Now in the <code>main()</code> you can call <code>etl_factory()</code>.</p> <p>Note:</p> <ul> <li>Please use factory pattern only when you data pipelines have similar structure.</li> <li>Ensure proper <code>logging</code> to know which function is executed.</li> </ul> <p>Link: SDE - Factory Patter - Coding Pattern</p>"},{"location":"2-Data-Engineering/etl_pipelines/#strategy-pattern","title":"Strategy Pattern","text":"<p>In simple terms, it takes out the actual transformation logic into separate function instead of being in the transformation() function itself.</p> <p>This ensure separation of concerns (definition, creation &amp; use) and hence enables easier code maintenance and testability.</p> <p>Eg:</p> <ul> <li><code>clean_name()</code>, <code>format_date()</code>, <code>calc_std_dev()</code> can be some of the transformations, now these are separate functions.</li> <li>There can be a factory to return these functions as and when required. Factory is nothing but a function having a dictionary which has key as \"some name to function\" and value as the function itself. It return the value of dict based on key passed.</li> <li>Depending on what you want to do in transformation, you can pass pass that transformation string to factory and it will return that transformation function.</li> </ul> <p>Note: The signature of transformation functions (input and output) is same, hence it is possible to make them switchable using factory pattern.</p>"},{"location":"2-Data-Engineering/etl_pipelines/#singleton-object-pool-patterns","title":"Singleton, &amp; Object pool patterns","text":"<p>Singleton - Only one object can be created and used. Eg, <code>db_conn</code> object. There can be only one database connection object and can be used across app.</p> <p>Object Pool - There is a pool of object from which an object can be taken to do the work and can then returned back to pool once done. Object should be returned in its original form. Eg, A pool of database connection from which objects can be taken when required, enabling parallel processing.</p>"},{"location":"2-Data-Engineering/etl_pipelines/#python-helpers","title":"Python Helpers","text":"<p>Python Type Validation</p> <ul> <li>It helps define param/return type so that run time issues are avoided.</li> <li><code>typing</code> package has classes that can be used to define complex types.</li> <li>Eg usage:</li> </ul> <pre><code>from typing import Callable, List\n\ndef transformation_factory(value: str) -&gt; Callable[[List[SocialMediaData]], List[SocialMediaData]]:\n    pass\n</code></pre> <p>Here, <code>Callable</code> defines that return type of function is a callable function. First param to Callable is input to callable function, Second param to Callable is return type of callable function.</p> <p><code>MyPy</code></p> <p>It is a python lib that helps validate python type checks. It is additional code check to coding standard like <code>flake8</code> is for formatting.</p> <pre><code>python -m mypy --no-implicit-reexport --ignore-missing-imports --no-namespace-packages ./\n</code></pre>"},{"location":"2-Data-Engineering/etl_pipelines/#dataclass","title":"DataClass","text":"<p>It helps store data as object of Dataclass. Instead of dictionary you can use class-object notation for handling data.</p> <pre><code>from dataclasses import dataclass\n\n@dataclass\nclass PostData:\n    title: str\n    body: int\n    author: str\n</code></pre> <p>This will make this usable in class-object notation.</p> <p>Using Data Class with JSON, like working with an API</p> <p>Suppose you have JSON data coming from API and you want to load it into a class.</p> <pre><code>from dataclasses import dataclass\nfrom dataclasses_json import dataclass_json\n\n@dataclass_json\n@dataclass\nclass Person:\n    is_active: int = 0          # data type\n    title_code: str = 'Mr'      # default value\n    first_name: str = ''\n    middle_name: str = ''\n\n    # define functions like this\n    def get_name(self):\n        name = self.first_name + ' ' + self.last_name\n        return name\n</code></pre> <p>To build the objects</p> <pre><code># build obj from json\nperson_items = None\nif res_json is not None:\n    person_items = []\n    for item in res_json['items']:\n        person = Person.from_dict(item)\n        person_items.append(person)\n</code></pre> <p>Here, you can also use <code>Person.from_json(item)</code> if <code>item</code> is JSON String.</p> <p>Link https://realpython.com/python-data-classes/#default-values</p> <p>Context Managers</p> <p>When there is a network call, like reading file or database connection, we should close the connection once done to avoid memory leaks and free up resources.</p> <p>Context manager makes a function context managed. A context managed functions can be called using <code>with</code> keyword. Eg, <code>with open(file):</code>.</p> <p>Upon call, a context managed function will provide an object, which is provided by using <code>yield</code>.</p> <p>Upon end of with block, the execution is returned back to the context managed function which can handle closing of connection in <code>finally</code> block.</p> <pre><code>from contextlib import contextmanager\n\nclass DatabaseConnection:\n\n    @contextmanager\n    def managed_cursor(self):\n        _conn = sqlite3.connect(self._db_file)\n        cur = _conn.cursor()\n        try:\n            yield cur         # is provided to `with` block\n        finally:\n            _conn.commit()    # executed after `with` block\n            cur.close()\n            _conn.close()\n\n\ndb = DatabaseConnection()\n\nwith db.managed_cursor() as cur: # cursor and connection are open\n    cur.execute(\"YOUR SQL QUERY\")\n# cursor and connection are close\n</code></pre> <p>Testing with <code>pytest</code></p> <p>Ensure code is correct and lets modify code with confidence.</p> <ul> <li>Fixtures - lets provide mock API data</li> <li>Schema Setup - Let define db-schema for tests, basically <code>setup</code> and <code>teardown</code> at different level.</li> <li>Mocking functions - lets run a function to be tested by modifying its behaviour.</li> </ul> <p>Decorators</p> <p>Decorators add functionality to other functions.</p>"},{"location":"2-Data-Engineering/etl_pipelines/#python-best-practice","title":"Python Best Practice","text":"<p>Makefile</p> <p>You do make aliases, like one work for long commands. This can be for command to run pytest, lint-test, type-test, formatting-test, run ETL, docker up or docker down, etc.</p> <p>Githooks</p> <p>You do run pytest, lint-test, type-test, formatting-test etc while approving a PR, but can make this mandatory and make it run before commit by adding a pre-commit hook. this will run all these checks automatically to force them before commit.</p>"},{"location":"2-Data-Engineering/etl_pipelines/#etl-pipeline","title":"ETL Pipeline","text":"<ul> <li>Simple - Using Pandas to read data, transform it and load is a pipeline. Doing this in distributed environment is big data pipeline.</li> </ul> <ul> <li>Distributed - Using PySpark read data from DB is extraction. Do transformation like groupBy or mean, join.</li> </ul> <ul> <li>Code organization - say in <code>etl_somejob.py</code><ul> <li>define extract functions. Eg: <code>def extract_table1_to_df():</code> that returns df.</li> <li>define transform functions. Eg: <code>def transform_avg_score(df1,df2):</code> returns df.</li> <li>define load function. Eg: <code>def load_df_to_db(df):</code></li> <li> <p>finally to execute them</p> <pre><code>if __name__ == \"__main__\":\n  df_table1 = extract_table1_to_df()\n  df_score = transform_avg_score(df_table1)\n  load_df_to_db(df_score)\n</code></pre> </li> </ul> </li> </ul> <ul> <li>Now you can use this file to run or schedule.</li> </ul>"},{"location":"2-Data-Engineering/etl_pipelines/#links","title":"Links","text":"<ul> <li>AIrflow - Automation Scheduling Orchestrate</li> </ul>"},{"location":"2-Data-Engineering/pandas/","title":"Pandas","text":"<ul> <li><code>df.T</code> - Transposes dataframe</li> </ul>"},{"location":"2-Data-Engineering/pandas/#read-csv","title":"Read CSV","text":"<pre><code># Data Types\ndtype_ = {\n    'Quantity' : 'int64',\n    'Amount' : 'float64',\n}\n\n# Date Parser\nmy_date_parser = lambda x: pd.to_datetime(x, format=\"%d/%m/%Y\", errors='coerce')\n\n# CSV with proper data and currency read\ndf = pd.read_csv(file, low_memory=False, dtype=dtype_, thousands=',',\n                 parse_dates=['load_date', 'transaction_date'], date_parser=my_date_parser)\n\n# find date range in huge CSV\ndf = pd.read_csv(filepath, usecols=['Date'], parse_dates=['Date'], date_parser=my_date_parser)\ndf.Date.max(), df.Date.min()\n</code></pre>"},{"location":"2-Data-Engineering/pandas/#select-data","title":"Select data","text":"<ul> <li>Indexing and Slicing Data - pandas.pydata.org</li> </ul> <ul> <li>You can select data by index or label. Not only select by also assign, that is write data.</li> <li><code>loc</code> take real labels, that is column-names, and row-names. Does not take index.</li> <li><code>iloc</code> takes indexes, and also in different sq brackets can take labels.</li> <li><code>at</code> and <code>iat</code> are for accessing <code>only single value</code> in dataframe.</li> <li>no function just slicing, <code>df[2:4]</code> works only on row indexes.</li> </ul> <pre><code>df.iloc[1:3,2:4] # pass row,column, takes index in same way as list.\ndf.iloc[1:3][2:4] # same as above\ndf.iloc[:,[2,4,6,22]] # all rows and specific index cols\ndf.iloc[1:3]['product','category'] # works\n\ndf.loc[1:3,'category']\ndf.loc[1:3,'product':'value'] # shows all columns between product and value\ndf.loc[1:3,['product','value']] # shows only prod and value, not in betweens.\ndf.loc[1,'product'] # returns cell, as the format of column, str, int.\n\ndf[2:20:2] - # row 3 to 20 with freq2, that is every alt row\n</code></pre>"},{"location":"2-Data-Engineering/pandas/#filter-conditions-where-clause","title":"Filter Conditions - Where clause","text":"<pre><code>df = df[df.column_name != value]\ndf[df.column_name.isin(values_list)]\ndf[~df.column_name.isin(values_list)] # reverse of condition\ndf = df[(df.height != 11) &amp; (df.age != 31)] # multiple conditions\n</code></pre>"},{"location":"2-Data-Engineering/pandas/#delete-data","title":"Delete Data","text":"<ul> <li>Ops1: You can use where-clause, then index slicing and copy result to df_new variable</li> <li>Ops2: Use <code>df.drop</code> and pass indexes to drop. To find indexes to drop, use <code>query</code> or df-slicing.</li> </ul> <pre><code># Ops1: find True-False list, slice dataframe, store results\ndf_new = df[df.col_name != value]\n\n# Ops2: where value is 0, slice dataframe, get index, pass to drop as row indexes to drop, do in place.\ndf.drop( df[df['value']==0].index, inplace=True)\n</code></pre>"},{"location":"2-Data-Engineering/pandas/#updates-inserts","title":"Updates / Inserts","text":"<ul> <li>the way you select cell/column, there you can assign value to make updates or inserts</li> </ul> <pre><code>df.loc[4,'emp_name'] = 'John' # updates cell at row_index 4 and column \"emp_name\" with value 'John'\n</code></pre> <p>To Insert new row in dataframe, you can</p> <ul> <li><code>concat</code> the new row as dataframe to the existing dataframe</li> <li><code>df.loc[-1]</code> can be used to insert at start.</li> </ul> <p>Insert DataFrame using Concat</p> <pre><code># df is existing dataframe\n\ntargets = [\n    {\n        \"year\": 2023,\n        \"brand\": \"Target\",\n        \"score\": 52\n    },\n    {\n        \"year\": 2024,\n        \"brand\": \"Target\",\n        \"score\": 68\n    }\n]\ndf_targets = pd.DataFrame(targets)\n\ndf = pd.concat([df, df_targets ]).reset_index(drop=True)\n</code></pre> <p>Here, new dataframe can have one or more dict as list. More on insert to pd -stackOverFlow</p> <p>Insert List at Index of Row, like start, end, middle etc</p> <pre><code> df.loc[-1] = [2, 3, 4]  # adding a row\n df.index = df.index + 1  # shifting index\n df = df.sort_index()  # sorting by index\n</code></pre>"},{"location":"2-Data-Engineering/pandas/#data-type-change","title":"Data Type Change","text":"<p>To avoid errors in converting float IDs to String and handle NULLs, Do</p> <pre><code>df.cust_id.fill_na('')\ndf.cust_id = df.cust_id.apply(lambda x: str(x).replace('.0', ''))\n</code></pre> <pre><code># this will change datatype but will keep decimal, 11.0 to '11.0'\ndf['emp_id'] = df['emp_id'].astype(str)\n\n# this will convert 1.04 to 1\ndf.cust_id = df.cust_id.apply(lambda x : str(int(x)) )\n\n# to check\ndf.dtypes\n</code></pre> <p>More on Stackoverflow</p>"},{"location":"2-Data-Engineering/pandas/#group-summarize-aggregate","title":"Group / Summarize / Aggregate","text":"<ul> <li><code>df.product.nunique()</code> - unique products in dataframe. pandas.Series.nunique</li> <li><code>df.product.value_counts()</code> - unique products and count of records for each. pandas.Series.value_counts</li> </ul> <p>Group Data</p> <ul> <li>aggregation functions - mean, sum, count, size, max, min, first, last. alos, <code>agg</code></li> </ul> <pre><code># Groupby multiple columns &amp; multiple aggregations\nresult = df.groupby('Courses').aggregate(\n    {\n        'Duration':'count',\n        'Fee':['min','max'],\n        'user_id': 'nunique'        # count distinct\n    }\n)\n\n# Aggregate and rename, x y z are new col names\ndf.agg(x=('A', 'max'), y=('B', 'min'), z=('C', 'mean'))\n\n# agg and aggregate are same.\n\n# Example\ndf_array2.groupby(['region']).aggregate(\n    p25 = ('score',lambda x: x.quantile(0.25) ),\n    min_salary = ('salary', 'min'),\n    p75 = ('score',lambda x: x.quantile(0.75) )\n)\n\n#Create a groupby object\ndf_group = df.groupby(\"Product_Category\")\n#Select only required columns\ndf_columns = df_group[[\"UnitPrice(USD)\",\"Quantity\"]]\n#Apply aggregate function\ndf_columns.mean()\n</code></pre> <p>User defined function with aggregate</p> <pre><code>def my_email_merge(x):\n    ...\n\ndf_grouped = df.groupby(grp_by_cols).aggregate(\n    email_id=('email_address', lambda x: my_email_merge(x)),\n)\n</code></pre> <p>In above, <code>my_email_merge()</code> is a user defined functions, it takes dataframe col <code>email_address</code> and does processing, result is stored in new column <code>email_id</code> in grouped dataFrame.</p> <p>Group and Work on Grouped Data</p> <pre><code>grouped = df_m[:].groupby(cols_to_group, dropna=False)\n# dropna false will keep rows that have null value in cols_to_group\n\ndef build_category(x):\n    # x is df in a group\n    ...\n\ndf['category'] = grouped.apply(lambda x: build_category(x), include_groups=False).values\n\n\n# returning multiple values from UDF on Grouped Data\nres = grouped.apply(lambda x: build_category(x), include_groups=False).values\ndf_g['category'] = [r[0] for r in res]\ndf_g['data'] = [r[1] for r in res]\n\n\n# add more columns from grouped\ndf_g['name'] = grouped['name'].first().values\ndf_g['surname'] = grouped['surname'].first().values\n</code></pre> <ul> <li>More on Pandas Pydata Aggregate</li> <li>Examples from PyData Pandas User Guide</li> </ul>"},{"location":"2-Data-Engineering/pandas/#sort-data","title":"Sort Data","text":"<pre><code># Sort Descending\ndf.sort_values(by='count',ascending=False)\n\n# Sort by custom order\nsource_sort_order = {'External': 1, 'Third Party': 2, 'Internal': 3}\ndf_m.sort_values(by=['source'], key=lambda x: x.map(source_sort_order), inplace=True)\n</code></pre>"},{"location":"2-Data-Engineering/pandas/#create-new-columns-or-calculated-fields","title":"Create New Columns or Calculated Fields","text":"<pre><code># applying col wise, so x is a row and function will apply column wise on each row\n\ndef build_some(x):\n    fname = x['first_name'].strip().title()\n\n    if pd.notnull(x['surname']) and len(x['surname']) &gt; 0:\n        name = fname + ' ' + x['surname']\n\n    return (name, fname)\n\ndf[ ['name', 'fname'] ] = df.apply(lambda x : build_some(x) , axis = 1, result_type='expand')\n\n# Example using function\ndf['item_type'] = df.apply(lambda x : get_item_type(x), axis=1)\ndef get_item_type(x):\n    y = \"\"\n    if x['order_type'] == \"Valid\" and len(x['inbound_reason']) &gt; 1:\n        y = \"Inbound\"\n    elif x['order_type'] == \"Need more info to respond\":\n        y = \"Need Info\"\n    elif x['order_type'] == \"Error in Data\":\n        y = \"Error in Reporting\"\n    else:\n        y = \"Awaiting Category\"\n    return y\n\n# Exmple single line\ndf['has_responded'] = df.apply(lambda x : 'Awaiting' if x['responded_by'] is None else 'Responded', axis=1)\n</code></pre>"},{"location":"2-Data-Engineering/pandas/#id-col","title":"ID col","text":"<pre><code>sql_max_id = \"\"\"\nselect max(id) as max_id from [dbo].[employee]\n\"\"\"\n\nconnection = sqlalchemy.create_engine(connection_string, echo=False)\ndf_max_id = pd.read_sql(sql=sql_max_id, con=connection)\n\n# hold max id in table\nmax_id = df_max_id.iat[0,0] or 0\n\n# adds id from max to new lenght\ndf_users_dvs['id'] = range(max_id+1, max_id+1+len(df_users_dvs))\n\n# Ideally use, auto-increment / Identity\n</code></pre>"},{"location":"2-Data-Engineering/pandas/#date-operations","title":"Date Operations","text":"<pre><code>df_wm['date_wm'].dt.year # year from datetime\n</code></pre>"},{"location":"2-Data-Engineering/pandas/#renaming-column-names","title":"Renaming Column Names","text":"<pre><code>import re\n\ndef readable_column_names(df):\n    \"\"\"Makes column name sentance readable\n\n    re.sub(pattern, repl, string)\n    find a pattern and replaces in a string\n\n    Args:\n        df (DataFrame): data frame to clean\n\n    Returns:\n        DataFrame: cleaned readable column names\n    \"\"\"\n    col_list = []  # holds names to check duplicates\n    renamer = dict()  # holds col name and its number of duplicates\n\n    for col in df.columns:\n\n        new_col_name = col\n\n        # camel case to space\n        new_col_name = re.sub('([a-z0-9])([A-Z])', r'\\1 \\2', new_col_name).lower()\n\n        new_col_name = new_col_name.replace(\"_\", \" \").replace(\"-\", \" \").title()\n\n        if new_col_name in col_list:\n            # rename\n            index = int(renamer.get(new_col_name, 0))\n            renamer[new_col_name] = index + 1\n            new_col_name += \"_\" + str(index + 1)\n            pass\n        else:\n            col_list.append(new_col_name)\n        # print(f'col: {col}, new: {new_col_name}')\n        df.rename(columns={col: new_col_name}, inplace=True)\n\n    return df\n\n\ndef system_column_names(df):\n    \"\"\"Converts column names to snake_case\n\n    Args:\n        df (dataframe): dataframe\n\n    Returns:\n        dataframe: updated columnname dataframe\n    \"\"\"\n    col_list = []     # holds names to check duplicates\n    renamer = dict()  # holds col name and its number of duplicates\n    for col in df.columns:\n\n        new_col_name = col.strip()\n\n        # Camel case to space\n        new_col_name = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', new_col_name).lower()\n\n        # convert multi space to single space\n        new_col_name = re.sub('([\\s]{2,})', r' ', new_col_name)\n\n        new_col_name = new_col_name.lower().replace(' ', '_').replace('-', '_')\n\n        # multi _ to single _\n        new_col_name = re.sub('([_]{2,})', r'_', new_col_name)\n\n\n        if new_col_name in col_list:\n            # rename\n            index = int(renamer.get(new_col_name, 0))\n            renamer[new_col_name] = index + 1\n            new_col_name += '_' + str(index + 1)\n            pass\n        else:\n            col_list.append(new_col_name)\n        # print(f'col: {col}, new: {new_col_name}')\n        df.rename(columns={col: new_col_name}, inplace=True)\n    return df\n</code></pre>"},{"location":"2-Data-Engineering/pandas/#plot-in-pandas","title":"Plot in Pandas","text":"<ul> <li>Params<ul> <li><code>kind</code>:str - The kind of plot to produce:<ul> <li>'line' : line plot (default)</li> <li>'bar' : vertical bar plot</li> <li>'barh' : horizontal bar plot</li> <li>'hist' : histogram</li> <li>'box' : boxplot</li> <li>'kde' : Kernel Density Estimation plot</li> <li>'density' : same as 'kde'</li> <li>'area' : area plot</li> <li>'pie' : pie plot</li> <li>'scatter' : scatter plot (DataFrame only)</li> <li>'hexbin' : hexbin plot (DataFrame only)</li> </ul> </li> </ul> </li> </ul> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\n# ...\ndf.plot(kind = 'scatter', x = 'Duration', y = 'Calories')\nplt.show()\n</code></pre>"},{"location":"2-Data-Engineering/pandas/#pandasdataframequery","title":"pandas.DataFrame.query","text":"<ul> <li>pandas.DataFrame.query</li> <li>pass where condition</li> </ul> <pre><code>df_new = df.query('sales != 0')\n</code></pre>"},{"location":"2-Data-Engineering/pandas/#pandasdataframedrop","title":"pandas.DataFrame.drop","text":"<ul> <li>pandas.DataFrame.drop</li> <li>is used for dropping whole columns or rows</li> <li>Params<ul> <li><code>labels</code> - list of index or column label, that is, index of row or column name.</li> <li><code>axis</code> - 0 for row, 1 for columns. default=0 or row.</li> </ul> </li> <li>Returns<ul> <li>DataFrame</li> </ul> </li> </ul> <pre><code>df.drop(labels=['Employee_Name2'], axis=1) # deletes column\ndf.drop([0, 1]) # deletes row at index 0 and 1, that is, first and second row.\n</code></pre>"},{"location":"2-Data-Engineering/pandas/#pandasto_datetime","title":"pandas.to_datetime","text":"<pre><code>df['date'] = pd.to_datetime(df['date_'], format='%d %b %Y')\ndf['load_datetime'] = pd.to_datetime(arg='now', utc=True)\n</code></pre> <ul> <li>Links:<ul> <li>https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf</li> <li>Towardsdatascience - 15 Ways To Create A Pandas Dataframe</li> </ul> </li> </ul>"},{"location":"2-Data-Engineering/python-data-wrangling/","title":"Python Data Wrangling","text":"<p>Pandas is a package in Python that can be used for data manipulation.</p>"},{"location":"2-Data-Engineering/python-data-wrangling/#what-is-data-manipulation","title":"What is Data Manipulation?","text":"<p>Data manipulations can be organized around six key verbs:</p> <ul> <li>arrange: order dataframe by index or variable or sort the data</li> <li>select: choose a specific variable or set of variables or select columns in data</li> <li>filter: subset a dataframe according to condition(s) in a variable(s) or select rows in data</li> <li>mutate: transform dataframe by adding new variables or add a calculated column</li> <li>group_by: create a grouped dataframe</li> <li>summarize: reduce variable to summary variable (e.g. mean)</li> </ul> <p>Here, variable is a column in data set.</p> <p>We'll cover how to perform above operations on a dataset using Pandas.</p>"},{"location":"2-Data-Engineering/python-data-wrangling/#quickest-data-in-pandas","title":"Quickest data in pandas","text":"<pre><code>text = '''colA colB\nJan 239\nFeb 234\n'''\n\nfrom io import StringIO\nimport pandas as pd\npd.read_csv(StringIO(text),delimiter=' ')\n</code></pre>"},{"location":"2-Data-Engineering/python-data-wrangling/#filter","title":"Filter","text":"<p>We can filter data to get a set of rows from complete dataset. It is similar to <code>WHERE</code> clause in SQL.</p>"},{"location":"2-Data-Engineering/python-data-wrangling/#doing-same-stuff-using-r","title":"Doing same stuff using R","text":"<p>R is also an excellent programming language for data manipulation. <code>dplyr</code> is a package in R that can be used to perform above operations.</p> <p>An excellent article by Ben, The 5 verbs of dplyr, can provide you more details on this.</p> <p>Another article that compares R and Python can be found here.</p> <p>Comparison of Pandas with SQL</p> <p>Pandas docs excellent details with examples.</p>"},{"location":"2-Data-Engineering/tableau/","title":"Tableau","text":"<p>Tableau is a data analysis and visualization tool.</p>"},{"location":"2-Data-Engineering/tableau/#date-calculations","title":"Date Calculations","text":"<p>Here are some basic common calculations that help in making KPIs and easy working with dates to find YOYs and MOMs</p> <ul> <li>Month-Year to Business Date <code>DATEPARSE('yyyy-MM-dd',[Business Month]+'-01')</code></li> <li>Month-Year to Business Year - <code>Left([Business Month],4)</code></li> <li>Max Date - <code>{MAX([Business Date])}</code></li> </ul> <ul> <li>Monthly</li> </ul> <pre><code>IF ( DATEDIFF('month', [Business Date], [Max Date], 'monday') = 0  AND DAY([Business Date])&lt;=DAY([Max Date]))\nTHEN 'Current Month'\nELSEIF ( \n    DATEDIFF('month', [Business Date], [Max Date], 'monday') = 1\n    //AND\n    //DAY([Business Date])&lt;=DAY([Max Date])\n)\nTHEN 'Last Month'\nELSEIF ( \n    DATEDIFF('month', [Business Date], [Max Date], 'monday') = 12 \n    // AND DAY([Business Date])&lt;=DAY([Max Date])\n)\nTHEN 'Last Year Month'\nEND\n</code></pre> <ul> <li>Rolling 13 months</li> </ul> <pre><code>[Business Date] &gt; DATEADD('month',-13,{MAX([Business Date])})\nand\n[Business Date] &lt;= {MAX([Business Date])}\n</code></pre> <ul> <li>yearly</li> </ul> <pre><code>IF ( DATEDIFF('year', [Business Date], [Max Date], 'monday') = 0   AND MONTH([Business Date])&lt;=MONTH([Max Date]))\nTHEN 'Current Year'\nELSEIF ( \n    DATEDIFF('year', [Business Date], [Max Date], 'monday') = 1  \n    AND ( \n            (  MONTH([Business Date]) &lt;= MONTH([Max Date])  ) \n            //OR \n            //( \n                // ( MONTH([Business Date])=MONTH([Max Date]) ) AND ( DAY([Business Date])&lt;=DAY([Max Date]) ) \n            //)\n        )\n)\nTHEN 'Last Year'\nEND\n</code></pre> <ul> <li>Current Month</li> </ul> <pre><code>SUM(IIF([Monthly]=='Current Month',[Closed Customers],NULL))\n</code></pre> <ul> <li>MOM</li> </ul> <pre><code>(\n    SUM(IIF ([Monthly] == 'Current Month',[Users],0)) - \n    SUM(IIF ([Monthly] == 'Last Month',[Users],0) ) \n)\n/\nSUM(IIF ([Monthly] == 'Last Month',[Users],0))\n</code></pre> <ul> <li>MOM up <code>IF [Closed Customers MOM] &gt; 0 THEN \"\u25b2\" END</code></li> <li>MOM Down <code>IF [Closed Customers MOM] &lt;= 0 THEN \"\u25bc\" END</code></li> </ul> <ul> <li>YOY</li> </ul> <pre><code>( SUM(IIF ([Yearly] == 'Current Year',[Closed Customers],0)) - SUM(IIF ([Yearly] == 'Last Year',[Closed Customers],0) ) )\n/\nSUM(IIF ([Yearly] == 'Last Year',[Closed Customers],0))\n</code></pre> <p>Above formulas are handy quickly build frequent calculations.</p>"},{"location":"2-Data-Engineering/tableau/#formatting","title":"Formatting","text":"<ul> <li>YOY Up <code>IF [Closed Customers YOY] &gt; 0 THEN \"\u25b2\" END</code></li> <li>YOY Down <code>IF [Closed Customers YOY] &lt;= 0 THEN \"\u25bc\" END</code></li> <li>YTD <code>SUM(IIF([Yearly] == 'Current Year',[Closed Customers],0))</code></li> </ul> <p>KPI Format Percentages</p> <ul> <li>small arrows <code>0%\u202f \u2bc5; -0% \u2bc6; 0%\u2800\u2800;</code> \u2bc7 \u2bc8 \u2bc5 \u2bc6</li> <li>\u2b9c \u2b9e \u2b9d \u2b9f <code>0%\u202f \u2b9d; -0% \u2b9f; 0%\u2800\u2800;</code> \\(U+2800\\)</li> <li>\ud83e\udc44 \ud83e\udc46 \ud83e\udc45 \ud83e\udc47 <code>0%\u202f \ud83e\udc45; -0% \ud83e\udc47; 0%\u2800\u2800;</code></li> <li>more arrows - http://xahlee.info/comp/unicode_arrows.html</li> </ul> <p>TUG Austia - https://github.com/tableau/community-tableau-server-insights - ready made events</p> <p>KPI Format Numbers</p> <p><code>#,##0\u202f \u25b2; #,##0 \u25bc; #,##0</code> add above to custom format</p> <p><code>+0; -0; 0</code> numbers with auto-sign</p> <p>Text Symbols for Legend and Info</p> <pre><code>\u2b1b - big box for legend\n\u24d8 - info\n</code></pre> <p>More on Alt Codes, compart</p> <p>Number Tweaks</p> <p>Number standardize between 0 and 1 per category for trend line colors</p> <pre><code>(\n    SUM([Value ]) - WINDOW_MIN(SUM([Value ])) \n)\n /\n(\n    WINDOW_MAX(SUM([Value ])) - WINDOW_MIN(SUM([Value ])) \n)\n</code></pre>"},{"location":"2-Data-Engineering/tableau/#tableau-js-embedded-api","title":"Tableau JS Embedded API","text":"<ul> <li>Everything starts by loading a <code>viz</code>, this can be a dashboard or a sheet. Viz gives you <code>workbook</code> and Async callback function.</li> <li><code>workbook</code> has <code>sheets</code>, but can have only one <code>activeSheet</code>, like active Tab. You can only do operations on activeSheet.</li> <li>Operations are <code>Async</code> and return a promise, so they can be chained.<ul> <li>Change Param  - <code>changeParameterValueAsync(\"param_name\", value)</code></li> <li>Change Filter - <code>applyFilterAsync(\"filter_name\", values, tableau.FilterUpdateType.REPLACE)</code></li> <li>Change ActiveSheet - <code>activateSheetAsync(\"sheet_name\")</code> this can be dashboard or sheet</li> <li>Get Data - <code>getSummaryDataAsync(data_options)</code> - returns table object with data and columns</li> </ul> </li> </ul> <p>Network calls are made when you call Async functions, else it is a JS execution only.</p> <p>Read Data:</p> <ul> <li>create sheet with all columns added to row pill.</li> <li>activate this sheet,</li> <li>then do <code>getData</code>,</li> <li>to skip cache, increment counter.</li> </ul> <p>Write-back:</p> <ul> <li>Add a proc as data source. Proc to have at least three inputs, Switch, Value and Counter.</li> <li>Create these params in workbook<ul> <li><code>switch</code><ul> <li>0 - no action</li> <li>1 - CREATE/insert</li> <li>2 - UPDATE</li> <li>3 - DELETE</li> </ul> </li> <li><code>psv</code> - pipe separated values</li> <li><code>counter</code> - increment it whenever you want Tableau to skip cache and call database server.</li> </ul> </li> <li>Create sheet <code>exec_proc</code>, whenever this sheet is activated, it will execute proc depending on the three params above.</li> </ul> <pre><code>async function execProcTabeauAsync(switch,psv) {\n    await workbook.changeParameterValueAsync(\"psv\", psv);\n    await workbook.changeParameterValueAsync(\"counter\", ++counter);\n    await workbook.changeParameterValueAsync(\"switch\", switch);\n    await workbook.activateSheetAsync(\"exec_proc\");\n    console.log('Action: ' + switch + '; Completed at: ' + new Date($.now()).toISOString());\n    return await workbook.changeParameterValueAsync(\"switch\", 0);\n}\n</code></pre>"},{"location":"2-Data-Engineering/tableau/#writeback","title":"Writeback","text":"<p>Parameters Required</p> <ul> <li><code>w_mega_string</code></li> <li><code>w_increment</code></li> <li><code>w_action_switch</code></li> </ul> <ul> <li>For each field to write<ul> <li><code>reset_field1</code></li> </ul> </li> </ul> <p>Create New</p> <ul> <li>Add sheet <code>add_button</code>, use <code>blank_db</code> having text \"+ Add new record\"</li> <li>Add sheet to dashboard</li> <li>Add actions<ul> <li>go to <code>create_record</code> sheet</li> <li>reset <code>field</code></li> <li>set param <code>action_switch</code> to 0</li> </ul> </li> </ul>"},{"location":"2-Data-Engineering/tableau/#links","title":"Links","text":"<ul> <li>Data Structuring for Analysis - https://help.tableau.com/current/pro/desktop/en-us/data_structure_for_analysis.htm</li> <li>Linkedin - Writeback to MS SQL using Proc</li> </ul>"},{"location":"3-Management-%26-Strategy/ajile-sprint-scrum/","title":"Agile, Scrum and Sprints","text":"<p>all about agile, scrum, sprints</p> <p>Agile is methodology that helps you move away from waterfall methodology of software development. Agility is a mindset. It helps adapt to changes which are inevital in software development.</p>"},{"location":"3-Management-%26-Strategy/ajile-sprint-scrum/#agile-approach","title":"Agile Approach","text":"<p>It is interactive and incremental, it not that you need to develop like manufacturing unit. It lets you develop interactivily with business users and lets you do incremental developments with entire development cycle involved in each increment and using best practices like TDD, XP, CI CD. Making flat structure and having a collective ownership. Having customer satisfaction as top priority. Keeping people over process, and always welcome change as it is inevitable. Frequent feedback from business people and developer is required and they work daily together. The measure is working software and value it brings. All this can be facilitated by Scrum.</p> <p>Following shows small batch of development lifecycle</p> <pre><code>graph LR\nsubgraph s1[Sprint 1]\nr1[Requirements] --&gt; a1[Analysis-Design] --&gt; d1[Development] --&gt; t1[Test] --&gt; m1[Deployment-Maintenance]\nend\n\nsubgraph s2[Sprint 2]\nr2[Requirements] --&gt; a2[Analysis-Design] --&gt; d2[Development] --&gt; t2[Test] --&gt; m2[Deployment-Maintenance]\nend\n\nsubgraph s3[Sprint 3]\nr3[Requirements] --&gt; a3[Analysis-Design] --&gt; d3[Development] --&gt; t3[Test] --&gt; m3[Deployment-Maintenance]\nend\n\ns1 --Release 1--&gt; s2 --Release 2--&gt; s3 --Release 3--&gt; fr[Final Product]\n</code></pre>"},{"location":"3-Management-%26-Strategy/ajile-sprint-scrum/#scrum","title":"Scrum","text":"<p>Scrum is a framework that facilitates development in an agile way, it is not a methodology. Scrum has three pillars on which Scrum values are defined. Empiricism \u2013 don't predict but keep adjusting based on situations. Scrum makes it possible. Feature over document.</p> <p>The three pillars are:</p> <ul> <li>Transparency - be transaparent with work and team members. share progress and artifacts</li> <li>Inspection - have boards, dashboards ready to inspect</li> <li>Adaptation - adapt to change based on inspection, what needs to be corrected.</li> </ul> <pre><code>graph TD;\n\nsubgraph s1[Scrum - The three Pillars TIA]\n  Transparency\n  Inspection\n  Adaptation\nend\n\ns1 --&gt; Focus\ns1 --&gt; Commitment\ns1 --&gt; Courage\ns1 --&gt; Openness\ns1 --&gt; Respect\n</code></pre>"},{"location":"3-Management-%26-Strategy/ajile-sprint-scrum/#scrum-roles","title":"Scrum Roles","text":"<p>Three team roles \u2013 product owner, scrum master, developers</p> <ul> <li>Product Owner<ul> <li>A person who created backlog and shows vision of stakeholder</li> <li>Build backlog and prioritise</li> </ul> </li> </ul> <ul> <li>Development Team \u2013 3-9 members, flat structure, all own everything, collective ownership</li> </ul> <ul> <li>Scrum Master \u2013 make product owner and developers align to scrum practise, solves conflicts, coaches individuals to implement scrum</li> </ul>"},{"location":"3-Management-%26-Strategy/ajile-sprint-scrum/#scrum-events","title":"Scrum Events","text":"<ul> <li>Sprint \u2013 is time bound container event</li> </ul> <ul> <li>Sprint Planning \u2013 what to work on and how<ul> <li>Ahead of our sprint planning:<ul> <li>Please update your capacity for the next sprint. link</li> <li>Please create, estimate and assign tasks with \"definition of done\" DoD</li> <li>Update tasks in current sprint.</li> </ul> </li> <li>Meeting for 2-4 hours for 2 week sprint</li> <li>Know Memebers, their role, capacity and leaves. Ideally tabular.</li> <li>In Last Sprint<ul> <li>Ensure all tasks are updated - in review, done or whatever</li> <li>End the sprint and move all unfinished tasks to backlog or new sprint</li> </ul> </li> <li>Start &amp; End Date - know and update it</li> <li>Sprint Goal(s) - have predefined and refine at end</li> <li>Tasks to have<ul> <li>Acceptace Criteria</li> <li>Definition of Done - covers product coming out of the sprint</li> <li>Estimated and assigned</li> <li>Reestimate tasks carried forward</li> </ul> </li> </ul> </li> </ul> <ul> <li>Daily Scrum \u2013 keep on track</li> </ul> <ul> <li>Sprint Review \u2013 reviews work</li> </ul> <ul> <li>Sprint Retrospective \u2013 discuss to be more effective<ul> <li>What did we do well?</li> <li>What should we have done better?</li> <li>actions to take based on \"What should we have done better\"</li> <li>actions taken from last retro actions? else carry them</li> <li>Learnings<ul> <li>don't under estimate tasks</li> <li>keep buffer capacity for meetings/PR-requests</li> </ul> </li> </ul> </li> </ul> <ul> <li>Backlog Grooming / Refinement (unofficial event) \u2013 break large items into small implementable items<ul> <li>Break stories into smaller tasks</li> <li>Tasks have \"Definition of Ready\" DoR - covers requirements coming into the sprint</li> <li>Tasks are prioritized, estimated</li> <li>Tasks may get assigned</li> <li>1-2 hour productive meeting</li> <li>link</li> </ul> </li> </ul> <ul> <li>Backlog Grooming vs Sprint Planning<ul> <li>Scope - BG looks at entire project for months, SP looks at near future for weeks</li> <li>Grain - BG breaks into tasks, SP breaks in to sub-tasks</li> <li>Detail - BG adds DoR, SP adds DoD</li> </ul> </li> </ul>"},{"location":"3-Management-%26-Strategy/ajile-sprint-scrum/#scrum-artifacts","title":"Scrum Artifacts","text":"<p>They are either done or not done</p> <ul> <li>Product backlog \u2013 has large and small tasks, small can be picked, large are broken in grooming<ul> <li>Requirements</li> <li>Enhancement requests</li> <li>Defects</li> <li>User stories</li> <li>New feature</li> </ul> </li> </ul> <ul> <li>Sprint backlog</li> </ul> <ul> <li>Product increment \u2013 done version, product itself, has value and is usable, not necessarily a release but is polished enough to be shippable.</li> </ul>"},{"location":"3-Management-%26-Strategy/ajile-sprint-scrum/#excellence-in-development","title":"Excellence in Development","text":"<p>Excellence in development help align with scrum principles like flat structure, collaborative ownership, welcoming change and frequent releases. One such methodology is Extreme Programming (XP) . It is standards that lets program effectively. Agile team combine XP programming with scrum to be highly productive. It has following ways of working:</p> <ul> <li>Execution \u2013 small tasks in scrum, whole team is accountable, readily changeable</li> </ul> <ul> <li>Incremental Design \u2013 not all at once, by one by one, refactoring code. Code 10 mins and run test \u2013 it gives quick feedback on quality.</li> </ul> <ul> <li>Pair Programming \u2013 two individual on one machine,  one types another reviews and suggested and they swap<ul> <li>Instant peer review, improved code quality</li> <li>Knowledge sharing \u2013 helps become T-shape knowledge than I-shaped</li> <li>Inclusiveness, more interactions, less distractions.  </li> </ul> </li> </ul> <ul> <li>Test Driven Development - TDD<ul> <li>do not code until you fail a test</li> <li>First write a test to fail, then code to pass the test, then improve the test to fail and complete the code to pass. Repeat it.</li> <li>Think of test before implementation</li> <li>There are tools to mock dependencies to function, it makes tests easy.</li> <li>Coverage is good to be 100%.</li> <li>Removes bugs in monolithic code, in early stage</li> </ul> </li> </ul> <pre><code>graph LR;\n\na[write a failing test] --&gt; b[make the test pass] --&gt; c[refactor] --&gt; a\n</code></pre> <ul> <li>DevOps<ul> <li>Problem \u2013 release bringing changes that can break prod. Agile brings frequent releases, hence frequent break in prod.</li> <li>Solution \u2013 a methodology that brings, development, QA and IT operations close together, automated and test driven, container based to have isolated similar environment so that the changes are minimal and are tested and hence don\u2019t break.</li> <li>Continuous integration \u2013 commit frequently, trigger build and test automatically to identify risks</li> <li>Continuous delivery and deployment \u2013 code is delivered and deployed continuously in prod</li> <li>Devops and Agile are mindset shift that remove hand-off in teams and bring them together with collective responsibility using automation in tests and builds.</li> </ul> </li> </ul> <pre><code>graph LR;\n\ncode --&gt; build --&gt; test --&gt; release --&gt; deploy --&gt; operate --&gt; monitor --&gt; plan --DevOps--&gt; code\n</code></pre>"},{"location":"3-Management-%26-Strategy/ajile-sprint-scrum/#user-story-tasks","title":"User Story / Tasks","text":"<ul> <li>User Story - it is user requirement in sticky note \u2013 having requirement in form \"As a \u2026 I want\u2026 so that I .. . Done when..\", where<ul> <li><code>As a..</code> - user role who will be benefited</li> <li><code>I want...</code> - what feature or outcome</li> <li><code>So that I...</code> \u2013 reason why this needs to be done</li> <li><code>Done when...</code> - acceptance criteria</li> <li>User story is a promise, not a contract. It can be small or large task or an epic.</li> </ul> </li> </ul> <ul> <li>Epic \u2013 large user story, a process to large to be estimated</li> </ul> <ul> <li>Themes \u2013 groups epic with tags</li> </ul> <ul> <li>Estimations<ul> <li>absolute it days, hours</li> <li>relative is estimating based on story relative to other stories. Use fibrinocci \u2013 1,2,3,5,8,13. or exponential scale. 1,2,4,8,16. Or t-shirt size. Xs,s,m,l,xl.</li> <li>Planning poker \u2013 lets users share a score. Have discussion based on score, why 3 or 8. Play again to get closer score. Discuss again, until you get same score from team members</li> </ul> </li> </ul>"},{"location":"3-Management-%26-Strategy/ajile-sprint-scrum/#agile-reports","title":"Agile reports","text":"<ul> <li>Burndown \u2013 work left and time to do</li> </ul> <ul> <li>Burnup \u2013 work done and time to do</li> </ul> <ul> <li>Cumulative  Flow Diagram \u2013 CFD \u2013 shows work done by state \u2013 to-do, in progress, done. Mostly used in Kanban, show bottleneck like acceptance is taking more time or developing, or delivering.</li> </ul>"},{"location":"3-Management-%26-Strategy/ajile-sprint-scrum/#kanban","title":"Kanban","text":"<ul> <li>Kanban \u2013 lean manufacturing principles like cars. Work process management methodology<ul> <li>Little\u2019s law = work in progress L = completion rate (\\(\\lambda\\)) X cycle time (W)</li> <li>\\(L = \\lambda \\times W\\)</li> <li>Visual mode to track.</li> <li>WIP is limited as we are not good multi-taskers. Every stage has a WIP limit. Like you can have on 2 tasks in progress, or 4 in review, the other tasks can come in only if the previous ones are done.</li> </ul> </li> </ul> <ul> <li> How to use Kanban or Agile for personal management.</li> </ul>"},{"location":"3-Management-%26-Strategy/ajile-sprint-scrum/#jira-atlassian","title":"Jira - Atlassian","text":"<p>Jira is a work management tool. We can create <code>Project</code>, which can have <code>issues</code>. Issues can be in <code>backlog</code> or can be part of <code>sprint</code>.</p> <p>Issues mostly have:</p> <ul> <li>Summary - one liner</li> <li>Type - task / story / subtask / bug / epic</li> <li>Description - As a.. I want to.. so that.. - has definition of done</li> <li>Reporter - Person who creted this</li> <li>Assignee - Person who will do this</li> <li>Status - Backlog / ToDo / In Progress / In Review / Done</li> <li>Epic Link - Broader work</li> <li>Story points - Estimate of duration</li> <li>Linked Issue - Dependencies / blocker</li> <li>Priority - Trivial / Critical / High / Medium / Low</li> <li>Version/Release</li> </ul> <p>Issues can be arranged and managed by versions / epics / sprints. Hierarchy is by portfolio outcome, business outcome, epic, task, sub-task.</p> <p>Boards are used to displays issues and to track progress of project(s). Kanban is simple board, shows tasks on board with swimlanes and state. Also</p> <ul> <li>Scrum is Ajile board concentrated on backlog and Sprints.</li> <li>Dashboards can display activity, filters, boards etc</li> <li>Filters can be created and shared. They have search criteria, can add JQL (Jira Query Language) to it.</li> <li>Project - can have confluence page to have documentation of project</li> </ul>"},{"location":"3-Management-%26-Strategy/ajile-sprint-scrum/#links","title":"Links","text":"<ul> <li>Your step-by-step guide to running a Sprint Planning meeting effectively</li> <li>Gantt Charts in PowerPoint - simple easy</li> </ul>"},{"location":"3-Management-%26-Strategy/project-management/","title":"Project Management","text":"<p>Project Management is balancing between \"the project scope\" and \"the time, resources and budget\" you have. Manage \"the time, resource and budget\" to cover \"the project scope\". It includes planning process like requirement gathering, planning, solution design, code, test, deploy. We need to break down the project into doable work tasks. Once broken, estimate them. Finally assign them to get started. It can include the steps below</p>"},{"location":"3-Management-%26-Strategy/project-management/#steps-for-end-to-end-project-management","title":"Steps for End to End Project Management","text":""},{"location":"3-Management-%26-Strategy/project-management/#1-initiation-and-ideation","title":"1 Initiation and Ideation","text":"<ul> <li>Business Requirement Gathering</li> <li>What are you solving and how will you solve it? The business use case.</li> <li>Define use case, scope and expectations. This is very high level and covers the business problem.</li> <li>Project should have a definite end - a product or a service. It should have definition of done.</li> <li>Above can result in <code>Business Requirements Document</code></li> </ul>"},{"location":"3-Management-%26-Strategy/project-management/#2-defining-goals-and-objectives","title":"2 Defining - Goals and Objectives","text":"<ul> <li>What needs to be done? How it can be done? Define the goal and objectives.</li> <li>Goal - should clearly and simply define a state considering most important factors.</li> <li>Objectives - they sould be specific, measurable, achievable, realistic and time-related. Documented. Also specify the category of aobjective:<ul> <li>qualitative - improve experience. measure by survey rating</li> <li>financial - inc revenue by 15%</li> <li>operational - reduce number of notifications</li> </ul> </li> <li>Definition of Done. This will help breaks down the problem into sub-tasks and defines what is expected.</li> <li>Above can shape the <code>High Level Document</code></li> <li>Identify the Stakeholders and in the document add their:<ul> <li>objectives requirements and interests</li> <li>contribution</li> <li>what are they concerned about</li> <li>their line mangers, eg, if you need somoeone from finance team then take approval from their line managers.</li> </ul> </li> <li>Be precise, take what matters and drop what doesn't. Clearly define what is in and out of scope, write it down.</li> <li>It can take several rounds with stakeholders to get correct objectives and scope.</li> </ul>"},{"location":"3-Management-%26-Strategy/project-management/#3-planning-choose-a-strategy","title":"3 Planning - Choose a Strategy","text":"<ul> <li>Brainstorm with group and let ideas flow in.</li> <li>Write possible options to achieve an objectives. Then pick one of the option that covers all scenarios and meets all objectives and goals.</li> <li>Considerations to be made<ul> <li>is the strategy feasible, achievable?</li> <li>are the risks acceptable: security, load balancing, new technologis challenges etc?</li> <li>culture - does it fit the org pattern?</li> </ul> </li> <li>above can shape <code>Low Level Document</code></li> <li>Get a F2F sign-off here, over email and treat it as approved.</li> </ul>"},{"location":"3-Management-%26-Strategy/project-management/#4-solution-design-modules-tasks-sub-tasks-deliverables","title":"4 Solution Design - Modules, Tasks, Sub-Tasks, Deliverables","text":"<ul> <li>Break the objectives/goals in to <code>modules</code>,  <code>technical work tasks</code> and <code>sub-tasks</code> using the strategy and define <code>deliverables</code>.</li> <li>Tasks - Add business understandings, definitions and calculations.<ul> <li>Sub-task - Add definition of done. Managable and doable tasks.</li> <li>Deliverables - Identify them, clearly and quantifiably measure them</li> </ul> </li> <li>Define Scenarios and map expectations - the above tasks should cover all scenarios and expectations.</li> <li>This sould define the <code>Software Design Document</code> can be planned on Jira and documented on Confluence.</li> <li>Take a technical sign-off and approval if required.</li> </ul>"},{"location":"3-Management-%26-Strategy/project-management/#5-delivery-plan-estimate-assign","title":"5 Delivery Plan - Estimate Assign","text":"<ul> <li>Arrange work tasks in sequence, link them with dependencies, add duration.</li> <li>Resource Allocation - assign the tasks to resources. These can be done in Jira. Look out for blockers and unavailability.</li> <li>Make a realistic schedule - include holidays, dependencies.</li> <li>Deadlines - Management can set a deadline, you need to adjust schedule to meet it. Add resource, or break into phase. Do phase analysis - define the MVP to deliver early. Do Phase II enhancements etc.</li> <li><code>Gantt Chart</code> - optional reporting.</li> </ul>"},{"location":"3-Management-%26-Strategy/project-management/#6-risk-assessment-clarify-assumptions","title":"6 Risk Assessment - Clarify Assumptions","text":"<ul> <li>Avoid risks that are based on assumptions, like someone will do the deployment, access would work.</li> <li>Will the business stop if solution is down? What if resource not available?</li> <li>What if data gets corrupt?</li> </ul>"},{"location":"3-Management-%26-Strategy/project-management/#7-other-optional-documentsplans","title":"7 Other optional documents/plans","text":"<ul> <li>Budget - Add costs, include resources, softwares.</li> <li>Communication plan - scrums, weekly, daily</li> <li>Change Management Plan - approvals, what changes when, impact</li> <li>Procurement plan - to buy software, resources, contracting</li> </ul>"},{"location":"3-Management-%26-Strategy/project-management/#8-execution-development","title":"8 Execution - Development","text":"<ul> <li>Write code and document it.</li> <li>Do pilot delivery - a quick delivery and test. If works, keep expanding by adding features.</li> <li>Monitoring and Controlling - evaluate and get it back on track if lagging or deviated.</li> <li>Keep unit testing the code.</li> <li><code>Deliverables</code> - code files, reports, documents. All should be in one place and version controlled.</li> </ul>"},{"location":"3-Management-%26-Strategy/project-management/#9-qa-prepration-testing","title":"9 QA Prepration - Testing","text":"<ul> <li>Make test cases and test scenarios as you go.</li> <li>Identify Testers from Stake Holders.</li> <li>Make a <code>bug tracker</code> where any one can report bugs and it can be tracked.</li> <li><code>QA Document</code> - add test cases and their results.</li> <li>Get a UAT Sign Off of the deliverables.</li> </ul>"},{"location":"3-Management-%26-Strategy/project-management/#10-deployment","title":"10 Deployment","text":"<ul> <li>Deploy in prod. Test it.</li> <li>Prod Env is secured and hence may require many access permissions. Please see this in advance.</li> <li>Change Management may be required here.</li> <li>Once deployed, do a Prod Testing.</li> <li>Finally release the product</li> </ul>"},{"location":"3-Management-%26-Strategy/project-management/#11-handover-user-training-socialisation","title":"11 Handover - User Training Socialisation","text":"<ul> <li>Prepare a <code>Training Guide</code> - for end users. This can be video as well.</li> <li>Make a <code>Handover Document</code> - if this need to be handed over to maintenance team to work on manual tasks.</li> <li>Manual tasks - include scope, work required, frequency, risks etc.</li> <li>Contracts - get signed-off if required.</li> </ul>"},{"location":"3-Management-%26-Strategy/project-management/#best-practices","title":"Best Practices","text":"<ul> <li>Deliverables and Documentation - All files and documentation at one place and all have access. Confluencem, Jira Boards and Shared Drives.</li> <li>Estimation - Set clear goals. Manage Workload.</li> <li>Communication - 1-1 meetings weekly. Daily Updates.</li> <li>Change Management - if change is required in between, follow a change management procedure and redo all steps and sign offs.</li> <li>Reviews - Doc, deliverable. Continuous review helps.</li> <li>Opennes - Be open, let them choose tool, let them choose way, keep them foucsed. Trust them.</li> <li>Risk Management - each team member is accountable to explain risk in their task to entire team. Don't let people assume the work, ask them and clarify it.</li> </ul>"},{"location":"3-Management-%26-Strategy/project-management/#documentation","title":"Documentation","text":"<p>Step 1: Plan the documentation</p> <p>Step 2: Prepare the document</p> <p>Great user documentation should include:</p> <ul> <li>Plain language</li> <li>Simplicity</li> <li>Visuals</li> <li>A focus on the problem</li> <li>A logical hierarchy and flow</li> <li>A table of contents</li> <li>Searchable content</li> <li>Accessible content</li> <li>Good design</li> <li>Feedback from real users</li> <li>Links to further resources</li> </ul> <p>Step 3: Test the document</p> <p>Step 4: Keep it upated.</p>"},{"location":"3-Management-%26-Strategy/project-management/#spinx","title":"Spinx","text":"<ul> <li>Sphinx is the de-facto documentation tool for Python.</li> <li>version controlled, sourced from repo</li> <li>read everywhere, confluence, wiki, PDF</li> <li>Also lets document functions and classes</li> </ul>"},{"location":"3-Management-%26-Strategy/project-management/#talk-product-strategy-systems-and-frameworks-with-sachin-rekhi","title":"Talk - Product Strategy, Systems, and Frameworks with Sachin Rekhi","text":"<ul> <li>Sachin built <code>LinkedIn Sales Navigator</code>, $200m in 1.5yr, 0-500 employee</li> <li>Learn to write and sell code.</li> <li>How to be in Product Management path?<ul> <li>Adjacent role, keep coding and start managing the product. Add values, show interest, then keep moving to product role.</li> <li>Be a domain expert, like expert in sales tech, expert in education-tech, med-tech, sports-tech.</li> </ul> </li> <li>Sales Navigator story<ul> <li>he built connected, personal CRM, it was acquired by linkedIn.</li> <li>showed delivering product quickly, initial traction, showed internal credibility in linkedin, then got the bigger and riskier bet.</li> <li>Credibility and Social-Capital is required to take new bigger opportunities.</li> <li>Share your aspirations with manager, but show credibility too for that.</li> </ul> </li> <li>Strategy to build a product - product should answer these quesitons, and in a compelling way<ul> <li>what is the problem you are solving? separate from solution? what is the pain? Exact knowledge helps</li> <li>who is the audience, as psecific as possible. understand exactly who the are, mroe specific more success</li> <li>value, benifit from solcution</li> <li>competitive, better than competitors, why? who will compete in long term and short term.</li> <li>growth strategy, how to get customers?</li> <li>buiness model, profit?</li> </ul> </li> <li>how to make it compelling?<ul> <li>does the problem <code>resonates with audience</code>?</li> <li>business model has <code>growth strategy</code>?</li> <li>have strong interplay in between these, specially when starting new product.</li> </ul> </li> <li>Growth Startegy<ul> <li>paid ads, then is customer giving that value, think of LTV, <code>lifetime value</code> or <code>customer acquisition cost</code>, CAC. Have greater LTV to afford CAC.</li> <li>product should be expensive enough to support sales team.</li> </ul> </li> <li>Framework / process for buy in (convincing)<ul> <li>get team of 8-10 engg, work as a venture</li> <li>do <code>prod research</code>, come with PPT, having screenshots and client feedback</li> <li>have detailed <code>compelling customer feedback</code>, is it compelling?</li> <li>have <code>convincing facts</code> for capitalists</li> <li>6 convincing style<ul> <li>framing - narrative in a way, set a context</li> <li>goal seek - align to their goals</li> <li>citation - ab tests, voice of customer</li> <li>narration - compelling story</li> <li>find which style will work</li> </ul> </li> </ul> </li> <li>Entrepreneur journey<ul> <li>idea of product came form office work<ul> <li>we dont have info we need, 90% info is not in wiki confluence, hence <code>notejoy</code></li> </ul> </li> <li><code>earned secrets</code>, give you start up ideas</li> </ul> </li> <li>Hurdles<ul> <li>fewer resources, no research team, less marketing team</li> <li>no client base, make your own customers, build growth strategy from day 1, share virally</li> </ul> </li> </ul>"},{"location":"3-Management-%26-Strategy/project-management/#links","title":"Links","text":"<ul> <li>Steps https://www.wrike.com/blog/foolproof-project-plan/</li> <li>Detailed https://www.smartsheet.com/content/software-project-management</li> <li>LinkedIn Learning - https://www.linkedin.com/learning/project-management-foundations-4</li> <li>PMP Certification https://www.pmi.org/certifications/project-management-pmp</li> <li> SAFe POPM</li> <li>TOGAF https://www.opengroup.org/togaf</li> </ul>"},{"location":"4-Functional-%26-Business/banking-finance-notes/","title":"Banking and Finance","text":"<p>All about banking and finance</p>"},{"location":"4-Functional-%26-Business/banking-finance-notes/#how-banks-make-money","title":"How Banks make money?","text":"<p>Banks makes money by following methods:</p> <ol> <li>Investment Banking: Most of the income The BarBank make is by being an advisor to big corporations, government and individuals.</li> <li>Corporate Banking: It provides services for multinationals and large domestic corporate. It charges fee, interests for loans from these companies.</li> <li>By charging interest from borrowers.</li> <li>By charging various fees on services like,   a.  Account Fees: For different products in bank, it charges fee for maintenance and services.   b.  ATM fees: This is charged for ATM usage outside limit and when using other bank\u2019s ATM for services.   c.  Penalty charges: This can be for late credit card payment or missing a loan EMI.   d.  Commissions: They provide asset management for which brokers ask for commission.</li> <li>Merchant Service Charges: The BarBank help merchants accept payments. For this bank charges fee for services. It also helps merchant by data analysis for this as well it charges other fees for value added services.</li> </ol>"},{"location":"4-Functional-%26-Business/banking-finance-notes/#net-interest-margin-nim","title":"Net Interest Margin (NIM)","text":"<ul> <li>(Interest Earned) - (Interest Paid).</li> <li>NIM% = NIM / (Avg earning asset), where</li> <li>AEA = Average earning asset = (Assets at the beginning of the year + Assets at the end of the year) / 2.</li> <li>The difference between the interest income generated by banks  and the amount of interest paid out to their lenders (for example, deposits), relative to the amount of their (interest-earning) assets.</li> </ul>"},{"location":"4-Functional-%26-Business/banking-finance-notes/#retail-banking-consumer-banking","title":"Retail banking (Consumer Banking)","text":"<ul> <li>Individual consumers can manage their money, take credit, and deposit their money</li> <li>Services include checking and savings accounts, mortgages, personal loans, credit cards, FD/RD.</li> </ul>"},{"location":"4-Functional-%26-Business/banking-finance-notes/#income-tax","title":"Income TAX","text":"<ul> <li>Form 26as has information on FD interest</li> </ul>"},{"location":"4-Functional-%26-Business/banking-finance-notes/#credit-risk-management","title":"Credit Risk Management","text":"<p>Credit risk management is the practice of mitigating losses by understanding the adequacy of a bank's capital and loan loss reserves at any given time \u2013 a process that has long been a challenge for financial institutions.</p>"},{"location":"4-Functional-%26-Business/banking-finance-notes/#tick-data-trade-data","title":"Tick Data (Trade Data)","text":"<p>Tick-by-tick data, often referred to as \"trade data\" or \"tick data\" or \"market data\", is a type of financial market data that records every individual transaction (trade) that occurs for a particular financial instrument, such as a stock, commodity, or currency pair. It is the most granular level of market data and provides detailed information about each trade, including the price at which the trade occurred, the quantity of assets bought or sold, and the exact timestamp of the trade.</p> <p>Tick Data is trade data, often know as \"tick-by-tick\" or \"market data\". It records each transaction that occurs for a financial instrument (stocks/commodity/currency etc). It is most granular market data. It has detailed information about each trade like price, quantity bought or sold, timestamp etc.</p> <p>It can be millions in milliseconds. There are special databases and frameworks to deal with this velocity and volume of data, some of them are arcticdb.</p>"},{"location":"4-Functional-%26-Business/banking-finance-notes/#book-building","title":"Book Building","text":"<p>Book Building is the process by which an underwriter determines the price at which the shares must be sold in an Initial Public Offer (IPO)</p>"},{"location":"4-Functional-%26-Business/business-analytics/","title":"Business Analytics","text":"<p>What is business analytics (BA)?</p> <ul> <li>A continuous and iterative exploration of past business performance.</li> <li>Gain insight on the past performance</li> <li>Get understanding of things that happened, improved or declined.</li> <li>Help make data driven decision making.</li> <li>Find patterns and reasons, understand customer behaviour.</li> <li>Help make adjustments to business activities and improve business outcomes.</li> </ul> <p>Business analytics compared</p> <ul> <li>Data Engineering<ul> <li>Integrate Data Sources</li> <li>Build Data Pipelines</li> <li>Process and Transform</li> <li>Store Data</li> </ul> </li> </ul> <ul> <li>Business Analytics / Intelligence<ul> <li>Dashboards/Reports</li> <li>Exploratory Analytics</li> <li>Statistical Modelling</li> <li>Machine Learning</li> <li>Business Actions</li> </ul> </li> </ul> <p>In Data Science, you do everything above.</p> <p>Stages of business analytics</p> <p>Descriptive - What happened? Exploratory - What is going on? Explanatory - Why did it happen? Predictive - What will happen? Prescriptive - How do I take advantage? Experimental - How well will it work?</p> <p>Business analytics process</p> <ul> <li>You move data from OLTP to OLAP. Build a data warehouse and data marts as required, then on top of it Analysis is done.</li> </ul>"},{"location":"4-Functional-%26-Business/business-analytics/#use-cases","title":"Use Cases","text":"<p>Heart Decease</p> <p>For an individual, it has measures like blood pressure, cholesterol, and sugar.</p> <ul> <li>Data<ul> <li>Age</li> <li>Age Group - in increment of 20</li> <li>Weight</li> <li>Weight Group - in increment of 50</li> <li>Sex - Male or Female</li> <li>Chest Pain Type - 1/2/3/4</li> <li>Resting BP</li> <li>Cholesterol</li> <li>High Sugar - 0/1</li> </ul> </li> </ul> <p>Email Campaign</p> <p>Email campaigns and conversions</p> <ul> <li>Computer business sends emails to customers about products</li> <li>Offer different discount rates from time to time</li> <li>Email readers click to view products and buy them</li> <li>One year of emails are used for analysis</li> <li>Captures details of the offers, customer types, and others</li> <li>For ease of use, size of data set is small</li> </ul> <ul> <li>Data<ul> <li>Offer Date - Date email was sent</li> <li>Offer Weekday - Day email was sent</li> <li>Offer Month - Month email was sent</li> <li>Product - Name of product</li> <li>Price - Price of product</li> <li>Discount Offered - Discount % offered</li> <li>Customer ID - Unique customer ID</li> <li>Gender - Male or Female</li> <li>Age - Age range</li> <li>Earning - Range in 1,000s</li> <li>Convert - 1/0</li> </ul> </li> </ul>"},{"location":"4-Functional-%26-Business/business-analytics/#descriptive-analytics-dea","title":"Descriptive Analytics (DEA)","text":"<ul> <li>Summarizes data to understand how business performed in a given time period</li> <li>Compares different segments of data</li> <li>Compares different time periods for trends and performance</li> <li>Predefined and pre-canned reports</li> <li>Bundled into software products or applications</li> <li>Custom built by IT organizations</li> <li>Schedule, run, export, and distribute reports</li> <li>All users get the same data</li> </ul>"},{"location":"4-Functional-%26-Business/business-analytics/#dea-tools-and-techniques","title":"DEA tools and techniques","text":""},{"location":"4-Functional-%26-Business/business-analytics/#dea-use-case","title":"DEA use case","text":""},{"location":"4-Functional-%26-Business/business-analytics/#dea-best-practices","title":"DEA best practices","text":""},{"location":"4-Functional-%26-Business/business-analytics/#exploratory-analytics-eda","title":"Exploratory Analytics (EDA)","text":"<p>The goal of exploratory data analytics is to deep dive into the data to understand patterns and confirm hypotheses.</p> <ul> <li>Get familiar with the data itself</li> <li>Deep dive into the data</li> <li>Typically ad hoc reporting</li> <li>Needs based</li> </ul>"},{"location":"4-Functional-%26-Business/business-analytics/#eda-tools-and-techniques","title":"EDA tools and techniques","text":""},{"location":"4-Functional-%26-Business/business-analytics/#eda-use-case","title":"EDA use case","text":""},{"location":"4-Functional-%26-Business/business-analytics/#eda-best-practices","title":"EDA best practices","text":""},{"location":"4-Functional-%26-Business/business-analytics/#explanatory-analytics-epa","title":"Explanatory Analytics (EPA)","text":"<p>The goal of explanatory data analytics is to identify reasons and root causes for business results.</p> <ul> <li>Storytelling with data</li> <li>Answer questions</li> <li>Present to an audience</li> <li>Done by analysts or managers</li> <li>Prelude to next actions</li> <li>Start with a fact or statistic</li> <li>Break down to interesting segments or profiles</li> <li>Focus on interesting insights</li> <li>Narrow down to possible causes</li> </ul>"},{"location":"4-Functional-%26-Business/business-analytics/#epa-tools-and-techniques","title":"EPA tools and techniques","text":""},{"location":"4-Functional-%26-Business/business-analytics/#epa-use-case","title":"EPA use case","text":""},{"location":"4-Functional-%26-Business/business-analytics/#epa-best-practices","title":"EPA best practices","text":""},{"location":"4-Functional-%26-Business/business-analytics/#emerging-trends-in-business-analytics","title":"Emerging Trends in Business Analytics","text":""},{"location":"4-Functional-%26-Business/business-analytics/#links","title":"Links","text":"<ul> <li>Business Analytics Foundations: Descriptive, Exploratory, and Explanatory Analytics</li> </ul>"},{"location":"5-Personal-Development/interview-career-growth/","title":"Career Growth Interview Mindset","text":"<p>career growth interview tips mindset shift</p>"},{"location":"5-Personal-Development/interview-career-growth/#talk-data-professional-growth-mindset","title":"Talk - Data Professional Growth Mindset","text":"<p>Link:  - Shachar Meir <p>Time is finite:</p> <ul> <li>Do most important and impactful thing that works towards the goal of business</li> <li>Do this thing in most efficient way to make best use of your time.</li> </ul> <p>This will get you recognised, and you will climb the ladder fast.</p> <p>Don't do because someone has asked to do. Do things consciously, you do because you have decided to do, and said yes as you have thought about it.</p> <p>Pre work, know the goals of the business, talk to, product, sales, marketing, customer support and finance teams, these are close to business. Lunch with them.</p> <p>Ask</p> <ul> <li>Why would you build a dashboard, or churn model, or a pipeline</li> <li>what impact</li> <li>what if not, impact if it is not built</li> <li>what will you stop doing if you say yes to new thing, as you have limited time.</li> </ul> <p>Ask as many why as possible, until you are satisfied with ask, or invalidate the ask, to come up with new ask that actually helps business to achieve the goal. Think like you know everything and you should ask question so that you can deliver the right thing, just like doctor.</p> - Can Do Can Do - Shouldn't Do Delegate/Refuse Execute Should Do Shouldn't Do Avoid* Learn Should Do - Can't Do Can't Do - <p>Avoiding is most important, many professionals get stuck there.</p> <p>Identify your role, create visibility through data.</p> <p>Metrics and Dimensions</p> <ul> <li>Are the metrics correct</li> <li>Do you have correct definitions</li> <li>Do you have enough dimensions to go under the hood and find the root cause of impact?</li> <li>Build missing data to get real insight.</li> </ul> <p>Learn Business Patterns</p> <p>There are common patterns in many business, identify them. It will help you when you will switch jobs, move roles. Eg</p> <ul> <li>ROI - can you map total investment and return on it, completely?</li> <li>Funnel - is funnel complete, how funnel look by different dimensions.</li> <li>Competition - add their data, scrape, news article, public info.</li> </ul> <p>Never eat alone. You eat 3 meals, 52 weeks, 5 days a week that gives, 780 opportunities to learn new things!</p> <p>To overcome shyness, be prepared with list of questions, you are not there to impress them or make them feel happy, treat it as an interview. Be prepared for rejection/ NOs, not good replies. But 50% will be Yes/Good and that will make an impact.</p>"},{"location":"5-Personal-Development/pd-courses/","title":"Personal Development Courses","text":""},{"location":"5-Personal-Development/pd-courses/#build-confidence-and-crush-doubt","title":"Build confidence and crush doubt","text":"<ul> <li>Start with belief and not talent. That is you can do it and not that you have talent to do it.</li> <li>build the confidence through repetition. Keep reminding.</li> <li>use physical clues to refocus. Like deep breath. Business suit.</li> <li>have affermations to yourself - you can code complex stuff, you can present to people clearly.</li> <li>build a grag sheet for yourself, you praise yourself. don't share this with others but keep reminding how good you are.</li> <li>draw energy from positive people, meet them. Like sunil mausaji, tony knock.</li> <li>Practice this as this is a skill, not one time read. You will shine and the people you lead will shine too.</li> <li>https://www.linkedin.com/learning/how-to-crush-self-doubt-and-build-self-confidence/</li> </ul>"},{"location":"6-Math-%26-Logic/logarithms/","title":"Logarithms","text":"<p>Read as:</p> <ul> <li>log of 1000 to the base 10 is equal to 3,</li> <li>log 1000 base 10 is 3,</li> <li>log is choppped tree, so, if base is 10, you need 3 power to get 1000</li> <li>log 8 base 2 is 3; if base is 2, you need 3 power to get 8</li> <li>how many power do you need?</li> <li>if base is $b$, how many power you need to get $a$, that is, $x$?</li> <li>$\\log_b a = x$</li> <li>$\\log_{base} {number} = {power}$</li> </ul>"},{"location":"6-Math-%26-Logic/logarithms/#logarithms","title":"Logarithms\u00b6","text":"<p>Log is opposite of \"power of\" something. Power of is called exponential, hence log is opposite of exponential.</p> <p>$$ 10^3 = 1000 =&gt; \\log_{10} 1000 = 3$$ $$ 2^3 = 8 =&gt; \\log_2 8 = 3$$</p>"},{"location":"6-Math-%26-Logic/probability/","title":"Probability","text":"<p>If an experiment is 'rolling 2 dice', then there are 6.6 = 36 outcomes. The possibility of outcome is the probability. (3,4) and (4,3) are two different events.</p> <p>Event is set of outcome of an experiment.</p> <p>Probability is a way to find likeliness of an event to happen. Permutation and combination are ways to count events and possibilities. Probability tells us:</p> <ul> <li>how likely event is going to happen.</li> <li>possibility of event that is fundamentally random.</li> <li>Quantifying the uncertainity.</li> </ul> <p>For example, if we flip a coin we can get either heads or tails. The possibility of heads is 50% and possibility ability of tails is 50%. So the probability of Heads is .5 and probability of tails is also .5, if it is a biased coin.</p> \\[ P(e) = \\frac{ Possibilities }{ Outcomes } \\] <p>Theoretical or Classical Probability</p> <ul> <li>It can be stated and seems fixed.</li> <li>For example flipping a coin.</li> </ul> <p>Experimental or Subjective Probability</p> <ul> <li>Finding an outcome based on past data and experience</li> <li>example prediction of the score. Probability gives a reasonable predictions about an outcome. It is highly likely but not hundred percent true.</li> </ul> <p>Simulation and Randomness</p> <ul> <li>We can use list of random numbers to simulate our experiment multiple times and average out to find confidence.</li> </ul> <p>Sample Space is collection of all possible outcomes of a random experiment. Hence, event can be any subset of sample space.</p> <p>Variable is anything whose value changes.</p> <p>Discrete variable is a variable whose value is calculated by counting. Eg, number of students in class, number of blue marbles in a jar, number of tails when flipping four coins.</p> <p>Continuous variable is a variable whose value is calculated by measuring. Eg, height of players in team, weight of students in class, time it takes to get to work</p> <p>Random variable is a variable whose possible values are outcomes of a random experiment. It is denoted usually X. P(X) is probability distribution of X, it tells values of X and its probability. Random variable can be continuous or discrete.</p> <p>Discrete random variable X has a countable number of possible values. Its probabilty distribution is histogram.</p> <p>Continuous random variable X takes all values in a given interval of numbers. The probability distribution of a continuous random variable is shown by a density curve. The probability that X is between an interval of numbers is the area under the density curve between the interval endpoints. The probability that a continuous random variable X is exactly equal to a number is zero.</p>"},{"location":"6-Math-%26-Logic/probability/#events-and-its-types","title":"Events and its Types","text":"<p>Every possible outcome of a variable is an event.</p> <p>Simple event</p> <ul> <li>described by a single characteristic.</li> <li>For eg, a day in January from all days in 2018.</li> <li>Complement of an event A (denoted A\u2019).<ul> <li>All events that are not part of event A.</li> <li>For eg, all days from 2018 that are not in January.</li> </ul> </li> </ul> <p>Joint event</p> <ul> <li>described by two or more characteristics.</li> <li>For eg, a day in January that is also a Wednesday from all days in 2018.</li> </ul> <p>Mutually Exclusive or Disjoint Sets</p> <ul> <li>cannot occur simultaneously.</li> <li>They have no intersection outcomes.</li> <li>For eg, A = day in Jan, B = day in Feb. A and B cannot occur simultaneously.<ul> <li>In this, P(A1 U A2 U A3...) = P(A1) + P(A2) + P(A3)...</li> <li>Also, P(A &amp; B) = 0.</li> </ul> </li> </ul> <p>Collectively Exhaustive Events</p> <ul> <li>One of the event must occur</li> <li>The set of events covers the entire sample space</li> <li>For eg, A = Weekday; B = Weekend; C = January; D = Spring;<ul> <li>Events A, B, C and D are collectively exhaustive (but not mutually exclusive \u2013 a weekday can be in January or in Spring).</li> <li>Events A and B are collectively exhaustive and also mutually exclusive.</li> </ul> </li> </ul> <p>Independent Events</p> <ul> <li>not dependent on each other. That is, occurrence of one does not affect occurrence of another event.</li> </ul> <p>Note: All mutually exclusive events are dependent but not all dependent events are mutually exclusive.</p>"},{"location":"6-Math-%26-Logic/probability/#addition-rule","title":"Addition Rule","text":"<p>Addition rule of probability.</p> \\[ P(A \\cup B ) = P(A) + P(B) -P(A \\cap B) \\] <p>if mutually exclusive, then \\(P(A \\cap B) = 0\\).</p> <p>And is intersection, or is union.</p> <p>For eg, P(Jan or Wed) = P(Jan) + P(Wed) - P(Jan and Wed) = 31/365 + 52/365 - 5/365 = 78/365</p>"},{"location":"6-Math-%26-Logic/probability/#multiplication-rule","title":"Multiplication Rule","text":"<p>For independent event, what happened in past event will have no effect on current event. For eg, P(HH) or P(at least 1H in 10 flips).</p> \\[ P(HH) = 0.5 \\times 0.5 \\] <p>P(at least 1H in 10 flips) = 1 - P(All T in 10 flips)</p> \\[ 1 - (0.5)^{10} = 1023 \\div 1024 = 99.9% \\]"},{"location":"6-Math-%26-Logic/probability/#marginal-or-unconditional-probability","title":"Marginal or Unconditional Probability","text":"<ul> <li>Simple probability like P(A) = 0.2, P(B) = 0.4</li> </ul>"},{"location":"6-Math-%26-Logic/probability/#joint-probability","title":"Joint Probability","text":"<ul> <li>P(A &amp; B) both events to happen simultanieously</li> </ul>"},{"location":"6-Math-%26-Logic/probability/#conditional-probability","title":"Conditional Probability","text":"<p>When we have to find a probability under a given condition.</p> <p>Dependent Events</p> <ul> <li>A|B is 'A happening after B' or 'conditional prob of A given that B has occurred'.</li> <li>B becomes the new sample space, because it's A given B. Hence,</li> </ul> \\[ P(A|B) = \\frac{P(A \\&amp; B)}{P(B)} \\] <p>Independent Events</p> <ul> <li>if independent (does not affect each other), then</li> </ul> \\[ P(A|B) = P(A) \\] <p>Important outcome:</p> <ul> <li>When finding  P(A &amp; B) we have to consider and analyse that whether A and B are dependent or not.</li> <li>Based on dependency, our P(A &amp; B) changes as follows:</li> </ul> <p>If dependent, the probability of A and B is:</p> \\[ P(A \\&amp; B) = P(A) \\times P(B|A) = P(B) \\times P(A|B) \\] <p>else</p> \\[ P(A \\&amp; B) = P(A) \\times P(B) \\] <p>because P(B|A) = P(B), occurrence of A has no effect on B.</p> <p>Probability of A or B</p> \\[ P(A \\space or \\space B) = P(A) + P(B) - P (A \\&amp; B) \\] <p>Add all the joint probability of colectively exhaustive and mutually exclusive events to get marginal probability of one event.</p> <p>eg, Consider industries and performance below.</p> <p>| Poor |Avg|Good|Marginal  ---|  small|0.02|0.07|0.01|0.1  medium|0.12|0.3|0.18|0.6  large|0.06|0.13|0.11|0.3 Marginal|0.2|0.5|0.3|1</p>"},{"location":"6-Math-%26-Logic/probability/#counting-events","title":"Counting Events","text":""},{"location":"6-Math-%26-Logic/probability/#permutation","title":"Permutation","text":"<p>Arrange \\(n\\) people in \\(k\\) seats. To count number of ways in which this can be done we use permutation.</p> <p>For eg, arrange 6 people in 3 seats, 6.5.4 = 6! / 3! = 120.</p> \\[ _nP_k = \\frac{n!}{(n - k)!} = n(n-1)... (k  times) \\] <p>Used when order matters and pick once (without replacement).</p> <p>For eg,</p> \\[ _{10}P_3 = 10.9.8 \\]"},{"location":"6-Math-%26-Logic/probability/#combinations","title":"Combinations","text":"\\[ _nC_k = \\binom{n}{k} = \\frac{_nP_k}{k!} = \\frac{n!}{k!(n - k)!} = \\frac{n(n-1)...[k \\space times]}{k!} \\] <p>We divide it by the number of ways in which k people can be arranged in k places, i.e, k! because ABCD and BCDA are same and we are counting this extra.</p> <p>Order doesn't matter, 123 = 312.</p> <p>For eg,</p> \\[ _{10}C_3 = \\frac{10.9.8}{3.2.1} \\]"},{"location":"6-Math-%26-Logic/probability/#approach-to-solve-a-problem","title":"Approach to solve a problem","text":"<p>We can take following approaches to solve a probability problem</p> <ol> <li> <p>use simple definition,     $$ P(e) = \\frac{events \\space possible}{sample \\space space} $$</p> </li> <li> <p>Make a Contingency Table with possibilities.</p> <ol> <li>To find P(A or B), use P(A)+ P(B) - P(A and B)</li> <li>To find P(A and B), simply use (joint event)/total.</li> <li>To find P(A|B), P(A and B) / P(B)</li> </ol> </li> <li> <p>Make a Decision Tree, use when question has \"after\".</p> <ol> <li>Find branches and outcomes</li> <li>Find effective value by multiplying with probabilities</li> <li>Roll back to find effective value at each branch.</li> </ol> </li> <li> <p>Use Venn Diagram when and/or is combined with not of a event.</p> </li> <li> <p>At least or at most, use</p> </li> </ol> \\[ P(at least/most) = 1 - P(e) \\] <p>Example</p> <p>Find number of ways to arrange 1 - 10 digits in 3 places,</p> <p>Repetition allowed, order matters = 10.10.10</p> <p>Repetition not allowed, order matters = Permutation = 10.9.8</p> <p>Repetition allowed, order doesn't matter =</p> \\[ \\frac{10.10.10}{3.2.1} \\] <p>Repetition not allowed, order doesn't matter = Combination =</p> \\[ \\frac{10.9.8}{3.2.1} \\] <p>References:</p> <ul> <li>Khan Academy.</li> <li>AMPBA - ISB</li> </ul>"},{"location":"7-Other/electronics-notes/","title":"Electronics &amp; IoT","text":"<p>Notes about IOT, Electronics etc.</p>"},{"location":"7-Other/electronics-notes/#volts-amps-ohms-watts-hours","title":"Volts Amps Ohms Watts Hours","text":"<p>Voltage (V) Volts - pressure of water, or how fast the electrons are moving. volt can be measured in parallel, stays same.</p> <p>Current (I) Amps - amount of water, or number of electrons moving. Amps can be measured in series, stays same. The current through the load depends on the load and not on the supply. 2A supply means it can support load upto 2A, means it has enough electrons to move current upto 2A, however, the current drawn from supply in a circuit depends on the resistance in it, it does not depend on the supply. Higher resistance is less electrons flowing thru, hence less Amps.</p> <p>Resistor (R) Ohm - can be added in series to resist current (drops volts), so that bulb doesn't blow. bands tell ohms.</p> \\[ V = I.R , Ohm's Law \\] <p>Power (P) Watts - Power or rate of energy transfer or used.</p> \\[ P = V.I = \\frac{V^2}{R} = I^2.R \\] <p>Que - Caculate resitance required to power less volt component (LED) from high volt battery. You need:</p> <ul> <li>fwd_voltage = electronic component = LED = 3.2 volts</li> <li>source_volt = battery used / supply = 9v</li> <li>amps = component amps = amps of LED = 24mA</li> </ul> <p>R = V / I , here find V remaining to be consumed by R, Amps remains same in series.</p> <p>R = (source_volt - fwd_volt) / Amps</p> <p>R = (9-3.2) / 0.024 = 240 ohms</p> <ul> <li>This resistor should be added in series to LED. Here, as in series, resistor is taking some volt and led is taking some volt.</li> </ul>"},{"location":"7-Other/electronics-notes/#how-current-flows-in-circuit","title":"How current flows in circuit?","text":"<ul> <li>current only flows when there is potential difference,</li> <li>current is drawn only when there is space for electrons to move, or the cricuit conducts.</li> <li>we can only change volts of source, but current drawn depends on load. we cannot provide high current.</li> <li>current is amount of electrons flowing, depends inversely on resistance in circuit.</li> <li>volts is pressure or how fast electrons are flowing.</li> <li>a load, say LED, can only allow few electrons to move through (current) and at some pressure/pace (volts). if we increase volts, it will burn.</li> <li>low resistance will allow more current to flow, hence more watts.</li> <li>high resistance allows less current to flow, in series it drops volts, or reduces pressure.</li> <li>in parallel current gets multiple routes to move, hence overall current increases and resistance drops.</li> <li>in series, resistances offer more restrictions hence less current and more resistance,</li> <li>Hence<ul> <li>Series, I is same, V is different</li> <li>Parallel, I is diff, V is same.</li> </ul> </li> <li>Example: A bulb, has resistance, this defines its volts and watts, say, 4V 1W for bike meter and 4v 20W for headlight. Volt can vary and hence Current and Watts will:<ul> <li>4V 1W =&gt; 16ohm (fixed), draws 0.25A</li> <li>now connect this to 12V source</li> <li>12V 16ohm =&gt; draws 0.75A, becomes 9W.</li> <li>this is why the bulb blows when connected to high volts.</li> </ul> </li> </ul> <p>Amp Hours - measures battery capacity, as in steady current flowing through one hour. 150Ah, inverter battery will give 15A for 10hours. A typical car battery with 12 volts rating has a capacity of 48 Ah. It means that when fully charged, the battery can deliver one amp for 48 hours, two amps for 24 hours and so on.</p>"},{"location":"7-Other/electronics-notes/#electonic-components","title":"Electonic Components","text":"<p>LEDs - bulb, usually added with resister. This give resistance to circuit. Longer leg is +ve.</p> <ul> <li>RGB has common anode and cathode LED. long to GND, light then cathode else anode.</li> <li>Control brightness by PWM - pulse width modulation.</li> <li>Mine is CC.</li> </ul> <p>Multimeter - measures volts, amps and resistance. Continuity, NPN and PNP, for AC and DC. When measuring AMPs do switch red com.</p> <p>Bread board - can be used for prototyping.</p> <p>Schematics is blueprint of a circuit.</p> <p>Potentiomenter is var resistor, or regulator.</p> <p>Capacitors - store chaege and act like battery</p> <ul> <li>unit is (F) Farads, and Volts it can handle.</li> <li>Prevents sudden start of motor, bulb, helps protect jerk in movements.</li> <li>ceramic have no polarity.</li> </ul> <p>Diodes, current in one direction, valve</p> <ul> <li>forward bias and reverse bias.</li> <li>They take volts from the circuit, cylinderical with silver strip, negative.</li> <li>Has volts drop, eg, 1.1v drop reduces volt by it. It eats volts in one direction.</li> <li>acts as protection for led which can accept current in one directions only.</li> <li>1N4148 diode can be used upto 4V, single diode</li> <li>1N4007 rectifier diode, used &lt;1000V. Most used.</li> </ul> <p>Complete circuit:</p> <ul> <li>resistor to reduce volts</li> <li>capacitor to fade out led, in parallel</li> <li>diode to prevent polarity</li> </ul> <p>Relays</p> <ul> <li>electronic switch, has electromagnet. another circuit makes switch operate. retangular box. its slow, so transistor was created.</li> <li>Types:<ul> <li>Electromechanical relay - has 5 terminals, 2 of electromagnet, 1 common and 1 Normally Open, 1 Normally Closed</li> <li>solid state relay</li> </ul> </li> </ul> <ul> <li>Relay Oscillator:<ul> <li>allows on-off loop, blinking light</li> <li>add capacitor in parallel</li> <li>make electromagnet go on and cut itself</li> <li>changing capacitor size will change frequency of on and off.</li> </ul> </li> </ul> <p>Transistor</p> <ul> <li>is base + cylindrical shaped.</li> <li>has 3 legs - base, collector and emmitter. Small +ve current in base completes the circuit.</li> <li>used for switching or amplifying.</li> <li>Types:<ul> <li>BJT - bipolar juction transistor - 2 types<ul> <li>NPN - emmiter out, +ve signal in base</li> <li>PNP - emitter in, -ve signal in base</li> </ul> </li> <li>MOSFET<ul> <li>used to switch or amplify voltages in circuits.</li> <li>3terminals, gate, drain and source.</li> <li>IRFZ24N - it blocks current until some volts is applied on gate. the more the volts on gate, more current it allows.</li> <li>A09N03N - MOsfet</li> </ul> </li> </ul> </li> <li>eg, to supply small current we add high resistor in series, this can be photo resistor to make day/night switch.</li> <li>models,<ul> <li>NPN - BC547, 2N3904</li> <li>PNP - BC557, 2N3906</li> </ul> </li> <li>2N2222 is a common NPN bipolar junction transistor (BJT) used for general purpose low-power amplifying or switching applications. It is designed for low to medium current, low power, medium voltage, and can operate at moderately high frequency.</li> </ul> <p>Integrated Cuircuits (IC):</p> <ul> <li>it has more than 1 circuit inside, can have componets like resistor, transistor or capacitor. called chip.</li> <li>save time, money, energy, space.</li> <li>types<ul> <li>Analogue/lienar IC - signal on gives cont output. eg, 7805, 555, LM386N</li> <li>Digital IC - no continuous output, but O/P is based on Logic Gates. eg, 7404 NOT Gate,  7408 AND Gate IC.</li> <li>Mixed IC -  .eg, ADC 0804, Analog to digital converter IC.</li> </ul> </li> <li>Forms - DIP, SMD, TO-220, eg,<ul> <li>ATMEGA328P - Programmabel IC</li> <li>ESP8266 - wifi programmable</li> </ul> </li> <li>555 timer IC is common,<ul> <li>contains more than 20 transistors + more components.</li> <li>8 pins, 8 is +ve, 1 -ve, 3 output.</li> <li>it has combination of logic to switch on and off.</li> <li>max load 200mA. if we power component more than this load the use mosfet.</li> </ul> </li> <li>Voltage regulator IC<ul> <li>used to control voltage, eg, give different volts to diff components.</li> <li>Types:<ul> <li>Linear volt regualtor - can only down volts, wasted volts as hear, to be used with heat sink. eg, 7805 - 5V, 7809 - 9V</li> <li>Switching volts regulator - wastes less energy, can down n up volts. eg, LM2678, LM2577.</li> </ul> </li> </ul> </li> </ul> <p>Transformer</p> <ul> <li>step down - 220v to 12v, mobile charger</li> <li>step up - inverter -</li> </ul> <p>Bridge Rectifier:</p> <ul> <li>Converts AC to DC</li> </ul> <p>MicroControllers:</p> <ul> <li>can be programmed to change current of output pins</li> <li>Arduino, tiny computer on IC.</li> <li>AtTiny85</li> </ul> <p>Logic Gates:</p> <ul> <li>XOR chip is IC with 14 pins for eg.</li> </ul> <p>Binary is number to base 2:</p> <ul> <li>8 bit computer can handle number till 8 bits.</li> <li>binary half adder is used to add two numbers.</li> <li>adders can be built using gates, AND XOR etc. which are ICs having pins.</li> <li>We can extrnd this to have full binday adder and subtractor to add 8 bits numbers.</li> </ul> <p>PCB - Printed Circut Board are circuits on mdf board with sigle or multi layer copper connections. Surface Mount Components SMC are electronic components soldered on top of PCB. Through Hole Components THC are passed through hole and soldered on back of plate. Pick-and-Place machine use SMT - surface mount technology to place SMD surface mount devices on PCBs, JUKI is the company. Altium is US based.</p> <p>RF Communication module - nRF24L01 -  Rs 60. for drones. can be used for transmission and receiver. single chip radio transceiver for the world wide 2.4 - 2.5 GHz ISM band.</p> <p>Gyro and Accelero - MPU-6050 is  6-axis, cost 50. Micro Electro-mechanical system (MEMS),  It helps us to measure velocity, orientation, acceleration, displacement and other motion like features, also measure temp, -40 to 85</p> <p>Bluetooth - HC-06 is slave bluetooth module.</p> <p>Veroboard is used for prototypying before pcb, its like breadboard.</p> <p>Magnetic Sensor - A3144 - detects magnet when close</p> <p>Touch Switch - TTP223 , rs. 20. - is a PCB with A/B modes.</p> <p>Time - DS3231 RTC Rs. 250, module Precise Real-Time Clock Module is a low-cost, extremely accurate I\u00b2C real-time clock (RTC) with an integrated temperature-compensated crystal oscillator (TCXO) and crystal. The device incorporates a battery input and maintains accurate timekeeping when the main power to the device is interrupted</p> <p>LED RGB ws2812b 5v led Strip is controllable via Arduino, we can specify color and brightness of every single LED.</p>"},{"location":"7-Other/electronics-notes/#tipsconcepts","title":"Tips/Concepts","text":"<ul> <li>Didode in parallel prevent volt volt reply on connct and disconnect.</li> <li>Capacitor in parallel prevent jerk and high volts reply. Reduces plasma.</li> <li>add 1kohm resistor in series to prevent damage by current.</li> <li>resistor could drop current however, the supply volt varies, to cover this use IC to for volt drop. To cover heat loss, and make efficient use 'DC-DC step-down buck converter'. 90% efficient. Ususally in car charger to convert 12v to 5v, more..</li> </ul>"},{"location":"7-Other/electronics-notes/#arduino-kit","title":"Arduino KIT","text":"<p>Arduino boards having microporcessor. single board computer. uno for begineers, nano for breadboard. Kit has related components. It is open source microcontroller. It comes original and compatible copies. Arduino nano is 125 each and can be used as processing unit.</p> <p>It has libraries same like python to do complex stuff. Install using library manager.</p> <p>LED with 220ohm</p> <ul> <li>12mA current, LED 2V, Resistor 2.7</li> </ul> <p>Active Buzzer makes sound. It has IC for sound, green circuit is passive, black is active. Active makes sound on current while passive needs square waves with 2K and 5K freq. So we can send high low signal to a pin, varying by a delay of 1 to 10 ms. THis will make sound with freq depending on delay. We can pass notes to passive buzzer and make play any song.</p> <p>Tilt Sensor are used to detect inclination or orientation. They are reliable, low-power, long- lasting and very inexpensive. Can tell arduino about on/off based on orientation, then based on that we can make another component operate, like making LED on/off.</p> <p>servo motor is a geared one, only capable of rotating 180 degrees and is commanded by transmitting electrical pulses from your Arduino. Brown wire is GND, Red is 5V, orange is signal. Signal can be position 0-180, this will make servo move that degree as quickly as possible, then we can delay.  Need servo lib.</p> <p>Ultrasonic sensor can measure distance, HC-SR04 is inexpensive and very easy to use. Need lib. capacity of 2cm to 400cm</p> <p>DHT11 Temperature and Humidity Sensor, The sensor includes a sensor of wet components and a temperature measurement device. It returns binary data string, which is the coverted by library to tell temp and humidity. has pin GND, Data and 5V,</p> <p>Analog Joystick Module is used to control components. 5 pins, GND, VCC +5, X, Y, SW Key. It has XY analog output which gives direction with magnitude. and key is digital.</p> <ul> <li>Key on press connects to GND. Todo: A pull-up resistor or pull-down resistor is a resistor used to ensure a known state for a signal. To get accurate readings from the Key/Select pin, it should be connected to VCC with a pull-up resistor, which we can do using the built in resistors on the UNO digital pins</li> <li>Range of X or Y is from 0-1024, mid value is approx 512.</li> <li>Switch is 1/0 pressed or free.</li> </ul> <p>IR Module using lib, we can program IR receiver.</p> <ul> <li>IR hexa decimal codes are required to interpret the OP.</li> <li>IR RECEIVER SENSOR - IR detectors are essentially small microchips with a photocell that are created to detect infrared light,</li> <li>They detect and send low signal else high 5v.</li> <li>3 pins, GND -, 5V, Signal. Signal send digitalvalues which are converted to HEX by library <code>case 0xFFA25D: Serial.println(\"CH-\"); break;</code>.</li> </ul> <p>LCD Display LCD1602</p> <ul> <li>16 pins<ul> <li>VSS: A pin that connects to ground</li> <li>VDD: A pin that connects to a +5V power supply</li> <li>VO: A pin that adjusts the contrast of LCD1602</li> <li>RS: A register select pin that controls where in the LCD\u2019s memory you are writing data: either the data register, which holds what is displayed on the screen, or an instruction register, which is where the LCD\u2019s controller looks for instructions on what to do next.</li> <li>R/W: A Read/Write pin that selects reading mode or writing mode</li> <li>E: An enabling pin that causes the LDC module to execute relevant instructions when supplied with low-level energy.</li> <li>D0-D7:Pins that read and write data</li> <li>A and K: Pins that control the LED backlight</li> </ul> </li> <li>The LCD display requires six UNO pins as digital outputs. Additionally, it needs 5V and GND connections.</li> <li>We need to set Potentiometer to control brightness of Letter (not backlit) and then reset Arduino to display.</li> </ul> <p>Thermistor is simply a thermal resistor - a resistor that changes its resistance according to the temperature. 100ohm or more per degree.</p> <p>74HC595 Shift Register</p> <ul> <li>The shift register is a kind of chip containing eight memory locations, with the values 1 or 0. We input the data using the 'Data' and 'Clock' pins of the chip to set these values on or off. 8 clock pins and 8 data pins. We can combine this with arduino analog PWN write to control brightness of LED.</li> <li>Serial to Parallel Converter - useful to power multiple LEDs from one output pin.</li> <li>Pin 1 of the chip is to the left of this notch. +5v</li> <li>Pin, Q0.</li> <li>right side is Q1-Q7 and ground at bottom</li> <li>Pin 14, 12 and 11 connect to UNO.</li> </ul> <p>Stepper Motor is an electromechanical device which converts electrical pulses into discrete mechanical movements. Used for movements and controlled with pulse of current. Has driver module.</p>"},{"location":"7-Other/electronics-notes/#small-projects-and-devices","title":"Small Projects and Devices","text":"<p>Electric Motor Speed Controller</p> <ul> <li>using 555 timer IC - IP 4.5v-16v, OP &lt;200mA. pin1 ground, pin8 +ve.</li> <li>Motor, 12V, 1.5A. It needs more current, so we use mosfet  to power 12v motor and use signal from 555.</li> <li>Mosfet - IRFZ24N - &lt;17A, &lt;55V - uses small current in gate pin1 to output more current from drain pin2. Source pin3 is ground. No current in gate, no flow of current. More volt in gate, more volts from drain.- Current to gate is given by 555 pin3. This volt is on/off pulse. this gives average volts and called Pulse Width Modulation.</li> <li>Add 1k ohm resistor in series b/w 555pin3 and mosfetpin1 to prevent 555 in case mosfet malfunctions and allows 12v to flow. Also add 1k ohm resistor in parallel to discharge this current. Need Explaination.</li> <li>When motor is turned off, it produces high volts to clear magnetic field, to prevent this add flyback diode 1N4007. parallel.</li> <li>ref - https://www.youtube.com/watch?v=UPTU6nYSaMo</li> </ul> <p>5V Regulator design</p> <ul> <li>Input is 9-12V with fluctuation, but output is constant 5v.</li> <li>use an IC LM7805, takes in random 9-12v outs 5v.</li> <li>add capacitor in 0.22uF cap, in parallel to smooth drops</li> <li>add 0.1uF ceramic cap, to smooth noise.</li> <li>10uf elector cap, and 0.1uF ceramic cap in OP to smooth out flow.</li> <li>add diode in IP to prevent polarity fault. schottky diode has less drop so add it.</li> <li>ref - https://www.youtube.com/watch?v=d-j0onzzuNQ</li> <li>this has isse of heat loss by 7805.</li> </ul> <p>DTH - Free Dish - Settop Box</p> <ul> <li>SMPS - 2658A1.PCB - 200 rs - converts 220v to 5v DC</li> </ul> <p>Fan Resistor 220v B1 R-783</p> <ul> <li>cap - 3.3uF 250V</li> <li>res - 1m in parallel</li> <li>res - 4.7ohm, 0.5w</li> <li>1p4t switch.</li> </ul> <ul> <li>SE 104j2A - capacitor, 100V 2A</li> <li>BT124 600E - Thyristors - TRIACs</li> <li>ref - https://www.youtube.com/watch?v=k4c3_yCfLWA</li> </ul> <p>diac triac light dimmer circuit</p> <ul> <li>triac - bt136</li> <li>diac - DB3</li> <li>capacitor - 104k 440v</li> <li>potentiomenter - 500k</li> <li>resistor - 10k</li> <li>ref<ul> <li>en https://www.youtube.com/watch?v=OmBu3emRdV8</li> <li>hi https://www.youtube.com/watch?v=C1qGVaGgGOo</li> </ul> </li> </ul> <p>Step-down buck converters</p> <ul> <li>reduces volts without wasting energy.</li> </ul> <p>WiFi Relay Switch</p> <ul> <li>ref - https://www.youtube.com/watch?v=TZnrHkjlgLk</li> </ul> <p>Malaysian Baloon</p> <ul> <li>6v, 40mA - working.</li> </ul> <p>Trimmer</p> <ul> <li>resistor - 100ohm 5%</li> </ul> <p>D Duke Adapter</p> <ul> <li>500mA or 0.5A at 12V = 6W max load.</li> </ul> <p>Balaji Adapter</p> <ul> <li>OP = 12V 2A = 24W max load.</li> </ul> <p>Lights LEDs</p> <ul> <li>MegaGold - 2.4W = 0.2A = 200mA. 10 lights with BalaJi Adapter.</li> <li>Car Music RGB - 0.23A at 12V = 2.76W</li> </ul> <p>2CH RC Remote Control 27MHz</p> <ul> <li>RC Remote Car</li> </ul>"},{"location":"7-Other/electronics-notes/#diy-notes","title":"DIY Notes","text":"<ul> <li>building material:<ul> <li>PVC sheets, PVC pipe to flat, PVC fanti.</li> <li>MDF boards</li> <li>Rubber sheets</li> </ul> </li> <li>tools:<ul> <li>Small Drill Bits</li> <li>Glue Gun</li> <li>Fevi Quick</li> </ul> </li> <li>Elec:<ul> <li>PCB Board, empty board</li> <li>Switches.</li> <li>Hot Glue Gun</li> </ul> </li> </ul>"},{"location":"7-Other/electronics-notes/#todo","title":"Todo","text":"<ul> <li>Add remote to symphony cooler</li> <li>Add mobile controlled Motor starter and water level monitor<ul> <li>Solar tank level monitor, wifi/rf to send signals.</li> <li>Relay to start motor.</li> </ul> </li> </ul>"},{"location":"7-Other/electronics-notes/#references","title":"References","text":"<ul> <li>Learn Beginner Electronics YouTube</li> <li>Multimeter Manual - https://www.petervis.com/meters/dt830d/dt830d-how-to-use-instructions.html</li> <li>https://quadstore.in/</li> <li>Water plants auto - https://www.electromaker.io/blog/article/elecrow-smart-plant-watering-system-using-arduino-uno-review-and-tutorial</li> <li>Channels:<ul> <li>Circuit Digest - https://www.youtube.com/channel/UCy3CUAIYgZdAOG9k3IPdLmw</li> <li>Manmohan Pal - https://www.youtube.com/c/ManmohanPal</li> <li>tech Ideas - https://www.youtube.com/channel/UCNtV2t2MX3qGkBSD_uRtCGg</li> <li>The engiuneering mindset - https://www.youtube.com/c/Theengineeringmindset</li> <li>Mega Electronics - https://www.youtube.com/channel/UCl9W8s1E1aXmODa_8fTSbhw</li> <li>DD ELectro Tech - https://www.youtube.com/user/Deba9681895487</li> </ul> </li> </ul>"},{"location":"8-Blog-%26-Articles/fake-data-using-faker/","title":"How to Create fake data using Faker","text":"<p>Faker is library in Python and it exists for Java as well.</p>"},{"location":"8-Blog-%26-Articles/fake-data-using-faker/#create-fake-records-in-database","title":"Create Fake Records in Database","text":"<p>Following script will insert fake records in database table</p> <pre><code>from datetime import datetime as dt\nimport sqlite3\nimport random\n\nfrom faker import Faker\n\n# Create a Faker instance\nfake = Faker()\n\n# Connect to the SQLite database\nconn = sqlite3.connect(r'/home/john/db-app-dummy-v1.sqlite')\ncursor = conn.cursor()\n\n# Generate and insert fake data into the 'users' table\n\nfor i in range(10):  # Insert 10 fake records as an example\n\n    uid = fake.bothify(\"uSr00000#??##?#?\")      # random alpha-numeric\n    name =  fake.name()\n    emp_number = 'EMP'+fake.numerify('########')\n    dob = fake.date_time()\n\n    sql = f'''\n    INSERT INTO app_users (\n        uid, name, emp_number, dob\n    ) VALUES (\n        \"{uid}\", \"{name}\", \"{emp_number}\", \"{dob}\"\n    );\n    '''\n    print (sql)\n\n    # cursor.execute(sql)\n\n\n# Commit the changes and close the connection\nconn.commit()\nconn.close()\n</code></pre>"},{"location":"8-Blog-%26-Articles/fake-data-using-faker/#create-fake-function-flask","title":"Create Fake function FLASK","text":"<p>Asuming you are using flask in <code>app.py</code> and it has <code>db</code> and <code>Employee</code> as ORM</p> <pre><code>import random\nimport sys\nfrom faker import Faker\nfrom app import db, Employee\nfrom datetime import datetime\n\ndef create_fake_employees(n):\n    \"\"\"Generate fake employees.\"\"\"\n    faker = Faker()\n    for i in range(n):\n        employee = Employee(\n            created_date = datetime.utcnow(),\n            department_id = faker.bothify(text='Dept_????-########'),\n            manager = faker.name()\n        )\n        db.session.add(employee)\n    db.session.commit()\n    print(f'Added {n} fake employees to the database.')\n\n\nif __name__ == '__main__':\n    if len(sys.argv) &lt;= 1:\n        print('Pass the number of employees you want to create as an argument.')\n        sys.exit(1)\n    create_fake_employees(int(sys.argv[1]))\n</code></pre> <p>Execution</p> <pre><code>$ python -m flask --app app.py shell\n&gt;&gt;&gt; from create_fake_employees import create_fake_employees\n&gt;&gt;&gt; create_fake_employees(10)\n</code></pre>"},{"location":"8-Blog-%26-Articles/fake-data-using-faker/#snippets","title":"Snippets","text":"<p>Below are ways to generate random things</p> <pre><code># Pick from a list, make choices\n\nmy_categories = ['dog', 'cat', ... , 'ant']\n\nanimal_category = random.choice(my_categories)\nmetric = random.choice(['abc', 'pqr', 'xyz'])      # inline\nrelationship_name = fake.name() + random.choice([' Relationship', ' Family', ''])\n\n\n# Random numbers, in range, between\n\nvalue_ = random.randint(3000,5000) * random.choice([1,-1])\n\n\n# bothify inserts random alpha for ? and random number for #\ncrm_id = fake.bothify(\"CR0######\")\nemp_num = 'EMP'+fake.numerify('########')\nteam_uid = fake.bothify(\"t35qG00000#??##?#?\")\noffice_id = 'OFC_'+str(random.randint(20, 80))\n\nbanker_name = None\n\nweek_number = random.randint(1,2)\nwc = [1, 8]\nweek_commencing = dt(2024,1,wc[week_number-1])\nload_datetime = dt.now()\nload_datetime = fake.date_time()\n\n\nuser_name = fake.name()\nuser_all_ids = user_uid+'|'+user_name.replace(' ','.').lower()\nteam_owner_id = user_name.replace(' ','.').lower()+\"@example.com\"\nteam_name = \"Team \" + fake.name()\n\n\n# Locale Settings\nfake = Faker('en-in')  # Locale ID (LCID) Chart\n</code></pre>"},{"location":"8-Blog-%26-Articles/fake-data-using-faker/#links","title":"Links","text":"<ul> <li>Providers - different type of things</li> <li>datetime randomness</li> <li>Geekflare - Python Faker Explained</li> <li>Faker Doc</li> </ul>"},{"location":"8-Blog-%26-Articles/github-pages-jekyll/","title":"Jekyll on Github","text":"<p>Github Pages are static sites that can be hosted on GitHub for free. Github Pages use Jekyll (a Ruby Gem) to build static site from markdown files.</p> <ul> <li>Do not remove this line (it will not be displayed) {:toc}</li> </ul>"},{"location":"8-Blog-%26-Articles/github-pages-jekyll/#get-started-quick","title":"Get Started - Quick","text":"<p>Use 'Jekyll Now', it is flat 30 seconds blog setup. Follow the steps below:</p> <ul> <li>You can setup Jekyll on GitHub by forking Jekyll Now repository.</li> <li>The readme.md in above repository is a very good tutorial that you can follow and setup Jekyll on your GitHub account.</li> <li>Modify config files and github settings as stated in above readme.</li> <li>your blog is live</li> </ul> <p>With this you can use your time on writing post rather than other geeky stuff, but if you need to setup everything or if it is required your can follow setting Jekyll locally below.</p> <p>Now that blog is working, we need to write posts.</p>"},{"location":"8-Blog-%26-Articles/github-pages-jekyll/#add-posts-to-the-site","title":"Add Posts to the Site","text":"<p>Posts can be published in 3 ways:</p> <ol> <li> <p>Directly write on GitHub.com: This is fastest way and requires no setup. You can go to <code>_posts</code> folder on this repository and create new .md file.</p> </li> <li> <p>Local MD files You can use Sublime, atom or any other text editor on your local machine and the upload it to GitHub or use Git locally then commit and push to GitHub.</p> </li> <li> <p>Local Jekyll setup You can install Jekyll locally on your machine. This will require you to install Ruby as well. Then on localhost you can render your entire website (blog) and see changes. Then you can push it to GitHub.</p> </li> </ol>"},{"location":"8-Blog-%26-Articles/github-pages-jekyll/#jekyll-local-setup","title":"Jekyll local setup","text":"<ul> <li>You need to have ruby, gem, gcc and g++ installed. else do <code>brew install gem</code> and all.</li> <li>Then you need to install <code>gem install bundler jekyll</code></li> <li>Next, <code>gem install github-pages</code> installs all gems required by github pages, all of the dependancies you\u2019ll need, like: kramdown, jemoji, and jekyll-sitemap</li> <li><code>jekyll new my_blog</code> creates scaffold for a new site. This is all you need to do.</li> <li><code>jekyll build</code> builds</li> <li><code>jekyll serve</code> serves the site to localhost:4000.</li> <li>Detailed article on installing jekyll, here.</li> <li>Tutorial with all steps, KBRoman.</li> <li>Advanced features: If you need to extend the functionality of Jekyll posts then advanced tutorial can be found at here.</li> </ul> <p>Issues:</p> <ul> <li>If you see permission issue on Mac, run using <code>sudo</code>. This may occur as gem and ruby are already installed on mac but in Library folder which is not writable.</li> <li>If you want to run locally already existing site, then create a new temp blog then copy 'Gemfile' and 'Gemfile.lock'. The site root should have these files. They are required to provide all gems that Jekyll requires for proper functionality.</li> </ul>"},{"location":"8-Blog-%26-Articles/github-pages-jekyll/#deploy-to-github","title":"Deploy to Github","text":"<p>Github can further be used to host your projects site. This is kind of a sub-site/sub-domain of main site.</p> <p>My site: <code>myname.github.io/</code></p> <p>Project Site: <code>myname.github.io/abc_project/</code></p> <p>All projects repository come under gh-pages branch and not master.</p> <p>Creating a sub site is same as creating a main site.</p>"},{"location":"8-Blog-%26-Articles/github-pages-jekyll/#jekyll-notes","title":"Jekyll Notes","text":"<p>Jekyll is a Ruby library to make blog and pages site.</p> <p>_config.yml has all configuration variables.</p> <p>Posts are markdown files store under _posts folder</p> <p>Pages are markdown files in root location.</p> <p>_layouts have different .html files that define the layout for example: default, pages or posts. These can include other templates from _includes folder. They have {{ content }} which gets populated by file that uses this layout.</p> <p>For eg. 'default.html' can include 'meta.html'.</p> <p>'post.html' can use 'default.html' as layout. So all code in 'post.html' will populate {{ content }} in default.html</p> <p>some_post.md can use <code>post.html</code> as layout. So all markdown from this file will be populated to {{ content }} of 'post.html'.</p> <p>To list all categories in site</p> <p>Category returns two array items, first is category name and second is another array of posts.</p> <p>Categories in site:</p> <pre><code>{\\% for category in site.categories \\%}\n- {{ category[0] }}\n{\\% endfor \\%}\n</code></pre>"},{"location":"8-Blog-%26-Articles/hello-humans/","title":"Hello Humans","text":"<p>Here is my first post on GitHub using Jekyll and I turn to my last in twenties. Phew..! life passes fast.. isn't it?</p> <p>Well, time ticks at it's own pace. So much to learn and so less time. World is running and so are we. Run.. chase your dreams.. rise and shine.. but remember, we are humans blessed enough to enjoy mother earth. Life is a journey, enjoy all ups and downs.</p> <p>Here, I basically write notes of what I learn and what I do. This helps me refer back and keep a log of how I did something. I was saving my notes locally and randomly so thought of putting them onto GitHub to centralize them and make them available to the world.</p> <p>A few posts here might not be very well document or in polished manner as they are quick notes to refer back. However, I do include the most essential part and try not to break the flow in understanding a concept.</p> <p>Thanks for landing here. A quote from Steve Jobs' speech at Harvard:</p> <p>Stay Hungry.. Stay Foolish.. ;)</p> <p>Cheers...!</p>"},{"location":"8-Blog-%26-Articles/syntax-highlight-jekyll/","title":"How to add syntax highlighting to Jekyll Sites","text":"<p>Jekyll supports syntax highlighting by default using gem <code>rouge</code>. It can highlight 100 different language.</p> <p>You need to add one line in <code>config.yml</code></p> <pre><code>highlighter: rouge\n</code></pre> <p>and need to ensure that <code>rouge</code> gem is installed. If you have forked any Jekyll site then you can skip this step, else run:</p> <pre><code>gem install rouge\n</code></pre> <p>Themes: There are many themes available for syntax highlighting. They can be previewed here. They can be downloaded from here.</p> <p>I personally prefer Github flavoured theme which I downloaded from here.</p> <p>Once you have decided the theme then you can replace the file <code>_syntax-highlighting.scss</code> file located in <code>_scss</code> directory. Every Jekyll site must have this file by default.</p> <p>Please see below some of the use cases.</p> <p>Ruby:</p> <pre><code>require 'redcarpet'\nmarkdown = Redcarpet.new(\"Hello World!\")\nputs markdown.to_html\n</code></pre> <p>Python:</p> <pre><code>import numpy as np\nimport pandas as pd\n\ndf = pd.read_csv('employee.csv')\n\ndf.head()\n</code></pre> <p>HTML</p> <pre><code>&lt;head&gt;\n  &lt;body&gt;\n    Hello..!\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"9-Drafts/dea-c01-u0-exam-course-intro-ps/","title":"AWS DEA-C01 Preparation","text":""},{"location":"9-Drafts/dea-c01-u0-exam-course-intro-ps/#exam-parts-and-weighting","title":"Exam Parts and Weighting","text":"<ul> <li>34% Data Ingestion and Transformation</li> <li>26% Data Store Management</li> <li>22% Data Operation and Support</li> <li>18% Data Security and Governance</li> </ul>"},{"location":"9-Drafts/dea-c01-u0-exam-course-intro-ps/#exam-details","title":"Exam Details","text":"<ul> <li>130 minutes (2hr 10 minutes)</li> <li>64 MCQ</li> <li>Ask additional time as English is second language</li> </ul>"},{"location":"9-Drafts/dea-c01-u0-exam-course-intro-ps/#de-fundamentals","title":"DE Fundamentals","text":"<ul> <li>Data from Sources lands on to AWS Data Lake</li> <li>AWS Data Lake is object file storage like S3.</li> <li>Data landed as is is called RAW Data.</li> <li>Then it is read by AWS Glue using Crawler.</li> <li>Crawler crawls and builds Catalog.</li> <li>Catalog is meta-data which lets do more operations and is schema-on-read.</li> <li>Then data is <code>processed</code> and <code>stored</code> as Transformed Data back into AWS Data Lake.</li> <li>Again <code>crawler</code> is built on Transformed Data, and then it can be used for Data Analytics by <code>QuickSight</code>, <code>SageMaker</code>, <code>Athena</code> or <code>OpenSearch</code></li> </ul> <pre><code>\nflowchart LR\n\ns1[Data Sources \\n\nDatabase\nObject\nIoT\nMobile]\n\ns2[Raw Data \\n\nAmazon S3\n]\n\ns31[[Glue \\n\nCrawler\nCatalog]]\n\ns32[[Glue \\n\nCrawler\nCatalog]]\n\n\ns4[Transformed Data \\n\nAmazon S3\n]\n\ns5[Data Analytics \\n\nAmazon QuickSight\nAmazon SageMaker\nAmazon OpenSearch\nAmazon Athena\n]\n\ns1 --&gt; s2 --&gt; s31 --&gt; s4 --&gt; s32 --&gt; s5\n\nsubgraph sub1[Data Lake]\ns2\ns31\ns4\ns32\nend\n</code></pre>"},{"location":"9-Drafts/dea-c01-u0-exam-course-intro-ps/#data-lake-vs-data-warehouse","title":"Data Lake vs Data Warehouse","text":"<p>Why Data Lake?</p> <ul> <li>Cost: DL is cheap as it is file storage whereas DW being RDBMS is expensive.</li> <li>DL is central repo for raw data, DW is central repo for historical data.</li> <li>Structure - DL supports any format, DW store organised and structured.</li> <li>Schema - DL is schema-on-read; DW is schema-on-write</li> <li>Type - DL can store text, images, videos; DW is good for transactional, CRM, ERP etc.</li> </ul>"},{"location":"9-Drafts/dea-c01-u1-data-ingestion-and-transformation-ps/","title":"U1 Data Ingestion and Transformation","text":"<p>Tools to ingest and transform the data.</p>"},{"location":"9-Drafts/dea-c01-u1-data-ingestion-and-transformation-ps/#aws-glue","title":"AWS Glue","text":"<ul> <li>Serverless solution to discover, prepare/combine data.</li> </ul> <ul> <li>Collection of<ul> <li>ETL Jobs</li> <li>Data Catalog</li> <li>Glue Studio</li> <li>Data Quality</li> <li>Data Brew</li> </ul> </li> </ul> <ul> <li>Can read from S3, RDS, DynamoDB etc.</li> <li>Lets build ETL job using Python Shell, Glue Streaming, Cloud Studio</li> <li>DPU s - Data Processing Units<ul> <li>G.1X - for memory intense workloads</li> <li>G.2X - for ML transformations</li> <li>G.0.25X - for low volume data</li> </ul> </li> </ul> <p>AWS Glue: Data Catalog</p> <ul> <li>Creates MetaData for assets</li> <li>Active Table Definition including Columns Names, data types and other attributes.</li> <li>Automatically infer the schema of data.</li> </ul> <p>AWS Glue: Glue Studio</p> <ul> <li>GUI to build ETL Jobs</li> <li>Drag and Drop Transformations</li> <li>GUI for Job Orchestration and Monitoring<ul> <li>Job can be scheduled, trigged on event or manual start.</li> </ul> </li> <li>Use Case: Getting, Storing and Transforming Data.</li> </ul> <p>AWS Glue: Data Brew</p> <ul> <li>Visual Data Preparation Tool, do build Data Profiling</li> <li>Use SQL or Programming Language to process data.</li> <li>Provide built-in transformation functions.</li> <li>Use Case: Gain insight and trends in data.</li> </ul>"},{"location":"9-Drafts/dea-c01-u1-data-ingestion-and-transformation-ps/#amazon-kinesis","title":"Amazon Kinesis","text":"<ul> <li>Fully managed, serverless.</li> <li>Data Streaming Service.</li> <li>Used to manage and process streaming data.</li> <li>Built in auto-scaling.</li> </ul> <ul> <li>Family of Services, to let us do collection, delivery and analytics of streaming data, :<ul> <li>Data Streams</li> <li>Firehose - delivery system - connects to stream and delivers to destination</li> <li>Video Streams</li> <li>Managed Apache Flink - allows processing</li> </ul> </li> </ul> <ul> <li>Kinesis Use Cases<ul> <li>Anomaly detection in IoT</li> <li>Log processing for ML</li> <li>Pattern Detection</li> <li>Click Stream Analytics</li> </ul> </li> </ul> <p>Amazon Kinesis : Data Streams</p> <ul> <li>Stream is sequence of shards containing data records</li> </ul> <ul> <li>Data records contains<ul> <li>sequence number - to order the shards</li> <li>partition key - to group the shards</li> <li>data blob - up to 1MB - this is actual data</li> </ul> </li> </ul> <ul> <li>Data Flow sides: There are two sides<ul> <li>Data Producers</li> <li>Data Consumers</li> </ul> </li> </ul> <ul> <li>Producers<ul> <li>application that ingests data in near real time.</li> </ul> </li> </ul> <ul> <li>Consumers<ul> <li>Application that collect then process or store the data.</li> </ul> </li> </ul> <pre><code>\nflowchart LR\n\na1[Data Producer \\n\nIoT Devices\nLog/event Generators\nExternal Apps]\n\na2[Kinesis \\n\nData Stream\nFirehose]\n\na3[Data Consumer \\n\nS3\nRedshift\nThird Party]\n\na1 --&gt; a2 --&gt; a3\n</code></pre> <p>Amazon Kinesis: Data Firehose</p> <ul> <li>It is a Delivery System.</li> <li>Lets collect from AWS service like Kinesis or from Third Party</li> <li>Allows to Store data to send to other services like Flink to process.</li> </ul> <p>Amazon Kinesis: Apache Flink (Managed Service)</p> <ul> <li>use Programming Lang or SQL to process.</li> <li>Run notebook commands against data.</li> </ul> <p>Kinesis vs Amazon SQS (Simple Queue Service)</p> <ul> <li>Storage Time: 24 hr to 7 days; 1 minute to 14 days</li> <li>Number of Consumers: Multiple; Single, once message is read it is deleted from queue.</li> <li>Ordering: More support for ordering with shards; no ordering, but can implement FIFO programmatically.</li> <li>Routing: Related records can be routed; no support.</li> </ul> <p>Kinesis vs Kafka</p> <ul> <li>Configuration: Limited as it is serverless, auto-scaled and aws managed; Highly configurable in how it writes the data, as it is self hosted.</li> <li>Cost: less with pay-as-you-go model on cloud; more as it needs sophisticated engineering.</li> <li>Use Case: easy to start, sets up in few clicks; heavy lifting to start and setup.</li> </ul>"},{"location":"9-Drafts/dea-c01-u1-data-ingestion-and-transformation-ps/#lab-performing-real-time-data-analysis-with-amazon-kinesis","title":"Lab: Performing Real-Time Data Analysis with Amazon Kinesis","text":"<p>Can be found under projects. Link</p>"},{"location":"9-Drafts/dea-c01-u1-data-ingestion-and-transformation-ps/#amazon-redshift","title":"Amazon RedShift","text":"<ul> <li>Supports distributed workloads for <code>Petabyte</code> scale.</li> <li>Store data in columns instead of rows.</li> </ul> <ul> <li>Querying can be done using<ul> <li>Redshift Query Editor</li> <li>Console</li> <li>Redshift HTTP API</li> </ul> </li> </ul> <ul> <li>Ready for both Structured and Semi-Structured data.</li> <li>Runs within a VPS, supports Multi AZ and offers serverless deployment option.</li> </ul> <ul> <li>It is PostgreSQL and hence all commands of postgres work.</li> </ul> <ul> <li>Connectivity: You can connect using similar ways you use to connect to any PostgreSQL DB<ul> <li>CLI using <code>pgcli</code> or <code>psql</code></li> <li>Python adapter using <code>psycopg2</code></li> <li>JDBC or ODBC</li> <li>or use any other method that you that lets you connect to postgres database.</li> </ul> </li> </ul> <ul> <li>Use Cases<ul> <li>OLAP - you can load data from traditional warehouse in to it for scalable analytics.</li> <li>Log Analysis - It lets you analyse logs. It can help understand user behaviour.</li> </ul> </li> </ul>"},{"location":"9-Drafts/dea-c01-u1-data-ingestion-and-transformation-ps/#redshift-architecture","title":"Redshift Architecture","text":"<ul> <li>Redshift uses Cluster architecture. Clusters have nodes.</li> </ul> <ul> <li>Nodes<ul> <li>They are computing resources.</li> <li>Max 128 computing nodes can be in a cluster.</li> <li>There are two type of nodes in each cluster:<ul> <li>Leader Node</li> <li>Compute Node</li> </ul> </li> </ul> </li> </ul> <ul> <li>Single Node Cluster<ul> <li>It has only one Compute Node with a Leader Node.</li> <li>Used for small dataset / testing.</li> </ul> </li> </ul> <ul> <li>Multi Node Cluster<ul> <li>It has at least two Compute Node with a Leader Node.</li> <li>Used for large dataset / production.</li> </ul> </li> </ul> <pre><code>flowchart TB\n\nsubgraph s2[Multi Node Cluster]\n    direction TB\n    b1[Leader Node]\n    b2[Compute Node]\n    b3[Compute Node]\n    b1 -.- b2\n    b1 -.- b3\nend\n\nsubgraph s1[Single Node Cluster]\n    direction TB\n    a1[Leader Node]\n    a2[Compute Node]\n    a1 -.- a2\nend</code></pre> <p>Leader Node</p> <ul> <li>Receives query from Client applications.</li> <li>Parses query and prepares Query Execution Plan.</li> <li>Coordinates parallel execution of plan.</li> <li>Aggregates results of the queries and returns to client.</li> </ul> <p>Compute Node</p> <ul> <li>Partitions jobs into slices (using slice management)</li> <li>Runs query execution plan.</li> <li>Sends intermediate data back to the leader node.</li> </ul> <pre><code>flowchart TB\n\na1[Client]\nb1[Leader Node]\nb2[Compute Node]\nb3[Compute Node]\nb4[Compute Node]\n\na1 &lt;--JDBC--&gt; b1\na1 &lt;--ODBC--&gt; b1\n\nb1 &lt;--Massively Parallel Processing--&gt; b2\nb1 &lt;--MPP--&gt; b3\nb1 -.Unutilised.- b4\n</code></pre> <p>Type of Compute Nodes</p> <ul> <li>Dense Compute (DC) For compute intensive tasks. Eg, analytics</li> <li>Dense Storage (DS) For large warehouse. Has slower performance but huge capacity.</li> <li>RA3<ul> <li>Scales compute and Storage independently.</li> <li>Automatically off-loads the data to S3 if it outgrows local SSD.</li> <li>Recommended over DS node.</li> </ul> </li> </ul> <p>Scaling Options</p> <ul> <li>You can scale by changing cluster, nodes or storage to S3. Lets explore then.<ul> <li>Add Redshift Clusters (Concurrency Scaling)</li> <li>Query Data from S3 via Redshift Spectrum</li> <li>Resizing Cluster by Updating Nodes</li> </ul> </li> </ul> <ul> <li>Add Redshift Clusters (Concurrency Scaling)<ul> <li>Automatically adds more compute power temporarily based on high demand.</li> </ul> </li> </ul> <ul> <li>Query Data from S3 via Redshift Spectrum<ul> <li>Ability to query large dataset directly from S3</li> <li>Avoids ETL from lake to database.</li> </ul> </li> </ul> <ul> <li>Resizing Cluster by Updating Nodes<ul> <li>Scaling horizontally by adding/removing nodes.</li> <li>Scaling vertically by upgrading node type.</li> </ul> </li> </ul> <p>Resizing Options</p> <ul> <li>You can resize using<ul> <li>Classical resize</li> <li>Elastic resize</li> </ul> </li> </ul> <ul> <li>Classical resize<ul> <li>Copies table and metadata from old source cluster to new target cluster.</li> <li>Cluster is in read-only mode, can't be edited.</li> <li>Takes more time to complete</li> <li>Does not retain the sys log of old cluster.</li> <li>Eg, you change compute node type from DS2 to RA3 then all data from DS2 nodes is copied to RA3 nodes in new cluster.</li> </ul> </li> </ul> <ul> <li>Elastic resize<ul> <li>Resize an existing cluster by adding/removing nodes in-place.</li> <li>Cluster is unavailable during resize.</li> <li>Only upgrading node is allowed within a cluster, can't downgrade.</li> <li>Completes in minutes is fast</li> <li>Retains system logs.</li> </ul> </li> </ul>"},{"location":"9-Drafts/dea-c01-u1-data-ingestion-and-transformation-ps/#loading-data-commands","title":"Loading Data Commands","text":"<ul> <li>Data can be loaded into Redshift from<ul> <li>S3</li> <li>Kinesis</li> <li>EMR</li> <li>DynamoDB</li> <li>RDS</li> <li>DMS</li> </ul> </li> </ul> <p>Below diagram shows an option to load data from different services to Redshift using the <code>COPY</code> command of PostgreSQL.</p> <pre><code>flowchart LR\nRDS --LOAD--&gt; S3\nFireHose --LOAD--&gt; S3 --COPY--&gt; Redshift\nDynamoDB --COPY---&gt; Redshift</code></pre> <p>Loading from S3 to Redshift Table</p> <ul> <li>Split the file into multiple chunks, this will let COPY command to load in parallel and hence leverage MPP architecture.</li> <li>Create IAM role with S3 read permission and attach it to cluster.</li> </ul> <pre><code># on pgcli or psql\nCOPY my_table\nfrom \"s3://my_bucket/my_table/table1.txt\"\niam_role 'arn:aws:iam::342341323:role/my_role_read_s3'\n</code></pre> <p>Here, <code>my_table</code> is the target table in redshift to which data will be copied from S3 source bucket.</p> <p>Unload to S3 from Redshift</p> <pre><code># on pgcli or psql\nUNLOAD ('SELECT * FROM my_table')\nTO \"s3://my_bucket/my_table\"\niam_role 'arn:aws:iam::342341323:role/my_role_write_s3'\nFORMAT PARQUET ;\n</code></pre> <p>Sorting Data in Redshift</p> <ul> <li>Use <code>VACUUM</code> command to sort and save space.</li> </ul> <pre><code># Reclaim space and re-sort rows based on default threshold\nVACUUM FULL sales_table ;\n\n# Re-sort rows only if less than 75% of rows are already sorted\nVACUUM sort only sales_table to 75 percent ;\n\n# Reindex and then vacuum\nVACUUM reindex listing;\n</code></pre>"},{"location":"9-Drafts/dea-c01-u1-data-ingestion-and-transformation-ps/#lab-loading-data-into-a-redshift-cluster","title":"Lab: Loading Data into a Redshift Cluster","text":"<p>Can be found under projects. Link</p>"},{"location":"9-Drafts/dea-c01-u1-data-ingestion-and-transformation-ps/#aws-lambda","title":"AWS Lambda","text":"<p>It has two parts:</p> <ul> <li>Lambda Functions</li> <li>Lambda Service</li> </ul> <ul> <li>Lambda Functions<ul> <li>Code and function to complete tasks when event happens that match the trigger you set for the function.</li> </ul> </li> </ul> <ul> <li>Lambda Service<ul> <li>Lets you handle the incoming calls to the functions</li> <li>Handles routing to and from the functions.</li> <li>Runs Lambda functions as and when needed and scales automatically.</li> </ul> </li> </ul> <ul> <li>Use Cases<ul> <li>IoT devices analysis - keep track</li> <li>Serverless websites - click stream analysis, where people are looking</li> <li>small event driven architecture</li> </ul> </li> </ul> <ul> <li>Benefits<ul> <li>Self managed, need not worry for hardware</li> <li>Operating System and security is managed</li> <li>Pay only for compute time measured in milliseconds.</li> <li>Highly customisable</li> <li>You can persist the data as required.</li> </ul> </li> </ul> <ul> <li>Workflow<ul> <li>create a function, you get a unique ARN</li> <li>write the code</li> <li>specify the trigger</li> </ul> </li> </ul> <ul> <li>Requirements<ul> <li>IAM Role to access services Lambda needs access to</li> <li>Memory size limit (RAM)<ul> <li>Specify memory size you think lambda would be consuming.</li> <li>Range allowed is 128 MB to 10,240 MB.</li> </ul> </li> <li>Execution Time<ul> <li>You can specify from 1 second to 15 minutes. Max is 15 minutes</li> <li>After this time the function terminates regardless of its execution state.</li> </ul> </li> <li>Event Source Mapping<ul> <li>These are triggers that will invoke the Lambda function.</li> </ul> </li> </ul> </li> </ul>"},{"location":"9-Drafts/dea-c01-u1-data-ingestion-and-transformation-ps/#lab-using-aws-lambda-with-amazon-kinesis-data-stream","title":"Lab: Using AWS Lambda with Amazon Kinesis Data Stream","text":"<p>In drafts.</p>"},{"location":"9-Drafts/dea-c01-u1-data-ingestion-and-transformation-ps/#triggering-data-ingestion","title":"Triggering Data Ingestion","text":"<ul> <li>Trigger<ul> <li>Trigger is an event that is generated from one service and invokes another service in AWS</li> <li>Eg, S3 Events, Redshift Events, EventBridge</li> </ul> </li> </ul> <ul> <li>EventBridge Triggers<ul> <li>Triggers are based off of event patterns that are defined within rules.</li> <li>It an event happens, it will be matched again pattern defined in rule. If it matches a pattern, it will be sent to other service and the action will be takes as defined.</li> </ul> </li> </ul> <ul> <li>Redshift Events<ul> <li>Log of things that happen within a cluster.Eg, table change, table update, cluster change, almost anything.</li> <li>Events can have notifications subscription and can be help for several weeks.</li> <li>Events have <code>date</code>, <code>description</code>, event <code>source</code>, and <code>source id</code>.</li> </ul> </li> </ul> <p>Event Driven Action - Example with Kinesis EventBridge and Lambda</p> <ul> <li>Kinesis - DataStream can be configured as an event source for a trigger. Eg, Data being pushes to singular mapped destination can trigger lambda (or any other service).</li> <li>EventBridge - It follows the event patterns defined in rule, and on a match it triggers an action on one or more services.</li> <li>Lambda - It can get triggered and Lambda provides most configurable actions and can pass on to one mapped destination (another service) for specific action.</li> </ul>"},{"location":"9-Drafts/dea-c01-u1-data-ingestion-and-transformation-ps/#consuming-data-apis","title":"Consuming Data APIs","text":"<p>accessing Redshift via HTTP API</p> <ul> <li>Four ways APIs work<ul> <li>SOAP</li> <li>RPC</li> <li>REST</li> <li>WebSocket</li> </ul> </li> </ul> <ul> <li>SOAP<ul> <li>Simple Object Access Protocol</li> <li>Client Server exchange message using XML</li> </ul> </li> </ul> <ul> <li>RPC<ul> <li>Remote Procedure Calls</li> <li>Client completes a function (procedure) on server and server sends back the output.</li> </ul> </li> </ul> <ul> <li>REST<ul> <li>Representational State Transfer</li> <li>Most flexible architecture</li> <li>Client sends request to server as data and server sends back response.</li> </ul> </li> </ul> <ul> <li>WebSocket<ul> <li>Two way communication between client and server</li> <li>Client can get update from server without requesting</li> <li>Uses JSON object to pass data</li> </ul> </li> </ul> <p>AWS Data Exchange API</p> <ul> <li>Requesters can access data via Subscriptions.</li> <li>Providers can create and manage your data before you publish it.</li> </ul> <ul> <li>Two Parts<ul> <li>Data Set</li> <li>Asset</li> </ul> </li> </ul> <ul> <li>Data Set<ul> <li>Collection of data that can changes over time</li> </ul> </li> </ul> <ul> <li>Asset<ul> <li>Any piece of data. The data itself, like image, file, record etc.</li> </ul> </li> </ul> <p>Accessing Redshift via Data Exchange API</p> <ul> <li>The Data Exchange API allows you to access Amazon Redshift.</li> <li>Doesn't require a persistent connection to your database.</li> <li>Provides a secure HTTP endpoint and integration with AWS SDK.</li> </ul> <p>Working with Amazon Redshift Data API</p> <ol> <li>Make sure you (The caller) are authorized to call on the database</li> <li>Determine if you have the authentication credentials, whether temporary or via Secrets Manager</li> <li>(Best Practice) Setup a secret using Secrets Manager</li> <li>Check your limitation while using the data API</li> <li>Call the API using the Command Line Interface (AWS CLI) from code or using the query editor in the Amazon Redshift console</li> </ol> <p>Lambda API</p> <ul> <li>It is best practice to use SDKs instead of calling an API from the Application (that is use Python SDK for Lambda and then write you lambda function)</li> <li>Lambda API is used with serverless applications (simply said, your lambda function).</li> <li>It uses the API gateway which supports: API routing, serving HTML pages or binary files, issuing redirects, and much more.</li> </ul>"},{"location":"9-Drafts/dea-c01-u1-data-ingestion-and-transformation-ps/#security-and-connections","title":"Security and Connections","text":"<p>Two basic ways of controlling security:</p> <p><code>NACLs</code> or Network Access Control Lists</p> <ul> <li>They are serverless, means they inspect all the traffic in n/w that is coming and going from VPC and subnets.</li> <li>Configurable to allow or deny on number of protocols and port ranges.</li> <li>This acts as a firewall in your VPC.</li> </ul> <p>Security Groups</p> <ul> <li>They also help filter out traffic coming into VPC.</li> <li>They are state-full, means they inspect packets with context you built into it. This lets create complex rules, and lets log in detail.</li> <li>You can allow/deny by using IP address or block of IPs.</li> </ul> <p>Architecture</p> <ul> <li>Within VPC you have subnets.</li> <li>Subnets are secured by \"Security Groups\" and NACLs.</li> </ul> <pre><code>\nflowchart TB\n\nsubgraph s1[AWS VPC]\n  direction TB\n  a1[Traffic]\n  a2[Private Subnet\n\n  Private Resources]\n  a1 --Security Group / NACLs--&gt; a2\n\n  a3[Traffic]\n  a4[Public Subnet\n\n  Public Assets\n  Public Resources]\n  a3 --Security Group / NACLs--&gt; a4\nend\n  b1[Traffic] --Internet--&gt; s1\n</code></pre> <p>Best Practices for RDS Security</p> <ul> <li>RDS should be in Private Subnet, so that it cannot be accessed using Internet.</li> <li>You can let other resources access this RDS using NAT Gateway.</li> <li>Ensure developers come from a singular IP address (or IP Group), and you only allow access to RDS via that IP Address.</li> <li>Limit the number of resources that use this particular subnet within the VPC.</li> <li>Set NACLs so that they only allow least necessary traffic to the subnet.</li> </ul>"},{"location":"9-Drafts/dea-c01-u1-data-ingestion-and-transformation-ps/#etl-pipelines","title":"ETL Pipelines","text":"<ul> <li>It is being done from old pen and paper time.</li> <li>It is process of converting data in to meaningful information.</li> </ul> <p>EMR vs Glue</p> <ul> <li>EMR is designed for Integration and ETL; Glue is designed with big data in mind.</li> <li>More customisable options; more built-in capabilities</li> <li>You need to load the data connectors as needed; in-built migration and ETL providers.</li> </ul> <p><code>AMWAA</code> or Amazon Managed Workflows for Apache Airflow</p> <ul> <li>Apache Airflow - Open Source tool used to build, schedule and monitor batch workflows.</li> <li>AMWAA is managed Airflow offering for managed orchestration.</li> <li>It lets you build your cluster, having your web server, scheduler and worker.</li> <li>You can upload DAG folder to S3.</li> <li>Then run DAG in Airflow.</li> </ul> <p>AWS Step Functions</p> <ul> <li>These provide serverless orchestration (AWS build functionality similar to Airflow).</li> <li>GUI to let you see application workflow as event-driven steps.</li> <li>Standard workflow execute each step exactly once.</li> <li>Express workflow have an at-least-once step execution but will run for at least five minutes.</li> </ul> <p>AMWAA vs Step Functions</p> <ul> <li>AMWAA lets auto scale with integration with AWS Fargate; Step Functions allow to scale to meet demand.</li> <li>Parses query and develops query execution plan; Pay-as-you-go model with first 4000 steps free per month.</li> <li>Allows managing complex code based workflows; Suitable for simplifying complex workflows.</li> </ul> <p>Glue Workflow Options</p> <ul> <li>Allows you to create and see complex ETL jobs. These jobs can have crawlers, triggers and other jobs.</li> <li>Workflows can be built using blueprint or can be created from scratch using one component at a time.</li> </ul>"},{"location":"9-Drafts/dea-c01-u1-data-ingestion-and-transformation-ps/#containers-in-data-pipelines","title":"Containers in Data Pipelines","text":"<ul> <li>Containers help making Data Processing in data pipeline more efficient.</li> <li>Monolith data pipeline will have everything on one machine and tightly coupled together.</li> <li>By using containers, we can make each part of pipeline isolated in a container.</li> <li>You can tweak individual pieces, slow them down or scale up to make it more efficient.</li> </ul> <p>Container Types</p> <ul> <li>Amazon Elastic Container Service or ECS</li> <li>Amazon Elastic Kubernetes Service or EKS</li> </ul> <p>Amazon Elastic Container Service or ECS</p> <ul> <li>Orchestrates the deployment of containerised applications and mapping of resources.</li> <li>It lets launch, manage and scale as app grows.</li> </ul> <ul> <li>There are three layers:<ul> <li>Capacity: the infrastructure layer on which the containers run.</li> <li>Controller: the deployment layer where you can manage container and applications.</li> <li>Provisioning: the layer which lets interact with schedulers for deploying and managing containers.</li> </ul> </li> </ul> <p>Amazon Elastic Kubernetes Service or EKS</p> <ul> <li>Managed service that let you interact with control panel of Kubernetes.</li> <li>It allows you to create cluster in which Kubernetes will run.</li> <li>Cluster can be made using Fargate containers or EC2 workers.</li> </ul> <p>ECS vs EKS</p> <ul> <li>AWS Orchestration Engine; EKS lets manage Kubernetes experience.</li> <li>Fine tune auto scalability; Allows multi-cloud deployments.</li> <li>Microservice architecture; Easy integration with other AWS services.</li> </ul>"},{"location":"9-Drafts/dea-c01-u1-data-ingestion-and-transformation-ps/#data-coming-from-everywhere","title":"Data Coming from Everywhere","text":"<p>Out tech stack should be ready to ingest data from multi-sources. For this AWS offers two solutions</p> <ul> <li>EMR Multi-Source and Redshift Multi Warehouse let handle incoming data.</li> </ul> <p>EMR</p> <ul> <li>Big Data Solution, for data processing and ML.</li> <li>Uses Open Source Frameworks like Hive, Spark and Pesto.</li> <li>It can run the app (with above frameworks) on ECS, EKS, Fargate or EC2.</li> <li>It can build cluster with primary, core and task nodes.<ul> <li>primary - manages running the cluster, distributing the data across nodes.</li> <li>core nodes - run the frameworks. They can be one or more.</li> <li>task nodes - don't store data, only transport/transform it. They are optional.</li> </ul> </li> <li>Multi Source setup has multiple primary nodes. So that it can take data simultaneously from many nodes.</li> </ul> <p>Redshift Multi Warehouse</p> <ul> <li>It is multi data warehousing on Redshift.</li> </ul> <ul> <li>It needs connection to a DataShare</li> <li>Can be setup on three clusters, <code>serverless workgroup</code> or <code>ra3.4xl</code> and <code>ra3.16xl</code></li> <li>You need to have metadata discovered in data-share. It can be seen using view catalog data.</li> <li>You need to have encryption set up correctly.</li> </ul> <ul> <li>You need a data warehouse, loaded data from S3, or using query editor.</li> </ul> <ul> <li>Limitations:<ul> <li>It cannot be accessed via Data API. You need to connect via dataShare.</li> <li>Auto Operations are not available, user needs to run queriers for auto-analysing.</li> <li>Users cannot see permission granted by data share.</li> </ul> </li> </ul>"},{"location":"9-Drafts/dea-c01-u1-data-ingestion-and-transformation-ps/#cost-optimisation-in-data-transformation","title":"Cost Optimisation in Data Transformation","text":"<ul> <li>In context of Glue Python Shells, PySpark Jobs and Spark Jobs.</li> <li>All these jobs use DPU on AWS (Data Processing Units)</li> <li>DPU is attached to CPUs. More DPUs can be added to CPU and they will offload the task from CPU hence can process more data.</li> <li>The more DPU your job uses the more you pay.</li> </ul> <ul> <li>Spark<ul> <li>It lets run batch job in Glue</li> <li>Script can be run at ETL script using Glue Command Line.</li> <li>Gives 10 DPUs by default.</li> <li>Billed 1-10 minutes interval.</li> <li>It is designed to run hefty jobs and can scale quickly.</li> <li>It can scale horizontally, vertically or both depending on data coming in.</li> </ul> </li> </ul> <ul> <li>PySpark lets use Spark using Python. Hence makes it easy to use spark and only python can be used to build ETL jobs.</li> </ul> <ul> <li>Python Shell on Glue is cheaper than running Glue on its own.</li> <li>Python Shell uses Spark in Glue and can be run using <code>pythonshell</code> command.</li> <li>It uses only one DPU at start.</li> <li>Billed per hour with minimum 1 minute.</li> <li>Designed to run light etl jobs.</li> </ul>"},{"location":"9-Drafts/dea-c01-u1-data-ingestion-and-transformation-ps/#module-3-applying-programming-concepts","title":"Module 3; Applying Programming Concepts","text":""},{"location":"9-Drafts/dea-c01-u1-data-ingestion-and-transformation-ps/#iac-infrastructure-as-a-code","title":"Iac Infrastructure as a Code","text":"<ul> <li>For autoscaling, you need to build new environments and deploy you code to it. Doing this manually can be challenging and error prone.</li> <li>Using IaC, you can do this using code.</li> <li>CloudFormation lets you build code that creates infra.</li> <li>IaC also gives blueprint of your environment, this lets you see whole env as one picture and see flow of data.</li> <li>It lets do automation and scaling.</li> </ul> <ul> <li>IaC options<ul> <li>Cloud Formation</li> <li>CDK</li> <li>Terraform (Third Party)</li> </ul> </li> </ul> <ul> <li>Cloud Formation AWS build. Can scale to any size.</li> </ul> <ul> <li>CDK layer on cloud formation. It lets build env in different languages. So not only yaml/json, you can use other languages.</li> </ul> <ul> <li>Terraform (Third Party)</li> </ul> <p>Cloud Formation</p> <ul> <li>Write in YAML/JSON</li> <li>It builds Stack.</li> <li>Stack lets build and connect resources, all at once.</li> <li>It offers Change Sets, that lets you see the change in stack and hence reduce error and have more clarity on actual changes.</li> <li>It is quick and easy deployment way.</li> <li>Helps easy roll out with visual instinct.</li> </ul> <p>CDK</p> <ul> <li>you can use JS/Py/C++</li> <li>In IDE, multi users at once.</li> <li>It can be declarative or imperative statements</li> <li>It also has Constructs which are per-build blocks that can be used in code.</li> <li>More developer friendly with write less, deploy more in mind.</li> <li>There are three levels of constructs<ul> <li>Level 1: Cloud Formation Constructs</li> <li>Level 2: Abstract Constructs</li> <li>Level 3: Pattern Constructs</li> </ul> </li> </ul> <ul> <li>Level 1: Cloud Formation Constructs<ul> <li>uses same basic syntax as cloud formation</li> <li>it lets explicitly define all aspects of resources</li> </ul> </li> </ul> <ul> <li>Level 2: Abstract Constructs<ul> <li>Add cloud resources, where some ability is extracted away by CDK.</li> </ul> </li> </ul> <ul> <li>Level 3: Pattern Constructs<ul> <li>These are pre-built custom patterns that define one or more AWS resources. Eg, \"new LambdaRestAPI\" may use Lambda and API Gateway together. So on line construct will create both services connected and working together.</li> </ul> </li> </ul>"},{"location":"9-Drafts/dea-c01-u1-data-ingestion-and-transformation-ps/#sql-data-pivoting","title":"SQL Data Pivoting","text":"<ul> <li>Data pivoting lets you create headers in both rows and columns, just like pivot tables.</li> <li>To do this you need to:<ul> <li>Row Identifiers - any row that contains a value that uniquely identifies rows in the table.</li> <li>Column identifiers - these are columns that can be converted into new columns in combined result set.</li> <li>Aggregates - These are main values you need to summarise or aggregate based on the row and column identifiers.</li> </ul> </li> </ul>"},{"location":"9-Drafts/dea-c01-u1-data-ingestion-and-transformation-ps/#tumbling-windows","title":"Tumbling WIndows","text":"<ul> <li>Streaming data if often received in windows of item or space. Theses windows can have time blocks or number of record blocks.</li> <li>Tumbling Window is when, a windowed query, processes the data inside a window.</li> <li>These windows are non-overlapping.</li> <li>This maintains order, rather than processing everything coming in bulk without order.</li> </ul> <p>Service that let do Tumbling WIndow operations are:</p> <ul> <li>Lambda<ul> <li>you can open your function to take data via window in range of 0 to 900 seconds.</li> <li>The max data it can take in is 5 MB.</li> </ul> </li> </ul> <ul> <li>DynamoDB<ul> <li>Works with Lambda. Lambda will trigger the dynamo DB.</li> <li>Hence, Dynamo DB will be triggered for a window only (as window is implemented in lambda)</li> </ul> </li> </ul> <ul> <li>Kinesis<ul> <li>It sends data to other services.</li> <li>Other services will have window implemented to accept the data, hence kinesis will send data in windowed way.</li> </ul> </li> </ul> <ul> <li>Tumbling Window Aggregation<ul> <li>Application code gets data</li> <li>Runs Queries</li> <li>Results are presented.</li> </ul> </li> </ul> <ul> <li>Windowed SQL Operation can be<ul> <li>Time Based - window size is timed, it requires timestamp column in stream.</li> <li>Row Based - window size is number of rows</li> </ul> </li> </ul> <ul> <li>Three type of Windows<ul> <li>Stagger Windows - Aggregates data using keyed time-based windows when data actually arrives. Allows for overlapping windows, hence reducing late or out of order data.</li> <li>Tumbling Windows - Opens/closes at scheduled intervals, thus allowing data at certain times only.</li> <li>Sliding Windows - window that continuously uses a fixed time or row count before closing.</li> </ul> </li> </ul> <ul> <li> more info required.</li> </ul>"},{"location":"9-Drafts/dea-c01-u1-data-ingestion-and-transformation-ps/#database-connectivity","title":"Database Connectivity","text":"<ul> <li>Four main type of JDBC Drivers<ol> <li>JDBC-ODBC Bridge Drivers translate and convert Java calls to ODBC function calls</li> <li>Native-API Drivers use client-side libraries of the target database</li> <li>The Network Protocol driver uses middleware to convert JDBC calls into a database call</li> <li>Database Protocol Drivers (or thin drivers) changes JDBC calls directly into vendor-specific database protocol</li> </ol> </li> </ul> <ul> <li>Four Steps of ODBC Connection<ol> <li>Applications process and call the ODBC functions then submit the SQL statements</li> <li>The driver manager loads drivers for each application sending a call</li> <li>The drivers handle the actual ODBC call and send the request to a data source</li> <li>The data source is the data being accessed.</li> </ol> </li> </ul> JDBC ODBC java any language any platform windows only Object Oriented Procedural"},{"location":"9-Drafts/dea-c01-u1-data-ingestion-and-transformation-ps/#the-big-data-all-stages-together","title":"The Big Data All Stages Together","text":"<pre><code>flowchart LR\n\na1[Data Sources\n\nFile\nAPI\nDatabase]\n\na2[Data Storage\n\nLands to S3]\n\na3[Real Time Ingestion\n\nKinesis]\n\na1 --&gt; a2\na1 --&gt; a3\n\na4[Batch Processing\n\nGlue]\n\na5[Machine Learning\n\nSageMaker]\n\na6[Stream Processing\n\nFlink]\n\na2 --&gt; a4\na2 --&gt; a5\na3 --&gt; a5\na3 --&gt; a6\n\na7[Analytic\nData Store\n\nData Analysis\nRedshift\nRDS\nDynamoDB]\n\na4 --&gt; a7\na5 --&gt; a7\na6 --&gt; a7\n\na8[Reporting\nand Dashboards]\n\na7 --&gt; a8\n</code></pre>"},{"location":"9-Drafts/dea-c01-u1-data-ingestion-and-transformation-ps/#summary-and-exam-tips-for-data-ingestion-and-transformation","title":"Summary and Exam Tips for Data Ingestion and Transformation","text":"<p>Same as above.</p>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/","title":"U2 Data Store Management","text":""},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#10-data-stores","title":"1.0 Data Stores","text":""},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#11-introduction-to-amazon-s3","title":"1.1 Introduction to Amazon S3","text":"<p>Shared Security Model</p> <ul> <li>Think of it as your responsibility as home owner vs tenant vs air-bnb vs hotel guest. Your responsibility goes down respectively for the examples here. Similarly in AWS responsibility varies.</li> <li>Infra Services such as EC2 requires lot of securing responsibility as you are owner of infra. You own configs, security patches, firewall</li> <li>Manages Services like RDS, Container; you need to do minor updates.</li> <li>Serverless things like S3, Dynamo; AWS manages everything.</li> <li>You need to do<ul> <li>Customer data security using IAM</li> <li>Client side encryption</li> </ul> </li> <li>AWS does:<ul> <li>Server side encryption</li> <li>Network Protection</li> <li>Manges platform, OS, NW, Firewall, Global Infra</li> </ul> </li> </ul> <p>Accessing Object via URL</p> <ul> <li>There are two URLs to access an object.<ul> <li><code>Object URL</code>: Also called public URL, anyone can only access this if they have permission and bucket has public access.</li> <li><code>Open</code>: When you click \"Open\" then it gives you a pre-signed URL that lets you in the session to view an object, this is not shareable. It has encoded credentials.</li> </ul> </li> </ul> <p>Security Layers</p> <ol> <li>VPC, this is first line of defence, like envelop</li> <li>IAM, or Authentication, this is 2nd line of defence, like name and address on letter.</li> <li>encryption, this is third line of defence, even if the message is read, it is meaning less unless you have a decryption key.</li> </ol> <p>Layer 1: S3 and VPC</p> <ul> <li>S3 is accessible over internet. It is independent of VPC.</li> <li>But, You can block public access.<ul> <li>Once blocked you can only make it accessible via VPC, by making private connection between your bucket and VPC.</li> <li>This private connection allows you to connect to S3 bucket internally without traversing via public internet.</li> </ul> </li> </ul> <p>Layer 2: Access Control</p> <p>This has two levels</p> <ol> <li> <p>Identity Based</p> <ul> <li>AWS IAM Policies: are attached to IAM users, groups, or roles</li> <li>Controls access at a granular level based on user identity</li> </ul> </li> <li> <p>Resourced-Based</p> <ul> <li>Access Control Lists (ACLs)<ul> <li>Grant specific permissions to individual AWS accounts/groups</li> <li>Can be configured on the object level or bucket level</li> </ul> </li> <li>Bucket Policies<ul> <li>JSON-based policies that you attach to control access at the bucket level</li> </ul> </li> </ul> </li> </ol> <p>Using above two, you can restrict access to resource as required at desired level.</p> <p>S3 Bucket Policies</p> <ul> <li>It is a JSON with following keys</li> <li><code>Effect</code> tells that it allows or denies</li> <li><code>Principal</code> tell who this policy applies to</li> <li><code>Action</code> tells get or write</li> <li><code>Resource</code> tells on which object the policy is applied to</li> </ul> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Deny\",\n      \"Principal\": \"*\",\n      \"Action\": \"s3:PutObject\",\n      \"Resource\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\",\n    }\n  ]\n}\n</code></pre> <ul> <li>S3 bucket policies allow you to specify a degree of fine-grained access to objects in your bucket.</li> <li>An eg is that, you can<ul> <li>Grant public read access to all objects in an S3 bucket</li> <li>Grace access across accounts</li> <li>Deny unencrypted uploads</li> <li>Define access based on criteria such as:<ul> <li>IP addresses</li> <li>IAM user/role</li> </ul> </li> </ul> </li> </ul> <p>S3 Access Points</p> <ul> <li>This lets more fine grain control on data within a bucket.</li> <li>You can allow an access point to read only certain data within a bucket. And this way you can manage different teams with different access to objects in bucket.</li> </ul> <p>S3 Encryption</p> <ul> <li>Server-Side Encryption (SSE) - Takes place in AWS.</li> <li>Client-side encryption - Takes place outside of AWS.</li> </ul> <ul> <li>Server Side Encryption</li> <li>Also called AT Rest encryption</li> <li>Either the client or AWS can be in charge of the encryption using Keys</li> <li>Two options to do SSE, you can do it using<ol> <li>Customer Provided Keys (SSE-C)<ul> <li>You use your managed keys during uploading objects on to S3.</li> </ul> </li> <li>AWS Key Management Service (KMS), within this you can manage three different keys<ol> <li>AWS Owned Keys - Free, AWS owns and manages</li> <li>AWS Managed Keys - Chargeable, You own, AWS manages</li> <li>Customer Managed Keys - Chargeable. You own and manage</li> </ol> </li> </ol> </li> </ul> <ul> <li>S3 in transit Encryption<ul> <li>S\u00a3 always does the encryption when data is in transit by using SSL/TLS protocols to secure the connection between client and S3</li> </ul> </li> </ul> <ul> <li>Client-side encryption<ul> <li>The client is always in charge of the encryption.</li> <li>Managed by client application.</li> <li>Client app will encrypt the data using keys before the data is sent to AWS.</li> </ul> </li> </ul>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#12-amazon-s3-features-and-capabilities","title":"1.2 Amazon S3 Features and Capabilities","text":"<ul> <li>S3 Versioning<ul> <li>new version is created each time object is updated or created.<ul> <li>eg, if you update <code>index.html</code> it will have a new version.</li> </ul> </li> <li>On delete, S3 keeps the object but adds a delete marker which tells that object is deleted.<ul> <li>to restore deleted bucket, delete the 'deleted marker' version of the bucket, this will make original bucket as latest version which is without delete marker.</li> </ul> </li> <li>Within a bucket, you can configure to version at<ul> <li>Object Level - versions specific objects. Eg, version important files but not logs.</li> <li>Bucket Level - if all objects are important.</li> </ul> </li> <li>It is chargeable</li> <li>It can't be disabled, only suspended.</li> <li>On suspend, future modifications are not versioned but previous versions remain.</li> </ul> </li> </ul> <ul> <li> <p>Replication</p> <ul> <li>Replicate objects from one bucket to another.</li> <li>Replication works only if versioning is enabled.</li> <li>The version number remains the same in both replication. So versions are replicated.</li> <li>Replication needs proper IAM permission in between buckets, so source bucket needs permission to write to destination bucket.</li> <li>Replication and Delete Marker<ul> <li>by default if delete markers are not replicated, that is, if you delete a file in source bucket is marked with delete marker, but in destination bucket, by default if is not marked as deleted.</li> <li>You can enable delete marker replication in settings, then delete marker version is replicated in destination bucket.</li> </ul> </li> </ul> <ul> <li>It can be done two ways<ul> <li>Cross-Region Replication (CRR)</li> <li>Same-Region Replication (SRR)</li> </ul> </li> </ul> <ul> <li>Cross-Region Replication (CRR)<ul> <li>us-east-2 to eu-west-1</li> <li>helpful in disaster recovery</li> <li>compliance and geo redundancy - some compliance may require you to store data in certain regions</li> <li>reduce latency - you can reduce latency by bringing data close to user region.</li> </ul> </li> </ul> <ul> <li>Same-Region Replication (SRR)<ul> <li>us-east-1 to us-east-1</li> <li>disaster recovery use case</li> <li>testing - you can sync test and prod</li> <li>caching - you can access frequently accessed data into caching layer</li> </ul> </li> </ul> <ul> <li>S3 supports Asynchronous Replication, that is, once object is written to primary bucket after that it will be written to replica bucket. This is near real time, not real time.</li> <li>Versioning is required for replication.</li> <li>Once on, old data is not replicated automatically, only newly added data will be. To replicate old data, use S3 Batch Replication.</li> <li>Delete Markers are not replicated by default.</li> </ul> </li> </ul> <ul> <li>Replication Handson<ul> <li>you need to create two buckets, origin and destination</li> <li>you need to enable versioning on both.</li> <li>In settings, Management &gt; 'create replication rules'</li> </ul> </li> </ul> <ul> <li>S3 Notifications<ul> <li>You can send notifications on specific events.</li> <li>You can configure S3 to notify you when one of the following events takes place:<ul> <li>Creating new objects</li> <li>Removing objects</li> <li>Restoring objects</li> <li>Replicating objects</li> <li>Expired S3 lifecycle events</li> <li>Transitioned S3 lifecycle events</li> <li>Automatic archival events from S3 Intelligent-Tiering</li> <li>Tagging objects</li> <li>PUT ACL objects (access control)</li> </ul> </li> <li>You can filter events, example notify create of object only for PNG extension.</li> <li>You can send notifications to:<ul> <li>Amazon SNS</li> <li>Event Bridge - can forward to other services</li> <li>Amazon SQS</li> <li>AWS Lambda</li> </ul> </li> </ul> </li> </ul>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#13-amazon-s3-storage-classes","title":"1.3 Amazon S3 Storage Classes","text":"<p>S3 Storage Classes</p> <ul> <li>S3 offers different storage classes with different pricing, like SSD, HHD etc.</li> </ul> <ul> <li>Determining Ideal Storage Class based on Frequency<ol> <li>Frequently Accessed with low-latency and high-throughput - You often use it, and need it quickly</li> <li>Infrequent access and slightly low latency but high throughput - You don't often use it, but when needed should be easily accessible and quick to read.</li> <li>Infrequent, high latency, low throughput - means you rarely access, takes time to fetch, is slow to read. Used for archival data.</li> </ol> </li> </ul> <ul> <li>Storages on S3<ol> <li>Amazon S3 Standard General Purpose Storage - Use case Gaming, Big Data Analytics.</li> <li>Amazon S3 Standard Infrequent Access Storage - Costs less than general. Less frequent accessed but rapid read.</li> <li>Amazon S3 One Zone Infrequent Access</li> <li>Amazon S3 Glacier Instant Retrieval</li> <li>Amazon S3 Glacier Flexible Retrieval</li> <li>Amazon S3 Glacier Deep Archive - Low cost. For archiving/backing up.</li> </ol> </li> </ul> <p>Infrequent Access - Sub storages</p> <ul> <li>Amazon S3 Standard-Infrequent Access (S3 Standard-IA)<ul> <li>99.9% availability in multiple zones</li> <li>Use Cases: Disaster recovery, backups</li> </ul> </li> </ul> <ul> <li>Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA)<ul> <li>99.5% availability in a single zone</li> <li>Use Cases: Storing secondary backup copies of your data</li> </ul> </li> </ul> <p>Amazon S3 Glacier Types</p> <ul> <li>Instant Retrieval<ul> <li>Offers millisecond retrieval</li> <li>Use Cases<ul> <li>Medical images</li> <li>News media assets</li> <li>Data that needs to be accessed on quarterly basis</li> </ul> </li> </ul> </li> </ul> <ul> <li>Flexible Retrieval<ul> <li>Minimum storage duration of 90 days</li> <li>Retrieval Options:<ul> <li>Expedited - 1 to 5 minutes</li> <li>Standard - 3 to 5 hours</li> <li>Bulk (Free to retrieve, only storage costs) - 5 to 12 hours</li> </ul> </li> </ul> </li> </ul> <ul> <li>Deep Archive<ul> <li>Minimum storage duration of 180 days</li> <li>For long term storage</li> <li>Retrieval Options:<ul> <li>Standard - 12 hours</li> <li>Bulk - 48 hours</li> </ul> </li> </ul> </li> </ul> <p>S3 Intelligent-Tiering</p> <ul> <li>Automatically moves objects between the Access Tiers.</li> <li>Incurs a small monthly fee for auto-tiering</li> </ul> <ul> <li>Frequent Access Tier - Automatically - Default tier</li> <li>Infrequent Access Tier - Automatically - Objects not accessed for 30 days</li> <li>Archive Instant Access Tier - Automatically - Objects not accessed for 90 days</li> <li>Archive Access Tier - Optional - Configurable from 90 days to 730 days</li> <li>Deep Archive Access Tier - Optional - Configurable from 180 days to 730 days</li> </ul> <p>S3 Select and Glacier Select</p> <ul> <li>You can use S3 Select to access data in S3. Select-enabled storage classes, such as Standard and Intelligent-Tiering.</li> <li>You can use Glacier Select to access in Amazon Glacier.</li> <li>Both allow you to use simple SQL statements to retrieve filtered data.</li> <li>Amazon performs server-side filtering which is up to 400% faster and 80% cheaper.</li> </ul>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#14-amazon-s3-lifecycle-rules","title":"1.4 Amazon S3 Lifecycle Rules","text":"<p>S3 Lifecycle Rules</p> <ul> <li>While Intelligent-Tier allows for automatic moves based on access pattern, but you can create your own rules to move things to different tiers.</li> <li>You can use S3 Lifecycle rules to move objects between storage classes and access tiers based on explicitly defined criteria. Eg, certain prefix e.g., <code>s3://my-bucket/jpg/*</code></li> </ul> <p>S3 Actions</p> <ul> <li>You can use actions to move objects from one tier to another or delete forever.</li> </ul> <ul> <li>Transition Actions<ul> <li>Define when and how objects should be moved from one access tier to another.</li> <li>Example: Move objects to Glacier for archiving after one year</li> </ul> </li> </ul> <ul> <li>Expiration Actions<ul> <li>Define at what age should objects expire (be permanently deleted)</li> <li>Examples: Delete after a specified retention period for compliance. Delete old version of files (if versioning is enabled)</li> <li>You won't be charged for storage after expiration time.</li> </ul> </li> </ul> <p>Lifecycle Rule Example Scenario</p> <ul> <li>Use case requirements<ul> <li>Imagine images is uploaded to S3 and then thumbnails are generated. Eg, movie ticket.</li> <li>Users only requires images to be frequently accessed for 60 days and then can wait for up to 6 hours.</li> <li>Thumbnails can be deleted after 60 days and may get regenerated from original image if required.</li> </ul> </li> <li>Lifecycle Setup<ul> <li>Images are stored in Standard for 60 days<ul> <li>Then rule moves them to Glacier after 60 days</li> </ul> </li> <li>Thumbnails are store on One-Zone IA as they don't need multi-zone disaster recovery<ul> <li>They can expire after 60 days.</li> </ul> </li> </ul> </li> </ul> <p>Scenario 2 For Versioning and Lifecycle Rules</p> <ul> <li>Use Case<ul> <li>Consider that you need to recover deleted file immediately for 30 days.</li> <li>After 30 days you can wait for 42 hours to recover a deleted file.</li> </ul> </li> <li>Versioning<ul> <li>You need to enable versioning so that deleted object get deleted marker</li> </ul> </li> <li>Lifecycle Rule<ul> <li>Keep objects in standard class for 30 days</li> <li>After 30 days, transition object having delete marker to Standard Infrequent Access</li> <li>After 1 year move them to Glacier Depp Archive to save cost.</li> </ul> </li> </ul>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#15-amazon-s3-security","title":"1.5 Amazon S3 Security","text":"<p>Analytics on Access Patter and Transition</p> <ul> <li>You can Generates CSV reports with recommendations on how to transition objects based on<ul> <li>Access patterns for S3 objects<ul> <li>Access frequency</li> <li>Last access time</li> <li>Total data scanned</li> </ul> </li> <li>Cost analysis</li> </ul> </li> <li>Using these patterns, you can come up with lifecycle rules<ul> <li>Object in Standard and Standard IA</li> <li>This feature isn't available for objects stored in One-Zone lA or Glacier</li> </ul> </li> </ul>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#16-introduction-to-amazon-ec2","title":"1.6 Introduction to Amazon EC2","text":"<ul> <li>EC2 is service on AWS that allows you to rent the virtual server (compute) on the cloud.</li> <li>Get it simply via console or by commands in API.</li> </ul> <ul> <li>Benefits compared to physical servers<ul> <li>EC2 can grow and shrink</li> <li>Easy to launch in minutes</li> <li>You pay less, only for what you use.</li> </ul> </li> </ul> <ul> <li>Choosing Instance Types based following<ul> <li>Power - how powerful machine is required for use case?</li> <li>CPUs - the CPUs required? eg, <code>M5-Large</code> gives 2 vCPUs, while <code>C54-X-Large</code> gives 16 vCPUs</li> <li>Memory - the Ram required?</li> </ul> </li> </ul> <ul> <li>General Purpose Instances<ul> <li>Power: A balanced performance of compute, memory, and networking resources</li> <li>vCPUS: A balanced ratio of vCPUs to memory</li> <li>Memory: A balanced ratio of memory to vCPUs</li> <li>Great for diverse workloads</li> <li>Examples include <code>t3, t3a, t4</code></li> </ul> </li> </ul> <ul> <li>Compute Optimized Instances Offer<ul> <li>Power: High computational power</li> <li>vCPUs: A higher number of vCPUS relative to memory</li> <li>Memory: Sufficient memory to support most workloads</li> <li>Great for applications that demand a lot of computational power</li> <li>Examples include <code>c6g, c5, c5a, c5n</code></li> </ul> </li> </ul> <ul> <li>Memory Optimized Instances<ul> <li>Power: A large amount of RAM</li> <li>vCPUS: Typically offer a lower number of vCPUs compared to other instance types</li> <li>Memory: Highest memory capacities</li> <li>Great for memory-intensive workloads</li> <li>Examples include <code>r5, r5a, r5n</code></li> </ul> </li> </ul> <ul> <li>Storage Optimized Instances<ul> <li>Power: High-speed storage</li> <li>vCPUs: Less vCPUs</li> <li>Memory: Moderate to high memory</li> <li>Great for low-latency storage for data-intensive workloads</li> <li>Examples include <code>13, 13en, d2, h1</code></li> </ul> </li> </ul> <ul> <li>EC2 doesn't offer persistent storage<ul> <li>that is, if EC2 is terminated the data stored is lost permanently.</li> <li>For persistence, you can use EBS or EFS to store data beyond life cycle of ec2.</li> <li> replace ec2 with EC2. s3 with S3.</li> </ul> </li> </ul> <ul> <li>Elastic Block Storage EBS<ul> <li>It is more like attached Local Disk to a Computer</li> <li>Each disc is mapped to individual EC2</li> <li>Data is persisted beyond ec2 life.</li> </ul> </li> </ul> <ul> <li>Instance Store<ul> <li>This is a block storage attached to EC2.</li> <li>It persists on reboot, but is lost when hibernated or terminated.</li> <li>It is used more for caching.</li> </ul> </li> </ul> <ul> <li>Elastic File System EFS<ul> <li>It is more like network storage or shared file storage.</li> <li>It is mounted on to ec2.</li> <li>It is offered by service like EFS for linux, FSx for windows.</li> <li>Used cases, content sharing and distributed file storage.</li> <li>The storage can content can be used by multiple EC2s for different purpose.</li> </ul> </li> </ul> <p>Different types of EBS volumes</p> <ul> <li>General Purpose (gp2)</li> <li>Provisioned IOPS (io1)</li> <li>Throughput Optimized (st1)</li> <li>Cold HDD (sc1)</li> <li>Magnetic</li> </ul> <p>EC2 Pricing Models</p> <ul> <li>On Demand Allows you pay by the hour or the second, depending on the type of instance.</li> </ul> <ul> <li>Reserved - Reserves capacity for 1 or 3 years depending on the contract for up to 72% discount on the hourly charge.</li> </ul> <ul> <li>Spot - Allows you to purchase unused capacity at a discount of up to 90%.</li> </ul> <ul> <li>Dedicated - A physical EC2 server dedicated for you.</li> </ul>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#20-data-formats","title":"2.0 Data Formats ===================","text":""},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#21-types-of-data-formats","title":"2.1 Types of Data Formats","text":""},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#organizational-styles-and-data-formats-analogy","title":"Organizational Styles and Data Formats Analogy","text":"<ol> <li> <p>Ultra Organizer (Structured Data):</p> <ul> <li>Highly organized and follows a predefined schema (tables, rows, columns).</li> <li>Data relationships are clearly defined, making it easily queryable.</li> <li>Example: SQL databases, CSV files, spreadsheets.</li> <li>Like a well-organized closet, you can quickly find what you're looking for.</li> </ul> </li> <li> <p>Semi-Structured (Semi-Structured Data):</p> <ul> <li>Contains organizational elements (tags, keys, attributes) but doesn\u2019t follow a rigid schema.</li> <li>Provides some sort of hierarchy</li> <li>More flexible than structured data, but less queryable.</li> <li>Example: JSON, XML.</li> <li>Similar to a semi-organized closet\u2014requires some effort to navigate but still manageable.</li> </ul> </li> <li> <p>Chaotic (Unstructured Data):</p> <ul> <li>Lacks a specific schema, and data relationships are not clear, making it hard to query.</li> <li>Requires pre-processing before analysis.</li> <li>Example: Images, audio, video, social media posts, text and docs.</li> <li>Like a messy closet, difficult to find what you need.</li> </ul> </li> </ol>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#common-data-formats","title":"Common Data Formats","text":"<ol> <li> <p>CSV (Comma-Separated Values):</p> <ul> <li>Structured data in tabular form.</li> <li>Use cases:<ul> <li>Data exchange between applications (e.g., database export/import).</li> <li>With Programming Languages, like python to read and write data. Libraries in programming languages (e.g., Python's CSV module) handle CSV files.</li> <li>Quick data analysis with tools like Excel or Google Sheets.</li> <li>Data backup and archival, especially for tabular data.</li> <li>Human Readable and Editable format.</li> </ul> </li> </ul> </li> <li> <p>JSON (JavaScript Object Notation):</p> <ul> <li>Semi-structured data organized in key-value pairs.</li> <li>Use cases:<ul> <li>Configuration files and settings. Simple, readable.</li> <li>Data exchange between web servers and browsers (common in APIs).</li> <li>High interoperability across different programming languages.</li> <li>Can be used for backup and archiving.</li> </ul> </li> </ul> </li> <li> <p>Avro:</p> <ul> <li>Represents data in a way that is easy to serialize (convert to binary) and deserialize (convert back to original structure)</li> <li>Row-based format with JSON-like syntax and an optional declarative schema.</li> <li>Use cases:<ul> <li>Compact binary format for smaller payloads.</li> <li>Ideal for big data processing (e.g., Hadoop, Spark, Kafka).</li> <li>Easily handles schema changes without breaking compatibility.</li> <li>Ensures data validation and strict typing for high interoperability.</li> </ul> </li> </ul> <pre><code>// avro data with schema defined\n{\n   \" type \" : \"record\",\n   \" name \" : \"user\",\n   \" fields \" : [\n      { \"name\" : \" Name\" , \"type\" : \"string\" },\n      { \"name\" : \"age\" , \"type\" : \"int\" }\n   ]\n}\n{\n   \"name\": Vaibhav,\n   \"age\": 32\n}\n</code></pre> </li> <li> <p>Parquet:</p> <ul> <li>Open source, Column-oriented format designed for data warehousing and analytical queries.</li> <li>Similar to avro, but data and schema are combined into one.</li> <li>Use cases:<ul> <li>Efficient data analysis (e.g., aggregation queries).</li> <li>Optimized for Hadoop, Spark, Kafka, and other big data tools.</li> <li>Supports frequent schema changes without breaking compatibility.</li> <li>Allows selective column reading for faster data access.</li> </ul> </li> </ul> </li> </ol>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#key-differences-between-avro-and-parquet","title":"Key Differences Between Avro and Parquet","text":"<ul> <li>Storage Type:<ul> <li>Avro: Row-based storage.</li> <li>Parquet: Column-oriented storage.</li> </ul> </li> </ul> <ul> <li>Schema Definition:<ul> <li>Avro: Uses a JSON-based schema.</li> <li>Parquet: Uses its own schema definition language.</li> </ul> </li> </ul> <ul> <li>Integration:<ul> <li>Both Avro and Parquet are widely used in Apache ecosystems (Hadoop, Spark, Kafka).</li> <li>Parquet is optimized for Impala, while Avro is supported by Impala.</li> </ul> </li> </ul>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#22-transforming-data-formats","title":"2.2 Transforming Data Formats","text":""},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#etl-and-data-transformation-with-aws-glue","title":"ETL and Data Transformation with AWS Glue","text":"<p>AWS Glue offers several ways to transform data:</p> <ul> <li>Python shell jobs for quick data manipulation.</li> <li>Spark ETL jobs for large-scale transformations.</li> <li>PySpark or Scala jobs for batch and streaming data processing.</li> </ul> <ol> <li> <p>Python Shell ETL Jobs in Glue:</p> <ul> <li>Suitable for simpler ETL tasks (e.g., small datasets).</li> <li>Provides pre-built libraries for transforming data between formats, such as CSV to Parquet.</li> <li>Libraries for aggregating data (e.g., sums, averages) and reading/writing formats.</li> <li>Use case: Reading a CSV file from an S3 bucket, converting it to Parquet using the <code>CSV</code> module and <code>PyArrow</code>.</li> </ul> </li> <li> <p>Spark ETL Jobs in Glue:</p> <ul> <li>Ideal for complex transformations and large-scale data.</li> <li>Apache Spark is a distributed computing system that supports:<ul> <li>Filtering data based on specific criteria.</li> <li>Aggregating data (e.g., sums, averages, counts).</li> <li>Joining data from multiple datasets using common keys.</li> </ul> </li> <li>Use case: Processing large datasets in Redshift to identify top-selling products through aggregation.</li> </ul> </li> <li> <p>PySpark or Scala for Batch and Streaming Jobs in Glue:</p> <ul> <li>Batch Processing: Processing data in fixed-size batches (e.g., daily sales reports).<ul> <li>Example: A retail company collects daily sales data in CSV format. PySpark or Scala can read the CSV, transform, and aggregate the data.</li> </ul> </li> <li>Streaming Processing: Processing data in micro-batches at regular intervals (e.g., real-time data analysis).<ul> <li>Example: analysing real-time clickstream data using Amazon Kinesis and processing it with PySpark or Scala for real-time insights.</li> </ul> </li> </ul> </li> </ol> <p>[ ] - do handson for above</p>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#30-databases","title":"3.0 Databases ======================","text":""},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#31-introduction-to-amazon-dynamodb","title":"3.1 Introduction to Amazon DynamoDB","text":""},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#aws-dynamodb","title":"AWS DynamoDB","text":"<p>Overview of DynamoDB</p> <ul> <li>DynamoDB is a fully managed, NoSQL non-relational database service.</li> <li>It supports key-value pairs and document data (JSON, HTML, XML).</li> <li>Flexible table adaptation, but lacks joins and analytical query capabilities.</li> <li>Access patterns must be defined before table creation?</li> <li>Unlimited storage, with single-digit millisecond response times.</li> <li>Use DynamoDB Accelerator (DAX) for microsecond latency.</li> <li>It is limitless, can handle Terabytes smoothly.</li> </ul> <p>Benefits of DynamoDB</p> <ol> <li> <p>Global Tables across Regions:</p> <ul> <li>Replicate data across AWS regions for fast, responsive access worldwide.</li> </ul> </li> <li> <p>DynamoDB Streams:</p> <ul> <li>Captures time-ordered modifications to database items.</li> <li>That is, it stores any modification to item in a table like created, updated etc.</li> </ul> </li> <li> <p>Partitioning and Availability:</p> <ul> <li>DynamoDB automatically scales by partitioning data. You need not take this step.</li> <li>Replicates data across three availability zones for fault tolerance.</li> </ul> </li> </ol> <p>Use Cases</p> <ul> <li>Media and metadata storage (photos, videos).</li> <li>Retail applications (high traffic, e.g., during holidays). It can handle millions of request per second.</li> <li>Gaming platforms (large-scale data handling).</li> <li>Online transaction processing (OLTP) (financial transactions, e-commerce).</li> <li>Hierarchical data models (e.g., employee directories).</li> <li>Fluctuating workloads (e.g., social media, flash sales).</li> <li>Mission-critical applications (healthcare, online banking). When downtime is not an option.</li> </ul> <p>DynamoDB compared to Relational Database</p> <ol> <li>Tables: Similar to SQL tables.</li> <li>Items: Equivalent to rows/records in SQL.</li> <li>Attributes: Equivalent to columns/fields in SQL.<ul> <li>A unique group of attributes forms a single item.</li> <li>Max item size is 400 KB.</li> </ul> </li> <li>Primary Keys:<ul> <li>Consist of one or two attributes.</li> <li>Used to retrieve data. Selecting the right primary key is crucial for table design.</li> <li>This is why knowing the access pattern is important as based on that you would define the primary key and then based on that you will retrieve the data.</li> </ul> </li> </ol> <p>Time to Live (TTL)</p> <ul> <li>Adds an expiry time to items.</li> <li>Items are automatically deleted once expired, including from indexes.</li> <li>Deletion occurs within 48 hours of expiration.</li> <li>Delete data might appear in result, you need to add filter to remove it.</li> </ul> <ul> <li>Use Case:<ul> <li>Use TTL for deleting sensitive data. Add expiry based on contract to keep PII.</li> <li>session data, event logs. Add expiry so that they get auto deleted after time and save on storage costs.</li> <li>for debugging. Add expiry to logs so that after debugging logs are auto deleted.</li> </ul> </li> </ul>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#32-amazon-dynamodb-dealing-with-rate-limits-and-throttling","title":"3.2 Amazon DynamoDB: Dealing with Rate Limits and Throttling","text":""},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#dynamodb-throughput","title":"DynamoDB Throughput","text":"<p>Throughput means rate at which data can be read and written to database. It depends on Capacity Modes. You basically can pre-define the unites required or pay as you go (more expensive) if load is unpredictable.</p> <p>Two capacity modes are:</p> <ol> <li>Provisioned Capacity Mode - You predict and specify the units in advance.</li> <li>On-Demand Capacity Mode - You are charges pay as you go.</li> </ol>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#dynamodb-capacity-modes","title":"DynamoDB Capacity Modes","text":"<p>Provisioned Capacity Mode:</p> <ul> <li>Throughput is calculated and provisioned using Read Capacity Units (RCUs) for read operations and Write Capacity Units (WCUs) for write operations.</li> <li>Suitable for predictable traffic and consistent capacity needs.</li> <li>Risk of over-provisioning (unnecessary charges) or under-provisioning (throttling).</li> <li>Offers consistent performance with a maximum of 40,000 RCUs/WCUs per table.</li> <li>Use Cases: Traffic that is predictable and then ramps up gradually.</li> </ul> <p>On-Demand Capacity Mode:</p> <ul> <li>No need to provision throughput; DynamoDB charges for Read/Write Request Units.</li> <li>Automatically scales based on demand.</li> <li>More expensive per request but removes the risk of under/over-provisioning.</li> <li>Best for unpredictable traffic or new tables with unknown workloads.</li> <li>Throttling occurs if demand exceeds 2x the previous peak within 30 minutes.</li> <li>Can switch between modes every 24 hours. (on demand to provision and vice versa)</li> <li>Use Cases: Where load is unpredictable.</li> </ul> <p>Scaling in Provisioned Mode</p> <ul> <li>Auto-scaling available to manage fluctuations in traffic.</li> <li>Auto-scaling: Adjusts capacity between a defined minimum and maximum range. Range helps control cost.</li> <li>Manual Scaling: You define the exact capacity units without auto-scaling.</li> <li>Recommended to switch to provisioned mode once the app\u2019s traffic becomes predictable.</li> </ul> <p>Over Throttling</p> <ul> <li>User may send more read/write request than provisioned. This will throttle and throw <code>ProvisionedThroughputExceededException</code>. To save from this, Burst Capacity comes in.</li> </ul> <p>Burst Capacity</p> <ul> <li>Burst capacity provides temporary additional throughput by storing unused capacity from partitions for up to 5 minutes (300 seconds).</li> <li>Helpful during traffic spikes, but shouldn't be relied upon as the primary solution.</li> </ul> <p>Solutions to Avoid Throttling</p> <ol> <li>Switch to On-Demand Mode: Avoids under-provisioning issues by automatically scaling capacity.</li> <li>Increase RCUs/WCUs: Increase provisioned throughput to match demand.</li> <li>AWS Application Auto Scaling: Automatically adjusts provisioned throughput based on traffic.</li> <li>Retry Logic with Exponential Backoff: Implement retry logic to delay retries after failures, using an increasing time delay.</li> <li>Optimize Queries:<ul> <li>Avoid querying unnecessary data.</li> <li>Use WHERE clauses to filter and retrieve only necessary data.</li> <li>Use Projection Expressions to retrieve specific attributes, reducing capacity usage.</li> </ul> </li> <li>Hot Partition Resolution:<ul> <li>Avoid hot partitions (when one partition receives disproportionate traffic).</li> <li>Analyze access patterns and partition keys to distribute traffic evenly across partitions.</li> </ul> </li> </ol>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#dynamodb-pricing-model","title":"DynamoDB Pricing Model","text":"<p>Cost is calculated separately for data storage, data read, data write. So:</p> <ul> <li>Idle table (not read or written) is only charged for storage and backups.</li> </ul>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#33-amazon-dynamodb-partiql","title":"3.3 Amazon DynamoDB: PartiQL","text":"<p>You can read from dynamo db by making API call (something like MongoDB calls), you cannot use SQL directly, however, DynamoDB offers a wrapper which helps you query by SQL Syntax (behind the scene it converts to API calls and hence may not be always efficient).</p> <p>PartiQL Editor</p> <ul> <li>PartiQL is a SQL-like syntax tool used to query DynamoDB tables via the AWS Console.</li> <li>It simplifies working with DynamoDB for developers familiar with SQL, supporting common statements like INSERT, UPDATE, SELECT, and DELETE.</li> <li>Query results can be displayed in table view or JSON view.</li> </ul> <p>Accessing PartiQL</p> <ul> <li>You can use PartiQL through:<ul> <li>AWS Console</li> <li>AWS CLI (Command Line Interface)</li> <li>DynamoDB APIs</li> <li>NoSQL Workbench (a graphical tool you can install locally).</li> </ul> </li> </ul> <p>Behind the Scenes of PartiQL</p> <ul> <li>PartiQL queries are automatically translated into DynamoDB API operations, but this process isn't always efficient.</li> <li>Scans and queries are the two DynamoDB operations relevant to PartiQL.</li> </ul> <p>Scans vs. Queries</p> <ul> <li>Scans:<ul> <li>It is equivalent to <code>SELECT *</code> in SQL.</li> <li>Command: <code>aws dynamodb scan --table-name</code>.</li> <li>Expensive because they read the entire table and then apply filters after consuming capacity units, leading to high throughput consumption.</li> <li>If table is large, scan and consume your throughput capacity quickly..</li> </ul> </li> </ul> <ul> <li>Queries:<ul> <li>It lets you find items using primary key, hence reads less data and is cost effective.</li> <li>Less expensive than scans, as they consume fewer capacity units.</li> <li>Command: <code>aws dynamodb query --table-name</code></li> </ul> </li> </ul> <p>Best Practices to Avoid Inefficient PartiQL Queries</p> <ol> <li> <p>Deny Scan Operations:</p> <ul> <li>Use AWS IAM policies to deny scan permissions for the identity running PartiQL statements.</li> </ul> </li> <li> <p>Create Secondary Indexes:</p> <ul> <li>Write queries that utilize these indexes to target specific data and improve performance.</li> </ul> </li> <li> <p>Monitor Query Performance:</p> <ul> <li>Regularly analyze query performance to detect and resolve full scans early.</li> </ul> </li> </ol>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#34-amazon-redshift-distribution-styles","title":"3.4 Amazon Redshift Distribution Styles","text":"<p>Distribution Styles Overview</p> <ul> <li>Distribution styles determine how data is spread across compute nodes in an Amazon Redshift cluster. There are four main styles:</li> </ul> <ol> <li> <p>EVEN Style:</p> <ul> <li>Data is distributed evenly across all compute nodes, regardless of any column values.</li> <li>Best suited for balanced workloads.</li> <li>Recommended for tables that don't participate in JOIN operations to avoid data skew.</li> </ul> </li> <li> <p>KEY Style:</p> <ul> <li>Data is distributed based on specific key values, where identical key values are placed on the same compute node.</li> <li>Ideal for JOIN-heavy queries when there is a clear distribution key.</li> </ul> </li> <li> <p>ALL Style:</p> <ul> <li>The entire table is replicated across all nodes.</li> <li>Suitable for small, static tables that don't undergo frequent updates.</li> <li>Not recommended for frequently written tables, as changes must be applied to every node in the cluster.</li> </ul> </li> <li> <p>AUTO Distribution:</p> <ul> <li>Redshift automatically determines the best distribution style based on the table size and characteristics.</li> <li>May initially use the ALL style for small tables and switch to EVEN style as the table grows.</li> <li>Recommended when the table size is unpredictable or if you're unsure which style to choose.</li> </ul> </li> </ol> <p>Recommendations</p> <ul> <li>Use EVEN for non-JOIN tables.</li> <li>Use KEY for JOIN-heavy queries with a clear distribution key.</li> <li>Use ALL for small, static tables that aren't frequently updated.</li> <li>Use AUTO if the table size is likely to change or when you're uncertain which distribution style fits best.</li> </ul>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#35-amazon-redshift-workload-management-wlm","title":"3.5 Amazon RedShift Workload Management (WLM)","text":"<p>Superstore Analogy</p> <ul> <li>Consider the checkout system of superstore, there can be different type of queues<ul> <li>based on quantity (customer with trolley, or customer with one item)</li> <li>based on type (premium customer, priority maternity disables old, regular)</li> </ul> </li> <li>These queues need to be manged, such as full trolley customer does not block single item person, old people can go to another queue and so on.</li> </ul> <ul> <li>Amazon Redshift Workload Management also offers queue management in similar way:<ul> <li>Long-running queries (quantity based)</li> <li>Short fast-running queries queue (quantity based)</li> <li>Data Analytics queue (role based)</li> <li>Superuser queue (role based)</li> </ul> </li> </ul> <p>Purpose of WLM</p> <ul> <li>Redshift Workload Management (WLM) helps prioritize and manage queries to optimize system performance.</li> <li>It categorizes queries into distinct queues based on roles, query types, or importance. For example, there can be queues for:<ul> <li>Long-running queries</li> <li>Short and fast-running queries</li> <li>Specific roles like data analytics teams</li> </ul> </li> <li>There is also a default super user queue for critical system operations such as administration and maintenance.</li> </ul> <p>Benefits of WLM</p> <ul> <li>Prevents long-running queries from holding up short queries.</li> <li>Ensures high-priority queries (e.g., system-critical tasks) are not delayed by exploratory or less important queries.</li> </ul> <p>Setting Up WLM</p> <ul> <li>WLM is configured through parameter groups in Redshift, which manage database settings.</li> <li>Up to eight queues can be created, each with its own concurrency level (number of concurrent queries).<ul> <li>For example, if the concurrency level is set to 1, only one query can run at a time in that queue; if set to 5, five queries can run simultaneously.</li> </ul> </li> </ul> <p>WLM Modes</p> <ol> <li> <p>Automatic Mode:</p> <ul> <li>Redshift manages concurrency and resource allocation (like memory) automatically based on query workload.</li> <li>Useful for demanding queries (e.g., hash joins between large tables) where Redshift will lower concurrency for better performance.</li> <li>Offers priority settings (CRITICAL, HIGHEST, HIGH, NORMAL, LOW, LOWEST) to assign importance to queries.</li> <li>Default mode for Redshift's default parameter group.</li> </ul> </li> <li> <p>Manual Mode:</p> <ul> <li>Requires creating a custom parameter group and manually managing concurrency.</li> <li>You can set a concurrency level of up to 50 per queue and for all queues combined.</li> <li>Redshift automatically creates two default queues:     - A queue with a concurrency level of 5.     - A super user queue with a concurrency level of 1.</li> </ul> </li> </ol> <p>Key Considerations</p> <ul> <li>Even in automatic mode, AWS recommends creating a separate custom parameter group for better control over configurations.</li> <li>WLM is ideal for balancing query workloads, avoiding bottlenecks, and ensuring critical tasks are prioritized effectively.</li> </ul>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#36-amazon-redshift-system-tables-and-views","title":"3.6 Amazon Redshift System Tables and Views","text":""},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#amazon-redshift-system-tables-and-views-study-notes","title":"Amazon Redshift System Tables and Views - Study Notes","text":"<p>System Tables</p> <ul> <li>Purpose Contain metadata and diagnostic information about the database, cluster performance, query execution, and overall health.</li> <li>Not meant to be modified directly by users but useful for diagnostics and troubleshooting.</li> </ul> <p>Examples of System Tables</p> <ol> <li> <p>STL Query</p> <ul> <li>Provides information about executed queries.</li> <li>Includes query ID, start and end times, query text, and error messages.</li> </ul> </li> <li> <p>STL Query Metrics</p> <ul> <li>Provides metrics on individual executed queries such as SPU usage, I/O statistics, and memory usage.</li> <li>Valuable for performance tuning and troubleshooting.</li> </ul> </li> <li> <p>STV Query Metrics</p> <ul> <li>Aggregates metrics from all queries executed on the cluster.</li> <li>Includes total execution time, total rows processed, and total bytes processed.</li> </ul> </li> <li> <p>STL WLM Query</p> <ul> <li>Stores information about queries executed or running within workload management queues.</li> <li>Includes resource consumption data, useful for assessing query performance and workload management.</li> </ul> </li> <li> <p>STL Alert Event Log</p> <ul> <li>Contains information about system alerts and events related to hardware failures, software errors, or resource constraints.</li> </ul> </li> <li> <p>STL Explain</p> <ul> <li>Stores execution plans generated by the query optimizer.</li> <li>Helps in optimizing queries by analysing execution plans.</li> </ul> </li> <li> <p>STL Scan</p> <ul> <li>Contains detailed information about table scans during query execution.</li> <li>Includes table ID, scan duration, and number of rows scanned.</li> </ul> </li> </ol> <p>System Views</p> <ul> <li>Purpose Use system tables to provide a consolidated summary and snapshot of the cluster\u2019s data.</li> <li>Types<ul> <li>STL Views Consolidate data from STL tables for monitoring database activity.</li> <li>SVV Views Track various system aspects like configuration, user sessions, and table distribution.</li> </ul> </li> </ul> <p>STV Tables</p> <ul> <li>Concept STV tables revolve around nodes and slices in Redshift, analogous to a library with shelves (nodes) and books (data blocks).</li> </ul> <p>Examples of STV Tables</p> <ol> <li> <p>STV Block List</p> <ul> <li>Contains information about all blocks in the cluster storage (books in the library).</li> </ul> </li> <li> <p>STV Slices</p> <ul> <li>Provides details on the mapping between slices and nodes.</li> <li>Useful for monitoring resource distribution across nodes.</li> </ul> </li> <li> <p>STV Table Perm</p> <ul> <li>Contains information about granted permissions.</li> <li>Each row includes details like table ID and username/role with granted permissions.</li> </ul> </li> </ol>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#37-dense-compute-versus-dense-storage-clusters","title":"3.7 Dense Compute versus Dense Storage clusters","text":"<p>Cluster Types</p> <ol> <li> <p>Single Node Cluster</p> <ul> <li>Consists of a single node that combines the functionalities of both leader and compute nodes.</li> <li>Leader Node Coordinates the overall operation of the cluster.</li> <li>Compute Node Processes data and executes queries.</li> </ul> </li> <li> <p>Multi-Node Cluster</p> <ul> <li>Consists of one leader node and one or more compute nodes.</li> </ul> </li> </ol> <p>Node Types</p> <ol> <li> <p>Dense Compute Nodes</p> <ul> <li>Optimized for computational performance.</li> <li>High CPU and memory resources.</li> <li>Prioritize compute over storage.</li> <li>Smaller capacity; higher cost per terabyte.</li> </ul> <ul> <li>Use Cases<ul> <li>High-query processing.</li> <li>Complex analytical queries.</li> <li>Concurrent queries or real-time analytics.</li> <li>Memory-intensive workloads.</li> </ul> </li> </ul> </li> <li> <p>Dense Storage Nodes</p> <ul> <li>Optimized for storage.</li> <li>Large storage capacity.</li> <li>Lower cost per terabyte; less expensive.</li> <li>Use Cases<ul> <li>Handling terabytes or petabytes of data.</li> <li>Balancing compute and storage needs.</li> <li>Lower cost for large storage but slower performance compared to dense compute.</li> </ul> </li> </ul> <ul> <li>Note Not inherently slower but may show slightly lower computational performance for intensive processing.</li> </ul> </li> <li> <p>RA3 Nodes</p> <ul> <li>Scales compute and storage independently.</li> <li>Automatically offloads data to S3 when local SSD capacity is exceeded.</li> <li>Use Cases<ul> <li>Recommended over dense storage nodes for better scalability.</li> </ul> </li> <li>Cost Structure<ul> <li>Charges compute and storage separately.</li> <li>Requires tracking costs for both compute nodes and data stored in S3.</li> </ul> </li> </ul> </li> </ol> <ul> <li>Dense Storage vs. RA3<ul> <li>Dense Storage Charges combine compute and storage; simpler cost tracking.</li> <li>RA3 Separates charges; potentially more complex cost management.</li> </ul> </li> </ul> <p>Summary</p> <ul> <li>Dense Compute Best for high performance and intensive computational tasks.</li> <li>Dense Storage Best for large data volumes and lower storage cost; suitable if performance is less critical.</li> <li>RA3 Offers flexible scaling and is recommended for scenarios where scalability and cost tracking are key considerations.</li> </ul>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#38-amazon-redshift-spectrum-and-materialized-views","title":"3.8 Amazon RedShift Spectrum and Materialized Views","text":"<p>Scaling Options</p> <ol> <li> <p>Concurrency Scaling</p> <ul> <li>Adds temporary compute power to handle spikes in concurrent read requests.</li> <li>Supports parallel query execution.</li> </ul> </li> <li> <p>Cluster Resizing</p> <ul> <li>Horizontal Scaling Add or remove nodes from the cluster.</li> <li>Vertical Scaling Change node types to scale up or down.</li> </ul> </li> </ol>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#redshift-spectrum","title":"Redshift Spectrum","text":"<ul> <li>Query large volumes of data stored in S3 without loading it into Redshift.</li> <li>Direct querying of exabytes of data in S3.</li> <li>Managed automatic scaling behind the scenes.</li> </ul> <ul> <li>Requirements<ul> <li>Requires a Redshift cluster for interface.</li> <li>Cluster and S3 bucket must be in the same region.</li> <li>Multiple Redshift clusters can query the same S3 data concurrently.</li> </ul> </li> </ul> <ul> <li>Data Handling<ul> <li>External read-only tables are created in Redshift to reference S3 data.</li> <li>Supports <code>SELECT</code> and <code>INSERT</code>; does not support <code>UPDATE</code> or <code>DELETE</code>.</li> <li>Uses external tables to specify data format, location, and structure.</li> </ul> </li> </ul> <ul> <li>Data Store Integration<ul> <li>To ingest data in External Tables you can use<ul> <li>AWS Glue Data Catalog</li> <li>Amazon Athena</li> <li>EMR cluster with an Apache Hive meta-store</li> </ul> </li> </ul> </li> </ul>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#federated-query","title":"Federated Query","text":"<ul> <li>Query data across various databases, warehouses, and data lakes.</li> <li>Combines data from Redshift with external databases like S3, RDS (PostgreSQL, MySQL), and Aurora.</li> <li>Perform complex joins and quick transformations without needing an ETL pipeline.</li> </ul>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#views-and-materialized-views","title":"Views and Materialized Views","text":"<p>Regular Views</p> <ul> <li>Virtual tables created from saved queries that retrieve data from underlying tables.</li> <li>Data is fetched each time the view is queried, similar to a social media feed that updates with the latest data.</li> </ul> <p>Materialized Views</p> <ul> <li>Physical snapshots of query results stored in the view itself.</li> <li>Queries retrieve data from the stored snapshot rather than executing the query each time.</li> <li>Use Case Suitable for predictable and recurring queries, such as end-of-quarter reports.</li> <li>Creation Example</li> </ul> <pre><code>CREATE MATERIALIZED VIEW view_name AS\nSELECT columns\nFROM employee_table\nJOIN department_table;\n</code></pre> <ul> <li>Refreshing Views</li> <li>Auto Refresh Enabled to update the view when the source data changes.</li> <li>Manual Refresh Use the command:</li> </ul> <pre><code>REFRESH MATERIALIZED VIEW view_name;\n</code></pre>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#39-migrating-data","title":"3.9 Migrating Data","text":"<p>Migration vs. Transfer</p> <ol> <li> <p>Migration</p> <ul> <li>Definition Moving an entire system or application, similar to moving houses.</li> <li>Process Typically involves planning, discovery, validation, and execution.</li> </ul> </li> <li> <p>Transfer</p> <ul> <li>Definition Moving individual files or objects, like moving a box or parcel.</li> <li>Services<ul> <li>AWS DataSync<ul> <li>Purpose Transfers files and objects between on-premises and AWS storage services (e.g., S3).</li> <li>Features Constant data synchronization for real-time data consistency and availability.</li> </ul> </li> <li>AWS Transfer Family<ul> <li>Purpose Fully managed file transfer services for protocols like SFTP, FTPS, and FTP.</li> <li>Advantages Seamless integration with existing authentication systems (e.g., Active Directory, LDAP).</li> </ul> </li> </ul> </li> </ul> </li> </ol> <p>AWS Migration Services</p> <ol> <li> <p>AWS Application Discovery Service (ADS)</p> <ul> <li>Purpose Assists in discovering and gathering information about on-premises applications.</li> <li>Assessments<ul> <li>Agentless Discovery<ul> <li>Method Uses AWS Agentless Discovery Connector to scan the network and infrastructure.</li> <li>Use Case Ideal when installing additional software (agents) is not feasible.</li> </ul> </li> <li>Agent-based Discovery<ul> <li>Method Deploys lightweight software agents to collect granular and real-time data.</li> <li>Use Case Suitable for scenarios requiring more control and customization.</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Application Migration Service</p> <ul> <li>Purpose Focuses on application-level migrations (rehosting or lifting and shifting).</li> <li>Features Minimizes downtime by either migrating applications or replicating data.</li> <li>Migration Lifecycle<ul> <li>Discovery Identify and analyze existing applications.</li> <li>Planning Develop a migration roadmap.</li> <li>Validation and Testing Set up a test environment to simulate and validate the migration.</li> <li>Final Migration Perform the migration after successful validation.</li> </ul> </li> </ul> </li> <li> <p>Snow Family</p> <ul> <li>Purpose Specialized in migrating large volumes of data when internet transfer is impractical.</li> <li>Devices<ul> <li>Snowball<ul> <li>Capacity Suitable for data volumes of at least 10 terabytes.</li> </ul> </li> <li>Snowball Edge<ul> <li>Capacity For data volumes over 10 terabytes.</li> <li>Features Includes onboard compute resources for data processing (transformation, analysis).</li> </ul> </li> <li>Snowmobile<ul> <li>Capacity Designed for data volumes exceeding 10 petabytes.</li> </ul> </li> </ul> </li> </ul> </li> </ol>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#310-database-migration-service-dms","title":"3.10 Database Migration Service (DMS)","text":""},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#aws-database-migration-service-dms-study-notes","title":"AWS Database Migration Service (DMS) - Study Notes","text":"<p>Overview</p> <ul> <li>Purpose AWS DMS is a fully managed service designed to migrate databases from on-premises to AWS or between databases.</li> <li>Analogy Like a smart flatbed transporting data from the source to the target database.</li> <li>Key Features<ul> <li>Discovery Identifies eligible source databases.</li> <li>Consolidation Merges multiple source databases into a single target database.</li> <li>Minimized Downtime Source database remains available during migration.</li> </ul> </li> </ul> <p>Migration Methods</p> <ol> <li> <p>One-Time Migration</p> <ul> <li>Description Moves data in a single operation from source to target.</li> <li>Use Case Ideal for migrating databases to a new environment (e.g., on-prem to cloud).</li> <li>Downtime May result in some downtime.</li> </ul> </li> <li> <p>Continuous Replication (CDC - Change Data Capture)</p> <ul> <li>Description Synchronizes changes between source and target databases in near real-time.</li> <li>Use Case Suitable for syncing without a full load, especially if initial data migration is done using other tools.</li> </ul> </li> <li> <p>Full Load Plus CDC</p> <ul> <li>Description Combines full load for initial migration and continuous replication for ongoing changes.</li> <li>Use Case When a full load is needed initially, followed by continuous data syncing.</li> </ul> </li> </ol> <p>CDC Streaming Options to S3</p> <ol> <li> <p>Kinesis Data Streams</p> <ul> <li>Process CDC data is captured and ingested directly into S3 in Parquet format.</li> </ul> </li> <li> <p>Kinesis Data Firehose</p> <ul> <li>Process Data is ingested into Kinesis Data Streams, then streamed into Kinesis Data Firehose for additional transformations before storing in S3.</li> </ul> </li> </ol> <p>Migration Types</p> <ol> <li> <p>Homogeneous Migration</p> <ul> <li>Description Migration between databases with compatible engines.</li> <li>Example On-prem MySQL to RDS MySQL.</li> </ul> </li> <li> <p>Heterogeneous Migration</p> <ul> <li>Description Migration between databases with different engines.</li> <li>Process<ul> <li>Schema Conversion Use AWS Schema Conversion Tool to convert schema.</li> <li>Data Migration Perform data migration after schema conversion.</li> </ul> </li> <li>Example On-prem Oracle to RDS PostgreSQL.</li> </ul> </li> </ol> <p>Schema Conversion Tool</p> <ul> <li>Purpose Resolves compatibility issues between source and target database schemas.</li> </ul> <p>DMS Components</p> <ol> <li> <p>Replication Instance</p> <ul> <li>Description EC2 instance running replication software within a VPC.</li> <li>Tasks<ul> <li>Create Set up and configure the replication instance.</li> <li>Endpoints Define source and target endpoints for database connections.</li> <li>Replication Task Defines the data migration or replication process.</li> </ul> </li> </ul> </li> <li> <p>IAM Role</p> <ul> <li>Purpose Provides DMS with the necessary permissions to access source and target databases.</li> </ul> </li> </ol> <p>Table Mappings and Transformation Rules</p> <ol> <li> <p>Table Mappings</p> <ul> <li>Description Specify relationships between columns in source and target tables.</li> <li>Example Mapping <code>email</code> to <code>contact_email</code>.</li> </ul> </li> <li> <p>Transformation Rules</p> <ul> <li>Purpose Transform data during migration.</li> <li>Types<ul> <li>Data Type Conversion E.g., string to number.</li> <li>Calculations E.g., summing columns.</li> <li>String Alterations E.g., modifying or combining strings.</li> </ul> </li> </ul> </li> </ol>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#40-data-cataloging-system","title":"4.0 Data Cataloging System ====================","text":""},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#41-components-of-a-data-catalog","title":"4.1 Components of a Data Catalog","text":""},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#aws-data-catalog-study-notes","title":"AWS Data Catalog - Study Notes","text":"<p>Purpose of a Data Catalog</p> <ul> <li>A data catalog systematically organizes data assets, similar to how a library organizes books.</li> <li>It helps users discover, understand, and categorize data, offering insights on:<ul> <li>Location of data</li> <li>Contents of the data (e.g., columns, tables)</li> <li>Users of the data (who accesses it)</li> <li>Data quality (e.g., missing values, inconsistencies)</li> <li>Data lineage (relationships and transformations from source to destination)</li> </ul> </li> </ul> <p>Components of a Data Catalog</p> <ol> <li> <p>Metadata Repository</p> <ul> <li>Centralized storage for metadata about datasets.</li> </ul> </li> <li> <p>Search and Discovery</p> <ul> <li>Capability to search by database names, tables, columns, and keywords.</li> <li>Tags and annotations provide context for searches.</li> </ul> </li> <li> <p>Data Lineage</p> <ul> <li>Visualizes data flow from source to final destination.</li> <li>Shows relationships between data elements (e.g., foreign keys).</li> </ul> </li> <li> <p>Data Asset Descriptions</p> <ul> <li>Includes descriptions of tables, such as purpose, owner, and creation date.</li> </ul> </li> <li> <p>Access and Security</p> <ul> <li>Implements access controls to restrict data viewing or modifications.</li> <li>Provides security labels and permissions for data confidentiality.</li> </ul> </li> </ol> <p>Examples of Data Catalog Systems</p> <ul> <li>AWS Glue Data Catalog</li> <li>Hive Metastore</li> </ul> <p>Hive Metastore Overview</p> <ul> <li>Stores metadata for tables (e.g., schemas, partitions, storage locations).</li> <li>SQL-like language (HiveQL) is used to query distributed data on systems like HDFS or Amazon S3.</li> <li>Can be integrated with EMR or replaced with AWS Glue Data Catalog.</li> </ul> <ul> <li>Elastic MapReduce (EMR)<ul> <li>Fully managed service for processing large-scale structured, semi-structured, or unstructured data.</li> <li>Supports Apache Hadoop and Apache Spark for distributed and parallel data processing.</li> <li>EMR Cluster Made of EC2 compute nodes, which scale up or down based on processing requirements.</li> </ul> </li> </ul>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#42-lets-look-at-metadata","title":"4.2 Let's Look at Metadata","text":""},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#metadata-study-notes","title":"Metadata - Study Notes","text":"<p>What is Metadata?</p> <ul> <li>Metadata is \"data about data,\" providing context and characteristics about the main data.</li> <li>It includes details like:<ul> <li>Location of data</li> <li>Schema (data types, table structure)</li> <li>Data Lineage (relationship between data elements)</li> </ul> </li> </ul> <p>Types of Metadata</p> <ol> <li> <p>Structural Metadata:</p> <ul> <li>Describes how the data is organized (e.g., table names, columns, data types).</li> <li>Used to show data lineage and relationships between tables.</li> </ul> </li> <li> <p>Descriptive Metadata:</p> <ul> <li>Provides information about the content and purpose of the data.</li> <li>Includes table descriptions, comments, and annotations to enable search and discovery.</li> </ul> </li> <li> <p>Administrative Metadata:</p> <ul> <li>Focuses on data management aspects like ownership, access permissions, and versioning.</li> </ul> </li> <li> <p>Technical Metadata:</p> <ul> <li>Describes technical details such as file formats (e.g., Parquet, Avro, CSV).</li> <li>Includes serialization/deserialization processes and indexing information.</li> </ul> </li> </ol> <p>Uses of Metadata</p> <ol> <li> <p>Data Lineage:</p> <ul> <li>Tracks data flow from source to final destination (e.g., databases, applications).</li> <li>Helps with impact analysis, showing how changes in data affect downstream processes.</li> <li>Supports compliance and auditing by tracking data handling.</li> <li>Aids in troubleshooting and identifying stages where problems occur.</li> </ul> </li> <li> <p>Data Quality Metrics:</p> <ul> <li>Accuracy: Measures how correct the data values are.</li> <li>Completeness: Measures how fully the data is populated.</li> <li>Consistency: Measures coherence of data across different sources or time periods.</li> <li>Validity: Measures conformance to predefined rules (e.g., valid ranges).</li> <li>Uniqueness: Measures uniqueness of data entries (e.g., unique IDs).</li> </ul> </li> </ol> <p>Benefits of Data Quality Metrics:</p> <ul> <li>Informed Decision-Making: Ensures data-driven decisions are based on accurate, reliable data.</li> <li>Cost Savings: Reduces expenses from errors, rework, and inefficiencies.</li> <li>Compliance: Ensures adherence to regulatory requirements, reducing risks of non-compliance.</li> </ul> <p>Collaboration Tools for Managing Metadata</p> <ol> <li> <p>Annotations and Comments:</p> <ul> <li>Add descriptions or tags to provide context and explain data to other teams.</li> <li>Supported in AWS Glue for enhanced data collaboration.</li> </ul> </li> <li> <p>Change Tracking:</p> <ul> <li>Use AWS CloudTrail to monitor who makes updates to metadata and when.</li> </ul> </li> <li> <p>Notifications:</p> <ul> <li>Configure AWS CloudWatch Events to trigger notifications (via SNS) when metadata is updated.</li> </ul> </li> </ol>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#43-demo-creating-a-data-catalog","title":"4.3 Demo: Creating a Data Catalog","text":""},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#aws-data-catalog-creation-study-notes","title":"AWS Data Catalog Creation - Study Notes","text":"<p>Overview</p> <ul> <li>A data catalog organizes virtual databases, tables, and metadata entries. It allows you to manage, query, and retrieve data efficiently.</li> </ul> <p>Steps to Create a Data Catalog</p> <ol> <li> <p>Set Up a Database:</p> <ul> <li>Use AWS Glue to create a database (e.g., \"Manhattan Insights\").</li> </ul> </li> <li> <p>Add Tables Using AWS Glue Crawler:</p> <ul> <li>The AWS Glue Crawler automatically scans and creates tables from a data source (e.g., Amazon S3).</li> <li>Data Source: Select S3 and navigate to the appropriate folder path (e.g., source data feed folder).</li> <li>Classifiers: Create a CSV classifier to define the file format (e.g., \"manhattan-csv-classifier\").</li> </ul> </li> <li> <p>IAM Role:</p> <ul> <li>Create an IAM role (e.g., \"Properties Analyst\") to allow Glue to access data from S3.</li> </ul> </li> <li> <p>Run the Crawler:</p> <ul> <li>After configuring the Crawler, review the summary and create it.</li> <li>Run the Crawler, and it will create a table based on the data in S3.</li> </ul> </li> <li> <p>Check the Table:</p> <ul> <li>Verify that the table was created correctly in Glue, with the appropriate S3 path, CSV classification, and partition key (e.g., neighbourhood).</li> </ul> </li> <li> <p>Query Data Using Amazon Athena:</p> <ul> <li>Use Amazon Athena to query the created table.</li> <li>Set the S3 output location for query results.</li> <li>Run the query in Athena to retrieve and view the data.</li> </ul> </li> </ol> <p>Example Setup</p> <ul> <li>Data Source: Amazon S3, partitioned by neighbourhoods (e.g., EastHarlem, Harlem).</li> <li>Columns: Price, bedrooms, bathrooms, square footage, status, and address.</li> <li>Database: Manhattan Insights.</li> <li>Table: source_datafeed with neighbourhood as the partition key.</li> </ul> <p>Final Steps in Athena</p> <ol> <li>Set the output path for query results (Athena Output Results in S3).</li> <li>Run the query to retrieve data with the correct schema and partition key.</li> </ol>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#44-aws-glue-versus-apache-hive","title":"4.4 AWS Glue versus Apache Hive","text":""},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#aws-glue-vs-apache-hive-study-notes","title":"AWS Glue vs Apache Hive - Study Notes","text":"<p>AWS Glue Overview</p> <ul> <li>Purpose: A fully managed ETL (Extract, Transform, Load) service designed for processing and analysing big data.</li> <li>Key Features:<ul> <li>Crawlers: Infer schema and create the AWS Glue Data Catalog to organize metadata.</li> <li>ETL Jobs: Automate data ingestion, transformation, and loading via the Glue console or custom Python scripts.</li> <li>Data Transformation: Built-in functions or custom code to transform data before loading it into target databases.</li> <li>Deployment: Fully managed, with no need for infrastructure setup or management.</li> <li>Processing Support: Supports batch and streaming processing for real-time or scheduled data jobs.</li> <li>AWS Integration: Seamlessly integrates with AWS services like S3, Athena, and RDS.</li> </ul> </li> </ul> <p>Apache Hive Overview</p> <ul> <li>Purpose: A data processing framework for transforming and analysing big data using a SQL-like language.</li> <li>Key Features:<ul> <li>HiveQL: Use SQL queries (HiveQL) to perform operations like filtering, aggregation, and joins on data stored in HDFS or S3.</li> <li>Hive Metastore: Centralized metadata store for organizing data in the Hive ecosystem.</li> <li>Processing Support: Primarily supports batch processing, making it less ideal for real-time analytics.</li> <li>Deployment: Runs on Hadoop clusters, requiring significant infrastructure setup and maintenance.</li> <li>Integration: Works with the Hadoop ecosystem, including HDFS, YARN, and Hadoop MapReduce.</li> </ul> </li> </ul> <p>Key Differences Between AWS Glue and Apache Hive</p> <ol> <li> <p>Architecture:</p> <ul> <li>AWS Glue: A fully managed service in AWS, handling all infrastructure needs, including resource provisioning, scaling, and maintenance.</li> <li>Apache Hive: Runs on Hadoop clusters, requiring manual infrastructure management and setup.</li> </ul> </li> <li> <p>Processing Types:</p> <ul> <li>AWS Glue: Supports both batch and real-time (streaming) processing.</li> <li>Apache Hive: Primarily supports batch processing and does not handle real-time data efficiently.</li> </ul> </li> <li> <p>ETL Job Translation:</p> <ul> <li>AWS Glue: Translates ETL jobs for AWS data environments.</li> <li>Apache Hive: Translates HiveQL queries into MapReduce jobs that run on Hadoop clusters.</li> </ul> </li> <li> <p>Integration:</p> <ul> <li>AWS Glue: Integrates with AWS services like S3, Athena, and RDS.</li> <li>Apache Hive: Integrates with Hadoop components like HDFS, YARN, and Hadoop MapReduce.</li> </ul> </li> </ol> <p>Use Cases:</p> <ul> <li>AWS Glue: Ideal for cloud-based, fully managed ETL workflows with real-time or batch processing needs.</li> <li>Apache Hive: Suitable for environments already using Hadoop clusters, with a focus on batch processing of large datasets.</li> </ul>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#45-self-discovering-schemas-in-aws-glue","title":"4.5 Self Discovering Schemas in AWS Glue","text":""},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#data-lake-and-aws-glue-study-notes","title":"Data Lake and AWS Glue - Study Notes","text":"<p>Data Lake Overview</p> <ul> <li>A data lake is a centralized repository designed to store, process, and secure large volumes of data in its raw format (native format).</li> <li>It acts as a single source of truth, consolidating various data types from multiple sources into a consistent store.</li> <li>Purpose: Avoids data silos by integrating different forms of data into one system.</li> </ul> <p>ETL Process in a Data Lake</p> <ul> <li>ETL (Extract, Transform, Load): A common data integration process used by data engineers.<ol> <li>Extract: Data is pulled from multiple sources.</li> <li>Transform: Data is validated and transformed to meet quality rules.</li> <li>Load: Data is loaded into target systems for downstream consumption (APIs, reports).</li> </ol> </li> </ul> <ul> <li>Flow Example:<ul> <li>Data extracted from sources enters the data lake (e.g., Amazon S3).</li> <li>Data is transformed and validated according to business rules.</li> <li>Processed data is loaded into a target database for business users.</li> </ul> </li> </ul> <p>AWS Glue in the ETL Process</p> <ul> <li>Storage Layer: Amazon S3 serves as the storage layer for the data lake.</li> <li>AWS Glue:<ul> <li>Crawls data in S3 to infer schema and creates the AWS Glue Data Catalog.</li> <li>Transforms and loads data into the target database using Glue ETL scripts.</li> </ul> </li> </ul> <p>AWS Glue Data Catalog Population</p> <ol> <li> <p>Automatic Schema Discovery:</p> <ul> <li>The Glue Crawler automatically analyses the data and identifies metadata (column names, data types) without manual intervention.</li> <li>Use Case: Ideal for scenarios where data changes frequently.</li> </ul> </li> <li> <p>Manual Schema Definition:</p> <ul> <li>Users explicitly define the schema (column names, data types, etc.).</li> <li>Use Case: Best for stable data environments or when specific schema requirements must be enforced.</li> </ul> </li> </ol>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#46-optimization-techniques-for-improving-query-performance","title":"4.6 Optimization Techniques for Improving Query Performance","text":""},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#query-performance-optimization-techniques-study-notes","title":"Query Performance Optimization Techniques - Study Notes","text":"<ol> <li> <p>Indexing</p> <ul> <li>Purpose: Speeds up data retrieval by avoiding full table scans.</li> <li>Analogy: Similar to a book index or table of contents that helps you find specific information without scanning the whole book.</li> <li>Examples:<ul> <li>Spreadsheets: Rows and columns are indexed to reference specific cells.</li> <li>SQL Databases: Indexes are created on table columns to speed up queries.</li> <li>Relational Tables: Primary keys and unique constraints act as indexes to improve retrieval performance.</li> </ul> </li> </ul> </li> <li> <p>Partitioning</p> <ul> <li>Purpose: Divides large datasets into smaller, more manageable subsets called partitions to reduce the amount of data scanned and enable parallel processing.</li> <li>Example Use Cases:<ul> <li>Time-Based Partitioning: Sales data partitioned by month, quarter, or year. Queries targeting a specific period (e.g., Q4) can access that partition directly, skipping irrelevant data.</li> <li>Alphabetical Partitioning: Customer data partitioned by the first letter of last names (e.g., A-C, D-F) for faster lookups.</li> </ul> </li> <li>Benefit: Allows parallel processing, which is faster than sequential scanning of the entire dataset.</li> </ul> </li> <li> <p>Compression</p> <ul> <li>Purpose: Reduces the storage space required for data by encoding it in a more compact form.</li> <li>Trade-Off: Compression and decompression consume CPU resources, so it's important to assess whether the storage savings justify the CPU overhead.</li> <li>Examples:<ul> <li>Common compression techniques include GZIP, ZIP, GZ, and RAR.</li> <li>Amazon Redshift has built-in compression algorithms like <code>LZOP</code>, BZIP2, and GZIP to optimize data storage in its fully managed warehouse.</li> </ul> </li> </ul> </li> </ol> <p>Summary of Benefits:</p> <ul> <li>Indexing: Improves query speed by providing direct access to data without full scans.</li> <li>Partitioning: Reduces the amount of data scanned and enables faster parallel processing.</li> <li>Compression: Reduces storage costs but requires careful consideration of CPU trade-offs.</li> </ul>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#47-schema-evolution-and-updating-data-catalogs","title":"4.7 Schema Evolution and Updating Data Catalogs","text":""},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#schema-changes-and-data-transformation-in-aws-glue-study-notes","title":"Schema Changes and Data Transformation in AWS Glue - Study Notes","text":"<p>Updating Data Catalogs</p> <ol> <li> <p>Manual Approach:</p> <ul> <li>Update the data catalog via the Glue Console or manually trigger Glue Crawlers to re-scan data sources and update metadata.</li> </ul> </li> <li> <p>Programmatic Approach:</p> <ul> <li>Use AWS Glue APIs, AWS SDKs (like Boto3 in Python), or ETL scripts (like pandas or PySpark) to update the catalog.</li> </ul> </li> <li> <p>Automated Approach:</p> <ul> <li>Schedule AWS Glue Crawlers to run periodically, ensuring the catalog stays up-to-date with schema changes.</li> <li>AWS CloudFormation can automate catalog updates alongside other infrastructure components.</li> </ul> </li> </ol> <p>Schema Evolution Methods:</p> <ol> <li> <p>Schema Updates:</p> <ul> <li>Directly modify schemas (e.g., add/delete columns) while preserving underlying data.</li> </ul> </li> <li> <p>Partitioning:</p> <ul> <li>Add or remove partitions (e.g., yearly data) by updating metadata about the partition's location, format, or compression.</li> </ul> </li> <li> <p>Table Updates:</p> <ul> <li>Create/delete tables, update table properties, or change table ownership (i.e., AWS account or IAM role).</li> </ul> </li> <li> <p>Index Changes:</p> <ul> <li>Alter indexes (used to improve query performance), which requires reflecting these changes in the data catalog.</li> </ul> </li> </ol> <p>Impact of Schema Changes:</p> <ol> <li> <p>Deleting Columns:</p> <ul> <li>You won\u2019t be able to query deleted columns, though the underlying data remains.</li> </ul> </li> <li> <p>Adding Columns:</p> <ul> <li>New columns can be queried. You may fill them with <code>null</code>, default values, or existing data.</li> </ul> </li> <li> <p>Changing Data Types:</p> <ul> <li>AWS will interpret columns according to the new data type, but this may cause conversion errors or inconsistencies if the data can't be cast to the new type.</li> </ul> </li> </ol> <p>ETL Data Transformation Techniques:</p> <ol> <li> <p>Custom Transformations:</p> <ul> <li>User-defined functions created from scratch (Python/Scala).</li> <li>Example: Converting data formats (JSON to Avro).</li> </ul> </li> <li> <p>Built-In Transformations:</p> <ul> <li>Predefined functions for common tasks:<ul> <li>DropFields: Remove specific columns.</li> <li>DropNullFields: Remove columns with all <code>null</code> values.</li> <li>Filter: Filter rows based on criteria.</li> <li>Join: Combine datasets on common keys (like SQL joins).</li> <li>Map: Modify each record, perform external lookups, etc.</li> <li>ResolveChoice: Resolve schema inconsistencies (e.g., convert mixed data types).</li> </ul> </li> </ul> </li> </ol> <p>Data Structures in AWS Glue:</p> <ol> <li> <p>DataFrames:</p> <ul> <li>Commonly used for structured data (rows/columns).</li> <li>Requires manual schema updates if there are changes.</li> <li>Supported by Apache Spark APIs.</li> </ul> </li> <li> <p>DynamicFrames:</p> <ul> <li>Designed for semi-structured data.</li> <li>Supports schema evolution automatically.</li> <li>Used with AWS Glue's built-in scripts for transformations.</li> </ul> </li> </ol> <p>Machine Learning Transformations in AWS Glue:</p> <ul> <li>AWS Glue has specialized ML transformations for tasks like:<ul> <li>Deduplication: Identifying duplicate records.</li> <li>Record Linkage: Linking related records from different sources.</li> <li>Data Quality Enhancement: Improving overall data quality.</li> </ul> </li> <li>Example: FindMatches operation for deduplication, determining the likelihood of records referring to the same entity.</li> </ul>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#50-conclusion","title":"5.0 Conclusion ====================","text":""},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#51-summary","title":"5.1 Summary","text":"<p>same as above</p>"},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#52-data-store-management-exam-tips","title":"5.2 Data Store Management: Exam Tips","text":""},{"location":"9-Drafts/dea-c01-u2-data-store-management-ps/#sample-exam-questions-study-notes","title":"Sample Exam Questions - Study Notes","text":"<p>Question 1: Data Catalogs</p> <ul> <li>Scenario: A data engineering team needs to create a central metadata repository for Amazon EMR and Amazon Athena queries. Some metadata is stored in Apache Hive and must be imported into the repository.</li> <li>Ask: Which solution minimizes development effort?</li> </ul> <p>Choices</p> <ol> <li>A. Deploy a Hive Metastore on an EMR cluster.</li> <li>B. Utilize Amazon EMR and Apache Ranger.</li> <li>C. Employ the AWS Glue Data Catalog.</li> <li>D. Implement a custom metadata import solution with AWS Lambda and Amazon S3.</li> </ol> <p>Correct Answer: C. Employ the AWS Glue Data Catalog</p> <ul> <li>Reasoning: AWS Glue Data Catalog is fully managed, automates metadata discovery, and integrates with EMR and Athena, reducing development overhead compared to the other options.</li> </ul> <p>Question 2: Optimizing Athena Queries</p> <ul> <li>Scenario: You need to optimize Amazon Athena queries. The data is stored in uncompressed <code>.csv</code> files, and users mainly run analytical queries with filters.</li> <li>Ask: What approach will most effectively improve query performance?</li> </ul> <p>Choices</p> <ol> <li>A. Convert the data to JSON format with Snappy compression.</li> <li>B. Apply Snappy compression to the existing <code>.csv</code> files.</li> <li>C. Switch the data format to Apache Parquet and use Snappy compression.</li> <li>D. Employ gzip compression on the <code>.csv</code> files.</li> </ol> <p>Correct Answer: C. Switch the data format to Apache Parquet and use Snappy compression</p> <ul> <li>Reasoning: Parquet is a columnar storage format ideal for analytical queries and filtering specific categories, and Snappy compression improves storage and query efficiency.</li> </ul> <p>Question 3: Purpose-Built Databases</p> <ul> <li>Scenario: A business uses an on-premise Microsoft SQL Server for financial data and transfers this data monthly to AWS. The company wants to reduce costs and minimize disruption during migration to Amazon RDS for SQL Server.</li> <li>Ask: Which AWS service should be used for cost-effective data migration?</li> </ul> <p>Choices</p> <ol> <li>A. AWS Direct Connect.</li> <li>B. AWS Database Migration Service (DMS).</li> <li>C. AWS Snowball.</li> <li>D. AWS Transfer Family.</li> </ol> <p>Correct Answer: B. AWS Database Migration Service (DMS)</p> <ul> <li>Reasoning: DMS provides a cost-effective solution for migrating data from on-premise SQL Server to Amazon RDS. It also supports near real-time replication, minimizing disruptions to applications accessing the database.</li> </ul> <p>Key Takeaways:</p> <ul> <li>AWS Glue Data Catalog: Best for automating metadata discovery and integration with EMR and Athena.</li> <li>Apache Parquet with Snappy Compression: Ideal for optimizing query performance in Athena for large analytical datasets.</li> <li>AWS DMS: The preferred tool for cost-effective and low-disruption database migration from on-premises to AWS.</li> </ul>"},{"location":"9-Drafts/dea-c01-u3-data-operations-and-support-ps/","title":"U3 Data Operations and Support","text":"<p>Automate, monitor and ensure the quality of data pipeline.</p>"},{"location":"9-Drafts/dea-c01-u3-data-operations-and-support-ps/#10-course-introduction","title":"1.0 Course Introduction","text":""},{"location":"9-Drafts/dea-c01-u3-data-operations-and-support-ps/#11-introduction","title":"1.1 Introduction","text":""},{"location":"9-Drafts/dea-c01-u3-data-operations-and-support-ps/#20-automate-data-processing-by-using-aws-services","title":"2.0 Automate Data Processing by Using AWS Services","text":""},{"location":"9-Drafts/dea-c01-u3-data-operations-and-support-ps/#21-data-apis","title":"2.1 Data APIs","text":"<p>Data APIs for External Data Sources</p> <ul> <li>When processing data in AWS, you may need to interact with external data sources (e.g., other cloud providers or on-premise sources).</li> <li>Data APIs allow secure data flow between AWS services and external sources.</li> </ul> <p>Examples of Data APIs:</p> <ol> <li> <p>JDBC (Java Database Connector) / ODBC (Open Database Connector):</p> <ul> <li>Used to connect AWS services like Redshift, Athena, and Glue to relational databases.</li> <li>Allows direct connections to external relational data sources.</li> </ul> </li> <li> <p>Amazon AppFlow:</p> <ul> <li>Integrates with third-party SaaS applications (e.g., Salesforce, SAP, Slack) to import data into AWS.</li> <li>Fully managed integration service for SaaS platforms.</li> <li>Allows data processing: Ability to filter, enrich, and validate data before importing it into AWS.</li> <li>Built-in integrations with platforms like Google Analytics, Facebook Ads, and ServiceNow.</li> <li>Exam: Recommended when dealing with SaaS data processing questions on exams.</li> </ul> </li> <li> <p>AWS Data Exchange:</p> <ul> <li>A marketplace for third-party data APIs.</li> <li>Allows consumers to subscribe to and use data APIs from data providers.</li> <li>Exam: Mentioned as a high-level concept for exams, mostly appearing as a distractor.</li> </ul> </li> </ol> <p>Key Takeaways:</p> <ul> <li>Use JDBC/ODBC for connecting AWS services to relational databases.</li> <li>For SaaS data (Salesforce, SAP, etc.), use Amazon AppFlow to import and process data.</li> <li>AWS Data Exchange is a marketplace for vetted data APIs, useful for machine learning models and large data needs.</li> </ul>"},{"location":"9-Drafts/dea-c01-u3-data-operations-and-support-ps/#22-intro-to-amazon-emr","title":"2.2 Intro to Amazon EMR","text":"<p>Overview of Amazon EMR</p> <ul> <li>Used for ETL of data at petabyte scale.</li> </ul> <ul> <li>MapReduce process: Splitting data, parallel processing, reassembling data.<ul> <li>Mapping: Distributing data for parallel processing.</li> <li>Reducing: Reassembling processed data.</li> </ul> </li> </ul> <p>Underlying Technology</p> <ul> <li>Apache Hadoop: Open-source framework for big data analytics, leveraging tools for data preparation and analysis.</li> <li>Apache Hive: Data warehousing tool with SQL-like interface.</li> <li>Apache Spark: Defines data transformations and trains ML models.</li> <li>Presto: Optimized big data query engine.</li> </ul> <p>Amazon EMR Infrastructure</p> <ul> <li>Cluster Management: Manages EC2 instances for big data operations.</li> <li>Single Availability Zone: For performance optimization, nodes are physically close.</li> </ul> <ul> <li>Node Types<ul> <li>Primary Nodes: Distribute data and tasks (can have standby nodes for high availability).</li> <li>Core Nodes: Data Handling, Host distributed file system and run tasks.</li> <li>Task Nodes: Amplify processing power without participating in the distributed file system.</li> </ul> </li> </ul> <p>File Systems in EMR</p> <ul> <li>HDFS (Hadoop Distributed File System): Default; stores distributed data.</li> <li><code>EMRFS</code>: Integrates HDFS with S3 for persistent storage.</li> <li>Local File System: Temporary storage for job-specific data.</li> </ul> <p>Cluster Types</p> <ul> <li>Ephemeral Cluster: Temporary; spins up and tears down with job completion (cost-effective).</li> <li>Long-standing Cluster: Persistent; for continuous or high-performance use.</li> </ul> <p>Hive Metastore</p> <ul> <li>Stores metadata about data (schemas, partitions, types).</li> <li>External Hive Metastore Options<ul> <li>AWS Glue Data Catalog: Integrates with AWS services; fully managed and highly available.</li> <li>Amazon Aurora/RDS: For third-party open-source apps (e.g., Apache Ranger).</li> </ul> </li> </ul> <p>EMR vs AWS Glue</p> <ul> <li>Commonality: Both use Apache Spark for data transformation.</li> <li>EMR Advantages<ul> <li>More open-source tools.</li> <li>Better cost-performance for large datasets or intensive processing.</li> </ul> </li> <li>AWS Glue Advantages<ul> <li>More built-in features.</li> <li>Less configuration management; better for operational efficiency.</li> </ul> </li> </ul> <p>Additional Resource</p> <ul> <li>EMR Studio: Integrated development environment for defining and developing EMR jobs (not likely on exams but useful).</li> </ul> <p>Summary</p> <ul> <li>EMR is ideal for complex, large-scale data processing with open-source tools and performance optimization.</li> <li>AWS Glue is suited for operational efficiency and integration with other AWS services.</li> </ul>"},{"location":"9-Drafts/dea-c01-u3-data-operations-and-support-ps/#23-intro-to-aws-glue-databrew","title":"2.3 Intro to AWS Glue DataBrew","text":"<p>like alteryx or tableau prep</p> <p>Overview</p> <ul> <li>Visual data preparation tool for non-technical data analysts.</li> <li>Functionality: Enables no-code or low-code transformations, data validation, and anomaly detection.</li> </ul> <p>Key Features</p> <ul> <li>Data Preparation: Provides over 250 prebuilt transformations for cleaning and normalizing data.</li> <li>Data Quality: Validates data against defined rules before staging it in S3 buckets.</li> <li>Data Lineage: Allows visual mapping of data lineage.</li> </ul> <p>Workflow</p> <ol> <li> <p>Data Collection</p> <ul> <li>Data resides in a data lake or warehouse.</li> <li>Subset of data migrated to an S3 bucket for further analysis.</li> </ul> </li> <li> <p>Using DataBrew</p> <ul> <li>Data Rules: Analysts define rules in DataBrew to ensure data quality.</li> <li>Validation: Rules are continuously applied; non-compliant data is not transferred to the staging bucket.</li> <li>Alerts: Analysts receive notifications about malformed data for remediation.</li> </ul> </li> <li> <p>Data Transformations</p> <ul> <li>Common Transformations<ul> <li>Remove or replace missing values.</li> <li>Combine different datasets.</li> <li>Create new columns (e.g., split timestamp into year, month, day).</li> <li>Filter data to select subsets for analysis.</li> <li>Label mapping (e.g., map categories to numerical values).</li> <li>Aggregate data.</li> </ul> </li> </ul> </li> </ol> <p>Exam Tip</p> <ul> <li>Focus: For scenarios involving automated data transformations or validation rules without coding, AWS Glue DataBrew is the relevant service.</li> </ul> <p>Summary</p> <ul> <li>AWS Glue DataBrew is ideal for visual data preparation and ensuring data quality with no-code/low-code solutions.</li> </ul>"},{"location":"9-Drafts/dea-c01-u3-data-operations-and-support-ps/#24-apache-offerings","title":"2.4 Apache Offerings","text":"<p>Understand how AWS services implement or act as alternatives to popular Apache applications</p> <p>Apache Flink</p> <ul> <li>Data analytics for streaming data.</li> <li>AWS Service: Amazon Managed Service for Apache Flink.</li> </ul> <ul> <li>Use Case<ul> <li>Stream data from sources like Amazon Kinesis for live analysis or ETL before storing in S3.</li> </ul> </li> <li>Capabilities<ul> <li>Streaming ETL</li> <li>Continuous metric generation</li> <li>Real-time analytics</li> <li>Interactive analysis</li> </ul> </li> </ul> <p>Apache Airflow</p> <ul> <li>Workflow management and orchestration.</li> <li>AWS Service<ul> <li>Amazon Managed Workflows for Apache Airflow: For migrating existing Airflow workflows.</li> <li>AWS Step Functions: Preferred for robust AWS service integration and lower operational overhead.</li> <li>AWS Glue Workflows: Specifically for ETL data pipelines.</li> <li>Amazon EventBridge: For event processing and scheduling.</li> </ul> </li> </ul> <p>Apache Kafka</p> <ul> <li>Distributed event streaming platform.</li> <li>AWS Service<ul> <li>Amazon Managed Streaming Service for Apache Kafka: For migrating existing Kafka applications.</li> <li>Amazon Kinesis: Simplified option for new projects on AWS.</li> </ul> </li> </ul> <p>Apache Hive</p> <ul> <li>Data warehousing on Hadoop, enabling SQL-like interaction with large datasets.</li> <li>AWS Service: Amazon EMR</li> <li>Use Case: Manage and interact with petabytes of data.</li> </ul> <p>Apache Spark</p> <ul> <li>Large-scale data processing and transformations.</li> <li>AWS Services<ul> <li>Amazon EMR: For processing big data with Spark.</li> <li>AWS Glue: Uses Spark for defining ETL jobs.</li> </ul> </li> <li>Example: Crawl S3 bucket to create a Glue Data Catalog, transform data, and store in a new S3 bucket for analysis with Amazon Athena.</li> </ul> <p>AWS vs Apache Offerings</p> <ul> <li>AWS Preference: For fully managed and integrated offerings (e.g., Step Functions, Kinesis, Glue) over Apache applications when building from scratch.</li> <li>Managed Apache Services: Used for migrating existing Apache-based applications to AWS.</li> </ul> <p>Exam Tip</p> <ul> <li>Focus: AWS tends to favour their integrated services for new implementations, but Managed Apache services are suitable for migrating existing applications.</li> </ul>"},{"location":"9-Drafts/dea-c01-u3-data-operations-and-support-ps/#25-intro-to-amazon-eventbridge","title":"2.5 Intro to Amazon EventBridge","text":"<p>Process and schedule asynchronous events across the AWS ecosystem</p> <ul> <li>Capabilities<ul> <li>Ingest and deliver events to/from AWS services.</li> <li>Schedule automated actions.</li> <li>Receive and route events based on defined rules.</li> </ul> </li> </ul> <p>Key Components</p> <ul> <li>Event Bus: Central component where events are received and routed.</li> <li>Event Rules: Define conditions to trigger automated actions.</li> </ul> <p>Event Movements</p> <ul> <li>Event Ingestion: Can handle events from AWS services or API calls.</li> <li>Event Routing: Route events to one or more targets (e.g., AWS Lambda, Glue workflows).</li> <li>Event Scheduling: Generate events on a schedule, independent of event sources.</li> </ul> <p>Use Cases</p> <ul> <li>Complex Event-Driven Architectures: Ideal for handling events from multiple sources or scheduling.</li> <li>Simple Workflows: For simpler scenarios (e.g., S3 file upload triggering a Lambda function), consider more direct solutions.</li> </ul> <p>Best Practices</p> <ul> <li>Simplest Solution: Choose the most straightforward method that meets your needs. For example, use S3 event notifications to trigger Lambda functions directly if possible.</li> </ul> <p>Exam Tip</p> <ul> <li>EventBridge: Use it when handling complex event routing or scheduling. For simpler use cases, consider more direct solutions to avoid overcomplication.</li> </ul> <p>Summary</p> <ul> <li>Amazon EventBridge is versatile for managing and scheduling events but evaluate if simpler solutions suffice for your specific scenario.</li> </ul>"},{"location":"9-Drafts/dea-c01-u3-data-operations-and-support-ps/#30-analytics","title":"3.0 Analytics","text":""},{"location":"9-Drafts/dea-c01-u3-data-operations-and-support-ps/#31-intro-to-amazon-athena","title":"3.1 Intro to Amazon Athena","text":"<p>Athena Overview</p> <ul> <li>Amazon Athena is ideal for querying large datasets in S3 with SQL</li> <li>It uses optimized formats and compression/partitioning to enhance performance and manage data effectively.</li> <li>Fully managed, serverless, interactive query service.</li> <li><code>Trino</code> (open-source) is underlying engine.</li> </ul> <p>Capabilities</p> <ul> <li>Data Querying: Directly query data in S3 using SQL.</li> <li>Data Formats Supported<ul> <li>Common: CSV, JSON, Apache Parquet.</li> <li>Other: Apache Avro, Apache ORC.</li> </ul> </li> <li>Apache Hive / Spark Support<ul> <li>create table and schema using Hive DDL</li> <li>Run ad-hoc Apache Spark Applications (Optional for testing, but Glue or EMR are preferred for implementation).</li> </ul> </li> </ul> <p>Best Practices / Optimization</p> <ul> <li>Partitioning</li> <li>Compression</li> <li>Columnar Formats</li> </ul> <p>Details of best practice</p> <ol> <li> <p>Data Partitioning</p> <ul> <li>Essential for efficient querying of large datasets.</li> <li>Group related data based on values (e.g., date, country).</li> <li>Benefit: Load only relevant data for each query.</li> <li>Imp: Data will not populate to your partitions by default. For Hive Style partitions, you will have to perform the <code>MSCK REPAIR TABLE</code> command. For non-Hive partitions, you will have to run the <code>ALTER TABLE ADD PARTITION</code> command.</li> </ul> <pre><code>CREATE EXTERNAL TABLE tickets (\n   user string,\n   text string,\n   timeStamp string,\n   impression string\n)\nPARTITIONED BY (month string)\nSTORED AS parquet\nLOCATION 's3://PS-EXAMPLE-BUCKET/tables/'\n</code></pre> </li> <li> <p>Data Compression</p> <ul> <li>Use supported formats to optimize performance.</li> <li>Supported Formats: gzip, Snappy.</li> <li>Splittable Formats<ul> <li>You can split the compressed data file for Parquet and ORC (due to segmentation and metadata).</li> </ul> </li> <li>JSON and CSV when compressed, cannot be split / processed in parallel.</li> </ul> </li> <li> <p>Columnar File Formats</p> <ul> <li>Use columnar format for faster query results.</li> <li>Prefer Parquet or ORC for better performance.</li> <li>Optimized Formats: Parquet and ORC.</li> </ul> </li> </ol> <p>Exam Tip</p> <ul> <li>Focus on Partitioning and Optimization: Partitioning and using columnar formats like Parquet or ORC are key for efficient querying in Athena.</li> </ul>"},{"location":"9-Drafts/dea-c01-u3-data-operations-and-support-ps/#32-creating-a-glue-crawler-for-athena-and-s3","title":"3.2 Creating a Glue Crawler for Athena and S3","text":"<p>lab</p>"},{"location":"9-Drafts/dea-c01-u3-data-operations-and-support-ps/#33-publishing-data-using-amazon-quicksight","title":"3.3 Publishing Data Using Amazon QuickSight","text":"<p>QuickSight Overview</p> <ul> <li>Create and share visualizations from various data sources.</li> <li>Serverless application for data visualization.</li> <li>Standalone application in AWS (has its own user and permission management)</li> </ul> <p>User Management</p> <ul> <li>QuickSight exists as standalone application in AWS.</li> <li>QuickSight Users &amp; Permission are for Administrators, data analysts, and business leaders with separate permissions. They are not same as IAM Users.</li> <li>IAM Users/Roles are for AWS resources, like s3 and athena. Quicksight needs IAM role to have permission to AWS resources. But QuickSight user don't, they only need access to viz/dashboard.</li> <li>Different users can be granted access to different dashboards.</li> </ul> <p>Data Sources</p> <ul> <li>It can connect to many internal and external data sources.</li> <li>Common Sources: Athena-S3, redshift, aurora, RDS, OpenSearch.</li> <li>External Sources: Relational databases, third-party sources (e.g., GitHub, Snowflake, Jira).</li> </ul> <p>AWS Resource Access Management</p> <ul> <li>IAM Role Requirement: QuickSight needs an IAM role to access AWS resources.</li> <li>Special Note: When integrating with Athena, ensure QuickSight has access to underlying S3 buckets.</li> </ul> <p>QuickSight vs CloudWatch</p> QuickSight CloudWatch Interactive Highly Less Use Case BI Resource Optimization Viz Selection Variety Limited Users Separate from IAM IAM Users <p>Exam Tip</p> <ul> <li>Choosing Between QuickSight and CloudWatch: Use QuickSight for advanced data visualizations and business intelligence; use CloudWatch for monitoring and automation of AWS resources.</li> </ul> <p>Summary</p> <ul> <li>Amazon QuickSight is a powerful tool for creating interactive and visually appealing dashboards for business intelligence, integrating with a wide range of data sources, and managing user permissions separately from AWS IAM.</li> </ul>"},{"location":"9-Drafts/dea-c01-u3-data-operations-and-support-ps/#34-visualizing-data","title":"3.4 Visualizing Data","text":"<p>You can use Athena, Glue Studio, QuickSight and Redshift Query Editor to \"visually\" prepare, transform and analyze data.</p> <p>AWS Glue Studio</p> <ul> <li>Visual interface for filtering and transforming data.</li> <li>Build Glue pipelines using a visual editor.</li> </ul> <p>Amazon Athena</p> <ul> <li>\"Athena Visual Query Analysis\" tool lets analyze query performance by showing logical steps and time taken.</li> <li>Use Case: Optimize Athena query performance.</li> </ul> <p>Amazon Redshift Query Editor</p> <ul> <li>Ver 2.0 lets create simple visualizations (e.g., line charts, bar charts) directly from the query editor.</li> <li>Use Case: Ad-hoc visualizations or testing query results.</li> </ul> <p>Amazon QuickSight</p> <ul> <li>Advanced data analysis and visualization tool.</li> <li>Features: Connects to various data sources and creates shareable visualizations.</li> <li>Use Case: Ideal for comprehensive organizational data sharing.</li> </ul> <p>Types of Visualizations</p> <ul> <li>Line Charts: Track trends over time.</li> <li>Bar Charts: Compare categories.</li> <li>Pie Charts: Illustrate percentages or parts of a whole.</li> <li>Scatter Plots: Compare two dimensions.</li> <li>Bubble Charts: Compare three dimensions (size/color represents additional dimension).</li> <li>Funnel Charts: Show stages in a process (e.g., customer journey).</li> <li>Histograms: Display distribution of values.</li> <li>Gauges: Display single metrics or key performance indicators.</li> </ul> <p>Exam Tips</p> <ul> <li>Choosing Visualizations: Understand when to use each type based on the data and the insights you want to convey.</li> <li>Service Choice: Select appropriate AWS tools based on your visualization needs, ranging from simple (Redshift Query Editor) to advanced (QuickSight).</li> </ul>"},{"location":"9-Drafts/dea-c01-u3-data-operations-and-support-ps/#40-maintaining-and-monitoring-data-pipelines","title":"4.0 Maintaining and Monitoring Data Pipelines","text":""},{"location":"9-Drafts/dea-c01-u3-data-operations-and-support-ps/#41-intro-to-amazon-macie","title":"4.1 Intro to Amazon Macie","text":"<p>Maintaining and Monitoring Data Pipelines with Amazon Macie</p> <ul> <li>Introduction to Amazon Macie<ul> <li>Simplifies monitoring for sensitive data within S3 buckets.</li> <li>Continuously scans S3 buckets for sensitive data, such as Personally Identifiable Information (PII) and intellectual property.</li> </ul> </li> </ul> <ul> <li>Key Features<ul> <li>Sensitive Data Detection: Scans and identifies sensitive data within S3 buckets.</li> <li>Alerts and Automation: Sends alerts or triggers automated actions if sensitive data is detected.</li> </ul> </li> </ul> <ul> <li>Multi-Account Management:<ul> <li>Can be activated in multiple accounts in an organization</li> <li>Centrally managed from a single account.</li> <li>Manages and aggregates findings across an AWS organization.</li> <li>Aggregates findings across accounts</li> <li>Automates actions centrally, like sending alerts.</li> </ul> </li> </ul> <ul> <li>Use Cases<ul> <li>Complex Data Pipelines: Ideal for monitoring data pipelines with multiple stages and buckets.</li> <li>Central Monitoring of S3 bucker across all org accounts.</li> </ul> </li> </ul> <ul> <li>Benefits<ul> <li>Operational Efficiency: Provides effective monitoring with minimal operational overhead.</li> <li>Centralized Management</li> </ul> </li> </ul>"},{"location":"9-Drafts/dea-c01-u3-data-operations-and-support-ps/#42-intro-to-amazon-cloudwatch-logs","title":"4.2 Intro to Amazon CloudWatch Logs","text":"<p>Monitoring Data Pipelines with Amazon CloudWatch</p> <p>CloudWatch Overview</p> <ul> <li>Monitors AWS services and data pipelines by collecting and analysing logs from various sources.</li> <li>Log Sources: Amazon EC2, CloudTrail, custom application logs, etc.</li> <li>Log Groups: Logs are organized into groups, retained indefinitely by default.</li> </ul> <p>Log Collection Methods</p> <ul> <li>Native Support: Enable logging in AWS services to send logs to CloudWatch.</li> <li>CloudWatch Agent: Install on EC2 instances or on-premises servers.</li> <li>AWS CLI use it send logs to cloud watch</li> <li>Programmatic Access: Use APIs, or SDK to send logs.</li> </ul> <p>Log Analysis</p> <ul> <li>Anomaly Detection<ul> <li>Functionality: Uses machine learning to create baselines and detect deviations.</li> <li>Benefits: Identifies novel errors by flagging outliers.</li> </ul> </li> </ul> <ul> <li>Insights<ul> <li>Query Capabilities: Create and execute queries, including natural language queries.</li> <li>Execution: Up to 50 log groups can be queried simultaneously.</li> </ul> </li> </ul> <ul> <li>Exporting Logs<ul> <li>To Amazon S3: Manual or programmatic export for periodic analysis.</li> <li>Cost Efficiency: S3 is cheaper for long-term storage compared to CloudWatch.</li> <li>Analysis Tools: Use Amazon Athena and QuickSight for querying and visualization.</li> </ul> </li> </ul> <p>Real-Time Streaming</p> <ul> <li>Log Streaming<ul> <li>To Amazon Kinesis: Stream logs for real-time processing.</li> <li>To Amazon OpenSearch Service: Ideal for near-real-time monitoring and search.</li> </ul> </li> <li>Subscription Filters<ul> <li>lets Triggers events based on specific patterns within logs in real time.</li> <li>Actions: Can stream to Amazon Kinesis or trigger AWS Lambda functions for automated responses.</li> </ul> </li> </ul> <ul> <li>Exam Tip<ul> <li>To Amazon OpenSearch Service: Ideal for near-real-time monitoring and search.</li> </ul> </li> </ul> <p>Summary</p> <ul> <li>Amazon CloudWatch provides comprehensive logging and monitoring capabilities, from basic log collection to advanced real-time analysis and anomaly detection. It integrates with various AWS services and offers multiple methods for analysing and reacting to log data, including using AWS-native tools and third-party services.</li> </ul>"},{"location":"9-Drafts/dea-c01-u3-data-operations-and-support-ps/#50-conclusion","title":"5.0 Conclusion","text":""},{"location":"9-Drafts/dea-c01-u3-data-operations-and-support-ps/#51-summary","title":"5.1 Summary","text":""},{"location":"9-Drafts/dea-c01-u3-data-operations-and-support-ps/#52-data-operations-and-support-exam-tips","title":"5.2 Data Operations and Support: Exam Tips","text":"<ol> <li> <p>Creating a Persistent, Central Data Catalog</p> <ul> <li>Scenario: Centralize metadata for an EMR Apache Hive metadata store with minimal operational overhead.</li> <li>Solution: Use AWS Glue Data Catalog.<ul> <li>AWS Glue Data Catalog: Provides a centralized, managed catalog that integrates easily with other AWS data sources.</li> <li>Alternatives: Amazon Aurora or RDS, but Glue is generally simpler and more effective.</li> </ul> </li> </ul> </li> <li> <p>Scanning S3 Buckets for PII</p> <ul> <li>Scenario: Ensure S3 buckets across your data pipeline do not contain Personally Identifiable Information (PII).</li> <li>Solution: Enable Amazon Macie.<ul> <li>Amazon Macie: Scans S3 buckets for sensitive data like PII and alerts or triggers actions if such data is found.</li> <li>Note: Macie is a key service to understand for exam scenarios, often appearing both as a correct answer and a distractor.</li> </ul> </li> </ul> </li> <li> <p>Enabling Non-Technical Data Analysts</p> <ul> <li>Scenario: Validate and enrich incoming data with a process that's easy to automate and suitable for non-technical data analysts.</li> <li>Solution: Use AWS Glue DataBrew.<ul> <li>AWS Glue DataBrew: A low or no-code tool that allows data analysts to define data validations and transformations.</li> <li>General Tip: For low or no-code solutions, consider Glue DataBrew.</li> </ul> </li> </ul> </li> <li> <p>Aggregating and Monitoring Custom CloudWatch Logs</p> <ul> <li>Scenario: Aggregate data, monitor logs, and set up anomaly detection for custom CloudWatch logs in real time.</li> <li>Solution: Use Amazon OpenSearch Service.<ul> <li>Amazon OpenSearch Service: Provides near-real-time monitoring and search capabilities for CloudWatch logs.</li> <li>Note: OpenSearch is ideal for real-time log analysis and searchability.</li> </ul> </li> </ul> </li> </ol> <p>Summary</p> <ul> <li>AWS Glue Data Catalog is best for centralizing metadata.</li> <li>Amazon Macie helps in scanning S3 buckets for sensitive data.</li> <li>AWS Glue DataBrew is suited for non-technical users needing low-code data transformations.</li> <li>Amazon OpenSearch Service is optimal for real-time log monitoring and analysis.</li> </ul> <p>Good luck with your exam preparation and future learning journey!</p>"},{"location":"9-Drafts/dea-c01-u4-data-security-and-governance-ps/","title":"U4 Data Security and Governance","text":""},{"location":"9-Drafts/dea-c01-u4-data-security-and-governance-ps/#20-limiting-access-to-data","title":"2.0 Limiting Access to Data","text":""},{"location":"9-Drafts/dea-c01-u4-data-security-and-governance-ps/#21-iam-foundations","title":"2.1 IAM Foundations","text":"<p>Identity and Access Management (IAM)</p> <ul> <li>Core service for managing access to AWS resources. Limiting access based on user roles and needs.</li> </ul> <ul> <li>Terms<ul> <li>Principals: Resources, applications, or users accessing AWS services. Entities requesting access to data.</li> <li>IAM Users: Directly map permissions to individual users.</li> <li>IAM Groups: Assign permissions to groups of users for easier management.</li> <li>IAM Policies:<ul> <li>JSON format document.</li> <li>They are crucial for managing access</li> <li>Eg, Policy allows actions like putting, getting objects, or retrieving object versions from a bucket.</li> </ul> </li> </ul> </li> </ul> <ul> <li>Permissions: Granted through IAM policies.</li> </ul> <ul> <li>Policy Types:<ul> <li>Identity-Based Policy: Attached to the principal (e.g., IAM user or role).</li> <li>Resource-Based Policy: Attached to the resource.</li> </ul> </li> </ul> <ul> <li>Access Evaluation:      1. Check for deny rules in any policy.      2. If no deny rule, check for allow rules.      3. Access granted if at least one policy allows it.</li> </ul> <p>Programmatic Access Risks</p> <ul> <li>Static Access Keys: Risk of keys being leaked or compromised \u26a0\ufe0f.</li> <li>Solution: Use IAM Identity Center to avoid long-lasting access keys.</li> </ul> <p>IAM Identity Center</p> <ul> <li>offers a more secure alternative to static access keys.</li> <li>Maps permission sets to IAM Roles not to users. Users get temporary sso login.</li> <li>Single Sign-On: Provides temporary access keys for the duration of a session.</li> <li>Benefits: Simplifies management and reduces security risks.</li> </ul> <p>IAM Roles</p> <ul> <li>Facilitate temporary access for AWS services and users.</li> <li>Usage: For temporary access to AWS resources by AWS services, applications, or users through IAM Identity Center.</li> </ul>"},{"location":"9-Drafts/dea-c01-u4-data-security-and-governance-ps/#22-accessing-private-data-stores","title":"2.2 Accessing Private Data Stores","text":"<p>Networking Scenarios for granting access to private resources in AWS.</p> <pre><code>flowchart LR\n\na1[IAM Users]\na2[AWS Lambda]\na3[Application]\n\nsubgraph s2[Security Layers]\n   direction TB\n   b1[IAM]\n   b2[Network]\n   b3[Authentication]\n   b4[Encryption]\nend\n\nb1 --&gt; b2 --&gt; b3 --&gt; b4\n\nsubgraph s3[AWS Resource]\n   direction TB\n   c1[Row Level Security]\n   c2[Resource]\n   c3[Monitoring &amp; Logging]\nend\n\nc1 --&gt; c2 --&gt; c3\na1 --&gt; s2\na2 --&gt; s2\na3 --&gt; s2\n\ns2 --&gt; s3\ns2 --&gt; s3\ns2 --&gt; s3</code></pre> <p>In the above diagram, consider you have setup level 1, that is proper IAM policies to connect. Then you need second thing setup, that is, proper network connection.</p> <pre><code>architecture-beta\n    group vpc(cloud)[VPC]\n    group psn1(cloud)[Private Subnet] in vpc\n    group psn2(cloud)[Private Subnet] in vpc\n\n    service db(database)[Database] in psn1\n    service server(server)[EC2] in psn2\n\n    service lambda(server)[Lambda]\n    service s3(server)[S3]\n\n    db:B &lt;-- T:lambda\n    lambda:R --&gt; L:s3\n    server:B --&gt; T:s3\n    server:L --&gt; R:db\n</code></pre> <p>Here, understand that S3 and Lambda are public services. They are not part of your private subnet. S3 is not in private subnet. Lambda has no fix VPC, rather it runs on AWS managed VPC. Hence they have access to public internet and same time do not have access to private subnet. While, EC2 and RDS are on your private network and are disconnected from public internet. You do need IAM permission but on top of that you also need proper network setup to allow the connections to correctly connect:</p> <ul> <li>S3 to private EC2</li> <li>Lambda to private RDS</li> <li>Lambda to S3</li> <li>from EC2 in one private Subnet to RDS in another private subnet.</li> </ul> <p>Network Connectivity for Private Resources</p> <ul> <li>Private resources need to talk to each other without using public internet.</li> <li>For this you need to configure the network security.</li> <li>You can use Security groups and NACL to define this connectivity. Or more importantly to troubleshoot it if connection fails.</li> </ul> <ul> <li>Examples of internal resources that need connectivity:<ul> <li>Private Database (e.g., RDS): Hosted in private subnets.</li> <li>Private Compute Services (e.g., EC2): Also in private subnets.</li> <li>AWS Lambda: Need to grant access to private resources.</li> </ul> </li> </ul> <ul> <li>Inbound rule and outbound rule for a security group define<ul> <li>what port is allowed</li> <li>what protocol is allowed</li> <li>What IP address / range is it allowed on</li> <li>eg,<ul> <li><code>198.51.100.0/24 TCP 22</code> allows SSH from internal range of IP</li> <li><code>0.0.0.0 all all</code> will allow all connection from all IP (internet)</li> <li><code>HTTP TCP 80 0.0.0.0/0 DENY</code> will deny any traffic on port 80 with protocol TCP.</li> </ul> </li> </ul> </li> </ul> <ul> <li>Security Groups:<ul> <li>Applied at the instance level. Just like firewall for the instance itself.</li> <li>Stateful, for allowed inbound traffic, the response is also allowed.</li> <li>Only define allow rules. What not allowed is denied by default.</li> <li>Order of rules don't matter. they are all applied at once.</li> </ul> </li> </ul> <ul> <li>Network Access Control Lists (NACLs):<ul> <li>Applied at the subnet level.</li> <li>Stateless, inbound traffic and outbound need explicit permission.</li> <li>Can define allow rules and deny rules.</li> <li>Rules are applied in order</li> </ul> </li> </ul> <p>Connecting Private EC2 Instances to Private RDS</p> <ul> <li>Private Resources: No public route through an internet gateway.</li> <li>Steps to check:<ol> <li>Ensure that route table is properly routing traffic within your VPC.</li> <li>Ensure database traffic is allowed at NACL level within your VPC.</li> <li>Ensure there is inbound rule on database-security-group that allows request traffic from instance-security-group.</li> <li>Ensure there is Outbound rule on instance-security-group that allows request traffic to database-security-group.</li> </ol> </li> </ul> <p>Connecting Private EC2 to S3</p> <ul> <li>EC2 is on private subnet and S3 are by default on your VPC.</li> <li>EC2 to S3 via internet is slow, expensive and insecure.</li> <li>You can connect internally by making, Gateway Endpoints.</li> <li>Gateway Endpoints are private end points for S3 or DynamoDB that permit private traffic within the same region. It keeps traffic within AWS and avoiding public internet.</li> </ul> <p>Connecting Lambda to Private RDS</p> <ul> <li>Lambda is not on fixed VPC. If not on your VPC it cannot connect to your Private RDS.</li> <li>While configuring Lambda, you can set them to have access to one of your private VPC.<ul> <li>Lambda still resided in AWS Managed Subnet.</li> <li>But will be given an Elastic Subnet that will allow network to your private subnet.</li> </ul> </li> <li>Elastic Network Interface: Allows Lambda functions to route requests to private resources.</li> </ul> <p>Connecting Lambda to S3</p> <ul> <li>IAM Permissions: Ensure Lambda functions have appropriate IAM permissions (identity-based or resource-based policy) to access S3 buckets. Rest all is managed by AWS.</li> </ul> <p>Differences between Security Groups and NACLs:</p> Security Group NACL Applied Instance Subnet State Stateful Stateless Traffic Both Side if one is allowed Each side needs explicit permission Rules Only define whats allowed Can define allow and deny Order Unordered, all at once Applied in order <p>Troubleshooting Connectivity Issues</p> <ul> <li>Check Routes: Ensure proper routing from EC2 to RDS.</li> <li>NACL Rules: Verify if NACL is blocking traffic.</li> <li>Security Group Rules: Ensure inbound and outbound rules are correct for the instance and database.</li> </ul> <p>Summary</p> <ul> <li>EC2 to RDS: Ensure proper security group and NACL configurations.</li> <li>Private Subnet to S3: Use gateway endpoints for secure and cost-effective access.</li> <li>Lambda Functions:<ul> <li>Access to Private Resources: Configure VPC access.</li> <li>Access to S3: Ensure IAM permissions are correctly set.</li> </ul> </li> </ul> <p>Next Steps: Proceed to the next lesson for further topics in data security and networking.</p>"},{"location":"9-Drafts/dea-c01-u4-data-security-and-governance-ps/#23-understanding-iam-and-basic-network-security-in-the-cloud","title":"2.3 Understanding IAM and Basic Network Security in the Cloud","text":"<p>lab</p>"},{"location":"9-Drafts/dea-c01-u4-data-security-and-governance-ps/#24-intro-to-secrets-manager-and-parameter-store","title":"2.4 Intro to Secrets Manager and Parameter Store","text":"<p>In the above diagram, consider you have setup level 1 and 2, that is proper IAM policies, and proper network setup. Then you need to setup third thing, that is, proper Authentication.</p> <p>Overview</p> <ul> <li>Authentication methods for accessing AWS databases, specifically RDS.</li> </ul> <p>Key Concepts</p> <p>Authentication Methods for RDS</p> <ol> <li> <p>IAM Authentication:</p> <ul> <li>Available for MariaDB, MySQL, and PostgreSQL.</li> <li>Limitations: May not handle very high connection rates efficiently</li> <li>Reduces read/write performance.</li> </ul> </li> <li> <p>Password Authentication:</p> <ul> <li>Uses hardcoded master username and password set during RDS database creation.</li> <li>This is not secure as it may expose password as plain text.</li> <li>To overcome this, use AWS Secrets Manager</li> </ul> </li> </ol> <p>AWS Services for Secrets Management</p> <ol> <li> <p>Parameter Store:</p> <ul> <li>Allows optional encryption of key-value parameters.</li> <li>Programmatically access key-value pairs</li> <li>Requires manual updates to parameters.</li> <li>Cost: 10,000 parameters for free.</li> <li>Use case: Suitable for centralized configuration and less sensitive information.</li> </ul> </li> <li> <p>Secrets Manager:</p> <ul> <li>Always encrypted, with automatic secret rotation.</li> <li>Cost: $0.40 per secret per month.</li> <li>Use Case: Best for sensitive data requiring high security and rotation. Ideal for high-security secrets and automatic rotation.</li> <li>Exam Tip: When security is top priority use secrets manager.</li> </ul> </li> </ol> <p>Integration with RDS</p> <ul> <li>Secrets Manager: Direct integration with RDS for automatic credential rotation.</li> </ul> <p>Accessing Secrets Programmatically</p> <ul> <li>Make lambda function</li> <li>Give it IAM access to secrets and secrets manager.</li> <li>Define secret's name and region.</li> <li>Initialize BOTO session and client.</li> <li>Retrieve secrets using <code>get_secret_value</code> function.</li> </ul> <p>Summary</p> <ul> <li>Authentication: Choose IAM or password authentication based on database and connection needs.</li> <li>Secrets Management:<ul> <li>Parameter Store: For general configurations and optional encryption.</li> <li>Secrets Manager: For high-security credentials and automatic rotation.</li> </ul> </li> <li>Integration: Secrets Manager integrates with RDS for secure, managed credential handling.</li> </ul>"},{"location":"9-Drafts/dea-c01-u4-data-security-and-governance-ps/#25-using-aws-secrets-manager-for-storing-and-rotating-database-credentials","title":"2.5 Using AWS Secrets Manager for Storing and Rotating Database Credentials","text":"<p>lab</p>"},{"location":"9-Drafts/dea-c01-u4-data-security-and-governance-ps/#26-securing-data-using-lake-formation","title":"2.6 Securing Data Using Lake Formation","text":"<p>IAM, Network, Authentication setup works for most use cases but may be problematic when dealing with Data Lake. For this AWS has created <code>AWS Lake Formation</code>. This manages the security using:</p> <ul> <li>IAM</li> <li>Row Level Security</li> <li>Monitoring and Logging</li> </ul> <p>It does not use much use of Networking, Authentication, Encryption.</p> <p>Data Lake is usually made of following services:</p> <ul> <li>S3 Buckets</li> <li>Data Catalog (in Glue)</li> <li>Data Analytics (in Athena)</li> </ul> <p>So now, each principal (user, service) need access to each of these resources. This will lead to create separate permission for each user for each resource, which becomes unmanageable. This is where \"AWS Lake Formation\" comes in. One permission can give access to all services in Lake.</p> <p>Overview</p> <ul> <li>Focus: Using AWS Lake Formation for managing permissions and access in data lakes.</li> </ul> <ol> <li> <p>AWS Lake Formation</p> <ul> <li>Simplifies the management of IAM permissions, granular permissions, and monitoring for data lakes.</li> <li>Features:<ul> <li>Centralizes access to all components of the data lake.</li> <li>Applies permissions across services within the data lake, not just per service.</li> </ul> </li> </ul> </li> <li> <p>Permissions Management</p> <ul> <li>IAM Permissions: Define actions principals can take.</li> <li>Lake Formation Permissions: Determine what data these actions apply to, using fine-grained permissions.</li> </ul> </li> <li> <p>Fine-Grained Permissions</p> <ul> <li>RDBMS Syntax: Used for restricting access at the database, table, or column level.</li> <li>Example:<ul> <li>Data Analyst: Grants access to specific tables (e.g., Food Item Sales) using IAM and Lake Formation permissions.</li> <li>Data Lake Administrator: Has broader access to manage and grant permissions to other users.</li> </ul> </li> </ul> </li> <li> <p>Cross-Account Access</p> <ul> <li>Functionality: Allows granting permissions to principals in different AWS accounts.</li> <li>Use Case: Centralized data lake accessed by multiple accounts within an organization.</li> </ul> </li> <li> <p>Event Logging</p> <ul> <li>CloudTrail Integration: Tracks API actions within the data lake.</li> <li>Lake Formation: Simplifies monitoring and auditing of data actions and permission changes.</li> </ul> </li> </ol> <p>Summary</p> <ul> <li>Lake Formation provides a centralized approach to managing data access and permissions within a data lake.</li> <li>IAM defines what actions can be taken, while Lake Formation specifies which data those actions can access.</li> <li>Fine-Grained Permissions are managed using RDBMS syntax.</li> <li>Cross-Account Access and Event Logging enhance flexibility and security.</li> </ul>"},{"location":"9-Drafts/dea-c01-u4-data-security-and-governance-ps/#27-using-lake-formation-granular-permissions","title":"2.7 Using Lake Formation Granular Permissions","text":"<p>Achieving super fine-grained permissions using Lake Formation</p> <p>Key Terms</p> <ul> <li>Data Catalog<ul> <li>Centralizes data from multiple sources.</li> <li>Organized into databases and tables accessible by analytic services and transformations.</li> </ul> </li> </ul> <ul> <li>Fine-Grained Access Control<ul> <li>Provide tailored access to specific tables and data within the catalog.</li> </ul> </li> </ul> <p>Types of Filters</p> <ul> <li>Column Masking Filter:<ul> <li>Hides specific columns from certain users.</li> <li>Eg, Mask the <code>shipping address</code> column in an item orders table.</li> </ul> </li> </ul> <ul> <li>Row Filter Expression:<ul> <li>Restricts access to rows based on criteria.</li> <li>Eg, Allow an analyst to access only orders from the U.S. by filtering rows where the <code>country</code> column equals \"U.S.\"</li> </ul> </li> </ul> <ul> <li>Cell Level Security:<ul> <li>Apply filters to individual cells within a table to achieve more granular control.</li> </ul> </li> </ul> <p>Application of Filters</p> <ul> <li>Filters can be combined and applied at the table level when granting access to users or groups.</li> <li>Permissions can be granted to individual users or groups with specific filters applied.</li> <li>Examples:<ul> <li>U.S. Sales Filter: Grants access to an analyst focusing on U.S. sales data.</li> <li>Canada Filter: Grants access to an analyst focusing on Canadian orders.</li> </ul> </li> </ul> <p>Summary</p> <ul> <li>Lake Formation allows for detailed permissions management using data catalogs and filters.</li> <li>Filters provide granular access control at the table level, including column masking, row filtering, and cell level security.</li> <li>Permission Granting: Filters can be customized and combined to fit specific user needs.</li> <li>Hence, using filter you can restrict access to table, column, row, or cell for user / group by making use of data catalog and filter.</li> </ul>"},{"location":"9-Drafts/dea-c01-u4-data-security-and-governance-ps/#30-ensuring-data-encryption-and-masking","title":"3.0 Ensuring Data Encryption and Masking","text":""},{"location":"9-Drafts/dea-c01-u4-data-security-and-governance-ps/#31-protecting-pii-and-masking-columns","title":"3.1 Protecting PII and Masking Columns","text":"<p>Protecting Personally Identifiable Information (PII) and masking data in AWS services, specifically S3 and Redshift</p> <p>S3 Data Masking (Data Lake)</p> <ul> <li>Protecting PII across multiple S3 buckets and accounts using Amazon Macie.</li> <li>Amazon Macie:<ul> <li>Scans S3 buckets for PII and provides alerts or automated actions.</li> <li>Can be set up at the organizational level to cover all S3 buckets in the organization.</li> </ul> </li> </ul> <p>Amazon Redshift Data Masking</p> <ul> <li>Protecting PII in data warehouses from exposure to analysts and data engineers.</li> <li> <p>It can be done in three ways:</p> <ol> <li> <p>Column-Level Masking:</p> <ul> <li>Mask columns by adjusting grants for <code>SELECT</code> and <code>UPDATE</code> privileges. That is, you can grant access to only select/update only certain column in a table.</li> <li>Masking can be applied to individual columns within tables.</li> </ul> </li> <li> <p>Row-Level Security (RLS) Policies:</p> <ul> <li>Limits access to specific rows based on user or role.</li> <li>Create and apply RLS policies to control row access.</li> <li>Example: A policy to restrict access to rows based on user roles.</li> </ul> </li> <li> <p>Dynamic Data Masking:</p> <ul> <li>Masks part or all of a column's data at query time.</li> <li>Benefit: Does not consume extra space or impact queries that do not involve masked data.</li> <li>Example: Masking a credit card number to show only the last four digits.</li> </ul> </li> </ol> </li> </ul> <p>Summary</p> <ul> <li>S3: Use Amazon Macie for scanning and protecting PII across S3 buckets and accounts.</li> <li>Redshift:<ul> <li>Column-Level Masking: Apply masking through access control grants.</li> <li>Row-Level Security: Use RLS policies to restrict access to specific rows.</li> <li>Dynamic Data Masking: Configure to mask data dynamically at query time based on user roles and permissions.</li> </ul> </li> </ul>"},{"location":"9-Drafts/dea-c01-u4-data-security-and-governance-ps/#32-data-encryption-options","title":"3.2 Data Encryption Options","text":"<p>Encryption methods for protecting data at rest and in transit in AWS</p> <p>Types of Encryption</p> <ol> <li>Data at Rest: Data stored in S3 buckets, EBS drives in EC2, or local storage on a device.</li> <li>Data in Transit: Data being transferred over networks, encrypted using TLS (Transport Layer Security), used in HTTPS.</li> </ol> <p>Encryption Methods</p> <ul> <li>Client-Side Encryption:<ul> <li>Data is encrypted by the application before sending it to S3.</li> <li>The application manages the encryption key.</li> <li>Data never leaves the application in an unencrypted state.</li> </ul> </li> </ul> <ul> <li>Server-Side Encryption:<ul> <li>Data is sent to S3 in an unencrypted form and encrypted once it reaches S3.</li> <li>Lower operational overhead as AWS manages encryption.</li> </ul> </li> </ul> <p>Server-Side Encryption Options in S3</p> <ol> <li> <p>S3-Managed Keys (SSE-S3):</p> <ul> <li>Enabled by default for S3 buckets.</li> <li>Uses 256-bit Advanced Encryption Standard (AES-256).</li> <li>Each object is encrypted with a unique key.</li> </ul> </li> <li> <p>AWS Key Management Service (KMS) Keys (SSE-KMS):</p> <ul> <li>Provides more granular access control over key permissions.</li> <li>Key policies can be managed and tracked via CloudTrail.</li> <li>Requires specifying KMS as the encryption type and providing the key ID.</li> </ul> </li> <li> <p>Customer-Provided Keys (SSE-C):</p> <ul> <li>Full control over encryption keys, which can be managed on-premises or in other clouds.</li> <li>Requires sending the encryption algorithm and Base64-encoded key with each request.</li> <li>Risk: Keys are compromised if sent over an unsecured network.</li> </ul> </li> </ol> <p>Encrypting Data in Amazon Redshift</p> <ul> <li>KMS Encryption:<ul> <li>Use AWS-managed or customer-managed KMS keys.</li> <li>Performance Impact: Encryption affects database performance. Must be enabled upon launch or through migration.</li> <li>Possible downtime during migration to an encrypted cluster.</li> </ul> </li> </ul> <p>Encrypting Data in AWS Glue</p> <ul> <li>Data in Transit: Ensure data is encrypted using TLS when transferred.</li> <li>Data at Rest: Use KMS or S3-managed keys for encryption of data catalog and stored data.</li> </ul> <p>Access Permissions for Encrypted Data</p> <ul> <li>Requirements: Users must have permissions not only to access the data but also to encrypt and decrypt it.</li> </ul> <p>Summary</p> <ul> <li>S3 Encryption: Choose between S3-managed keys, KMS keys, or customer-provided keys based on security needs.</li> <li>Redshift Encryption: Use KMS for encryption; consider performance implications and manage encryption at database launch or migration.</li> <li>AWS Glue Encryption: Secure data in transit with TLS and data at rest with KMS or S3-managed keys. Ensure users have appropriate encryption permissions.</li> </ul>"},{"location":"9-Drafts/dea-c01-u4-data-security-and-governance-ps/#40-data-privacy-and-governance","title":"4.0 Data Privacy and Governance","text":"<p>cloudTrail and CloudWatch for Audit, Monitoring and Governance</p>"},{"location":"9-Drafts/dea-c01-u4-data-security-and-governance-ps/#41-preparing-logs-for-audit-tracking-api-calls-via-cloudtrail","title":"4.1 Preparing Logs for Audit: Tracking API Calls via CloudTrail","text":"<p>Data privacy and governance, specifically tracking and auditing access with CloudTrail</p> <ul> <li>CloudTrail Overview<ul> <li>Records API calls made within your AWS account.</li> <li>Provides a trail of all actions taken, useful for auditing activities.</li> </ul> </li> </ul> <ul> <li>Storing CloudTrail Logs<ol> <li>S3 Bucket: Store logs in an S3 bucket and use tools like Amazon Athena to query and analyze logs.</li> <li>CloudTrail Lake: A managed solution for storing, querying, and filtering CloudTrail logs by event types.</li> </ol> </li> </ul> <ul> <li>Centralized Log Management<ul> <li>In an org, you can have multiple accounts generating logs, you need to analyze them all together, for this case you can create a separate account for centralized storing and analysing log trail.</li> <li>Best Practice: Use a centralized account to aggregate CloudTrail logs from multiple accounts.</li> <li>Advantages:<ul> <li>Easier management of permissions and security.</li> <li>Simplifies access control and prevents accidental deletion or tampering.</li> <li>Enables cross-account log analysis.</li> </ul> </li> </ul> </li> </ul> <ul> <li>Access Control for Centralized Logs<ul> <li>Storage: Logs are centrally stored in a dedicated account.</li> <li>Security: Restrict access to logs and ensure they are secure and tamper-proof.</li> <li>Analytics: Grant access to analytics services like Amazon Athena for querying centralized logs.</li> </ul> </li> </ul> <p>Summary</p> <ul> <li>CloudTrail: Essential for tracking API activity and auditing in AWS.</li> <li>Log Storage: Use S3 for flexibility or CloudTrail Lake for a managed solution.</li> <li>Centralized Logging: Recommended for managing logs from multiple accounts and simplifying access and security.</li> </ul>"},{"location":"9-Drafts/dea-c01-u4-data-security-and-governance-ps/#42-using-aws-config-and-cloudtrail","title":"4.2 Using AWS Config and CloudTrail","text":"<p>Lab</p>"},{"location":"9-Drafts/dea-c01-u4-data-security-and-governance-ps/#43-cloudwatch-alarms-and-logs","title":"4.3 CloudWatch Alarms and Logs","text":"<p>Using CloudWatch for monitoring resource performance and application logs</p> <ul> <li>CloudWatch Overview<ul> <li>Monitors resource performance and application logs.</li> <li>Complementary to CloudTrail: While CloudTrail tracks API activity, CloudWatch focuses on performance metrics and logs.</li> </ul> </li> </ul> <ul> <li>Log Collection and Management<ul> <li>Log Groups: Collect CloudWatch logs into log groups.</li> <li>Subscriptions: Subscribe services to log events from specific log groups or across an entire account.</li> <li>Streaming and Filtering: Stream entire log groups or filter to capture specific event types.</li> </ul> </li> </ul> <ul> <li>Metrics and Alarms<ul> <li>Metric Filters: Attach to log groups to define alarms based on metrics.</li> <li>Alarms: Triggered when metrics exceed defined thresholds for a certain period.</li> <li>Notifications: Use SNS notifications or automated actions in response to alarms (e.g., email alerts for DDL commands in Redshift).</li> </ul> </li> </ul> <ul> <li>Real-Time Analysis<ul> <li>Amazon OpenSearch: Use for near real-time analysis of CloudWatch logs.</li> </ul> </li> </ul> <ul> <li>CloudWatch vs. CloudTrail<ul> <li>CloudTrail: Records all API activity for auditing.</li> <li>CloudWatch: Monitors resource performance and captures application logs.</li> <li>Use Cases:<ul> <li>CloudTrail: Tracking and auditing actions.</li> <li>CloudWatch: Resource monitoring and performance metrics.</li> </ul> </li> </ul> </li> </ul> <ul> <li>Log Storage and Cost Management<ul> <li>Cost Considerations: CloudWatch logs can become expensive over time.</li> <li>Archiving: Backup CloudWatch logs to Amazon S3 for cost savings.</li> <li>Analysis: Analyze archived logs with Amazon Athena.</li> </ul> </li> </ul> <ul> <li>Log Retention<ul> <li>Default Setting: CloudWatch logs are stored indefinitely by default.</li> <li>Recommendation: Adjust log retention settings to avoid unnecessary storage costs.</li> </ul> </li> </ul> <p>Summary</p> <ul> <li>CloudWatch: Essential for monitoring and managing resource performance and logs.</li> <li>Cost Management: Backup logs to S3 to manage costs effectively.</li> <li>Retention Settings: Adjust default log retention to control expenses.</li> </ul>"},{"location":"9-Drafts/dea-c01-u4-data-security-and-governance-ps/#44-sharing-data-across-redshift-clusters","title":"4.4 Sharing Data Across Redshift Clusters","text":"<p>Sharing data across Redshift clusters using datashares</p> <p>Redshift Datashare</p> <ul> <li>Definition: A datashare allows you to share Redshift data objects across clusters.</li> <li>Components: Can include databases, tables, user-defined functions, materialized views, or schemas.</li> </ul> <p>Sharing Data</p> <ul> <li>Creation: Define and create a datashare in the Redshift console.</li> <li>Inclusion: Add objects like schemas and tables to the datashare.</li> <li>Granting Access: Grant usage of the datashare to specific Redshift clusters.</li> <li>Access Types: Shared clusters will have read-only access to the datashare objects.</li> </ul> <p>Sharing Capabilities</p> <ul> <li>Cluster Types: Share across different types of Redshift clusters.</li> <li>Availability Zones/Regions: Share across different availability zones and AWS regions.</li> <li>Accounts: Share datashares across different AWS accounts.</li> </ul> <p>Automatic Updates</p> <ul> <li>Live Data: Datashares provide real-time access to live data from the source cluster. No need for manual refreshes to update data.</li> </ul> <p>Example Commands</p> <p>Summary</p> <ul> <li>Datashare Functionality: Facilitates data sharing across clusters, regions, and accounts.</li> <li>Access Control: Grants read-only access to shared data.</li> <li>Real-Time Data: Ensures up-to-date data access without manual refreshes.</li> </ul>"},{"location":"9-Drafts/dea-c01-u4-data-security-and-governance-ps/#50-conclusion","title":"5.0 Conclusion","text":""},{"location":"9-Drafts/dea-c01-u4-data-security-and-governance-ps/#51-summary","title":"5.1 Summary","text":""},{"location":"9-Drafts/dea-c01-u4-data-security-and-governance-ps/#52-data-security-and-governance-exam-tips","title":"5.2 Data Security and Governance: Exam Tips","text":"<p>Key Scenarios and Solutions</p> <ol> <li> <p>Connecting EC2 to Private RDS Instance</p> <ul> <li>Issue: Unable to connect an EC2 instance to a private RDS instance.</li> <li>Troubleshooting Steps:<ul> <li>Verify routing and IAM permissions.</li> <li>Check security group settings:<ul> <li>RDS Security Group: Should allow inbound traffic from the EC2 security group.</li> <li>EC2 Security Group: Should allow outbound traffic to the RDS security group.</li> </ul> </li> <li>Also verify Network ACL (NACL) settings to ensure they allow database traffic.</li> </ul> </li> </ul> </li> <li> <p>Accessing Sensitive Data in Redshift</p> <ul> <li>Scenario: Redshift database contains Personally Identifiable Information (PII) that should not be exposed.</li> <li>Solutions:<ul> <li>Column Masking: Mask sensitive columns when granting access.</li> <li>Row-Level Security (RLS) Policies: Use RLS to restrict access to specific rows.</li> <li>Dynamic Data Masking: Optionally use dynamic data masking to show portions or transformed versions of the data.</li> </ul> </li> </ul> </li> <li> <p>Encrypting Data at Rest in S3</p> <ul> <li>Requirement: Control over encryption keys and their policies.</li> <li>Options:<ul> <li>S3 Managed Keys: Default but do not allow control over key policies.</li> <li>KMS Keys: Provides access to key policies and is easier to manage than customer-provided keys.</li> <li>Customer-Provided Keys: Offers full control but requires more management effort.</li> </ul> </li> </ul> </li> <li> <p>Sharing Materialized Views Across AWS Accounts</p> <ul> <li>Scenario: Share a materialized view from a Redshift cluster with data engineers in another AWS account.</li> <li>Solution: Use Datashares.<ul> <li>Create a datashare and include the materialized view.</li> <li>Grant read-only access to the datashare for the other Redshift cluster, which can be in a different account, region, or cluster type.</li> </ul> </li> </ul> </li> </ol> <p>Conclusion</p> <ul> <li>Exam Preparation: Review these scenarios and solutions to gauge readiness for the Data Engineer Associate exam.</li> <li>Final Advice: Ensure familiarity with troubleshooting connections, managing sensitive data access, encryption options, and data sharing techniques.</li> </ul>"},{"location":"9-Drafts/ll-py-design-patterns/","title":"Python: Design Patterns","text":"In\u00a0[40]: Copied! <pre>class Dog():\n    def __init__(self, name) -&gt; None:\n        self._name = name\n    \n    def speak(self) -&gt; str:\n        return \"Woof!\"\n\nclass Cat():\n    def __init__(self, name) -&gt; None:\n        self._name = name\n    \n    def speak(self) -&gt; str:\n        return \"Meow!\"\n\n# factory method\ndef get_pet(pet) -&gt; object:\n    pets = {\n        \"dog\": Dog(\"Moti\"),\n        \"cat\": Cat(\"Billu\")\n    }\n    return pets[pet]\n\n\nd = get_pet(\"dog\")\nprint(d.speak())\n\nc = get_pet(\"cat\")\nprint(c.speak())\n</pre> class Dog():     def __init__(self, name) -&gt; None:         self._name = name          def speak(self) -&gt; str:         return \"Woof!\"  class Cat():     def __init__(self, name) -&gt; None:         self._name = name          def speak(self) -&gt; str:         return \"Meow!\"  # factory method def get_pet(pet) -&gt; object:     pets = {         \"dog\": Dog(\"Moti\"),         \"cat\": Cat(\"Billu\")     }     return pets[pet]   d = get_pet(\"dog\") print(d.speak())  c = get_pet(\"cat\") print(c.speak()) <pre>Woof!\nMeow!\n</pre> <p>Here, you have flexibility to add new class when required, and user is unaffected by this change.</p> In\u00a0[41]: Copied! <pre>class Dog():\n    \n    def __init__(self) -&gt; None:\n        self._name = \"Dog\"\n        \n    def speak(self):\n        return \"Whof!\"\n\n# factory\nclass DogFactory():\n\n    def get_pet(self):\n        return Dog()\n\n    def get_food(self):\n        return \"Dog Roti\"\n\n# abstract factory house\nclass PetStore():\n\n    # abstract factory\n    def __init__(self, pet_factory=None):\n        self._pet_factory = pet_factory\n\n    def show_pet(self):\n        # utility to display details\n\n        # concrete\n        pet = self._pet_factory.get_pet()\n        pet_food = self._pet_factory.get_food()\n\n        print(f\"Our pet is {pet._name}\")\n        print(f\"Our pet says '{pet.speak()}'\")\n        print(f\"Its food is '{pet_food}'\")\n\n# concrete factory\ndog = DogFactory()\n\n# housing abstract\nshop = PetStore(dog)\n\n# invoke utility method\nshop.show_pet()\n</pre>  class Dog():          def __init__(self) -&gt; None:         self._name = \"Dog\"              def speak(self):         return \"Whof!\"  # factory class DogFactory():      def get_pet(self):         return Dog()      def get_food(self):         return \"Dog Roti\"  # abstract factory house class PetStore():      # abstract factory     def __init__(self, pet_factory=None):         self._pet_factory = pet_factory      def show_pet(self):         # utility to display details          # concrete         pet = self._pet_factory.get_pet()         pet_food = self._pet_factory.get_food()          print(f\"Our pet is {pet._name}\")         print(f\"Our pet says '{pet.speak()}'\")         print(f\"Its food is '{pet_food}'\")  # concrete factory dog = DogFactory()  # housing abstract shop = PetStore(dog)  # invoke utility method shop.show_pet() <pre>Our pet is Dog\nOur pet says 'Whof!'\nIts food is 'Dog Roti'\n</pre> <p>Here, concrete is dog object and dogfood object. class <code>PetStore</code> is abstract as it is nothing without a concrete object <code>dog</code> passed to it.</p> <p>[?] Overcomplicated...!!</p> In\u00a0[42]: Copied! <pre>\"\"\"Singleton implementation\"\"\"\n\nclass Borg:\n    _shared_data = {}       # attribute dictionary\n    \n    def __init__(self):\n        # Make an attribute dictionary\n        self.__dict__ = self._shared_data\n\n# singleton class, inherits Borg\nclass Singleton(Borg):\n    def __init__(self, **kwargs):\n        Borg.__init__(self)\n        self._shared_data.update(kwargs)\n        \n    def __str__(self):\n        return str(self._shared_data)\n\nx = Singleton(name = \"John\")\nprint(x)\ny = Singleton(age = 24)\nprint(y)\n</pre> \"\"\"Singleton implementation\"\"\"  class Borg:     _shared_data = {}       # attribute dictionary          def __init__(self):         # Make an attribute dictionary         self.__dict__ = self._shared_data  # singleton class, inherits Borg class Singleton(Borg):     def __init__(self, **kwargs):         Borg.__init__(self)         self._shared_data.update(kwargs)              def __str__(self):         return str(self._shared_data)  x = Singleton(name = \"John\") print(x) y = Singleton(age = 24) print(y) <pre>{'name': 'John'}\n{'name': 'John', 'age': 24}\n</pre> <p>Here, <code>update()</code> is dict method to update dict with args. Borg, build dict, Singleton uses it and exposes as shared to all Singleton object created. <code>y</code> automatically has 'name' added, because the object is shared resource.</p> In\u00a0[43]: Copied! <pre># Implement Builder\n\nclass Director():\n    def __init__(self, builder):\n        self._builder = builder\n        \n    def construct_car(self):\n        self._builder.create_new_car()\n        self._builder.add_model()\n        self._builder.add_tires()\n        self._builder.add_engine()\n    \n    def get_car(self):\n        return self._builder.car\n\n# Abstract builder    \nclass Builder():\n    def __init__(self):\n        self.car = None\n    \n    def create_new_car(self):\n        self.car = Car()\n\n# concrete builder\nclass MustangBuilder(Builder):\n    def add_model(self):\n        self.car.model = \"Mustang\"\n    \n    def add_tires(self):\n        self.car.tires = \"Radials\"\n    \n    def add_engine(self):\n        self.car.engine = \"Muscle Turbo\"\n    \n# product    \nclass Car():\n    def __init__(self):\n        self.model = None\n        self.tires = None\n        self.engine = None\n        \n    def __str__(self):\n        return f\"{self.model} | {self.tires} | {self.engine}\"\n    \n# concrete builder\nbuilder = MustangBuilder()\ndirector = Director(builder)\ndirector.construct_car()\ncar = director.get_car()\n\nprint(car)\n</pre> # Implement Builder  class Director():     def __init__(self, builder):         self._builder = builder              def construct_car(self):         self._builder.create_new_car()         self._builder.add_model()         self._builder.add_tires()         self._builder.add_engine()          def get_car(self):         return self._builder.car  # Abstract builder     class Builder():     def __init__(self):         self.car = None          def create_new_car(self):         self.car = Car()  # concrete builder class MustangBuilder(Builder):     def add_model(self):         self.car.model = \"Mustang\"          def add_tires(self):         self.car.tires = \"Radials\"          def add_engine(self):         self.car.engine = \"Muscle Turbo\"      # product     class Car():     def __init__(self):         self.model = None         self.tires = None         self.engine = None              def __str__(self):         return f\"{self.model} | {self.tires} | {self.engine}\"      # concrete builder builder = MustangBuilder() director = Director(builder) director.construct_car() car = director.get_car()  print(car) <pre>Mustang | Radials | Muscle Turbo\n</pre> <p>This makes the process that require same steps more efficient and managable. Now if you have to add new model, it can be done easily.</p> In\u00a0[44]: Copied! <pre>import copy\n\nclass Prototype:\n    \n    def __init__(self):\n        self._objects = {}\n        \n    def register_object(self, name, obj):\n        \"\"\"Register an object\"\"\"\n        self._objects[name] = obj\n        \n    def unregister_object(self, name):\n        \"\"\"Unregister an object\"\"\"\n        del self._objects[name]\n        \n    def clone(self, name, **attr):\n        \"\"\"Clone a registered object and update its attributes\"\"\"\n        obj = copy.deepcopy(self._objects.get(name))\n        obj.__dict__.update(attr)\n        return obj\n        \nclass Car:\n    def __init__(self):\n        self.name = \"Skylark\"\n        self.color = \"Red\"\n        self.options = \"Ex\"\n        \n    def __str__(self):\n        return '{} | {} | {}'.format(self.name, self.color, self.options)\n        \nc = Car()\nprototype = Prototype()\nprototype.register_object('skylark',c)\n\nc1 = prototype.clone('skylark')\n\nprint(c1)\n</pre> import copy  class Prototype:          def __init__(self):         self._objects = {}              def register_object(self, name, obj):         \"\"\"Register an object\"\"\"         self._objects[name] = obj              def unregister_object(self, name):         \"\"\"Unregister an object\"\"\"         del self._objects[name]              def clone(self, name, **attr):         \"\"\"Clone a registered object and update its attributes\"\"\"         obj = copy.deepcopy(self._objects.get(name))         obj.__dict__.update(attr)         return obj          class Car:     def __init__(self):         self.name = \"Skylark\"         self.color = \"Red\"         self.options = \"Ex\"              def __str__(self):         return '{} | {} | {}'.format(self.name, self.color, self.options)          c = Car() prototype = Prototype() prototype.register_object('skylark',c)  c1 = prototype.clone('skylark')  print(c1) <pre>Skylark | Red | Ex\n</pre> In\u00a0[45]: Copied! <pre># Decorator Pattern\n\nfrom functools import wraps\n\ndef make_blink(function):\n\t\"\"\"Defines the decorator\"\"\"\n\n\t#This makes the decorator transparent in terms of its name and docstring\n\t@wraps(function)\n\n\t#Define the inner function\n\tdef decorator():\n\t\t#Grab the return value of the function being decorated\n\t\tret = function() \n\n\t\t#Add new functionality to the function being decorated\n\t\treturn \"&lt;blink&gt;\" + ret + \"&lt;/blink&gt;\"\n\n\treturn decorator\n\n#Apply the decorator here!\n@make_blink\ndef hello_world():\n\t\"\"\"Original function! \"\"\"\n\n\treturn \"Hello, World!\"\n\n#Check the result of decorating\nprint(hello_world())\n\n#Check if the function name is still the same name of the function being decorated\nprint(hello_world.__name__)\n\n#Check if the docstring is still the same as that of the function being decorated\nprint(hello_world.__doc__)\n</pre> # Decorator Pattern  from functools import wraps  def make_blink(function): \t\"\"\"Defines the decorator\"\"\"  \t#This makes the decorator transparent in terms of its name and docstring \t@wraps(function)  \t#Define the inner function \tdef decorator(): \t\t#Grab the return value of the function being decorated \t\tret = function()   \t\t#Add new functionality to the function being decorated \t\treturn \"\" + ret + \"\"  \treturn decorator  #Apply the decorator here! @make_blink def hello_world(): \t\"\"\"Original function! \"\"\"  \treturn \"Hello, World!\"  #Check the result of decorating print(hello_world())  #Check if the function name is still the same name of the function being decorated print(hello_world.__name__)  #Check if the docstring is still the same as that of the function being decorated print(hello_world.__doc__) <pre>&lt;blink&gt;Hello, World!&lt;/blink&gt;\nhello_world\nOriginal function! \n</pre> <p>Here, we have added brink around string without modifying the original function.</p> In\u00a0[46]: Copied! <pre># Proxy Implementation\n\nimport time\n\nclass Producer:\n\t\"\"\"Define the 'resource-intensive' object to instantiate!\"\"\"\n\tdef produce(self):\n\t\tprint(\"Producer is working hard!\")\n\n\tdef meet(self):\n\t\tprint(\"Producer has time to meet you now!\")\n\nclass Proxy:\n\t\"\"\"\"Define the 'relatively less resource-intensive' proxy to instantiate as a middleman\"\"\"\n\tdef __init__(self):  \n\t\tself.occupied = 'No'\n\t\tself.producer = None\n\n\tdef produce(self):\n\t\t\"\"\"Check if Producer is available\"\"\"\n\t\tprint(\"Artist checking if Producer is available ...\")\n\n\t\tif self.occupied == 'No':\n\t\t\t#If the producer is available, create a producer object!\n\t\t\tself.producer = Producer()\n\t\t\ttime.sleep(2)\n\n\t\t\t#Make the prodcuer meet the guest!\n\t\t\tself.producer.meet()\n\t\t\t\n\t\telse:\n\t\t\t#Otherwise, don't instantiate a producer \n\t\t\ttime.sleep(2)\n\t\t\tprint(\"Producer is busy!\")\n\n#Instantiate a Proxy\np = Proxy()\n\n#Make the proxy: Artist produce until Producer is available\np.produce()\n\n#Change the state to 'occupied'\np.occupied = 'Yes'\n\n#Make the Producer produce\np.produce()\n</pre> # Proxy Implementation  import time  class Producer: \t\"\"\"Define the 'resource-intensive' object to instantiate!\"\"\" \tdef produce(self): \t\tprint(\"Producer is working hard!\")  \tdef meet(self): \t\tprint(\"Producer has time to meet you now!\")  class Proxy: \t\"\"\"\"Define the 'relatively less resource-intensive' proxy to instantiate as a middleman\"\"\" \tdef __init__(self):   \t\tself.occupied = 'No' \t\tself.producer = None  \tdef produce(self): \t\t\"\"\"Check if Producer is available\"\"\" \t\tprint(\"Artist checking if Producer is available ...\")  \t\tif self.occupied == 'No': \t\t\t#If the producer is available, create a producer object! \t\t\tself.producer = Producer() \t\t\ttime.sleep(2)  \t\t\t#Make the prodcuer meet the guest! \t\t\tself.producer.meet() \t\t\t \t\telse: \t\t\t#Otherwise, don't instantiate a producer  \t\t\ttime.sleep(2) \t\t\tprint(\"Producer is busy!\")  #Instantiate a Proxy p = Proxy()  #Make the proxy: Artist produce until Producer is available p.produce()  #Change the state to 'occupied' p.occupied = 'Yes'  #Make the Producer produce p.produce()  <pre>Artist checking if Producer is available ...\nProducer has time to meet you now!\nArtist checking if Producer is available ...\nProducer is busy!\n</pre> In\u00a0[47]: Copied! <pre>class Korean:\n\t\"\"\"Korean speaker\"\"\"\n\tdef __init__(self):\n\t\tself.name = \"Korean\"\n\n\tdef speak_korean(self):\n\t\treturn \"An-neyong?\"\n\nclass British:\n\t\"\"\"English speaker\"\"\"\n\tdef __init__(self):\n\t\tself.name = \"British\"\t\n\n\t#Note the different method name here!\n\tdef speak_english(self):\n\t\treturn \"Hello!\"\t\n\nclass Adapter:\n\t\"\"\"This changes the generic method name to individualized method names\"\"\"\n\n\tdef __init__(self, object, **adapted_method):\n\t\t\"\"\"Change the name of the method\"\"\"\n\t\tself._object = object\n\n\t\t#Add a new dictionary item that establishes the mapping between the generic method name: speak() and the concrete method\n\t\t#For example, speak() will be translated to speak_korean() if the mapping says so\n\t\tself.__dict__.update(adapted_method)\n\n\tdef __getattr__(self, attr):\n\t\t\"\"\"Simply return the rest of attributes!\"\"\"\n\t\treturn getattr(self._object, attr)\n\t\t\n#List to store speaker objects\nobjects = []\n\n#Create a Korean object\nkorean = Korean()\n\n#Create a British object\nbritish = British()\n\n#Append the objects to the objects list\nobjects.append(Adapter(korean, speak=korean.speak_korean))\nobjects.append(Adapter(british, speak=british.speak_english))\n\n\nfor obj in objects:\n\tprint(\"{} says '{}'\\n\".format(obj.name, obj.speak()))\n</pre> class Korean: \t\"\"\"Korean speaker\"\"\" \tdef __init__(self): \t\tself.name = \"Korean\"  \tdef speak_korean(self): \t\treturn \"An-neyong?\"  class British: \t\"\"\"English speaker\"\"\" \tdef __init__(self): \t\tself.name = \"British\"\t  \t#Note the different method name here! \tdef speak_english(self): \t\treturn \"Hello!\"\t  class Adapter: \t\"\"\"This changes the generic method name to individualized method names\"\"\"  \tdef __init__(self, object, **adapted_method): \t\t\"\"\"Change the name of the method\"\"\" \t\tself._object = object  \t\t#Add a new dictionary item that establishes the mapping between the generic method name: speak() and the concrete method \t\t#For example, speak() will be translated to speak_korean() if the mapping says so \t\tself.__dict__.update(adapted_method)  \tdef __getattr__(self, attr): \t\t\"\"\"Simply return the rest of attributes!\"\"\" \t\treturn getattr(self._object, attr) \t\t #List to store speaker objects objects = []  #Create a Korean object korean = Korean()  #Create a British object british = British()  #Append the objects to the objects list objects.append(Adapter(korean, speak=korean.speak_korean)) objects.append(Adapter(british, speak=british.speak_english))   for obj in objects: \tprint(\"{} says '{}'\\n\".format(obj.name, obj.speak()))  <pre>Korean says 'An-neyong?'\n\nBritish says 'Hello!'\n\n</pre> In\u00a0[48]: Copied! <pre>class Component(object):\n\t\"\"\"Abstract class\"\"\"\n\n\tdef __init__(self, *args, **kwargs):\n\t\tpass\n\n\tdef component_function(self):\n\t\tpass\n\nclass Child(Component): #Inherits from the abstract class, Component\n\t\"\"\"Concrete class\"\"\"\n\n\tdef __init__(self, *args, **kwargs):\n\t\tComponent.__init__(self, *args, **kwargs)\n\n\t\t#This is where we store the name of your child item!\n\t\tself.name = args[0]\n\n\tdef component_function(self):\n\t\t#Print the name of your child item here!\n\t\tprint(\"{}\".format(self.name))\n\nclass Composite(Component): #Inherits from the abstract class, Component\n\t\"\"\"Concrete class and maintains the tree recursive structure\"\"\"\n\n\tdef __init__(self, *args, **kwargs):\n\t\tComponent.__init__(self, *args, **kwargs)\n\n\t\t#This is where we store the name of the composite object\n\t\tself.name = args[0]\n\n\t\t#This is where we keep our child items\n\t\tself.children = []\n\n\tdef append_child(self, child):\n\t\t\"\"\"Method to add a new child item\"\"\"\n\t\tself.children.append(child)\n\n\tdef remove_child(self, child):\n\t\t\"\"\"Method to remove a child item\"\"\"\n\t\tself.children.remove(child)\n\n\tdef component_function(self):\n\n\t\t#Print the name of the composite object\n\t\tprint(\"{}\".format(self.name))\n\n\t\t#Iterate through the child objects and invoke their component function printing their names\n\t\tfor i in self.children:\n\t\t\ti.component_function()\n\n#Build a composite submenu 1\nsub1 = Composite(\"submenu1\")\n\n#Create a new child sub_submenu 11\nsub11 = Child(\"sub_submenu 11\")\n#Create a new Child sub_submenu 12\nsub12 = Child(\"sub_submenu 12\")\n\n#Add the sub_submenu 11 to submenu 1\nsub1.append_child(sub11)\n#Add the sub_submenu 12 to submenu 1\nsub1.append_child(sub12)\n\n#Build a top-level composite menu\ntop = Composite(\"top_menu\")\n\n#Build a submenu 2 that is not a composite\nsub2 = Child(\"submenu2\")\n\n#Add the composite submenu 1 to the top-level composite menu\ntop.append_child(sub1)\n\n#Add the plain submenu 2 to the top-level composite menu\ntop.append_child(sub2)\n\n#Let's test if our Composite pattern works!\ntop.component_function()\n</pre> class Component(object): \t\"\"\"Abstract class\"\"\"  \tdef __init__(self, *args, **kwargs): \t\tpass  \tdef component_function(self): \t\tpass  class Child(Component): #Inherits from the abstract class, Component \t\"\"\"Concrete class\"\"\"  \tdef __init__(self, *args, **kwargs): \t\tComponent.__init__(self, *args, **kwargs)  \t\t#This is where we store the name of your child item! \t\tself.name = args[0]  \tdef component_function(self): \t\t#Print the name of your child item here! \t\tprint(\"{}\".format(self.name))  class Composite(Component): #Inherits from the abstract class, Component \t\"\"\"Concrete class and maintains the tree recursive structure\"\"\"  \tdef __init__(self, *args, **kwargs): \t\tComponent.__init__(self, *args, **kwargs)  \t\t#This is where we store the name of the composite object \t\tself.name = args[0]  \t\t#This is where we keep our child items \t\tself.children = []  \tdef append_child(self, child): \t\t\"\"\"Method to add a new child item\"\"\" \t\tself.children.append(child)  \tdef remove_child(self, child): \t\t\"\"\"Method to remove a child item\"\"\" \t\tself.children.remove(child)  \tdef component_function(self):  \t\t#Print the name of the composite object \t\tprint(\"{}\".format(self.name))  \t\t#Iterate through the child objects and invoke their component function printing their names \t\tfor i in self.children: \t\t\ti.component_function()  #Build a composite submenu 1 sub1 = Composite(\"submenu1\")  #Create a new child sub_submenu 11 sub11 = Child(\"sub_submenu 11\") #Create a new Child sub_submenu 12 sub12 = Child(\"sub_submenu 12\")  #Add the sub_submenu 11 to submenu 1 sub1.append_child(sub11) #Add the sub_submenu 12 to submenu 1 sub1.append_child(sub12)  #Build a top-level composite menu top = Composite(\"top_menu\")  #Build a submenu 2 that is not a composite sub2 = Child(\"submenu2\")  #Add the composite submenu 1 to the top-level composite menu top.append_child(sub1)  #Add the plain submenu 2 to the top-level composite menu top.append_child(sub2)  #Let's test if our Composite pattern works! top.component_function() <pre>top_menu\nsubmenu1\nsub_submenu 11\nsub_submenu 12\nsubmenu2\n</pre> In\u00a0[49]: Copied! <pre>class DrawingAPIOne(object):\n\t\"\"\"Implementation-specific abstraction: concrete class one\"\"\"\n\tdef draw_circle(self, x, y, radius):\n\t\tprint(\"API 1 drawing a circle at ({}, {} with radius {}!)\".format(x, y, radius))\n\n\nclass DrawingAPITwo(object):\n\t\"\"\"Implementation-specific abstraction: concrete class two\"\"\"\n\tdef draw_circle(self, x, y, radius):\n\t\tprint(\"API 2 drawing a circle at ({}, {} with radius {}!)\".format(x, y, radius))\n\nclass Circle(object):\n\t\"\"\"Implementation-independent abstraction: for example, there could be a rectangle class!\"\"\"\n\n\tdef __init__(self, x, y, radius, drawing_api):\n\t\t\"\"\"Initialize the necessary attributes\"\"\"\n\t\tself._x = x\n\t\tself._y = y\n\t\tself._radius = radius\n\t\tself._drawing_api = drawing_api\n\n\tdef draw(self):\n\t\t\"\"\"Implementation-specific abstraction taken care of by another class: DrawingAPI\"\"\"\n\t\tself._drawing_api.draw_circle(self._x, self._y, self._radius)\n\n\tdef scale(self, percent):\n\t\t\"\"\"Implementation-independent\"\"\"\n\t\tself._radius *= percent\n\n\n#Build the first Circle object using API One\ncircle1 = Circle(1, 2, 3, DrawingAPIOne())\n#Draw a circle\ncircle1.draw()\n\n#Build the second Circle object using API Two\ncircle2 = Circle(2, 3, 4, DrawingAPITwo())\n#Draw a circle\ncircle2.draw()\n</pre> class DrawingAPIOne(object): \t\"\"\"Implementation-specific abstraction: concrete class one\"\"\" \tdef draw_circle(self, x, y, radius): \t\tprint(\"API 1 drawing a circle at ({}, {} with radius {}!)\".format(x, y, radius))   class DrawingAPITwo(object): \t\"\"\"Implementation-specific abstraction: concrete class two\"\"\" \tdef draw_circle(self, x, y, radius): \t\tprint(\"API 2 drawing a circle at ({}, {} with radius {}!)\".format(x, y, radius))  class Circle(object): \t\"\"\"Implementation-independent abstraction: for example, there could be a rectangle class!\"\"\"  \tdef __init__(self, x, y, radius, drawing_api): \t\t\"\"\"Initialize the necessary attributes\"\"\" \t\tself._x = x \t\tself._y = y \t\tself._radius = radius \t\tself._drawing_api = drawing_api  \tdef draw(self): \t\t\"\"\"Implementation-specific abstraction taken care of by another class: DrawingAPI\"\"\" \t\tself._drawing_api.draw_circle(self._x, self._y, self._radius)  \tdef scale(self, percent): \t\t\"\"\"Implementation-independent\"\"\" \t\tself._radius *= percent   #Build the first Circle object using API One circle1 = Circle(1, 2, 3, DrawingAPIOne()) #Draw a circle circle1.draw()  #Build the second Circle object using API Two circle2 = Circle(2, 3, 4, DrawingAPITwo()) #Draw a circle circle2.draw()  <pre>API 1 drawing a circle at (1, 2 with radius 3!)\nAPI 2 drawing a circle at (2, 3 with radius 4!)\n</pre> In\u00a0[50]: Copied! <pre>class Subject(object): #Represents what is being 'observed'\n\n\tdef __init__(self):\n\t\tself._observers = [] # This where references to all the observers are being kept\n\t\t\t\t\t\t\t # Note that this is a one-to-many relationship: there will be one subject to be observed by multiple _observers\n\n\tdef attach(self, observer):\n\t\tif observer not in self._observers: #If the observer is not already in the observers list\n\t\t\tself._observers.append(observer) # append the observer to the list\n\n\tdef detach(self, observer): #Simply remove the observer\n\t\ttry:\n\t\t\tself._observers.remove(observer)\n\t\texcept ValueError:\n\t\t\tpass\n\n\tdef notify(self, modifier=None):\n\t\tfor observer in self._observers: # For all the observers in the list\n\t\t\tif modifier != observer: # Don't notify the observer who is actually updating the temperature \n\t\t\t\tobserver.update(self) # Alert the observers!\n\nclass Core(Subject): #Inherits from the Subject class\n\n\tdef __init__(self, name=\"\"):\n\t\tSubject.__init__(self)\n\t\tself._name = name #Set the name of the core\n\t\tself._temp = 0 #Initialize the temperature of the core\n\n\t@property #Getter that gets the core temperature\n\tdef temp(self):\n\t\treturn self._temp\n\n\t@temp.setter #Setter that sets the core temperature\n\tdef temp(self, temp):\n\t\tself._temp = temp\n\t\tself.notify() #Notify the observers whenever somebody changes the core temperature\n\nclass TempViewer:\n\n\tdef update(self, subject): #Alert method that is invoked when the notify() method in a concrete subject is invoked\n\t\tprint(\"Temperature Viewer: {} has Temperature {}\".format(subject._name, subject._temp))\n\n#Let's create our subjects\nc1 = Core(\"Core 1\")\nc2 = Core(\"Core 2\")\n\n#Let's create our observers\nv1 = TempViewer()\nv2 = TempViewer()\n\n#Let's attach our observers to the first core\nc1.attach(v1)\nc1.attach(v2)\n\n#Let's change the temperature of our first core\nc1.temp = 80\nc1.temp = 90\n</pre> class Subject(object): #Represents what is being 'observed'  \tdef __init__(self): \t\tself._observers = [] # This where references to all the observers are being kept \t\t\t\t\t\t\t # Note that this is a one-to-many relationship: there will be one subject to be observed by multiple _observers  \tdef attach(self, observer): \t\tif observer not in self._observers: #If the observer is not already in the observers list \t\t\tself._observers.append(observer) # append the observer to the list  \tdef detach(self, observer): #Simply remove the observer \t\ttry: \t\t\tself._observers.remove(observer) \t\texcept ValueError: \t\t\tpass  \tdef notify(self, modifier=None): \t\tfor observer in self._observers: # For all the observers in the list \t\t\tif modifier != observer: # Don't notify the observer who is actually updating the temperature  \t\t\t\tobserver.update(self) # Alert the observers!  class Core(Subject): #Inherits from the Subject class  \tdef __init__(self, name=\"\"): \t\tSubject.__init__(self) \t\tself._name = name #Set the name of the core \t\tself._temp = 0 #Initialize the temperature of the core  \t@property #Getter that gets the core temperature \tdef temp(self): \t\treturn self._temp  \t@temp.setter #Setter that sets the core temperature \tdef temp(self, temp): \t\tself._temp = temp \t\tself.notify() #Notify the observers whenever somebody changes the core temperature  class TempViewer:  \tdef update(self, subject): #Alert method that is invoked when the notify() method in a concrete subject is invoked \t\tprint(\"Temperature Viewer: {} has Temperature {}\".format(subject._name, subject._temp))  #Let's create our subjects c1 = Core(\"Core 1\") c2 = Core(\"Core 2\")  #Let's create our observers v1 = TempViewer() v2 = TempViewer()  #Let's attach our observers to the first core c1.attach(v1) c1.attach(v2)  #Let's change the temperature of our first core c1.temp = 80 c1.temp = 90  <pre>Temperature Viewer: Core 1 has Temperature 80\nTemperature Viewer: Core 1 has Temperature 80\nTemperature Viewer: Core 1 has Temperature 90\nTemperature Viewer: Core 1 has Temperature 90\n</pre> In\u00a0[51]: Copied! <pre>class House(object): #The class being visited \n\tdef accept(self, visitor):\n\t\t\"\"\"Interface to accept a visitor\"\"\"\n\t\tvisitor.visit(self) #Triggers the visiting operation!\n\n\tdef work_on_hvac(self, hvac_specialist):\n\t\tprint(self, \"worked on by\", hvac_specialist) #Note that we now have a reference to the HVAC specialist object in the house object!\n\n\tdef work_on_electricity(self, electrician):\n\t\tprint(self, \"worked on by\", electrician) #Note that we now have a reference to the electrician object in the house object!\n\n\tdef __str__(self):\n\t\t\"\"\"Simply return the class name when the House object is printed\"\"\"\n\t\treturn self.__class__.__name__\n\n\nclass Visitor(object):\n\t\"\"\"Abstract visitor\"\"\"\n\tdef __str__(self):\n\t\t\"\"\"Simply return the class name when the Visitor object is printed\"\"\"\n\t\treturn self.__class__.__name__\n\n\nclass HvacSpecialist(Visitor): #Inherits from the parent class, Visitor\n\t\"\"\"Concrete visitor: HVAC specialist\"\"\"\n\tdef visit(self, house):\n\t\thouse.work_on_hvac(self) #Note that the visitor now has a reference to the house object\n\n\nclass Electrician(Visitor): #Inherits from the parent class, Visitor\n\t\"\"\"Concrete visitor: electrician\"\"\"\n\tdef visit(self, house):\n\t\thouse.work_on_electricity(self) #Note that the visitor now has a reference to the house object\n\n#Create an HVAC specialist\nhv = HvacSpecialist()\n#Create an electrician\ne = Electrician()\n\n#Create a house\nhome = House()\n\n#Let the house accept the HVAC specialist and work on the house by invoking the visit() method\nhome.accept(hv)\n\n#Let the house accept the electrician and work on the house by invoking the visit() method\nhome.accept(e)\n</pre> class House(object): #The class being visited  \tdef accept(self, visitor): \t\t\"\"\"Interface to accept a visitor\"\"\" \t\tvisitor.visit(self) #Triggers the visiting operation!  \tdef work_on_hvac(self, hvac_specialist): \t\tprint(self, \"worked on by\", hvac_specialist) #Note that we now have a reference to the HVAC specialist object in the house object!  \tdef work_on_electricity(self, electrician): \t\tprint(self, \"worked on by\", electrician) #Note that we now have a reference to the electrician object in the house object!  \tdef __str__(self): \t\t\"\"\"Simply return the class name when the House object is printed\"\"\" \t\treturn self.__class__.__name__   class Visitor(object): \t\"\"\"Abstract visitor\"\"\" \tdef __str__(self): \t\t\"\"\"Simply return the class name when the Visitor object is printed\"\"\" \t\treturn self.__class__.__name__   class HvacSpecialist(Visitor): #Inherits from the parent class, Visitor \t\"\"\"Concrete visitor: HVAC specialist\"\"\" \tdef visit(self, house): \t\thouse.work_on_hvac(self) #Note that the visitor now has a reference to the house object   class Electrician(Visitor): #Inherits from the parent class, Visitor \t\"\"\"Concrete visitor: electrician\"\"\" \tdef visit(self, house): \t\thouse.work_on_electricity(self) #Note that the visitor now has a reference to the house object  #Create an HVAC specialist hv = HvacSpecialist() #Create an electrician e = Electrician()  #Create a house home = House()  #Let the house accept the HVAC specialist and work on the house by invoking the visit() method home.accept(hv)  #Let the house accept the electrician and work on the house by invoking the visit() method home.accept(e)   <pre>House worked on by HvacSpecialist\nHouse worked on by Electrician\n</pre> In\u00a0[52]: Copied! <pre>def count_to(count):\n\t\"\"\"Our iterator implementation\"\"\"\n\t\n\t#Our list\n\tnumbers_in_german = [\"eins\", \"zwei\", \"drei\", \"vier\", \"funf\"]\n\n\t#Our built-in iterator\n\t#Creates a tuple such as (1, \"eins\")\n\titerator = zip(range(count), numbers_in_german)\n\t\n\t#Iterate through our iterable list\n\t#Extract the German numbers\n\t#Put them in a generator called number\n\tfor position, number in iterator:\n\t\t\n\t\t#Returns a 'generator' containing numbers in German\n\t\tyield number \n\n#Let's test the generator returned by our iterator\nfor num in count_to(3):\n\tprint(\"{}\".format(num))\n\nprint('\\n')\n\nfor num in count_to(4):\n\tprint(\"{}\".format(num))\n</pre> def count_to(count): \t\"\"\"Our iterator implementation\"\"\" \t \t#Our list \tnumbers_in_german = [\"eins\", \"zwei\", \"drei\", \"vier\", \"funf\"]  \t#Our built-in iterator \t#Creates a tuple such as (1, \"eins\") \titerator = zip(range(count), numbers_in_german) \t \t#Iterate through our iterable list \t#Extract the German numbers \t#Put them in a generator called number \tfor position, number in iterator: \t\t \t\t#Returns a 'generator' containing numbers in German \t\tyield number   #Let's test the generator returned by our iterator for num in count_to(3): \tprint(\"{}\".format(num))  print('\\n')  for num in count_to(4): \tprint(\"{}\".format(num))  <pre>eins\nzwei\ndrei\n\n\neins\nzwei\ndrei\nvier\n</pre> In\u00a0[53]: Copied! <pre>import types #Import the types module\n\nclass Strategy:\n    \"\"\"The Strategy Pattern class\"\"\"\n    \n    def __init__(self, function=None):\n        self.name = \"Default Strategy\"\n        \n        #If a reference to a function is provided, replace the execute() method with the given function\n        if function:\n            self.execute = types.MethodType(function, self)\n            \n    def execute(self): #This gets replaced by another version if another strategy is provided.\n        \"\"\"The defaut method that prints the name of the strategy being used\"\"\"\n        print(\"{} is used!\".format(self.name))\n\n#Replacement method 1\ndef strategy_one(self):\n    print(\"{} is used to execute method 1\".format(self.name))\n\n#Replacement method 2    \ndef strategy_two(self):\n    print(\"{} is used to execute method 2\".format(self.name))\n    \n#Let's create our default strategy\ns0 = Strategy()\n#Let's execute our default strategy\ns0.execute()\n\n#Let's create the first varition of our default strategy by providing a new behavior\ns1 = Strategy(strategy_one)\n#Let's set its name\ns1.name = \"Strategy One\"\n#Let's execute the strategy\ns1.execute()\n\ns2 = Strategy(strategy_two)\ns2.name = \"Strategy Two\"\ns2.execute()\n</pre> import types #Import the types module  class Strategy:     \"\"\"The Strategy Pattern class\"\"\"          def __init__(self, function=None):         self.name = \"Default Strategy\"                  #If a reference to a function is provided, replace the execute() method with the given function         if function:             self.execute = types.MethodType(function, self)                  def execute(self): #This gets replaced by another version if another strategy is provided.         \"\"\"The defaut method that prints the name of the strategy being used\"\"\"         print(\"{} is used!\".format(self.name))  #Replacement method 1 def strategy_one(self):     print(\"{} is used to execute method 1\".format(self.name))  #Replacement method 2     def strategy_two(self):     print(\"{} is used to execute method 2\".format(self.name))      #Let's create our default strategy s0 = Strategy() #Let's execute our default strategy s0.execute()  #Let's create the first varition of our default strategy by providing a new behavior s1 = Strategy(strategy_one) #Let's set its name s1.name = \"Strategy One\" #Let's execute the strategy s1.execute()  s2 = Strategy(strategy_two) s2.name = \"Strategy Two\" s2.execute() <pre>Default Strategy is used!\nStrategy One is used to execute method 1\nStrategy Two is used to execute method 2\n</pre> In\u00a0[54]: Copied! <pre>class Handler: #Abstract handler\n\t\"\"\"Abstract Handler\"\"\"\n\tdef __init__(self, successor):\n\t\tself._successor = successor # Define who is the next handler\n\n\tdef handle(self, request):\n\t\t\thandled = self._handle(request) #If handled, stop here\n\n\t\t\t#Otherwise, keep going\n\t\t\tif not handled:\n\t\t\t\tself._successor.handle(request)\t\n\n\tdef _handle(self, request):\n\t\traise NotImplementedError('Must provide implementation in subclass!')\n\nclass ConcreteHandler1(Handler): # Inherits from the abstract handler\n\t\"\"\"Concrete handler 1\"\"\"\n\tdef _handle(self, request):\n\t\tif 0 &lt; request &lt;= 10: # Provide a condition for handling\n\t\t\tprint(\"Request {} handled in handler 1\".format(request))\n\t\t\treturn True # Indicates that the request has been handled\n\nclass DefaultHandler(Handler): # Inherits from the abstract handler\n\t\"\"\"Default handler\"\"\"\n\n\tdef _handle(self, request):\n\t\t\"\"\"If there is no handler available\"\"\"\n\t\t#No condition checking since this is a default handler\n\t\tprint(\"End of chain, no handler for {}\".format(request))\n\t\treturn True # Indicates that the request has been handled\n\nclass Client: # Using handlers\n\tdef __init__(self):\n\t\tself.handler = ConcreteHandler1(DefaultHandler(None)) # Create handlers and use them in a sequence you want\n\t\t                                                      # Note that the default handler has no successor\n\n\tdef delegate(self, requests): # Send your requests one at a time for handlers to handle\n\t\tfor request in requests:\n\t\t\t\tself.handler.handle(request)\n\n# Create a client\nc = Client()\n\n# Create requests\nrequests = [2, 5, 30]\n\n# Send the requests\nc.delegate(requests)\n</pre> class Handler: #Abstract handler \t\"\"\"Abstract Handler\"\"\" \tdef __init__(self, successor): \t\tself._successor = successor # Define who is the next handler  \tdef handle(self, request): \t\t\thandled = self._handle(request) #If handled, stop here  \t\t\t#Otherwise, keep going \t\t\tif not handled: \t\t\t\tself._successor.handle(request)\t  \tdef _handle(self, request): \t\traise NotImplementedError('Must provide implementation in subclass!')  class ConcreteHandler1(Handler): # Inherits from the abstract handler \t\"\"\"Concrete handler 1\"\"\" \tdef _handle(self, request): \t\tif 0 &lt; request &lt;= 10: # Provide a condition for handling \t\t\tprint(\"Request {} handled in handler 1\".format(request)) \t\t\treturn True # Indicates that the request has been handled  class DefaultHandler(Handler): # Inherits from the abstract handler \t\"\"\"Default handler\"\"\"  \tdef _handle(self, request): \t\t\"\"\"If there is no handler available\"\"\" \t\t#No condition checking since this is a default handler \t\tprint(\"End of chain, no handler for {}\".format(request)) \t\treturn True # Indicates that the request has been handled  class Client: # Using handlers \tdef __init__(self): \t\tself.handler = ConcreteHandler1(DefaultHandler(None)) # Create handlers and use them in a sequence you want \t\t                                                      # Note that the default handler has no successor  \tdef delegate(self, requests): # Send your requests one at a time for handlers to handle \t\tfor request in requests: \t\t\t\tself.handler.handle(request)  # Create a client c = Client()  # Create requests requests = [2, 5, 30]  # Send the requests c.delegate(requests)   <pre>Request 2 handled in handler 1\nRequest 5 handled in handler 1\nEnd of chain, no handler for 30\n</pre>"},{"location":"9-Drafts/ll-py-design-patterns/#python-design-patterns","title":"Python: Design Patterns\u00b6","text":"<p>Link: Python: Design Patterns LinkedIn Learning</p>"},{"location":"9-Drafts/ll-py-design-patterns/#understanding-design-patterns","title":"Understanding Design Patterns\u00b6","text":"<p>Three types of design pattern</p> <ul> <li>Creational - <code>polymorphism</code> - different object from same class</li> <li>Structural - <code>inheritance</code> - relationship between components</li> <li>Behavioural - Method <code>Signatures</code> - objects interacting with each other</li> </ul> <p><code>Interfaces</code> help in all these design patterns.</p> <p>Functional requirement - what job is done by sw, Non-functional - how better sq does the job.</p>"},{"location":"9-Drafts/ll-py-design-patterns/#understanding-pattern-context","title":"Understanding Pattern Context\u00b6","text":"<p>To use design patterns effectively you need to know the context in which the design patterns work best.</p> <p>Lets start by learning how to describe the design pattern context. The pattern context consists of the following.</p> <ul> <li>Participants - the classes involved</li> <li>Quality attributes - non functional requirements such as usability, modifiability, reliability, performance and so on. Quality attributes have an impact on the entire software, and architectural solutions address them.</li> <li>Forces - various factors or trade-offs you consider when trying to adopt a particular design pattern. Forces are defined by quality attributes.</li> <li>Consequences - result of above when using a design pattern</li> </ul>"},{"location":"9-Drafts/ll-py-design-patterns/#creational-patterns","title":"Creational Patterns\u00b6","text":""},{"location":"9-Drafts/ll-py-design-patterns/#factory","title":"Factory\u00b6","text":"<ul> <li><p>Factory is an method creating objects</p> </li> <li><p>Good when not sure of type of objects required.</p> </li> <li><p>Implementation</p> <ul> <li>a method, returns object based on param.</li> <li>in this method, a dict has objects as value and its name as key</li> <li>based on param, object is picked from dict value by matching key to param.</li> </ul> </li> </ul>"},{"location":"9-Drafts/ll-py-design-patterns/#abstract-factory","title":"Abstract Factory\u00b6","text":"<ul> <li>Abstract means existance as idea, not physical or concrete.</li> <li>It is like layering on top of your classes, so if you have class dog and cat, you can create pet factory class, this can be abstract ?</li> <li>real class <code>Dog</code>, on top you have class <code>DogFactory</code> abstract, on top you have <code>PetStore</code> class</li> </ul>"},{"location":"9-Drafts/ll-py-design-patterns/#singelton","title":"Singelton\u00b6","text":"<p>Create only one object from a class. There can be multiple instance, but all share the same attributes. So you implement global variable in OOP style. Useful when you have to cache some data that can can be shared by different things.</p>"},{"location":"9-Drafts/ll-py-design-patterns/#builder","title":"Builder\u00b6","text":"<p>Helps decomplex when lot of constructors are used. Reduce complexity by divide and conque. It divides roles as:</p> <ul> <li>Director</li> <li>Abstract Builder - interfaces</li> <li>Concrete Builder - implements the interfaces</li> <li>Product - object being built</li> </ul>"},{"location":"9-Drafts/ll-py-design-patterns/#prototype","title":"Prototype\u00b6","text":"<p>Useful when you have to clone things, which makes carbon copy without repeating things.</p>"},{"location":"9-Drafts/ll-py-design-patterns/#structural-pattern","title":"Structural Pattern\u00b6","text":""},{"location":"9-Drafts/ll-py-design-patterns/#decorator-pattern","title":"Decorator Pattern\u00b6","text":"<p>Add additional feature to class without subclass or modifying it, using built-in feature decorator</p>"},{"location":"9-Drafts/ll-py-design-patterns/#porxy","title":"Porxy\u00b6","text":"<p>When there is resource intensive object, we can delay its creation or limit its creation and instead let proxy be used instead. Adapter and Decorator are related to the proxy design pattern.</p>"},{"location":"9-Drafts/ll-py-design-patterns/#adapter-pattern","title":"Adapter Pattern\u00b6","text":"<ul> <li>Lets interface used with different name, makes it simple for client to use different named function on server.</li> <li>Bridges and decorators are relared to this</li> </ul>"},{"location":"9-Drafts/ll-py-design-patterns/#composite","title":"Composite\u00b6","text":"<ul> <li>Useful when data is tree structure, like menu, sub-menu, sub-sub-menu and so on.</li> <li>Like building a recursive tree data structure.</li> <li>Elements in implementation<ul> <li>Component - abstract</li> <li>Child - concrete, inherits component</li> <li>Composite - concrete, inherites component, manages child in tree</li> </ul> </li> </ul>"},{"location":"9-Drafts/ll-py-design-patterns/#bridge","title":"Bridge\u00b6","text":"<p>The abstract factory and adapter patterns are the related patterns to the bridge design pattern.</p> <p>Untangles complicated class hierarchy.</p>"},{"location":"9-Drafts/ll-py-design-patterns/#behavioural","title":"Behavioural\u00b6","text":""},{"location":"9-Drafts/ll-py-design-patterns/#observer","title":"Observer\u00b6","text":"<p>When there is somthing that needs to be observed and notify the observer when there is a change.</p>"},{"location":"9-Drafts/ll-py-design-patterns/#visitor","title":"Visitor\u00b6","text":"<p>Lets add new features to existing class hierarchy without changing it.</p>"},{"location":"9-Drafts/ll-py-design-patterns/#iterator","title":"Iterator\u00b6","text":"<ul> <li>The iterator pattern allows a client to have sequential access to the elements of an aggregate object without exposing its underlying structure.</li> <li>The composite design pattern is related to the iterator pattern.</li> </ul>"},{"location":"9-Drafts/ll-py-design-patterns/#strategy","title":"Strategy\u00b6","text":"<ul> <li>It lets change algotithm to a client.</li> <li>Useful when object behaviour need to be dynamic.</li> <li>A <code>Strategy</code> class has default behaviour, and that can be changed dynamically with new one.</li> <li>Function is swapped using <code>types</code> module.</li> </ul>"},{"location":"9-Drafts/ll-py-design-patterns/#chain-of-responsibility","title":"Chain of Responsibility\u00b6","text":"<ul> <li>Opens various possibility fo a request</li> <li>if current handler cannot handle the request, it passess to successor.</li> <li>Composite is related to the chain of responsibility, design pattern.</li> <li>It is used not to tie a specific solution to a request.</li> </ul>"},{"location":"9-Drafts/ll-py-design-patterns/#design-best-practices","title":"Design Best Practices\u00b6","text":"<ul> <li>Consistency is the key, use same patther and design solution for different projects</li> <li>Identify software architecture that fits you need and keep using it.</li> <li>Or use framework, it is pre built with a design pattern to make it flexible, or capable etc. Like flask / Django</li> <li>Completeness - build over base of design patter to complete it.</li> </ul>"},{"location":"9-Drafts/ll-py-design-patterns/#oops","title":"OOPs\u00b6","text":"<p>Class, object, attribute and methods.</p> <ul> <li><p><code>__init__</code> in a class sets its attributes. Takes in param.</p> </li> <li><p>attribite in init can be with or without underscore. It is naming convention, userscore prefix indicates internal use, it does not mandate.</p> </li> <li><p>[?] Implement abstract factory without using inheritance because Python is a dynamically typed language, and therefore does not require abstract classes.</p> </li> </ul>"},{"location":"9-Drafts/ll-software-architecture-domain-driven-design/","title":"Software Architecture: Domain-Driven Design","text":"<p>INSTRUCTOR: Allen Holub | Linkedin Learning</p> <p>Domain-Driven Design or DDD is good to manage change and agile way of development.</p>"},{"location":"9-Drafts/ll-software-architecture-domain-driven-design/#what-is-ddd","title":"What is DDD?","text":"<p>Agile - The philosophy of DDD is similar to Agile manifesto, \"Developers and Business People must work together everyday throughout the project\".</p> <p>Modeling - DDD works on model and says that the structure of code models 1-1 to domain you are solving. So it makes sense to business and change is domain can be mapped to change in code.</p> <p>Incremental - Code for MVP (only enough to solve immediate problem), then evolve as you learn more about the problem.</p> <p>DD, follows the Agile Cycle</p> <pre><code>flowchart LR\n    code --&gt; assess --&gt; adapt --&gt; improve --&gt; code</code></pre>"},{"location":"9-Drafts/ll-software-architecture-domain-driven-design/#microservices-and-monoliths","title":"Microservices and Monoliths","text":"<p>Monoliths have problem with changes, deployment, maintainance.</p> <p>Microservives split it but has to be architectured nicely. It should be</p> <ul> <li>small: as big as something you can hold in your head</li> <li>Independently deployable</li> <li>Changing one should have no change required on other</li> <li>Modeled around business</li> <li>decentralized</li> <li>observable - you can see errors</li> <li>autonomous - runs on its own</li> </ul>"},{"location":"9-Drafts/ll-software-architecture-domain-driven-design/#bounded-contexts-and-entities","title":"Bounded Contexts and Entities","text":"<p>Contexts means boundary or scope in business. Eg, for 'A book Company, you have two contexts, Warehouse and Store, both are speparate from each other in real world and have different definition, attributes, properties, roles and functions. However, they have similarities too like product, cost etc. To manage this difference and similarities we use DDD which keeps them separate using microservice and links them only via single channel. Lets understand more..</p> Details / Context Warehouse Store Handles shipping of books sales of books Responsibilities only ship no sales only sales no shipping Ubiquitous Language Properties Weight, Height Author, Review, Length, Readability Ubiquitous Language Actions Pick books, Box Books Sell, Shelf, Organize Actors Warehouse Manager Store Cashier Roles Ship on time Balancesheet updates \\[\\text{Fig: Monolith Design}\\] <pre><code>flowchart TD\nsubgraph Warehouse\n    Trolley --&gt; Picker\n    Trolley ---&gt; Shipping\nend\n\nsubgraph Store\n    Cart --&gt; Product --&gt; Search\nend\n\nsubgraph Finance\n    Payments\nend\n\nProduct --&gt; Order\nPicker --&gt; Order\nOrder --&gt; Cutomer --&gt; Finance</code></pre> <p>In DDD, every entity should be associated with some context and same model in Database may be broken up by context, so in below, Trolley, Product can be broken into separate entity based on context like store or warehouse.</p> \\[\\text{Fig: Domain Driven Design}\\] <pre><code>flowchart LR\n\nsubgraph Warehouse\n    Trolley --&gt; Picker\n    Trolley ---&gt; Shipping\n    Order ---&gt; Customer\\nAddress\n    Order --&gt; p[Product\\nSKU\\nShelf\\nSize\\nWeight]\nend\n\nsubgraph Store\n    Cart --&gt; p1\n    o[Order] ---&gt; c[Customer\\nID]\n    o --&gt; p1[Product\\nSKU\\nimage\\nprice]\n    p1 --&gt; s[Search\\nsub service]\nend\n\nsubgraph Finance\n    Customer\n    Payments\nend\n\nWarehouse &lt;--&gt; Store\nFinance &lt;--&gt; Store</code></pre> <p>Hence, when we get into a domain driven design approach, we're going to move away from the relational database thinking of trying to make giant components, if you will, that could be used everywhere. It is considered a bad thing in the domain driven design world to have a single product-object for example, a single product component, that could be used in multiple context. Because that product component is going to be too big. It's going to be hard to maintain. It's going to be containing lots of internal dependencies and so forth that you just don't want to have. So, in the DDD world there will be two different product entities. One that makes sense in the store context, another one which makes sense in the warehouse context and the fact that they're different is good actually. You do not try and put them together. Now this leads to something that's important in microservices that people often don't understand, which is that if we are implementing a DDD design in microservices, we'll have two different product services. One product service will be warehouse focused, the other product will be store focused. Those two services will have completely different databases. You will not have a single table that contains everything that a product has to have in it. So everything stays context sensitive, context specific, withinside a domain driven design.</p>"},{"location":"9-Drafts/ll-software-architecture-domain-driven-design/#reactive-vs-declarative-system","title":"Reactive vs Declarative System","text":"<p>How entities communicate with each other.</p> <p>In Monolith System, entity sends instruction the same way as in real-world, like shopping card issues invoice to billing service and tells warehouse service to ship items. It declares it, hence Declarative System, they are all tightly-coupled.</p> \\[\\text{Fig: Declarative System in Monolith System}\\] <pre><code>flowchart LR\n\nss[Shopping Cart\\nService] --\"issueInvoice()\"--&gt; bs[Billing Service]\nss --\"queueItemForShipping()\"--&gt; ws[Warehouse Service]\nss --\"emailCustomer()\"--&gt; es[Email Service]\n\nsubgraph us[Upstream Services]\n    ss\nend\n\nsubgraph ds[Downstream Services]\n    bs\n    ws\n    es\nend</code></pre> <p>In Microservice System these function calls are replaced by Network Calls, which adds a complexity. Upstream service needs to know downstream service. So any change in downstream affects upstream which is a problem.</p> <p>Solution is Reactive Systems which make Asynchronous Services, which takes away the dependencies of communication and relying on other service, rather the service announces, eg, shopping-cart-service announces that order has been placed, now other services may listen to it and do the work. So email-service sends email, shipping-service ships and billing-service generates invoice. All this is done by decoupling the services. So services need not know each other, and hence we can change them or add new downstream service. This is implemented using publisher-subscriber model.</p> <p>In Publisher-Subscriber Model, the publishers don't know who the subscribers are and vice-versa. More on implementation details in messaging systems like Kafka, RabbitMQ or ZeroMQ.</p>"},{"location":"9-Drafts/ll-software-architecture-domain-driven-design/#event-storming","title":"Event Storming","text":"<p>Best way to do DDD is using technique Event Storming by Alberto Brandolini</p> <p>It is working with Business People to come up with events that mimic the real world events. Do not do all at once, pick a story, model it and repeat.</p> <p>Event is something that happens at business level that customers care about. Eg, 'Order Submitted', 'Payment Received', 'Nightly Reconciliation Done'. Nightly Reconciliation is different as in real world it might not happen but is an automated event in modelling. Keeping in past tense is good convention.</p> <p>How to do Event Storming?</p> <p>Start with a sticky note, write an event, and then arrange these events in timeline. You can start anywhere.</p> \\[\\text{Fig: Sticky Note Color and Meaning}\\] <pre><code>flowchart\ns1[Event\\nOf interest to the\\nBusiness]\nstyle s1 fill:#ffa930,stroke-width:0,color:#fff,width:150px,height:150px\n\ns2[Action\\n What do we want\\nto have happen]\nstyle s2 fill:#a9edf1,stroke-width:0,width:150px,height:150px\n\ns3[Question?\\nThings we have to\\ndo]\nstyle s3 fill:#ff32b2,stroke-width:0,color:#fff,width:150px,height:150px</code></pre> <pre><code>flowchart\ns1[Policy\\nControl how the\\naction\\nplays out]\nstyle s1 fill:#74ed4b,stroke-width:0,width:150px,height:150px\n\ns2[Human Activity\\nWho does the\\nevent?\\nClerk, Cashier etc.]\nstyle s2 fill:#f1f58f,stroke-width:0,width:150px,height:150px\n\ns3[External Systems?\\nOccured in\\noutside\\nworld]\nstyle s3 fill:#d9b8ff,stroke-width:0,width:150px,height:150px</code></pre> <p>You can do this digitally on miro.com and do these steps:</p> <ul> <li>Just think you are working in a physical world and see events that are happening, add them.</li> <li>Add action that are taking place due to the events.</li> <li>Add external Systems in these actions like Payment handler, shipping handler.</li> <li>Add human roles who do these actions, like ,cashier, clerk etc. Entities.</li> <li>Add questions that you have no answers for, like how to talk to payment gateway.</li> <li>Add policies that govern the flow of actions, like cannot ship to certain geo.</li> <li>Add context for entities, like cashier is working in warehouse or store.</li> </ul> <p>This will map the whole business process and makes us ready to think if it would be monolith or microservice, but is where we have mapped domain and can begin domain driven development.</p> <p>Lastly list down all the contexts, eg:</p> <ul> <li>warehouse</li> <li>finance</li> <li>order processing</li> <li>store</li> <li>customer service</li> <li>shipping</li> </ul> <p>Then see how entities in contexts talk to each other. This will give a final map.</p> <p>In case of monolith, these would be classes and objects. In case of microservices these can be individual services.</p> <p>Next Step: Do this for a project</p>"},{"location":"9-Drafts/ll-software-architecture-domain-driven-design/#abreviations","title":"Abreviations","text":"<ul> <li>DDD - Domain Driven Design</li> </ul>"},{"location":"9-Drafts/ll-software-architecture-domain-driven-design/#links","title":"Links","text":"<ul> <li>Linkedin - Learning Software Architecture Domain Driven Design</li> </ul>"},{"location":"9-Drafts/mblog-notes/","title":"App - mBlog","text":"<p>These notes are taken when learning from \"React Mega Tutorial\" by Migulel Grinberg. The code or notes text may be similar to what the author wrote and belongs to them.</p> <p>Here we will implement react and build Micro Blog project.</p>"},{"location":"9-Drafts/mblog-notes/#set-up-basics","title":"Set up Basics","text":"<p>Start the basic stuff</p> <p>Start from scratch</p> <pre><code>npx create-react-app app2-mblog\ncd app2-mblog\nnpm start\n</code></pre> <p>It builds basic structure and adds dependencies.</p> <p>Add Third Party Packages</p> <p>We will need following dependencies:</p> <pre><code>npm install bootstrap@5.2.0 react-bootstrap react-router-dom serve\n</code></pre> <p>Here, the packages are:</p> <ul> <li><code>bootstrap</code>: UI framework.</li> <li><code>react-bootstrap</code>: a React component library wrapper for the bootstrap package.</li> <li><code>react-router-dom</code>: a React component library that implements client-side routing.</li> <li><code>serve</code>: is a static file web server that can be used to run the production version of the React application.</li> </ul> <p>Update Meta Info</p> <p>Update defaults with project specific stuff in <code>index.html</code>.</p> <pre><code>    &lt;meta\n      name=\"description\"\n      content=\"Web site is Micro Blog\"\n    /&gt;\n    ...\n    &lt;title&gt;Microblog&lt;/title&gt;\n</code></pre> <p>To <code>src/index.js</code> Add the bootstrap framework</p> <pre><code>import 'bootstrap/dist/css/bootstrap.min.css';\n</code></pre> <p>Remove not required default files</p> <pre><code>git rm src/logo.svg src/App.css\n</code></pre> <p>Modify <code>src/App.js</code></p> <pre><code>export default function App() {\n  return (\n    &lt;h1&gt;Microblog&lt;/h1&gt;\n  );\n}\n</code></pre>"},{"location":"9-Drafts/mblog-notes/#webpage-components","title":"Webpage components","text":"<p>So, basically any app will have components like <code>header</code>, <code>sidebar</code>, <code>content</code> and <code>footer</code>. Which may have nested components like content has <code>posts</code>, <code>tables</code>, action <code>cards</code> etc. They are all built using UI framework like Bootstrap which beneath uses HTML standard components.</p> <p>Bootstrap in React - Grid Layout</p> <p>You might have used bootstrap traditionally using div with columns for grid-layout. Bootstrap in react is implemented using bootstrap-components called <code>Container</code> and <code>Stack</code> that implement same grid-layout.</p> <p>In any <code>src/MyComp.js</code>, add</p> <pre><code>import Container from 'react-bootstrap/Container';\n\nexport default function App() {\n  const posts = [\n    ...  // &lt;-- no changes to fake blog posts\n  ];\n\n  return (\n    &lt;Container fluid className=\"App\"&gt;\n      ...  // &lt;-- no changes to JSX content\n    &lt;/Container&gt;\n  );\n}\n</code></pre> <p>to make bootstrap-components container available use <code>import X from 'react-bootstrap/X'</code>. All components can be found here React Bootstrap Components</p> <p>Adding Header</p> <p>Now, you can add your own components like header, footer. Use <code>src/components</code> dir to arrange them. Add following to <code>src/components/Header.js</code>, you are making a component.</p> <pre><code>import Navbar from 'react-bootstrap/Navbar';\nimport Container from 'react-bootstrap/Container';\n\nexport default function Header() {\n  return (\n    &lt;Navbar bg=\"light\" sticky=\"top\" className=\"Header\"&gt;\n      &lt;Container&gt;\n        &lt;Navbar.Brand&gt;mBlog&lt;/Navbar.Brand&gt;\n      &lt;/Container&gt;\n    &lt;/Navbar&gt;\n  );\n}\n</code></pre> <p>Now you can import it in <code>src/App.js</code></p> <pre><code>import Container from 'react-bootstrap/Container';\nimport Header from './components/Header';\n\nexport default function App() {\n  const posts = [\n    ...  // &lt;-- no changes to fake blog posts\n  ];\n\n  return (\n    &lt;Container fluid className=\"App\"&gt;\n      &lt;Header /&gt;\n      &lt;Container&gt;\n        {posts.length === 0 ?\n          ...\n        }\n      &lt;/Container&gt;\n    &lt;/Container&gt;\n  );\n}\n</code></pre> <p>Adding Sidebar</p> <p>Any css goes to <code>src/index.css</code></p> <p>Same way you can add a sidebar, to <code>src/components/Sidebar.js</code>: A sidebar component</p> <pre><code>import Navbar from \"react-bootstrap/Navbar\";\nimport Nav from \"react-bootstrap/Nav\";\n\nexport default function Sidebar() {\n  return (\n    &lt;Navbar sticky=\"top\" className=\"flex-column Sidebar\"&gt;\n      &lt;Nav.Item&gt;\n        &lt;Nav.Link href=\"/\"&gt;Feed&lt;/Nav.Link&gt;\n      &lt;/Nav.Item&gt;\n      &lt;Nav.Item&gt;\n        &lt;Nav.Link href=\"/explore\"&gt;Explore&lt;/Nav.Link&gt;\n      &lt;/Nav.Item&gt;\n    &lt;/Navbar&gt;\n  );\n}\n</code></pre> <p>Here, you can see that <code>flex-column</code> makes navbar items vertical. And <code>sticky=\"top\"</code> make it visible if you scroll down.</p> <p>To add it to App we will use <code>Stack</code> component of Bootstrap.</p> <p>Using Stack Bootstrap Component</p> <p>You can stack content and sidebar side-by-side. Add following code to <code>src/App.js</code>:</p> <pre><code>import Container from 'react-bootstrap/Container';\nimport Stack from 'react-bootstrap/Stack';          // New\nimport Header from './components/Header';\nimport Sidebar from './components/Sidebar';         // New\n\nexport default function App() {\n  const posts = [\n    ...  // &lt;-- no changes to fake blog posts\n  ];\n\n  return (\n    &lt;Container fluid className=\"App\"&gt;\n      &lt;Header /&gt;\n      &lt;Container&gt;\n        &lt;Stack direction=\"horizontal\"&gt;\n          &lt;Sidebar /&gt;\n          &lt;Container&gt;\n            {posts.length === 0 ?\n              ...  // &lt;-- no changes to render loop\n            }\n          &lt;/Container&gt;\n        &lt;/Stack&gt;\n      &lt;/Container&gt;\n    &lt;/Container&gt;\n  );\n</code></pre> <p>Here you see <code>&lt;Stack direction=\"horizontal\"&gt;</code> makes sidebar and container stack side by side.</p> <p>This completes basic webpage that can be used in any app.</p> <p>Content sub Components</p> <p>Now <code>src/App.js</code> imports <code>header</code>, <code>sidebar</code> and also shows <code>content</code>. Next, what you can do is, to build <code>components</code> that can be used in <code>content</code>, like <code>Posts</code> to view, <code>Friends</code> to follow, <code>Table</code> of activity.</p> <p>Posts Component</p> <p>Now lets make first content component that shows list of posts for user. Add code in <code>src/components/Posts.js</code> to Render a list of blog posts:</p> <pre><code>export default function Posts() {\n  const posts = [\n    {\n      id: 1,\n      text: 'Hello, world!',\n      timestamp: 'a minute ago',\n      author: {\n        username: 'susan',\n      },\n    },\n    {\n      id: 2,\n      text: 'Second post',\n      timestamp: 'an hour ago',\n      author: {\n        username: 'john',\n      },\n    },\n  ];\n\n  return (\n    &lt;&gt;\n      {posts.length === 0 ?\n        &lt;p&gt;There are no blog posts.&lt;/p&gt;\n      :\n        posts.map(post =&gt; {\n          return (\n            &lt;p key={post.id}&gt;\n              &lt;b&gt;{post.author.username}&lt;/b&gt; &amp;mdash; {post.timestamp}\n              &lt;br /&gt;\n              {post.text}\n            &lt;/p&gt;\n          );\n        })\n      }\n    &lt;/&gt;\n  );\n}\n</code></pre> <p>Now we can use it in <code>src/App.js</code> to show Posts component:</p> <pre><code>import Container from 'react-bootstrap/Container';\nimport Stack from 'react-bootstrap/Stack';\nimport Header from './components/Header';\nimport Sidebar from './components/Sidebar';\nimport Posts from './components/Posts';     // New\n\nexport default function App() {\n  return (\n    &lt;Container fluid className=\"App\"&gt;\n      &lt;Header /&gt;\n      &lt;Container&gt;\n        &lt;Stack direction=\"horizontal\"&gt;\n          &lt;Sidebar /&gt;\n          &lt;Container&gt;\n            &lt;Posts /&gt;                       // New\n          &lt;/Container&gt;\n        &lt;/Stack&gt;\n      &lt;/Container&gt;\n    &lt;/Container&gt;\n  );\n}\n</code></pre> <p>That's how we do it, YaYYY....!! But with fake data. Let's now try to play with data and pass it from one component to another.</p> <p>Parameters in Components</p> <p>Simply, paramenters or arguments that we pass to component are same as attributes in HTML and are called props in react. Eg,</p> <pre><code>&lt;Body sidebar={true}&gt;\n  &lt;Posts /&gt;\n&lt;/Body&gt;\n</code></pre> <p>Here, props is <code>sidebar</code>. Also, you see that in <code>Body</code> component we nest <code>Posts</code> component, this also gets passed to <code>Body</code> definition, that is, in <code>Body</code> function all children are available, see below. In <code>src/components/Body.js</code>: A body component</p> <pre><code>import Container from 'react-bootstrap/Container';\nimport Stack from 'react-bootstrap/Stack';\nimport Sidebar from './Sidebar';\n\nexport default function Body(props) {\n  return (\n    &lt;Container&gt;\n      &lt;Stack direction=\"horizontal\" className=\"Body\"&gt;\n        {props.sidebar &amp;&amp; &lt;Sidebar /&gt;}\n        &lt;Container className=\"Content\"&gt;\n          {props.children}\n        &lt;/Container&gt;\n      &lt;/Stack&gt;\n    &lt;/Container&gt;\n  );\n}\n</code></pre> <p>Here, <code>{props.sidebar &amp;&amp; &lt;Sidebar /&gt;}</code> is way of if-then in JSX. Also, above can be simplified further using destructuring</p> <pre><code>// destructuring\n{a,b} = {a:1, c:3, b:2, d:4}\n//a=1, b=2\n</code></pre> <p>So above can be re-written as</p> <pre><code>export default function Body({ sidebar, children }) {       // modified\n  return (\n    &lt;Container&gt;\n      &lt;Stack direction=\"horizontal\" className=\"Body\"&gt;\n        {sidebar &amp;&amp; &lt;Sidebar /&gt;}                            // modified\n        &lt;Container className=\"Content\"&gt;\n          {children}                                        // modified\n        &lt;/Container&gt;\n      &lt;/Stack&gt;\n    &lt;/Container&gt;\n  );\n}\n</code></pre> <p>Lets modify App to have a Body component that show Sidebar based on condition, and the children.</p> <p><code>src/App.js</code>: Refactored application component</p> <pre><code>import Container from 'react-bootstrap/Container';\nimport Header from './components/Header';\nimport Body from './components/Body';\nimport Posts from './components/Posts';\n\nexport default function App() {\n  return (\n    &lt;Container fluid className=\"App\"&gt;\n      &lt;Header /&gt;\n      &lt;Body sidebar&gt;\n        &lt;Posts /&gt;\n      &lt;/Body&gt;\n    &lt;/Container&gt;\n  );\n}\n</code></pre>"},{"location":"9-Drafts/mblog-notes/#routing-and-page-navigation","title":"Routing and Page Navigation","text":"<p>Now that we have the base structure, (same as flask template base.html with index.html). Lets add, more pages like login, profile, users etc. Pages are simply components which make use of base-components.</p> <p>Add dir,</p> <pre><code>mkdir src/pages\n</code></pre> <p>Feed Page</p> <p>Lets make feed page, that shows user feeds, <code>src/pages/FeedPage.js</code>. Basically we will use base-components here, <code>Body</code> and <code>Posts</code>.</p> <p><code>src/pages/FeedPage.js</code>: the Feed page, that says return body with sidebar and posts. Simple?</p> <pre><code>import Body from '../components/Body';\nimport Posts from '../components/Posts';\n\nexport default function FeedPage() {\n  return (\n    &lt;Body sidebar&gt;\n      &lt;Posts /&gt;\n    &lt;/Body&gt;\n  );\n}\n</code></pre> <p>Similarly add <code>src/pages/ExplorePage.js</code>: a placeholder for the explore page</p> <pre><code>import Body from '../components/Body';\n\nexport default function ExplorePage() {\n  return (\n    &lt;Body sidebar&gt;\n      &lt;h1&gt;Explore&lt;/h1&gt;\n      &lt;p&gt;TODO&lt;/p&gt;\n    &lt;/Body&gt;\n  );\n}\n</code></pre> <p>and a login page without sidebar, <code>src/pages/LoginPage.js</code>: a placeholder for the login page</p> <pre><code>import Body from '../components/Body';\n\nexport default function LoginPage() {\n  return (\n    &lt;Body&gt;\n      &lt;h1&gt;Login form&lt;/h1&gt;\n      &lt;p&gt;TODO&lt;/p&gt;\n    &lt;/Body&gt;\n  );\n}\n</code></pre> <p>Now that we have pages, let us add routing</p> <p>Route links</p> <p><code>src/App.js</code>: Page routing</p> <pre><code>import Container from 'react-bootstrap/Container';\nimport { BrowserRouter, Routes, Route, Navigate } from 'react-router-dom';  // New\nimport Header from './components/Header';\nimport FeedPage from './pages/FeedPage';        // New\nimport ExplorePage from './pages/ExplorePage';  // New\nimport LoginPage from './pages/LoginPage';      // New\n\nexport default function App() {\n  return (\n    &lt;Container fluid className=\"App\"&gt;\n      &lt;BrowserRouter&gt;                           // New\n        &lt;Header /&gt;\n        &lt;Routes&gt;                                // New\n          &lt;Route path=\"/\" element={&lt;FeedPage /&gt;} /&gt;\n          &lt;Route path=\"/explore\" element={&lt;ExplorePage /&gt;} /&gt;\n          &lt;Route path=\"/login\" element={&lt;LoginPage /&gt;} /&gt;\n          &lt;Route path=\"*\" element={&lt;Navigate to=\"/\" /&gt;} /&gt;\n        &lt;/Routes&gt;\n      &lt;/BrowserRouter&gt;\n    &lt;/Container&gt;\n  );\n}\n</code></pre> <p>Okay, lot of changes here.</p> <p><code>BrowserRouter</code> component is transparent. It shows nothing but id required because, in SPA browser needs to know all pages. This adds all pages links to DOM. This should be added too above in heirarhy.</p> <p><code>Routes</code> is a component that needs to be inserted in the place in the component tree where the contents need to change based on the current page. Here it is after <code>Header</code> and replaces <code>Content</code> with subsequent routes.</p> <p><code>Route</code> is used to define a route inside the Routes component. This is where we define, the component to render based on the path. It is simply, if path = this, then show this component. Simple... right?</p> <p><code>Navigate</code> is a special component that allows to redirect from one route to another (just like <code>redirect_to</code> in flask).</p> <p>Now, the js in app knows the routes to handle on JS level (neither browser, nor server, remember this is SPA).</p> <p>If you use normal href links, the components work but will reload page, however, in React, navigation should move from browser to JS. to do this.</p> <p>React Navigation</p> <p>Following components help implement JS level navigation without page-reloads</p> <ul> <li><code>Nav.Link</code> this is bootstrap component and build Nav DOM</li> <li><code>Link</code> this is React-Router package component and prevents page reload to do JS routing.</li> <li><code>NavLink</code> same as Link from React-Router package but adds link \"active\" by matching route. It is good to use in nav-bar.</li> </ul> <p>Now, NavLink and Nav.Link both need to be used together, for this, use bootstrap's <code>as</code> attribute to specify a different base component. Gulp it.... :)</p> <pre><code>&lt;Nav.Link href=\"/\"&gt;Feed&lt;/Nav.Link&gt;              // Old\n&lt;Nav.Link as={NavLink} to=\"/\" end&gt;Feed&lt;/Nav.Link&gt;   // New\n</code></pre> <p>Here, <code>href</code> is replaced with <code>to</code>, and end tells react to do exact match to <code>/</code> and not make link active for anything else like <code>/home</code>, gulp again...</p> <p>Now, browser load does not happen, JS handles rounting with back and forward buttons working.</p> <p>Varibales in Route</p> <p>Just use special syntax in path</p> <pre><code>&lt;Route path=\"/user/:username\" element={&lt;UserPage /&gt;} /&gt;\n</code></pre> <p>Here, <code>:</code> is placeholder that matches any value. And this placeholder can be accessed as an object in the component defined in <code>element</code>, or any of its children, using  <code>useParams()</code> as an object. So to use the param in component add to <code>src/pages/UserPage.js</code> a simple user profile page</p> <pre><code>import { useParams } from 'react-router-dom';\nimport Body from '../components/Body';\n\nexport default function UserPage() {\n  const { username } = useParams();     // destructuring\n\n  return (\n    &lt;Body sidebar&gt;\n      &lt;h1&gt;{username}&lt;/h1&gt;\n      &lt;p&gt;TODO&lt;/p&gt;\n    &lt;/Body&gt;\n  );\n}\n</code></pre> <p>Here, <code>useParams</code> is a hook, anythin that has use... is hook in react. hook gives access to application state.</p>"},{"location":"9-Drafts/mblog-notes/#adding-backend","title":"Adding Backend","text":"<p>App needs data, and data is values read and write, which is get and set.. simple.. right?</p> <p>React gives, getter and setter right out of the box and can be implemented using hook called <code>useState()</code>, we will come to this later, so for now this does the job.</p> <p>Now, lets see how things render on screen. So components render asap on screen, to speed up this skips data fetch from server and sends it to background job and meanwhile shows 'loading..', as soon as data is served from backend server, the components re-renders with the data. Same happens when a value is set, components show the new data and background job sends data to server. Cool...!</p> <p>This \"background job\" is handled using Side Effect Functions.</p> <p>Now, lets add useState to app in <code>src/components/Posts.js</code>: Add a posts state variable</p> <pre><code>import { useState } from 'react';\nimport Spinner from 'react-bootstrap/Spinner';\n\nexport default function Posts() {\n  const [posts, setPosts] = useState();\n\n  // TODO: add a side effect function to request posts here\n\n  return (\n    &lt;&gt;\n      {posts === undefined ?\n        &lt;Spinner animation=\"border\" /&gt;\n      :\n        &lt;&gt;\n          ... // &lt;-- no changes to blog post JSX\n        &lt;/&gt;\n      }\n    &lt;/&gt;\n  );\n}\n</code></pre> <p>Side Effect Functions</p> <p>To tell backend URL to frontend, use environment variable. Create <code>./.env</code></p> <pre><code>REACT_APP_BASE_API_URL=http://localhost:4000\n</code></pre> <p>Imp: Since react runs in front-end, to avoid exposing env variable to client, it only pics env var starting with <code>REACT_APP_*</code>.</p> <p>The vaiable will be accessible everywhere with <code>process.env.REACT_APP_BASE_API_URL</code>.</p> <p>The fetch() function, is standard JS function available in all modern browser, we will use the same.</p> <pre><code>const BASE_API_URL = process.env.REACT_APP_BASE_API_URL;\nconst response = await fetch(BASE_API_URL + '/api/feed');\n</code></pre> <p>The fetch() function uses promises, so it needs to be awaited when you are in a function declared as async. It returns Response object.</p> <p>In <code>Response</code> object:</p> <ul> <li><code>response.ok</code> is true on success</li> <li><code>response.json()</code> parses the data in body of response and returns it as a JavaScript object or array</li> </ul> <p>Side Effect for Background Job</p> <p><code>useEffect()</code> is a function to do something based on a variables.</p> <pre><code>useEffect( `arg1` , `arg2` )\n\nuseEffect( `do this` , `based on these vars` )\n\nuseEffect( `function` , `array` )\n\nuseEffect( () =&gt; {}, [] )\n\nuseEffect( () =&gt; { (define)(call) }, [] )\n\nuseEffect( () =&gt; { ( async () =&gt; {} )() }, [] )\n\nuseEffect( () =&gt; { ( async () =&gt; { await fetch(); } )() }, [] )\n</code></pre> <p>Here, <code>arg1</code> is a function that is executed, the execution is controlled by <code>arg2</code> which is array of variables.</p> <p>You can implement this to do background-job. The arg1 will do <code>fetch()</code> to backend server.</p> <p>Imp: A simple rule to remember, is that when <code>arg2</code> is set to an empty array, the <code>arg1</code> function runs once when the component is first rendered and never again.</p> <p>So, when component is first rendered, and <code>arg2</code> is empty, <code>arg1</code> func will do <code>fetch()</code> to backend server. Simple..</p> <p>Now in <code>src/components/Posts.js</code>: Load blog posts as a side effect</p> <pre><code>import { useState, useEffect } from 'react';                // New\nimport Spinner from 'react-bootstrap/Spinner';\n\nconst BASE_API_URL = process.env.REACT_APP_BASE_API_URL;    // New\n\nexport default function Posts() {\n  const [posts, setPosts] = useState();\n                                                            // New\n  useEffect(() =&gt; {\n    (async () =&gt; {                                          // arg1 func\n      const response = await fetch(BASE_API_URL + '/api/feed');\n      if (response.ok) {\n        const results = await response.json();\n        setPosts(results.data);\n      }\n      else {\n        setPosts(null);\n      }\n    })();                                                   // arg1 func ENDS\n  }, []);       // arg2, empty array\n\n  return (\n    &lt;&gt;\n      {posts === undefined ?\n        &lt;Spinner animation=\"border\" /&gt;\n      :\n        &lt;&gt;\n          {posts === null ?                                 // New\n             &lt;p&gt;Could not retrieve blog posts.&lt;/p&gt;          // New\n          :\n            &lt;&gt;\n              ... // &lt;-- no changes to blog post JSX\n            &lt;/&gt;\n          }\n        &lt;/&gt;\n      }\n    &lt;/&gt;\n  );\n}\n</code></pre> <p>Here, we added side effect and handled null condition in JSX.</p> <p>Understanding <code>arg1</code> function</p> <pre><code>() =&gt; {\n    (\n        async () =&gt; {\n            const response = await fetch(BASE_API_URL + '/api/feed');\n            if (response.ok) {\n                const results = await response.json();\n                setPosts(results.data);\n            }\n            else {\n                setPosts(null);\n            }\n        }   // async defined closed\n    )();    // async called / executed immediately\n}   // main closed\n</code></pre> <p>Here, it is <code>arg1</code> as a function, within which we define an inner async function and immediately call it.</p> <p>In async function, we <code>fetch()</code>, then pass result to <code>setPosts()</code> which is setter from <code>useState</code> hook, the setter sets state-variable posts. When state variable is set, it triggers re-render of component.</p> <p>Simply, on first render it displays-blank and in background it fetches then sets the posts, on set it re-renders with data.</p> <p>Or, on first render, useEffect fetches in background, and useState sets the posts, then re-renders component with the data.</p> <p>Bravoo..! render empty.. fetch.. render.. :)</p> <p>Now you can further break down <code>Posts</code> component by making a sub component <code>Post.js</code>.</p> <p>Time Ago - Displaying Relative Times</p> <p>The following can be skipped. It is a better way to do things but hard to understand, if you want you can skip to 'Build an API Client' section.</p> <p>Add another component <code>TimeAgo.js</code> which shows relative time, compared to now. In this, there are two functionalities:</p> <ol> <li>Show realtive time, like, 'a second ago', '2 months ago'.</li> <li>Update this as time passes, so 'a second ago' becomes '20 seconds ago' after 20 seconds.</li> </ol> <p>While, 1 is simple to do. 2 is good to have and can be done on client side without making call to server. To do this, you can use <code>useState</code> and <code>useEffect</code> function of react.</p> <p>We are doing something new, \"to change a value on browser even though the inputs have not changed\", re-render the timeago even if the data has not changed. Remember, react only re-renders when the state of variable is changed.</p> <p>Code is added to <code>src/components/TimeAgo.js</code>: TimeAgo component</p> <pre><code>export default function TimeAgo({ isoDate }) {\n  const date = new Date(Date.parse(isoDate));\n  const [time, unit, interval] = getTimeAgo(date);\n\n  const [, setUpdate] = useState(0);\n\n  useEffect(() =&gt; {\n    const timerId = setInterval(\n      () =&gt; setUpdate(update =&gt; update + 1),\n      interval * 1000\n    );\n    return () =&gt; clearInterval(timerId);\n  }, [interval]);\n\n  return (\n    &lt;span title={date.toString()}&gt;{rtf.format(time, unit)}&lt;/span&gt;\n  );\n}\n</code></pre> <p>Here, above two lines and last return is what you know. So <code>const [, setUpdate] = useState(0);</code> is use of setState in a different way. Only setter function is stored. As we want to re-render timeago even when no input has changed so we will make a dummy variable that is not used anywhere but is changed when a re-render is needed.</p> <p>Simply, when you want to change timeago component change its state so that is re-render. Here, state is a dummy-variable.</p> <p>Next, <code>useEffect()</code> implements a function,</p> <pre><code>() =&gt; {\n    const timerId = setInterval(\n      () =&gt; setUpdate(update =&gt; update + 1),\n      interval * 1000\n    );\n    return () =&gt; clearInterval(timerId);\n}\n</code></pre> <p>This is WILD-CODE.. get your brain here...</p> <p>It implements, <code>setInterval</code> and returns a function, <code>clearInterval</code>.</p> <pre><code>const timerId = setInterval(func,val);\nreturn () =&gt; clearInterval(timerId);\n</code></pre> <p>func in <code>setInterval</code> is:</p> <pre><code>() =&gt; setUpdate(\n    (update) =&gt; {\n        returns update + 1;\n        }\n    )\n</code></pre> <p>which is again a function, that calls setter function of state, <code>setUpdate</code>, within which, is another function having arg <code>update</code> and returns <code>update + 1</code>.</p> <p>Simply, in specific interval, increment state by 1.</p> <p>Remember, useEffect is do-something based on value. So here, it is incrementing and setting that value to a state so that the component re-renders. And this is done whenever the value of arg2, <code>interval * 1000</code> is changed.</p> <p>Here, return function, <code>clearInterval</code> has no purpose in component update but is used to clean up the resources when component is removed from the page. On removal react will call the function returned by useEffect, which here will clearInterval and hence prevents memory leakage.</p> <p>As a result, A dummy write-only state variable can be used to force a component to re-render when none of its inputs have changed.</p> <p>If you don't get this (like me), move on..! This is specific to a need and you can do wonders without understanding this... :)</p>"},{"location":"9-Drafts/mblog-notes/#build-an-api-class-and-context","title":"Build an API Class and Context","text":"<p>You know how to do <code>fetch()</code> (or API calls) within a component (we did in <code>Posts</code>).</p> <p>Now, lets do all API calls in central place that can be used by any component and does following things:</p> <ul> <li>authentication</li> <li>has info of server domain and host</li> <li>knows common api path</li> <li>knows pagination arguments</li> <li>does error handling</li> <li>parses JSON</li> </ul> <p>Note: We will do this by making a class that has methods to do <code>get post put delete</code> and returns a <code>response</code> object. Simple.. right?</p>"},{"location":"9-Drafts/mblog-notes/#building-api-request-class","title":"Building API Request Class","text":"<p>In <code>src/MicroblogApiClient.js</code>: An API client class</p> <pre><code>const BASE_API_URL = process.env.REACT_APP_BASE_API_URL;\n\nexport default class MicroblogApiClient {\n  constructor() {\n    this.base_url =  BASE_API_URL + '/api';\n  }\n\n  async request(options) {\n    let query = new URLSearchParams(options.query || {}).toString();\n    if (query !== '') {\n      query = '?' + query;\n    }\n\n    let response;\n    try {\n      response = await fetch(this.base_url + options.url + query, {\n        method: options.method,\n        headers: {\n          'Content-Type': 'application/json',\n          ...options.headers,\n        },\n        body: options.body ? JSON.stringify(options.body) : null,\n      });\n    }\n    catch (error) {\n      response = {\n        ok: false,\n        status: 500,\n        json: async () =&gt; { return {\n          code: 500,\n          message: 'The server is unresponsive',\n          description: error.toString(),\n        }; }\n      };\n    }\n\n    return {\n      ok: response.ok,\n      status: response.status,\n      body: response.status !== 204 ? await response.json() : null\n    };\n  }\n\n  async get(url, query, options) {\n    return this.request({method: 'GET', url, query, ...options});\n  }\n\n  async post(url, body, options) {\n    return this.request({method: 'POST', url, body, ...options});\n  }\n\n  async put(url, body, options) {\n    return this.request({method: 'PUT', url, body, ...options});\n  }\n\n  async delete(url, options) {\n    return this.request({method: 'DELETE', url, ...options});\n  }\n}\n</code></pre> <p>Mostly it is self explainory. Some changes are, now you are using <code>request()</code> insted of <code>fetch()</code> because it is more elaborative.</p> <p>Second important thing here is, <code>options</code> object passed as param to <code>MicroblogApiClient.request()</code> method. It gives all details like url, body, method, query and other params as object (or dictionary).</p> <p>So keys in <code>options</code> object are:</p> <ul> <li><code>options.url</code> - set by <code>get post put</code> and <code>delete</code></li> <li><code>options.method</code> - set by <code>get post put</code> and <code>delete</code></li> <li><code>options.body</code> - set by <code>post</code> and <code>put</code></li> <li>Any additional options that the caller might need, such as custom headers or query string parameters, are accepted as a last argument on the four helper methods.</li> </ul> <p><code>URLSearchParams</code> is helper class in browser that build query string in proper format.</p> <p>It finally does the request using try/catch handling all scenarios and builds the response object. The object it returns has:</p> <ul> <li><code>ok</code> - boolean, shows success or faliure</li> <li><code>status</code>: HTTP code like 200, 404, 500 etc as returned from server.</li> <li><code>body</code>: object having payload returned in body of the response from server.</li> </ul>"},{"location":"9-Drafts/mblog-notes/#skippable-understanding-react-context","title":"Skippable - Understanding React-Context","text":"<p>In short, most efficient way to use the above API class is by using react-context and implement it by making a custom hook. Read on for details. or skip to 'Add an API Provider'</p> <p>One way is create instance of it in the component you want to use, problem is one page each component will create new instance which is inefficient.</p> <p>Another way is to create one instance and share with components, problem is passing from top to bottom in hierarchy of tree is not easy.</p> <p>Solution that lets the instance to be shared in tree is, contexts provided by React.</p> <p>Creating context</p> <pre><code>import { createContext } from 'react';\nconst MyDataContext = createContext();\n</code></pre> <p>Now the object <code>MyDataContext</code> can be used as JSX, <code>&lt;MyDataContext.Provider value={'data-to-share'}&gt;</code>.</p> <p>To make context available, insert it in JSX Tree high enough so all members in tree can access it. Eg, in <code>App.js</code> you can do this:</p> <pre><code>export default function MyApp() {\n  return (\n    &lt;Container&gt;\n      &lt;Header /&gt;\n      &lt;MyDataContext.Provider value={'data-to-share'}&gt;      // New\n        &lt;Sidebar /&gt;\n        &lt;Content /&gt;\n      &lt;/MyDataContext.Provider&gt;                             // New\n    &lt;/Container&gt;\n  );\n}\n</code></pre> <p>Here, you can see, context will be available to sidebar and content but not to header as it is not in the tree hierarchy.</p> <p>To use context value the child component can use the <code>useContext</code> hook as follows:</p> <pre><code>import { useContext } from 'react';\nimport { MyDataContext } from './MyDataContext';\n\nexport default function Sidebar() {\n  const myData = useContext(MyDataContext);\n  // ...\n}\n</code></pre>"},{"location":"9-Drafts/mblog-notes/#add-an-api-provider","title":"Add an API Provider","text":"<p>Simply, here we will import MicroblogApiClient class, create its instance, wrap instance in context. Make hook to access context. Gulp it.</p> <p>Another way to understand:</p> <ul> <li>There is a simple class</li> <li>Class has instance</li> <li>Instance is wrapped in context</li> <li>Context is made availabe as hook</li> </ul> <p>Same thing with reasons</p> <ul> <li>There is a simple class - so we don't repeat and do fetch/try/catch in each component</li> <li>Class has instance - to use class. This instance can be used, however...</li> <li>Instance is wrapped in context - so that one instance is used in all components efficiently. This context can be used, however...</li> <li>Context is made availabe as hook - that's react way, hooks are special and can be used outside render to interact with other functions. Also, it makes code neat (less imports).</li> </ul> <p>Same thing with code</p> <ul> <li>There is a simple class, <code>MicroblogApiClient</code></li> <li>Class has instance, <code>const api = new MicroblogApiClient();</code></li> <li>Instance is wrapped in context<pre><code>&lt;ApiContext.Provider value={api}&gt;   // Instance added here\n    {children}\n&lt;/ApiContext.Provider&gt;\n</code></pre> </li> </ul> <ul> <li>Context is made availabe as hook, <code>return useContext(ApiContext);</code></li> </ul> <p>This is react-way of doing things efficiently. Trust it..!</p> <p>Adding it all together to make APIContext, in <code>src/contexts/ApiProvider.js</code>: An API context, add</p> <pre><code>import { createContext, useContext } from 'react';\nimport MicroblogApiClient from '../MicroblogApiClient';\n\nconst ApiContext = createContext();\n\nexport default function ApiProvider({ children }) {\n  const api = new MicroblogApiClient();\n\n  return (\n    &lt;ApiContext.Provider value={api}&gt;\n      {children}\n    &lt;/ApiContext.Provider&gt;\n  );\n}\n\nexport function useApi() {\n  return useContext(ApiContext);\n}\n</code></pre> <p>Here, there two exports, one is default which is same as any other react-componenet, another is <code>useApi()</code> a custom hook.</p> <p><code>{children}</code> in <code>&lt;ApiContext.Provider&gt;</code> makes the context available to all child components.</p> <p>Simply, understand it as an invisible component high-above in JSX tree to make api-instance avilable to all components (children) in JSX tree. Let's add it to <code>src/App.js</code>: Add the API context to the application</p> <pre><code>import Container from 'react-bootstrap/Container';\nimport { BrowserRouter, Routes, Route, Navigate } from 'react-router-dom';\nimport ApiProvider from './contexts/ApiProvider';   // New\nimport Header from './components/Header';\nimport FeedPage from './pages/FeedPage';\nimport ExplorePage from './pages/ExplorePage';\nimport UserPage from './pages/UserPage';\nimport LoginPage from './pages/LoginPage';\n\nexport default function App() {\n  return (\n    &lt;Container fluid className=\"App\"&gt;\n      &lt;BrowserRouter&gt;\n        &lt;ApiProvider&gt;                               // New\n          &lt;Header /&gt;\n          &lt;Routes&gt;\n            &lt;Route path=\"/\" element={&lt;FeedPage /&gt;} /&gt;\n            &lt;Route path=\"/explore\" element={&lt;ExplorePage /&gt;} /&gt;\n            &lt;Route path=\"/user/:username\" element={&lt;UserPage /&gt;} /&gt;\n            &lt;Route path=\"/login\" element={&lt;LoginPage /&gt;} /&gt;\n            &lt;Route path=\"*\" element={&lt;Navigate to=\"/\" /&gt;} /&gt;\n          &lt;/Routes&gt;\n        &lt;/ApiProvider&gt;                              // New\n      &lt;/BrowserRouter&gt;\n    &lt;/Container&gt;\n  );\n}\n</code></pre> <p>Now you have the class available as hook. Lets use this in <code>src/components/Posts.js</code>: Using the useApi() hook</p> <pre><code>import { useState, useEffect } from 'react';\nimport Spinner from 'react-bootstrap/Spinner';\nimport { useApi } from '../contexts/ApiProvider';       // added\nimport Post from './Post';\n\nexport default function Posts() {\n  const [posts, setPosts] = useState();\n  const api = useApi();                                 // added\n\n  useEffect(() =&gt; {\n    (async () =&gt; {\n      const response = await api.get('/feed');          // updated\n      if (response.ok) {\n        setPosts(response.body.data);                   // updated\n      }\n      else {\n        setPosts(null);\n      }\n    })();\n  }, [api]);                                            // updated\n\n  ... // &lt;-- no changes in the rest of the function\n}\n</code></pre> <p>Here, <code>api</code> is passed as dependency to <code>useEffect</code> because it is good to have dependency on the variable (api) used in <code>useEffect</code> function.</p>"},{"location":"9-Drafts/mblog-notes/#add-pagination","title":"Add Pagination","text":"<p>You can make a generic \"More\" button as a component. This uses props to have two params <code>{ pagination, loadNextPage }</code>. Pagination has pagination data from API, loadNextPage if true will fetch more data from API and set pagination and posts. So a new state to manage Pagination was also added to <code>Posts</code> page.</p>"},{"location":"9-Drafts/mblog-notes/#forms-and-validations","title":"Forms and Validations","text":"<p>React-Bootstrap offers components that can be used to build DOM elements of form. It offers at low level that is, a component for label, input, placeholder, help text etc. You can group them all to make a high level custom-component that uses low level and groups them so you use only one custom component and it in backend uses all. this is done to avoid code repetition.</p> <p>Building Form - Using low level Bootstrap-react form components`</p> <pre><code>&lt;Form&gt;\n  &lt;Form.Group className=\"mb-3\" controlId=\"formBasicEmail\"&gt;\n    &lt;Form.Label&gt;Email address&lt;/Form.Label&gt;\n    &lt;Form.Control type=\"email\" placeholder=\"Enter email\" /&gt;\n    &lt;Form.Text className=\"text-muted\"&gt;\n      {\"We'll never share your email with anyone else.\"}\n    &lt;/Form.Text&gt;\n  &lt;/Form.Group&gt;\n\n  // ... more fields here\n\n&lt;/Form&gt;\n</code></pre> <p>Code Reusability - You can make your custom high level component with props so it is reusable. In <code>src/components/InputField.js</code>: A generic form input field</p> <pre><code>import Form from 'react-bootstrap/Form';\n\nexport default function InputField(\n  { name, label, type, placeholder, error, fieldRef }\n) {\n  return (\n    &lt;Form.Group controlId={name} className=\"InputField\"&gt;\n      {label &amp;&amp; &lt;Form.Label&gt;{label}&lt;/Form.Label&gt;}\n      &lt;Form.Control\n        type={type || 'text'}\n        placeholder={placeholder}\n        ref={fieldRef}\n      /&gt;\n      &lt;Form.Text className=\"text-danger\"&gt;{error}&lt;/Form.Text&gt;\n    &lt;/Form.Group&gt;\n  );\n}\n</code></pre> <p>Implementing custom-template - Now you can use following in any component to build form, eg in login page, <code>src/pages/LoginPage.js</code>:</p> <pre><code>import { useState } from 'react';\nimport Form from 'react-bootstrap/Form';\nimport Button from 'react-bootstrap/Button';\nimport Body from '../components/Body';\nimport InputField from '../components/InputField';\n\nexport default function LoginPage() {\n  const [formErrors, setFormErrors] = useState({});\n\n  const onSubmit = (ev) =&gt; {\n    ev.preventDefault();\n    console.log('handle form here');\n  };\n\n  return (\n    &lt;Body&gt;\n      &lt;h1&gt;Login&lt;/h1&gt;\n      &lt;Form onSubmit={onSubmit}&gt;\n        &lt;InputField\n          name=\"username\" label=\"Username or email address\"\n          error={formErrors.username} /&gt;\n        &lt;InputField\n          name=\"password\" label=\"Password\" type=\"password\"\n          error={formErrors.password} /&gt;\n        &lt;Button variant=\"primary\" type=\"submit\"&gt;Login&lt;/Button&gt;\n      &lt;/Form&gt;\n    &lt;/Body&gt;\n  );\n}\n</code></pre> <p>Here, errors is kind of data, to handle data, you need state variable, One state variable for all fields. To submit, you need a funciton, hence added. And you can see use of custom-input-field-componenent that we created.</p> <p>Handling Form Submit</p> <p>Now, lets think about getting form-values, remember, here everything is client-side. So start with <code>event.preventDefault();</code>. Then to get value, traditional way with vanilla javascript is <code>document.getElementById()</code> but rather than maintaining <code>id</code> for each element, react has smarter way of using references. This is done by using hook, <code>useRef()</code> inside component's render function.</p> <pre><code>export default function MyForm() {\n  const usernameField = useRef();\n\n  const onSubmit = (ev) =&gt; {\n    ev.preventDefault();\n    alert('Your username is: ' + usernameField.current.value);\n  };\n\n  return (\n    &lt;form onSubmit={onSubmit}&gt;\n      &lt;input type=\"text\" ref={usernameField} /&gt;\n    &lt;/form&gt;\n  );\n}\n</code></pre> <p>Simply, initialize a variable using hook <code>useRef()</code>, then add <code>ref</code> prop to input field, now this maps the input field with this variable and makes available. To access var, use <code>usernameField.current.value</code>. All linked togther.. bravo..!</p> <p>So, now you can add ref like this, here we are using custom-component:</p> <pre><code>// create ref var\nconst usernameField = useRef();\n\n// add link using fieldRef, which is ref in actual\n&lt;InputField\n    name=\"password\" label=\"Password\" type=\"password\"\n    error={formErrors.password} fieldRef={passwordField} /&gt;\n\n// now access like this\nconst onSubmit = (ev) =&gt; {\n    ev.preventDefault();\n    const username = usernameField.current.value;\n    console.log(`You entered username: ${username}`);\n};\n\n// or do something like this\nuseEffect(() =&gt; {\n    usernameField.current.focus();\n}, []);\n</code></pre> <p>All crystal clear, right? It is same as id, but <code>ref</code> is \"react-way\".</p> <p>Form Validations - This is done in <code>onSubmit()</code> function.</p> <pre><code>const [formErrors, setFormErrors] = useState({});\n\nconst onSubmit = (ev) =&gt; {\n    ev.preventDefault();\n\n    const username = usernameField.current.value;\n    const errors = {}\n\n    if (!username) {\n        errors.username = 'Username must not be empty!'\n    }\n\n    setFormErrors(errors);\n\n    if (Object.keys(errors).length &gt; 0) {\n        return;\n    }\n};\n</code></pre> <p>Form Submit - send validated data to server</p> <p>Sending data is simple as an async post request to server. In <code>src/pages/RegistrationPage.js</code></p> <pre><code>import { useNavigate } from 'react-router-dom';\nimport { useApi } from '../contexts/ApiProvider';\n\nexport default function RegistrationPage() {\n    ... // &lt;-- no changes to state variables and references\n    const navigate = useNavigate();\n    const api = useApi();\n\n    const onSubmit = async (event) =&gt; {\n        event.preventDefault();\n\n        // validation\n        if (passwordField.current.value !== password2Field.current.value) {\n            setFormErrors({password2: \"Passwords don't match\"});\n        }\n        else {\n            // submit\n            const response = await api.post(\n                '/users',\n                {\n                    username: usernameField.current.value,\n                    email: emailField.current.value,\n                    password: passwordField.current.value,\n                }\n            );\n\n            // if error\n            if(!response.ok) {\n                setFormErrors(response.body.errors.json);\n            }\n            // success\n            else {\n                setFormErrors({});\n                navigate('/login');\n            }\n        }\n    }\n\n    ... // &lt;-- no changes to returned JSX\n};\n</code></pre> <p>Imp: Interesting thing to see there is that in case the validation error from server, response has error, and that has the same structure as is used in the form. Notice, how <code>setFormErrors(response.body.errors.json);</code> sets error and applies to the form to the frontend. Hence, when designing keep data structure same from db to orm to api json to react form to errors. Keeping same format is very convinient.</p>"},{"location":"9-Drafts/mblog-notes/#showing-flash-messages","title":"Showing Flash Messages","text":"<p>Flash message will be displayed from one component hierarchy to another. This requires data (flash message) created in one component (register) to render in another (login). To share data react provides context (used in Api sharing previously). You can use <code>import { createContext, useContext, useState } from 'react';</code> to implement Flash messaging. In <code>src/contexts/FlashProvider.js</code>: A Flash context</p> <pre><code>import { createContext, useContext, useState } from 'react';\n\nexport const FlashContext = createContext();\nlet flashTimer;\n\nexport default function FlashProvider({ children }) {\n  const [flashMessage, setFlashMessage] = useState({});\n  const [visible, setVisible] = useState(false);\n\n  const flash = (message, type, duration = 10) =&gt; {\n    if (flashTimer) {\n      clearTimeout(flashTimer);\n      flashTimer = undefined;\n    }\n    setFlashMessage({message, type});\n    setVisible(true);\n    if (duration) {\n      flashTimer = setTimeout(hideFlash, duration * 1000);\n    }\n  };\n\n  const hideFlash = () =&gt; {\n    setVisible(false);\n  };\n\n  return (\n    &lt;FlashContext.Provider value={{flash, hideFlash, flashMessage, visible}}&gt;\n      {children}\n    &lt;/FlashContext.Provider&gt;\n  );\n}\n\nexport function useFlash() {\n  return useContext(FlashContext).flash;\n}\n</code></pre> <p>Then add this to JSX hierarchy high so that it is available to all components, best place is to add to <code>src/App.js</code></p> <pre><code>... // &lt;-- no changes to existing imports\nimport FlashProvider from './contexts/FlashProvider';   // Added\n\nexport default function App() {\n  return (\n    &lt;Container fluid className=\"App\"&gt;\n      &lt;BrowserRouter&gt;\n        &lt;FlashProvider&gt;                                 // Added\n          &lt;ApiProvider&gt;\n            &lt;Header /&gt;\n            &lt;Routes&gt;\n              ... // &lt;-- no changes to routes\n            &lt;/Routes&gt;\n          &lt;/ApiProvider&gt;\n        &lt;/FlashProvider&gt;                                // Added\n      &lt;/BrowserRouter&gt;\n    &lt;/Container&gt;\n  );\n}\n</code></pre> <p>This facilitates, sharing of flash message as data and making it available via context to all components, next part is showing the flash message which can be done using Flash Component. In <code>src/components/FlashMessage.js</code>: Display a flashed message</p> <pre><code>import { useContext } from 'react';\nimport Alert from 'react-bootstrap/Alert';\nimport Collapse from 'react-bootstrap/Collapse';\nimport { FlashContext } from '../contexts/FlashProvider';\n\nexport default function FlashMessage() {\n  const { flashMessage, visible, hideFlash } = useContext(FlashContext);\n\n  return (\n    &lt;Collapse in={visible}&gt;\n      &lt;div&gt;\n        &lt;Alert variant={flashMessage.type || 'info'} dismissible\n          onClose={hideFlash}&gt;\n          {flashMessage.message}\n        &lt;/Alert&gt;\n      &lt;/div&gt;\n    &lt;/Collapse&gt;\n  );\n}\n</code></pre> <p>Finally, add it to <code>src/components/Body.js</code>: Show a flashed message in the page</p> <pre><code>... // &lt;-- no changes to existing imports\nimport FlashMessage from './FlashMessage';\n\nexport default function Body({ sidebar, children }) {\n  return (\n    &lt;Container&gt;\n      &lt;Stack direction=\"horizontal\" className=\"Body\"&gt;\n        {sidebar &amp;&amp; &lt;Sidebar /&gt;}\n        &lt;Container className=\"Content\"&gt;\n          &lt;FlashMessage /&gt;          // Added\n          {children}\n        &lt;/Container&gt;\n      &lt;/Stack&gt;\n    &lt;/Container&gt;\n  );\n}\n</code></pre> <p>Imp: A React context is not only useful when a parent needs to share data with its children. It can also be used to enable children components to pass information between themselves with the parent as intermediary.</p>"},{"location":"9-Drafts/mblog-notes/#authentication","title":"Authentication","text":"<p>Enable auth in backend by changing .env to <code>DISABLE_AUTH=false</code>.</p> <p>The auth in this API works by:</p> <ul> <li>sending <code>POST</code> username and password to server</li> <li>on success, get <code>access_token</code> from server</li> <li>now to make \"authenticated API calls\" the requests must include \"Bearer Authentication Header\" with a valid access-token in each call.</li> </ul> <p>Add Auth Headers</p> <p>To do this, modify the API class fetch code to include this header. In <code>src/MicroblogApiClient.js</code>: Include bearer token header</p> <pre><code>  response = await fetch(this.base_url + options.url + query, {\n    method: options.method,\n    headers: {\n      'Content-Type': 'application/json',\n      'Authorization': 'Bearer ' + localStorage.getItem('accessToken'), // added\n      ...options.headers,\n    },\n    body: options.body ? JSON.stringify(options.body) : null,\n  });\n}\n</code></pre> <p>Here, you notice that <code>accessToken</code> is passed in each request without check if it is set, idea is that mostly api calls will need it, for those that are public, that is <code>login</code> will overwrite this.</p> <p>Add auth methods</p> <p>The following code is added to class to facilitate login with overwritten auth header. In <code>src/MicroblogApiClient.js</code>: add Login method</p> <pre><code>  async login(username, password) {\n\n    // submit POST\n    const response = await this.post('/tokens', null, {\n      headers: {\n        Authorization:  'Basic ' + btoa(username + \":\" + password)\n      }\n    });\n\n    // check error\n    if (!response.ok) {\n      return response.status === 401 ? 'fail' : 'error';\n    }\n\n    // success\n    localStorage.setItem('accessToken', response.body.access_token);\n    return 'ok';\n  }\n  ```\n\nSimilarly, add a `logout` method and `isAuthenticated` in `src/MicroblogApiClient.js`: Logout method\n\n```js\n  async logout() {\n    await this.delete('/tokens');\n    localStorage.removeItem('accessToken');\n  }\n\n  isAuthenticated() {\n    return localStorage.getItem('accessToken') !== null;\n  }\n</code></pre> <p>User Context - or g.user in flask</p> <p>UserContext provides information of logged in user to all the component in app. It can be used to handle these attributes:</p> <ul> <li><code>user</code>: user object, null if logged out.</li> <li><code>setUser</code>: setter for above.</li> <li><code>login</code>: helper method to login with username and password.</li> <li><code>logout</code>: helper method to logout.</li> </ul> <p>Here, doing things react-way. User information is data, so use <code>useState</code>, and a setter to setUser. Next, to do api calls, you need side-effect, so use <code>useEffect</code>.</p> <p>Create <code>src/contexts/UserProvider.js</code>: User context and hook</p> <pre><code>import { createContext, useContext, useState, useEffect } from 'react';\nimport { useApi } from './ApiProvider';\n\nconst UserContext = createContext();\n\nexport default function UserProvider({ children }) {\n  const [user, setUser] = useState();\n  const api = useApi();\n\n  useEffect(() =&gt; {\n    (async () =&gt; {\n      if (api.isAuthenticated()) {\n        const response = await api.get('/me');\n        setUser(response.ok ? response.body : null);\n      }\n      else {\n        setUser(null);\n      }\n    })();\n  }, [api]);\n\n  const login = async (username, password) =&gt; {\n    const result = await api.login(username, password);\n    if (result === 'ok') {\n      const response = await api.get('/me');\n      setUser(response.ok ? response.body : null);\n    }\n    return result;\n  };\n\n  const logout = async () =&gt; {\n    await api.logout();\n    setUser(null);\n  };\n\n  return (\n    &lt;UserContext.Provider value={{ user, setUser, login, logout }}&gt;\n      {children}\n    &lt;/UserContext.Provider&gt;\n  );\n}\n\nexport function useUser() {\n  return useContext(UserContext);\n}\n</code></pre> <p>Here, context has a companion hook, that is context is made available making hook, other components can use this hook to access entire object in context and can obtain requried attributes using destructuring.</p> <p>Again to make it available, add to <code>src/App.js</code>: Add user context</p> <pre><code>... // &lt;-- no changes to existing imports\nimport UserProvider from './contexts/UserProvider';\n\nexport default function App() {\n  return (\n    &lt;Container fluid className=\"App\"&gt;\n      &lt;BrowserRouter&gt;\n        &lt;FlashProvider&gt;\n          &lt;ApiProvider&gt;\n            &lt;UserProvider&gt;\n              &lt;Header /&gt;\n              &lt;Routes&gt;\n                ... // &lt;-- no changes to routes\n              &lt;/Routes&gt;\n            &lt;/UserProvider&gt;\n          &lt;/ApiProvider&gt;\n        &lt;/FlashProvider&gt;\n      &lt;/BrowserRouter&gt;\n    &lt;/Container&gt;\n  );\n}\n</code></pre>"},{"location":"9-Drafts/mblog-notes/#login-required-privateroute","title":"Login Required - PrivateRoute","text":"<p>Some routes are only available when logged-in, else redirect user to login and the back to requested page. To implement this, <code>PrivateRoute</code> component. It can be added as parent to any component that requires auth (same as <code>@login_requried</code> decorator in flask). Add <code>src/components/PrivateRoute.js</code>: Private route component</p> <pre><code>import { useLocation, Navigate } from 'react-router-dom';\nimport { useUser } from '../contexts/UserProvider';\n\nexport default function PrivateRoute({ children }) {\n  const { user } = useUser();\n  const location = useLocation();\n\n  if (user === undefined) {\n    return null;\n  }\n  else if (user) {\n    return children;\n  }\n  else {\n    const url = location.pathname + location.search + location.hash;\n    return &lt;Navigate to=\"/login\" state={{next: url}} /&gt;\n  }\n}\n</code></pre> <p>Simply, add it high in hierarchy. It has children of JSX tree. Return the children passed, only if user is logged in.. simple.</p> <p>As an added functionality, else, it navigates to <code>/login</code> with information of request-url. This is done using <code>useLocation</code> hook and <code>Navigate</code> component. <code>useLocation</code> reads browser url with params. <code>Navigate</code> componet has prop <code>state</code> to store any data that might be useful, you can use this to pass <code>url</code>, it will be available in key <code>next</code>.</p> <p>Similarly, you can craete PublicRoute for login and logout. Create <code>src/components/PublicRoute.js</code>: Public route component</p> <pre><code>import { Navigate } from 'react-router-dom';\nimport { useUser } from '../contexts/UserProvider';\n\nexport default function PublicRoute({ children }) {\n  const { user } = useUser();\n\n  if (user === undefined) {\n    return null;\n  }\n  else if (user) {\n    return &lt;Navigate to=\"/\" /&gt;\n  }\n  else {\n    return children;\n  }\n}\n</code></pre> <p>Now, you just need to wrap the routes. Below are the changes to App to apply the route wrappers <code>src/App.js</code>: Routing of public and private routes</p> <pre><code>... // &lt;-- no changes to existing imports\nimport PrivateRoute from './components/PrivateRoute';\nimport PublicRoute from './components/PublicRoute';\n\nexport default function App() {\n  return (\n    ... // &lt;-- no changes to outer components\n\n    &lt;Routes&gt;\n      &lt;Route path=\"/login\" element={\n        &lt;PublicRoute&gt;&lt;LoginPage /&gt;&lt;/PublicRoute&gt;\n      } /&gt;\n      &lt;Route path=\"/register\" element={\n        &lt;PublicRoute&gt;&lt;RegistrationPage /&gt;&lt;/PublicRoute&gt;\n      } /&gt;\n      &lt;Route path=\"*\" element={\n        &lt;PrivateRoute&gt;\n          &lt;Routes&gt;\n            &lt;Route path=\"/\" element={&lt;FeedPage /&gt;} /&gt;\n            &lt;Route path=\"/explore\" element={&lt;ExplorePage /&gt;} /&gt;\n            &lt;Route path=\"/user/:username\" element={&lt;UserPage /&gt;} /&gt;\n            &lt;Route path=\"*\" element={&lt;Navigate to=\"/\" /&gt;} /&gt;\n          &lt;/Routes&gt;\n        &lt;/PrivateRoute&gt;\n      } /&gt;\n    &lt;/Routes&gt;\n\n    ... // &lt;-- no changes to outer components\n  );\n}\n</code></pre> <p>After doing, this, once you login, the status becomes authenticated and login ang logout routes are no-longer available. You can type <code>/register</code> in browser but nothing happens as this route is not returned from <code>&lt;Routes&gt;</code> component.</p> <p>Now that we have all set-up to make login and logout modular, reusable, react-way and flask-way. Lets add them all to login page.</p> <p>Logging User In</p> <p>Modify <code>src/pages/LoginPage.js</code>: Log users in</p> <pre><code>import { useState, useEffect, useRef } from 'react';\nimport { Link, useNavigate, useLocation } from 'react-router-dom';  // Added\nimport Form from 'react-bootstrap/Form';\nimport Button from 'react-bootstrap/Button';\nimport Body from '../components/Body';\nimport InputField from '../components/InputField';\nimport { useUser } from '../contexts/UserProvider';       // Added\nimport { useFlash } from '../contexts/FlashProvider';     // Added\n\nexport default function LoginPage() {\n  ... // &lt;-- no changes to existing state and references\n  const { login } = useUser();                         // Added\n  const flash = useFlash();                            // Added\n  const navigate = useNavigate();                      // Added\n  const location = useLocation();                      // Added\n\n  ... // &lt;-- no changes to side effect function\n\n  const onSubmit = async (ev) =&gt; {                        // Updated\n    ... // &lt;-- no changes to existing submit logic\n\n    const result = await login(username, password);       // Added this &amp; below\n    if (result === 'fail') {\n      flash('Invalid username or password', 'danger');\n    }\n    else if (result === 'ok') {\n      let next = '/';\n      if (location.state &amp;&amp; location.state.next) {\n        next = location.state.next;\n      }\n      navigate(next);\n    }\n  };\n\n  ... // &lt;-- no changes to returned JSX\n}\n</code></pre>"},{"location":"9-Drafts/mblog-notes/#using-user-info-in-header","title":"Using user info in header","text":"<p>Now we can show username and login/logout buttons as the app is user aware. Modify <code>src/components/Header.js</code>: Show a user account dropdown</p> <pre><code>import Navbar from 'react-bootstrap/Navbar';\nimport Container from 'react-bootstrap/Container';\nimport Nav from 'react-bootstrap/Nav';\nimport NavDropdown from 'react-bootstrap/NavDropdown';\nimport Image from 'react-bootstrap/Image';\nimport Spinner from 'react-bootstrap/Spinner';\nimport { NavLink } from 'react-router-dom';\nimport { useUser } from '../contexts/UserProvider';\n\nexport default function Header() {\n  const { user, logout } = useUser();\n\n  return (\n    &lt;Navbar bg=\"light\" sticky=\"top\" className=\"Header\"&gt;\n      &lt;Container&gt;\n        &lt;Navbar.Brand&gt;Microblog&lt;/Navbar.Brand&gt;\n        &lt;Nav&gt;\n          {user === undefined ?\n            &lt;Spinner animation=\"border\" /&gt;\n          :\n            &lt;&gt;\n              {user !== null &amp;&amp;\n                &lt;div className=\"justify-content-end\"&gt;\n                  &lt;NavDropdown title={\n                    &lt;Image src={user.avatar_url + '&amp;s=32'} roundedCircle /&gt;\n                  } align=\"end\"&gt;\n                    &lt;NavDropdown.Item as={NavLink} to={'/user/' + user.username}&gt;\n                      Profile\n                    &lt;/NavDropdown.Item&gt;\n                    &lt;NavDropdown.Divider /&gt;\n                    &lt;NavDropdown.Item onClick={logout}&gt;\n                      Logout\n                    &lt;/NavDropdown.Item&gt;\n                  &lt;/NavDropdown&gt;\n                &lt;/div&gt;\n              }\n            &lt;/&gt;\n          }\n        &lt;/Nav&gt;\n      &lt;/Container&gt;\n    &lt;/Navbar&gt;\n  );\n}\n</code></pre>"},{"location":"9-Drafts/mblog-notes/#refreshing-tokens","title":"Refreshing Tokens","text":"<p>Access token has timeout of 15 mins, to auto refresh it and hide the functionality from client, it is good to do that in class. Steps to follow:</p> <ul> <li>Send a requet</li> <li>if response is not 401, return response to caller</li> <li>else, refresh token, resend original request with new token, return response.</li> </ul> <p>One thing to note is, access token is not returned from server in each request. It is only returned when the request type is from login, that is, it has following in request header.</p> <pre><code>headers: {Authorization: 'Basic ' + btoa(username + \":\" + password)}\n</code></pre> <p>We again need the access token in response, so that it can be set to cookit, but need this without sending username and password, as we are not again asking user for credentials. To do this, add below code to request options.</p> <pre><code>credentials: options.url === '/tokens' ? 'include' : 'omit',\n</code></pre> <p>Now cookie is returned from server when token is requested. This now needs to to be added to localStorage. For this, rename <code>request</code> to <code>requestInternal</code> and add another <code>request</code> method as wrapper that handles to refresh token using following code to <code>src/MicroblogApiClient.js</code>: Refresh token logic:</p> <pre><code>export default class MicroblogApiClient {\n  async request(options) {\n    let response = await this.requestInternal(options);\n    if (response.status === 401 &amp;&amp; options.url !== '/tokens') {\n      const refreshResponse = await this.put('/tokens', {\n        access_token: localStorage.getItem('accessToken'),\n      });\n      if (refreshResponse.ok) {\n        localStorage.setItem('accessToken', refreshResponse.body.access_token);\n        response = await this.requestInternal(options);\n      }\n    }\n    return response;\n  }\n\n  ... // &lt;-- no changes to other methods\n}\n</code></pre> <p>Adding <code>credentials: include</code> returned <code>refreshResponse.body.access_token</code> hence we can do <code>localStorage.setItem()</code>.</p>"},{"location":"9-Drafts/mblog-notes/#building-application-features","title":"Building Application Features","text":"<p>Now you can use above react-concepts to build rest of the application parts.</p>"},{"location":"9-Drafts/mblog-notes/#submitting-blog-posts","title":"Submitting Blog Posts","text":"<p>Task here is to:</p> <ul> <li>let user write a new post - add new component <code>Write.js</code></li> <li>then submit that post to server - do <code>onSubmit</code> an async req</li> <li>show that post to user by updating posts state - use prop in Write that uses existing <code>posts</code> state <code>setPosts()</code> in <code>Posts.js</code></li> <li>make the form blank - in <code>Write.js</code></li> </ul> <p>Since <code>Posts.js</code> is reusable and is used in explore, feed and user posts. We need to add a prop for <code>write</code> in <code>Posts()</code> that will show write post only when we want.</p> <p>Add <code>src/components/Write.js</code>: Blog post write form</p> <pre><code>import { useState, useEffect, useRef } from 'react';\nimport Stack from \"react-bootstrap/Stack\";\nimport Image from \"react-bootstrap/Image\";\nimport Form from 'react-bootstrap/Form';\nimport InputField from './InputField';\nimport { useApi } from '../contexts/ApiProvider';\nimport { useUser } from '../contexts/UserProvider';\n\nexport default function Write({ showPost }) {\n  const [formErrors, setFormErrors] = useState({});\n  const textField = useRef();\n  const api = useApi();\n  const { user } = useUser();\n\n  useEffect(() =&gt; {\n    textField.current.focus();\n  }, []);\n\n  const onSubmit = async (ev) =&gt; {\n    ev.preventDefault();\n    const response = await api.post(\"/posts\", {\n      text: textField.current.value\n    });\n    if (response.ok) {\n      showPost(response.body);\n      textField.current.value = '';\n    }\n    else {\n      if (response.body.errors) {\n        setFormErrors(response.body.errors.json);\n      }\n    }\n  };\n\n  return (\n    &lt;Stack direction=\"horizontal\" gap={3} className=\"Write\"&gt;\n      &lt;Image\n        src={ user.avatar_url + '&amp;s=64' }\n        roundedCircle\n      /&gt;\n      &lt;Form onSubmit={onSubmit}&gt;\n        &lt;InputField\n          name=\"text\" placeholder=\"What's on your mind?\"\n          error={formErrors.text} fieldRef={textField} /&gt;\n      &lt;/Form&gt;\n    &lt;/Stack&gt;\n  );\n}\n</code></pre> <p>Here, <code>showPost</code> is a callback function provided by the parent component to perform action of adding newPost to posts.</p>"},{"location":"9-Drafts/mblog-notes/#user-page-actions","title":"User Page Actions","text":"<p>Add Edit-Profile, Follow/Unfollow.</p> <p>To determine when to show edit/follow/unfollow you need the know who is logged in and who is being viewed, if they are same show edit. if the logged in user has not followed the user being viewed show follow else unfollow.</p> <p>In <code>src/pages/UserPage.js</code>: Action buttons in user page</p> <pre><code>... // &lt;-- no changes to existing imports\nimport Button from 'react-bootstrap/Button';          // added\nimport { useNavigate } from 'react-router-dom';       // added\nimport { useUser } from '../contexts/UserProvider';   // added\nimport { useFlash } from '../contexts/FlashProvider'; // added\n\nexport default function UserPage() {\n  ... // &lt;-- no changes to existing state, references and custom hooks\n  const [isFollower, setIsFollower] = useState();\n  const { user: loggedInUser } = useUser();\n  const flash = useFlash();\n  const navigate = useNavigate();\n\n  useEffect(() =&gt; {\n    (async () =&gt; {\n      const response = await api.get('/users/' + username);\n      if (response.ok) {                                          // updated\n        setUser(response.body);\n        if (response.body.username !== loggedInUser.username) {   // added &amp; below\n          const follower = await api.get(\n            '/me/following/' + response.body.id);\n          if (follower.status === 204) {\n            setIsFollower(true);\n          }\n          else if (follower.status === 404) {\n            setIsFollower(false);\n          }\n        }\n        else {\n          setIsFollower(null);\n        }\n      }\n      else {\n        setUser(null);\n      }\n    })();\n  }, [username, api, loggedInUser]);\n\n  const edit = () =&gt; {\n    // TODO\n  };\n\n  const follow = async () =&gt; {\n    // TODO\n  };\n\n  const unfollow = async () =&gt; {\n    // TODO\n  };\n\n  return (\n    &lt;Body sidebar&gt;\n      {user === undefined ?\n        &lt;Spinner animation=\"border\" /&gt;\n      :\n        &lt;&gt;\n          {user === null ?\n            &lt;p&gt;User not found.&lt;/p&gt;\n          :\n            &lt;&gt;\n              &lt;Stack direction=\"horizontal\" gap={4}&gt;\n                &lt;Image src={user.avatar_url + '&amp;s=128'} roundedCircle /&gt;\n                &lt;div&gt;\n                  ... // &lt;-- no changes to user details\n\n                  {isFollower === null &amp;&amp;                         // added &amp; below\n                    &lt;Button variant=\"primary\" onClick={edit}&gt;\n                      Edit\n                    &lt;/Button&gt;\n                  }\n                  {isFollower === false &amp;&amp;\n                    &lt;Button variant=\"primary\" onClick={follow}&gt;\n                      Follow\n                    &lt;/Button&gt;\n                  }\n                  {isFollower === true &amp;&amp;\n                    &lt;Button variant=\"primary\" onClick={unfollow}&gt;\n                      Unfollow\n                    &lt;/Button&gt;\n                  }\n                &lt;/div&gt;\n              &lt;/Stack&gt;\n              &lt;Posts content={user.id} /&gt;\n            &lt;/&gt;\n          }\n        &lt;/&gt;\n      }\n    &lt;/Body&gt;\n  );\n}\n</code></pre> <p>Follow/unfollow is data and needs a new state to be created. <code>isFollower</code> has three values, null then same user, true then show unfollow, false then show follow. The value of this is determined by request to API.</p> <p><code>const { user: loggedInUser } = useUser();</code> is a JS syntax that renames user to loggedInUser as user is already a variable in component and represents the user being viewed.</p> <p>The buttons that present the actions to the user are going to have onClick handlers edit(), follow() and unfollow() respectively, all placeholders for now.</p>"},{"location":"9-Drafts/mblog-notes/#edit-user-profile","title":"Edit User Profile","text":"<p>Simply navigate to new <code>/edit</code> route. This is new page, <code>src/pages/EditUserPage.js</code>.</p> <p>To edit a resource:</p> <ul> <li>Build a resource state to build getter object and setter function for <code>resource</code>. <code>useState()</code> hook.</li> <li>Build a form-error state to handle <code>formErrors</code>. <code>useState()</code> hook.</li> <li>Build form references using <code>useRef()</code> for all fields.</li> <li>Build useApi from api-context.</li> <li>Build side-effect using <code>useEffect()</code> to call API, then <code>setResource()</code>. This pre-fills form.</li> </ul> <ul> <li>Use <code>&lt;Form onSubmit={onSubmit}&gt;</code> to let form know submit function.</li> </ul> <ul> <li>Build a <code>onSubmit()</code> function, that that implements <code>async</code> api <code>PUT</code> request with values from form. Within onSubmit:<ul> <li>prevent default <code>event.preventDefault()</code></li> <li>check validations - <code>if (!name)</code></li> <li>if no client side form error, do API PUT.</li> <li>if response is ok,<ul> <li>set new response and remove form errors - <code>setResource(response.body)</code>, <code>setFormErrors({})</code>.</li> <li>flash <code>flash('woo hoo..!', 'success');</code></li> <li>navigate - <code>navigate('/user')</code></li> </ul> </li> <li>else response is not okay, server returns errors<ul> <li>set errors in form, <code>setFormErrors(response.body.errors.json)</code></li> </ul> </li> </ul> </li> </ul> <p>Here is implementation of above in <code>src/pages/EditUserPage.js</code>: Edit user form</p> <pre><code>import { useState, useEffect, useRef } from 'react';\nimport Form from 'react-bootstrap/Form';\nimport Button from 'react-bootstrap/Button';\nimport { useNavigate } from 'react-router-dom';\nimport Body from '../components/Body';\nimport InputField from '../components/InputField';\nimport { useApi } from '../contexts/ApiProvider';\nimport { useUser } from '../contexts/UserProvider';\nimport { useFlash } from '../contexts/FlashProvider';\n\nexport default function EditUserPage() {\n  const [formErrors, setFormErrors] = useState({});\n  const usernameField = useRef();\n  const emailField = useRef();\n  const aboutMeField = useRef();\n  const api = useApi();\n  const { user, setUser } = useUser();\n  const flash = useFlash();\n  const navigate = useNavigate();\n\n  useEffect(() =&gt; {\n    usernameField.current.value = user.username;\n    emailField.current.value = user.email;\n    aboutMeField.current.value = user.about_me;\n    usernameField.current.focus();\n  }, [user]);\n\n  const onSubmit = async (event) =&gt; {\n    event.preventDefault();\n    const response = await api.put('/me', {\n      username: usernameField.current.value,\n      email: emailField.current.value,\n      about_me: aboutMeField.current.value,\n    });\n    if (response.ok) {\n      setFormErrors({});\n      setUser(response.body);\n      flash('Your profile has been updated.', 'success');\n      navigate('/user/' + response.body.username);\n    }\n    else {\n      setFormErrors(response.body.errors.json);\n    }\n  };\n\n  return (\n    &lt;Body sidebar={true}&gt;\n      &lt;Form onSubmit={onSubmit}&gt;\n        &lt;InputField\n          name=\"username\" label=\"Username\"\n          error={formErrors.username} fieldRef={usernameField} /&gt;\n        &lt;InputField\n          name=\"email\" label=\"Email\"\n          error={formErrors.email} fieldRef={emailField} /&gt;\n        &lt;InputField\n          name=\"aboutMe\" label=\"About Me\"\n          error={formErrors.about_me} fieldRef={aboutMeField} /&gt;\n        &lt;Button variant=\"primary\" type=\"submit\"&gt;Save&lt;/Button&gt;\n      &lt;/Form&gt;\n    &lt;/Body&gt;\n  );\n}\n</code></pre>"},{"location":"9-Drafts/mblog-notes/#follow-unfollow","title":"Follow / Unfollow","text":"<p>Send API request and <code>setIsFollower()</code> with true/false. Modify <code>src/pages/UserPage.js</code>: Follow and unfollow handlers</p> <pre><code>  const follow = async () =&gt; {\n    const response = await api.post('/me/following/' + user.id);\n    if (response.ok) {\n      flash(\n        &lt;&gt;\n          You are now following &lt;b&gt;{user.username}&lt;/b&gt;.\n        &lt;/&gt;, 'success'\n      );\n      setIsFollower(true);\n    }\n  };\n\n  const unfollow = async () =&gt; {\n    const response = await api.delete('/me/following/' + user.id);\n    if (response.ok) {\n      flash(\n        &lt;&gt;\n          You have unfollowed &lt;b&gt;{user.username}&lt;/b&gt;.\n        &lt;/&gt;, 'success'\n      );\n      setIsFollower(false);\n    }\n  };\n</code></pre>"},{"location":"9-Drafts/mblog-notes/#change-the-password","title":"Change the Password","text":"<p>To do this:</p> <ul> <li>Add new link to nav-bar</li> <li>Add new route to <code>App.js</code></li> <li>Add new page to show change password form</li> </ul> <p><code>src/components/Header.js</code>: Change password menu option</p> <pre><code>&lt;NavDropdown.Item as={NavLink} to=\"/password\"&gt;\n  Change Password\n&lt;/NavDropdown.Item&gt;\n</code></pre> <p><code>src/App.js</code>: Change password route</p> <pre><code>// add this import at the top\nimport ChangePasswordPage from './pages/ChangePasswordPage';\n\nexport default function App() {\n  ... // &lt;-- no changes to logic in this function\n\n  return (\n    ...\n\n    // add this route in the private routes section, above the \"*\" route\n    &lt;Route path=\"/password\" element={&lt;ChangePasswordPage /&gt;} /&gt;\n\n    ...\n  );\n}\n</code></pre> <p>The form to change password, this is very similar to <code>edit a resource</code> structure, except that you don't have to per-fill the form.</p> <p><code>src/pages/ChangePasswordPage.js</code>: Change password form</p> <pre><code>import { useState, useEffect, useRef } from 'react';\nimport Form from 'react-bootstrap/Form';\nimport Button from 'react-bootstrap/Button';\nimport { useNavigate } from 'react-router-dom';\nimport Body from '../components/Body';\nimport InputField from '../components/InputField';\nimport { useApi } from '../contexts/ApiProvider';\nimport { useFlash } from '../contexts/FlashProvider';\n\nexport default function ChangePasswordPage() {\n  const [formErrors, setFormErrors] = useState({});\n  const oldPasswordField = useRef();\n  const passwordField = useRef();\n  const password2Field = useRef();\n  const navigate = useNavigate();\n  const api = useApi();\n  const flash = useFlash();\n\n  useEffect(() =&gt; {\n    oldPasswordField.current.focus();\n  }, []);\n\n  const onSubmit = async (event) =&gt; {\n    event.preventDefault();\n    if (passwordField.current.value !== password2Field.current.value) {\n        setFormErrors({password2: \"New passwords don't match\"});\n    }\n    else {\n      const response = await api.put('/me', {\n        old_password: oldPasswordField.current.value,\n        password: passwordField.current.value\n      });\n      if (response.ok) {\n        setFormErrors({});\n        flash('Your password has been updated.', 'success');\n        navigate('/me');\n      }\n      else {\n        setFormErrors(response.body.errors.json);\n      }\n    }\n  };\n\n  return (\n    &lt;Body sidebar&gt;\n      &lt;h1&gt;Change Your Password&lt;/h1&gt;\n      &lt;Form onSubmit={onSubmit}&gt;\n        &lt;InputField\n          name=\"oldPassword\" label=\"Old Password\" type=\"password\"\n          error={formErrors.old_password} fieldRef={oldPasswordField} /&gt;\n        &lt;InputField\n          name=\"password\" label=\"New Password\" type=\"password\"\n          error={formErrors.password} fieldRef={passwordField} /&gt;\n        &lt;InputField\n          name=\"password2\" label=\"New Password Again\" type=\"password\"\n          error={formErrors.password2} fieldRef={password2Field} /&gt;\n        &lt;Button variant=\"primary\" type=\"submit\"&gt;Change Password&lt;/Button&gt;\n      &lt;/Form&gt;\n    &lt;/Body&gt;\n  );\n}\n</code></pre>"},{"location":"9-Drafts/mblog-notes/#password-resets","title":"Password Resets","text":"<p>This needs two new routes:</p> <ul> <li>Request reset - where user enters valid email address and requests a link.</li> <li>Reset page - when user clicks link on email, this lets reset password by entering new password twice.</li> </ul> <p>Add two new routes in, <code>src/App.js</code>: Password reset routing updates</p> <pre><code>// add these imports at the top\nimport ResetRequestPage from './pages/ResetRequestPage';\nimport ResetPage from './pages/ResetPage';\n\nexport default function App() {\n  ... // &lt;-- no changes to logic in this function\n\n  return (\n    ...\n\n    // add these routes in the public routes section\n    &lt;Route path=\"/reset-request\" element={\n      &lt;PublicRoute&gt;&lt;ResetRequestPage /&gt;&lt;/PublicRoute&gt;\n    } /&gt;\n    &lt;Route path=\"/reset\" element={\n      &lt;PublicRoute&gt;&lt;ResetPage /&gt;&lt;/PublicRoute&gt;\n    } /&gt;\n\n    ...\n  );\n}\n</code></pre> <p>Modify login page, <code>src/pages/LoginPage.js</code>: Reset password link</p> <pre><code>// add this above the registration link\n&lt;p&gt;Forgot your password? You can &lt;Link to=\"/reset-request\"&gt;reset it&lt;/Link&gt;.&lt;/p&gt;\n</code></pre> <p>Build, <code>src/pages/ResetRequestPage.js</code>: Reset request form</p> <pre><code>import { useState, useEffect, useRef } from 'react';\nimport Form from 'react-bootstrap/Form';\nimport Button from 'react-bootstrap/Button';\nimport Body from '../components/Body';\nimport InputField from '../components/InputField';\nimport { useApi } from '../contexts/ApiProvider';\nimport { useFlash } from '../contexts/FlashProvider';\n\nexport default function ResetRequestPage() {\n  const [formErrors, setFormErrors] = useState({});\n  const emailField = useRef();\n  const api = useApi();\n  const flash = useFlash();\n\n  useEffect(() =&gt; {\n    emailField.current.focus();\n  }, []);\n\n  const onSubmit = async (event) =&gt; {\n    event.preventDefault();\n    const response = await api.post('/tokens/reset', {\n      email: emailField.current.value,\n    });\n    if (!response.ok) {\n      setFormErrors(response.body.errors.json);\n    }\n    else {\n      emailField.current.value = '';\n      setFormErrors({});\n      flash(\n        'You will receive an email with instructions ' +\n        'to reset your password.', 'info'\n      );\n    }\n  };\n\n  return (\n    &lt;Body&gt;\n      &lt;h1&gt;Reset Your Password&lt;/h1&gt;\n      &lt;Form onSubmit={onSubmit}&gt;\n        &lt;InputField\n          name=\"email\" label=\"Email Address\"\n          error={formErrors.email} fieldRef={emailField} /&gt;\n        &lt;Button variant=\"primary\" type=\"submit\"&gt;Reset Password&lt;/Button&gt;\n      &lt;/Form&gt;\n    &lt;/Body&gt;\n  );\n}\n</code></pre> <p>If you have setup mail server you will get an email.</p> <p>You can get real email, or on localhost:8025 you can start email server using</p> <pre><code>python -m smtpd -n -c DebuggingServer localhost:8025\n</code></pre> <p>The link in email can be clicked to go to reset page, which is handled by route below.</p> <p>Now, lets build reset request page <code>src/pages/ResetPage.js</code>: Reset password</p> <pre><code>import { useState, useEffect, useRef } from 'react';\nimport Form from 'react-bootstrap/Form';\nimport Button from 'react-bootstrap/Button';\nimport { useNavigate, useLocation } from 'react-router-dom';\nimport Body from '../components/Body';\nimport InputField from '../components/InputField';\nimport { useApi } from '../contexts/ApiProvider';\nimport { useFlash } from '../contexts/FlashProvider';\n\nexport default function ResetPage() {\n  const [formErrors, setFormErrors] = useState({});\n  const passwordField = useRef();\n  const password2Field = useRef();\n  const navigate = useNavigate();\n  const { search } = useLocation();\n  const api = useApi();\n  const flash = useFlash();\n  const token = new URLSearchParams(search).get('token');\n\n  useEffect(() =&gt; {\n    if (!token) {\n      navigate('/');\n    }\n    else {\n      passwordField.current.focus();\n    }\n  }, [token, navigate]);\n\n  const onSubmit = async (event) =&gt; {\n    event.preventDefault();\n    if (passwordField.current.value !== password2Field.current.value) {\n        setFormErrors({password2: \"New passwords don't match\"});\n    }\n    else {\n      const response = await api.put('/tokens/reset', {\n        token,\n        new_password: passwordField.current.value\n      });\n      if (response.ok) {\n        setFormErrors({});\n        flash('Your password has been reset.', 'success');\n        navigate('/login');\n      }\n      else {\n        if (response.body.errors.json.new_password) {\n          setFormErrors(response.body.errors.json);\n        }\n        else {\n          flash('Password could not be reset. Please try again.', 'danger');\n          navigate('/reset-request');\n        }\n      }\n    }\n  };\n\n  return (\n    &lt;Body&gt;\n      &lt;h1&gt;Reset Your Password&lt;/h1&gt;\n      &lt;Form onSubmit={onSubmit}&gt;\n        &lt;InputField\n          name=\"password\" label=\"New Password\" type=\"password\"\n          error={formErrors.password} fieldRef={passwordField} /&gt;\n        &lt;InputField\n          name=\"password2\" label=\"New Password Again\" type=\"password\"\n          error={formErrors.password2} fieldRef={password2Field} /&gt;\n        &lt;Button variant=\"primary\" type=\"submit\"&gt;Reset Password&lt;/Button&gt;\n      &lt;/Form&gt;\n    &lt;/Body&gt;\n  );\n}\n</code></pre> <p>This completes build of the app.</p>"},{"location":"9-Drafts/mblog-notes/#memoization","title":"Memoization","text":"<p>Memoization is technique to optimize calls and implementing caching.</p> <p>React renders top component <code>App</code> followed by rendering child components. However, it first builds virtual DOM and then real DOM on browser. This helps in re-render, only those real DOM elements are updated which are different from re-built virtual DOM.</p> <p>Now state var can do side effect and side effect do state change, this cycle continues but settles down and then things are shared.</p> <p>You can memoization a post using a wrapper. In <code>src/components/Post.js</code>: Memoize the component</p> <pre><code>import { memo } from 'react';\n... // &lt;-- no changes to existing imports\n\nexport default memo(function Post({ post }) {\n  ... // &lt;-- no changes to function body\n});\n</code></pre> <p>Render Loops</p> <p>Sometimes cyclic dependency may introduce this and it causes high CPU usage and poor performance.</p> <p>To handle unresponsive behaviour, we can have app wide error handler (just like flask 500 or 404 handler) to report no connectivity or unresponsive behaviour. In <code>src/MicroblogApiClient.js</code>: custom error handler, add</p> <pre><code>export default class MicroblogApiClient {\n  constructor(onError) {\n    this.onError = onError;\n    this.base_url =  BASE_API_URL + '/api';\n  }\n\n  async request(options) {\n    let response = await this.requestInternal(options);\n    if (response.status === 401 &amp;&amp; options.url !== '/tokens') {\n      ... // &lt;-- no changes to retry logic\n    }\n    if (response.status &gt;= 500 &amp;&amp; this.onError) {\n      this.onError(response);\n    }\n    return response;\n  }\n\n  ... // &lt;-- no changes to the rest of the class\n}\n</code></pre> <p>This sets onError with erros sent from server. Next we need to flash this error, make this change global in ApiProvider at <code>src/contexts/ApiProvider.js</code>: Error handling</p> <pre><code>import { createContext, useContext } from 'react';\nimport MicroblogApiClient from '../MicroblogApiClient';\nimport { useFlash } from './FlashProvider';               // Added\n\nexport const ApiContext = createContext();\n\nexport default function ApiProvider({ children }) {\n  const flash = useFlash();                               // Added\n\n  const onError = () =&gt; {                                 // Added\n    flash('An unexpected error has occurred. Please try again.', 'danger');\n  };\n\n  const api = new MicroblogApiClient(onError);            // updated\n\n  // &lt;-- no changes to the returned JSX\n}\n\n... // &lt;-- no changes to the hook function\n</code></pre> <p>Now, when we use it, it make a circular dependency, api request causes error, errors build flash. Now ApiProvider component depends on the flash() function, so now API is rebuilt, this make a loop.</p> <p>To prevent this, react provides the <code>useCallback()</code> and <code>useMemo()</code> hooks to memoize functions and other values.</p> <p>In <code>src/contexts/FlashProvider.js</code>, update code to memoize flash() and hideFlash()</p> <pre><code>import { createContext, useContext, useState, useCallback } from 'react';\n\nexport const FlashContext = createContext();\nlet flashTimer;\n\nexport default function FlashProvider({ children }) {\n  const [flashMessage, setFlashMessage] = useState({});\n  const [visible, setVisible] = useState(false);\n\n  const hideFlash = useCallback(() =&gt; {               // updated\n    ... // &lt;-- no changes in the function body\n  }, []);\n\n  const flash = useCallback((message, type, duration = 10) =&gt; {\n    ... // &lt;-- no changes in the function body\n  }, [hideFlash]);\n\n  // &lt;-- no changes to the returned JSX\n}\n\n... // &lt;-- no changes to the hook function\n</code></pre> <p><code>hideFlash()</code> is moved above because now it is a dependency on <code>flash()</code>.</p> <p>The next listing shows the memoizing changes for ApiProvider. <code>src/contexts/ApiProvider.js</code>: Memoize onError and api</p> <pre><code>import { createContext, useContext, useCallback, useMemo } from 'react';\nimport MicroblogApiClient from '../MicroblogApiClient';\nimport { useFlash } from './FlashProvider';\n\nexport const ApiContext = createContext();\n\nexport default function ApiProvider({ children }) {\n  const flash = useFlash();\n\n  const onError = useCallback(() =&gt; {\n    flash('An unexpected error has occurred. Please try again later.', 'danger');\n  }, [flash]);\n\n  const api = useMemo(() =&gt; new MicroblogApiClient(onError), [onError]);\n\n  // &lt;-- no changes to the returned JSX\n}\n\n... // &lt;-- no changes to the hook function\n</code></pre> <p>Now we have removed all circular dependencies.</p>"},{"location":"9-Drafts/mblog-notes/#testing-automation","title":"Testing Automation","text":"<p>When you modify code to add new feature, you test that manually. At same time, take some time to write test case for it so that testing can be automated.</p> <p>Jest is a testing framework for React apps.</p> <p>Naming convenstion is to use, <code>.test.js</code> suffix. So tests for <code>src/App.js</code> are written in <code>src/App.test.js</code>.</p> <p>How to do test:</p> <ul> <li>You can render a component in test.</li> <li>Then read text from screen.</li> <li>the asset that text to be something</li> </ul> <pre><code>import { render, screen } from '@testing-library/react';\nimport App from './App';\n\ntest('renders learn react link', () =&gt; {\n  render(&lt;App /&gt;);\n  const linkElement = screen.getByText(/learn react/i);\n  expect(linkElement).toBeInTheDocument();\n});\n</code></pre> <p>Here, the function has this signature, <code>test( test-description , function with test logic)</code>.</p> <p>You can re-write this as,</p> <pre><code>import { render, screen } from '@testing-library/react';\nimport App from './App';\n\ntest('renders brand element', () =&gt; {\n  render(&lt;App /&gt;);\n\n  const element = screen.getByText(/Microblog/);\n\n  expect(element).toBeInTheDocument();\n  expect(element).toHaveClass('navbar-brand');\n});\n</code></pre> <p>Most of the tests will have same structure, render.. read screen and assert.</p> <p>To read screen you can use funcitons like <code>getBy...()</code>, <code>queryBy...()</code>, <code>findBy...()</code> and when you need all as list, <code>getAllBy...()</code>, <code>queryAllBy...()</code>, <code>findAllBy...()</code>.</p> <p>The expect can have <code>toBeNull()</code>, <code>toBeInTheDocument()</code> and <code>toHaveClass()</code>.</p> <p>Advanced testing, Jest allows the tests to mock timers, remote services and other external entities required by the application, so that the test runs in an isolated, controlled and reproducible environment.</p>"},{"location":"9-Drafts/mblog-notes/#production-builds","title":"Production Builds","text":"<p>Development build is slow as it facilitates debudding and reloads, production build is built for fast and small size.</p> <p>Generate build</p> <pre><code>npm run build\n</code></pre> <p>The to serve that</p> <pre><code>npx serve -s build\n</code></pre> <p>This servers the prod build to <code>localhost:3000</code> for you to test.</p> <p>The prod build is dir <code>./build</code> which has files that can be hosted on prod web server. It has <code>index.html</code> and static folder has css and js.</p> <p>Now you can run this locally using a simple python server</p> <pre><code>c:\ncd \\code\\repos\\tutorials\\react-mblog\\app2-mblog\\build\npython -m http.server --bind 127.0.0.1 3001\n</code></pre> <p>Now you can access app at http://127.0.0.1:3001</p>"},{"location":"9-Drafts/mblog-notes/#deployment","title":"Deployment","text":"<p>You can load these to any static web-server like github pages, netlify or using docker.</p> <p><code>DockerFile</code>: A simple Dockerfile for the React application</p> <pre><code>FROM nginx\nCOPY build/ /usr/share/nginx/html/\n</code></pre> <p>Build image <code>docker build -t react-microblog</code></p> <p>Run Container <code>docker run -p 8080:80 --name microblog -d react-microblog</code></p> <p>Remove containter <code>docker rm -f microblog</code></p>"},{"location":"9-Drafts/mblog-notes/#docker-compose-for-front-end-and-back-end","title":"Docker-Compose for front-end and back-end","text":"<p>You can define two service in docker compose so that it starts multiple containers, one for front end and another for backend.</p> <p><code>docker-compose.yml</code>: A Docker Compose configuration</p> <pre><code>version: '3.2'\nservices:\n  frontend:\n    build: .\n    image: react-microblog\n    ports:\n      - \"8080:80\"\n    restart: always\n  api:\n    build: ../microblog-api\n    image: microblog-api\n    volumes:\n      - type: volume\n        source: data\n        target: /data\n    env_file: .env.api\n    environment:\n      DATABASE_URL: sqlite:////data/db.sqlite\n    restart: always\nvolumes:\n  data:\n</code></pre> <p>To run <code>docker-compose up -d</code> -d is detached in background</p> <p>To stop <code>docker-compose down</code></p> <p>All above steps are:</p> <pre><code>npm run build\ndocker-compose build\ndocker-compose up -d\n</code></pre> <p>This can be tedious, another option is to use <code>npm run deploy</code> to do all in one command. For that add your scripts in <code>package.json</code></p> <p><code>package.json</code>: Custom deploy command</p> <pre><code>  \"scripts\": {\n    \"start\": \"react-scripts start\",\n    \"build\": \"react-scripts build\",\n    \"deploy\": \"npm run build &amp;&amp; docker-compose build &amp;&amp; docker-compose up -d\",\n    \"test\": \"react-scripts test\",\n    \"eject\": \"react-scripts eject\"\n  },\n</code></pre> <p>Brilliant...! All done... :)</p> <p>Nect, React Native, a framework for building React applications that run natively on Android and iOS devices.</p>"},{"location":"9-Drafts/mblog-notes/#references","title":"References","text":"<ul> <li>React Mega Tutorial</li> <li>Realpython - Python Http Server</li> <li>Freecodecamp - News React Crud App How To Create A Book Management App From Scratch</li> </ul>"},{"location":"9-Drafts/mongodb-notes/","title":"Mongo DB","text":"<p>all about no sql mongo-db, notes from certification prepration</p>"},{"location":"9-Drafts/mongodb-notes/#setup","title":"Setup","text":"<p><code>~/code/mogodb</code> - it is base for project setup and is imported to eclipse.</p> <ul> <li>added new softwares are /usr/local/mogodb, apache-maven.</li> <li>spark and front end framework, FreeMarker, is added as jar to project.</li> </ul> <ul> <li>Non relational JSON db</li> <li>Does not save in table, instead stored as JSON document.</li> <li>It does not support joins and sql.</li> <li>It does not support transactions.</li> <li>It supports indexes and secondary indexes.</li> </ul> <ul> <li>MongoD is the process which runs the mongoDB.</li> <li>Shell is js shell which connects via TCP to mongoD.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#java-driver-and-web-framework","title":"Java Driver and Web Framework","text":"<ul> <li>SparkJava and FreeMarker are framework which well use to interact with mongoDB.</li> </ul> <ul> <li>SparkJava is micro web fw to setup routes.</li> <li>FreeMarker is used for HTML views.</li> </ul> <ul> <li>runs under JVM</li> <li>all this talks to mongoDB via mongoDB java driver.</li> </ul> <ul> <li>@todo: Quick Introduction to the Mongo Shell - watch at home.</li> </ul> <ul> <li>mongod is server daemon and mongo is shell that connects to it .</li> <li>data is saved in /data/db by default</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#select-where-find","title":"Select &amp; Where <code>find()</code>","text":""},{"location":"9-Drafts/mongodb-notes/#using-findone-function","title":"Using findOne function","text":"<ul> <li>It takes first argument as document in which we have key and value.</li> <li>this acts as key = column, value = value like we give in where clause.</li> </ul> <ul> <li>the second argument to findOne is the document having keys as columns we want to see and value as true to see and false to skip.</li> </ul> <ul> <li>to use gr and lt we use a sub document in argument.</li> </ul> <ul> <li>eg. db.score.find ( { score : { $gt : 35 } } )</li> </ul> <ul> <li>db.score.find ( { score : { $gt : 35 , $lte : 60} , type : \"English\" } )</li> </ul> <ul> <li>where score &gt; 35 and score &lt;= 60 and type = \"English\";</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#introduction-to-find","title":"Introduction to find()","text":"<ul> <li>the result is returned in form of batches. say 20.</li> <li>to page through type it.</li> <li>cursor on server is open. it is closed in say 10 mins on the server.</li> <li>.pretty() makes result more readable.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#querying-using-field-selection","title":"Querying using field selection","text":"<ul> <li>the 1st arg is like where clause. filter the result on matching conditions.</li> <li>it accepts doc and that is matched for results.</li> </ul> <ul> <li>@Note: We can give key without quotes but the value has to be in quotes when writing a document.</li> </ul> <ul> <li>the 2nd arg takes what we want to see in results. by default the _id column is selected. it is analogous to select clause in rdbms.</li> </ul> <ul> <li>e.g.</li> <li> <p>db.scores.find({student:19,type:\"essay\"} , {score:true,_id:false})</p> </li> </ul>"},{"location":"9-Drafts/mongodb-notes/#querying-using-gt-and-lt","title":"Querying using $gt and $lt","text":"<ul> <li>e.g.</li> <li> <p>db.scores.find({ score: { $gt : 95  }  })</p> </li> <li> <p>db.scores.find({ score: { $gt : 95 , $lte: 96 }  })</p> </li> <li> <p>db.scores.find({ score: { $gt : 95 , $lte: 96 } , type:\"essay\"  })</p> </li> </ul>"},{"location":"9-Drafts/mongodb-notes/#inequalities-on-string","title":"Inequalities on String","text":"<ul> <li>We can use lt, gt, lte, gte but this gives result based on UTF sorting of alphabets.</li> <li>eg. { name : { $lte : \"D\" } } # it may produce absurd results as well.</li> </ul> <ul> <li>We have $type in which we match the datatype of a field. It accepts a number based on BSON type.</li> <li>e.g: name : { $type : 2 } - this gives names having string value</li> </ul> <ul> <li>We have $regex which accepts regular expressions.</li> </ul> <ul> <li>We have $exists, it accepts a key name. It returns only that document which has this key in it.</li> <li>e.g: db.emp.find ( { profession : { $exists : true } } )</li> <li>Nested content is not matched, only top level array element is matched.</li> </ul> <ul> <li>$all is is like a sub set of elements. all should be there but in any order</li> </ul> <ul> <li>$in is like any of it should match.</li> </ul> <ul> <li>We can use gt and lt in string Comparisons.</li> </ul> <pre><code>    &gt; db.people.find()\n    { \"_id\" : ObjectId(\"57befca2daf90e8c76d1910e\"), \"name\" : \"vaibhav\", \"age\" : 30 }\n    { \"_id\" : ObjectId(\"57befdbddaf90e8c76d1910f\"), \"name\" : \"neeraj\", \"age\" : 31 }\n    { \"_id\" : ObjectId(\"57bfa58cdaf90e8c76d19cc8\"), \"name\" : \"rahul\" }\n    { \"_id\" : ObjectId(\"57bfa594daf90e8c76d19cc9\"), \"name\" : \"Kashish\" }\n    { \"_id\" : ObjectId(\"57bfa59bdaf90e8c76d19cca\"), \"name\" : \"Patnam\" }\n    { \"_id\" : ObjectId(\"57bfa5a3daf90e8c76d19ccb\"), \"name\" : \"Jaspreet\" }\n    { \"_id\" : ObjectId(\"57bfa5aadaf90e8c76d19ccc\"), \"name\" : \"Ankit\" }\n    { \"_id\" : ObjectId(\"57bfa5b1daf90e8c76d19ccd\"), \"name\" : \"Hardeep\" }\n    { \"_id\" : ObjectId(\"57bfa5bcdaf90e8c76d19cce\"), \"name\" : \"Anuj\" }\n    { \"_id\" : ObjectId(\"57bfa5c0daf90e8c76d19ccf\"), \"name\" : \"Arjun\" }\n    &gt; db.people.find({name:{$lt:\"D\"}})\n    { \"_id\" : ObjectId(\"57bfa5aadaf90e8c76d19ccc\"), \"name\" : \"Ankit\" }\n    { \"_id\" : ObjectId(\"57bfa5bcdaf90e8c76d19cce\"), \"name\" : \"Anuj\" }\n    { \"_id\" : ObjectId(\"57bfa5c0daf90e8c76d19ccf\"), \"name\" : \"Arjun\" }\n    &gt; db.people.find({name:{$gt:\"D\",$lt:\"M\"}})\n    { \"_id\" : ObjectId(\"57bfa594daf90e8c76d19cc9\"), \"name\" : \"Kashish\" }\n    { \"_id\" : ObjectId(\"57bfa5a3daf90e8c76d19ccb\"), \"name\" : \"Jaspreet\" }\n    { \"_id\" : ObjectId(\"57bfa5b1daf90e8c76d19ccd\"), \"name\" : \"Hardeep\" }\n</code></pre> <ul> <li>In mongodb the name field can have numeric value as well. But with above query we will get only the result we got. name:42 for eg will not be retrieved.</li> <li>hence mongodb Comparison operator donot cross the data type boundary.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#using-regex-exists-and-type","title":"Using $regex, $exists and $type","text":"<ul> <li>exists is used to find if a certain filed is present in the document.</li> <li>type is used to find doc having value of particular type say string or number. the type code can be found from bson.org.</li> <li>regex is used to match regular expressions.</li> </ul> <pre><code>    &gt; db.people.find({age:{$exists:false}})\n    { \"_id\" : ObjectId(\"57bfa58cdaf90e8c76d19cc8\"), \"name\" : \"rahul\" }\n    { \"_id\" : ObjectId(\"57bfa594daf90e8c76d19cc9\"), \"name\" : \"Kashish\" }\n    { \"_id\" : ObjectId(\"57bfa59bdaf90e8c76d19cca\"), \"name\" : \"Patnam\" }\n    { \"_id\" : ObjectId(\"57bfa5a3daf90e8c76d19ccb\"), \"name\" : \"Jaspreet\" }\n    { \"_id\" : ObjectId(\"57bfa5aadaf90e8c76d19ccc\"), \"name\" : \"Ankit\" }\n    { \"_id\" : ObjectId(\"57bfa5b1daf90e8c76d19ccd\"), \"name\" : \"Hardeep\" }\n    { \"_id\" : ObjectId(\"57bfa5bcdaf90e8c76d19cce\"), \"name\" : \"Anuj\" }\n    { \"_id\" : ObjectId(\"57bfa5c0daf90e8c76d19ccf\"), \"name\" : \"Arjun\" }\n    { \"_id\" : ObjectId(\"57bfa78fdaf90e8c76d19cd0\"), \"name\" : 42 }\n    &gt; db.people.find({age:{$exists:true}})\n    { \"_id\" : ObjectId(\"57befca2daf90e8c76d1910e\"), \"name\" : \"vaibhav\", \"age\" : 30 }\n    { \"_id\" : ObjectId(\"57befdbddaf90e8c76d1910f\"), \"name\" : \"neeraj\", \"age\" : 31 }\n    &gt; db.people.find({name:{$type:2}}) //2 is code for string\n    { \"_id\" : ObjectId(\"57befca2daf90e8c76d1910e\"), \"name\" : \"vaibhav\", \"age\" : 30 }\n    { \"_id\" : ObjectId(\"57befdbddaf90e8c76d1910f\"), \"name\" : \"neeraj\", \"age\" : 31 }\n    { \"_id\" : ObjectId(\"57bfa58cdaf90e8c76d19cc8\"), \"name\" : \"rahul\" }\n    { \"_id\" : ObjectId(\"57bfa594daf90e8c76d19cc9\"), \"name\" : \"Kashish\" }\n    { \"_id\" : ObjectId(\"57bfa59bdaf90e8c76d19cca\"), \"name\" : \"Patnam\" }\n    { \"_id\" : ObjectId(\"57bfa5a3daf90e8c76d19ccb\"), \"name\" : \"Jaspreet\" }\n    { \"_id\" : ObjectId(\"57bfa5aadaf90e8c76d19ccc\"), \"name\" : \"Ankit\" }\n    { \"_id\" : ObjectId(\"57bfa5b1daf90e8c76d19ccd\"), \"name\" : \"Hardeep\" }\n    { \"_id\" : ObjectId(\"57bfa5bcdaf90e8c76d19cce\"), \"name\" : \"Anuj\" }\n    { \"_id\" : ObjectId(\"57bfa5c0daf90e8c76d19ccf\"), \"name\" : \"Arjun\" }\n    &gt; db.people.find({name:{ $regex: \"a\"  }}) // all that have a somewhere\n    { \"_id\" : ObjectId(\"57befca2daf90e8c76d1910e\"), \"name\" : \"vaibhav\", \"age\" : 30 }\n    { \"_id\" : ObjectId(\"57befdbddaf90e8c76d1910f\"), \"name\" : \"neeraj\", \"age\" : 31 }\n    { \"_id\" : ObjectId(\"57bfa58cdaf90e8c76d19cc8\"), \"name\" : \"rahul\" }\n    { \"_id\" : ObjectId(\"57bfa594daf90e8c76d19cc9\"), \"name\" : \"Kashish\" }\n    { \"_id\" : ObjectId(\"57bfa59bdaf90e8c76d19cca\"), \"name\" : \"Patnam\" }\n    { \"_id\" : ObjectId(\"57bfa5a3daf90e8c76d19ccb\"), \"name\" : \"Jaspreet\" }\n    { \"_id\" : ObjectId(\"57bfa5b1daf90e8c76d19ccd\"), \"name\" : \"Hardeep\" }\n    &gt; db.people.find({name:{ $regex: \"e$\"  }}) //ends with e\n    &gt; db.people.find({name:{ $regex: \"a$\"  }}) //ends with a\n    { \"_id\" : ObjectId(\"57bfa59bdaf90e8c76d19cca\"), \"name\" : \"Patnam\" }\n\n    &gt; db.people.find({name:{ $regex: \"^A\"  }}) //begins with A\n    { \"_id\" : ObjectId(\"57bfa5aadaf90e8c76d19ccc\"), \"name\" : \"Ankit\" }\n    { \"_id\" : ObjectId(\"57bfa5bcdaf90e8c76d19cce\"), \"name\" : \"Anuj\" }\n    { \"_id\" : ObjectId(\"57bfa5c0daf90e8c76d19ccf\"), \"name\" : \"Arjun\" }\n</code></pre>"},{"location":"9-Drafts/mongodb-notes/#using-or","title":"Using $or","text":"<ul> <li>$or takes in an array of documents and combines them with or conditions.</li> </ul> <ul> <li>when the value is array in document and we specify it to match in find, then only the outer array is looked in.</li> <li>no recursion occurs or inner depth arrays are matched.</li> </ul> <ul> <li>MongoDB has no sql language. Instead it has functions that have arguments passes to them.</li> </ul> <pre><code>    &gt; db.people.find({ $or : [ {name:{$regex:\"e\"}} , { age : { $exists:true } } ] });\n    { \"_id\" : ObjectId(\"57befca2daf90e8c76d1910e\"), \"name\" : \"vaibhav\", \"age\" : 30 }\n    { \"_id\" : ObjectId(\"57befdbddaf90e8c76d1910f\"), \"name\" : \"neeraj\", \"age\" : 31 }\n    { \"_id\" : ObjectId(\"57bfa5a3daf90e8c76d19ccb\"), \"name\" : \"Jaspreet\" }\n    { \"_id\" : ObjectId(\"57bfa5b1daf90e8c76d19ccd\"), \"name\" : \"Hardeep\" }\n</code></pre>"},{"location":"9-Drafts/mongodb-notes/#using-and","title":"Using $and","text":"<ul> <li>Same as we use $or.</li> </ul> <pre><code>    &gt; db.people.find({ $and : [ {name:{$regex:\"a\"}} , { name : { $gt:\"K\" } } ] });\n    { \"_id\" : ObjectId(\"57befca2daf90e8c76d1910e\"), \"name\" : \"vaibhav\", \"age\" : 30 }\n    { \"_id\" : ObjectId(\"57befdbddaf90e8c76d1910f\"), \"name\" : \"neeraj\", \"age\" : 31 }\n    { \"_id\" : ObjectId(\"57bfa58cdaf90e8c76d19cc8\"), \"name\" : \"rahul\" }\n    { \"_id\" : ObjectId(\"57bfa594daf90e8c76d19cc9\"), \"name\" : \"Kashish\" }\n    { \"_id\" : ObjectId(\"57bfa59bdaf90e8c76d19cca\"), \"name\" : \"Patnam\" }\n\n    &gt; db.people.find( {name:{$regex:\"a\"}, name : { $gt:\"K\" } } );\n</code></pre> <ul> <li>This query also gives same result as the above one. And this more efficient as well.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#querying-inside-arrays","title":"Querying inside Arrays","text":"<ul> <li>MongoDB has polymorphic find. It also evaluates for matching array elements.</li> </ul> <pre><code>    &gt; db.accounts.find().pretty();\n    { \"_id\" : ObjectId(\"57bff4973df8f8d7306e7918\") }\n    {\n           \"_id\" : ObjectId(\"57bff4d93df8f8d7306e7919\"),\n           \"name\" : \"vaibhav\",\n           \"favorites\" : [\n                   \"ice cream\",\n                   \"beer\"\n           ]\n    }\n    {\n           \"_id\" : ObjectId(\"57bff5043df8f8d7306e791a\"),\n           \"name\" : \"Neeraj\",\n           \"favorites\" : [\n                   \"beer\",\n                   \"Spring Roll\"\n           ]\n    }\n\n    &gt; db.accounts.find({favorites:\"beer\"});\n    { \"_id\" : ObjectId(\"57bff4d93df8f8d7306e7919\"), \"name\" : \"vaibhav\", \"favorites\"\n    : [ \"ice cream\", \"beer\" ] }\n    { \"_id\" : ObjectId(\"57bff5043df8f8d7306e791a\"), \"name\" : \"Neeraj\", \"favorites\" :\n    [ \"beer\", \"Spring Roll\" ] }\n    &gt;\n</code></pre> <ul> <li>Here nested contents are not matched. Only first level is looked into.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#using-in-and-all","title":"Using $in and $all","text":"<ul> <li>The $all operator matches all the elements that are specified with elements present inside the array.</li> </ul> <pre><code>    db.accounts.find({favorites : {$all : [ \"beer\", \"cheeze\" ]} })\n</code></pre> <ul> <li>The $in operator is used to filter results by having value in the values specified in $in array.</li> </ul> <pre><code>    &gt; db.accounts.find({name: {$in: [\"Sahiba\",\"Neeraj\"]}});\n    { \"_id\" : ObjectId(\"57bff5043df8f8d7306e791a\"), \"name\" : \"Neeraj\", \"favorites\" :\n    [ \"beer\", \"Spring Roll\" ] }\n    { \"_id\" : ObjectId(\"57c017fc3df8f8d7306e791b\"), \"name\" : \"Sahiba\", \"favorites\" :\n    [ \"beer\", \"Momo\", \"cheeze\" ] }\n    &gt;\n</code></pre>"},{"location":"9-Drafts/mongodb-notes/#queries-with-dot-notation","title":"Queries with dot notation","text":"<ul> <li>to match nested documents, if we specify document in nested way then it is mactched exactly.</li> <li>we cannot match one key value. Not even in reversed order.</li> </ul> <pre><code>    {\n        \"_id\" : ObjectId(\"57c06919daf90e8c76d19cd2\"),\n        \"name\" : \"Rahul\",\n        \"email\" : {\n            \"work\" : \"rahul@info.com\",\n            \"personal\" : \"rgw@live.in\"\n        }\n    }\n</code></pre> <ul> <li>to find with email we have to pass exact email doc in find clause.</li> <li>Even the order of work and personal needs to be same.</li> </ul> <ul> <li>To query one part of doc,</li> </ul> <pre><code>    &gt; db.users.find({\"email.work\":\"rahul@info.com\"});\n    { \"_id\" : ObjectId(\"57c06919daf90e8c76d19cd2\"), \"name\" : \"Rahul\", \"email\" : { \"work\" : \"rahul@info.com\", \"personal\" : \"rgw@live.in\" } }\n    &gt;\n</code></pre> <ul> <li>if we use . notation then we can match on value.</li> <li>e.g. email : <code>{ work: \"abc\", personal: \"xyz\"}</code></li> </ul> <ul> <li>then email.work : \"abc\" - this fetches and gives result, while</li> <li>email : { work: \"abc\" } - this will not match as personal is also there in doc and byte by byte match will fail.</li> </ul> <ul> <li>Suppose a simple e-commerce product catalog called catalog with documents that look like this:</li> </ul> <pre><code>    { product : \"Super Duper-o-phonic\",\n     price : 100000000000,\n     reviews : [ { user : \"fred\", comment : \"Great!\" , rating : 5 },\n                 { user : \"tom\" , comment : \"I agree with Fred, somewhat!\" , rating : 4 } ],\n     ... }\n</code></pre> <ul> <li>Write a query that finds all products that cost more than 10,000 and that have a rating of 5 or better.</li> </ul> <ul> <li>Ans:</li> </ul> <pre><code>    db.catalog.find(\n       {\n           \"price\" : {\"$gt\" : 10000},\n           \"reviews.rating\" : {\"$gte\" : 5}\n       }\n    );\n</code></pre>"},{"location":"9-Drafts/mongodb-notes/#querying-cursors","title":"Querying, Cursors","text":"<ul> <li>cur = db.sb.find();</li> <li>cur.next(); - returns next doc</li> <li>cur.hasNext(); - true if there is next doc</li> <li>cur.sort({name:-1}) - sorts by descending name order</li> <li>cur.limit(5) - limits result set to 5, database only returns 5 docs.</li> <li>cur.sort({name:-1}).limit(5) - can be combined this way.</li> <li>cur.sort({name:-1}).limit(5).skip(2) - this sorts, skips 2 and shows 3 results. this sequence is followed by db engine,</li> <li>the sort, skip and limit is sent to db and performed on server, not on cursor in the memory.</li> </ul> <ul> <li>limit and sort are processed on server side and not in memory on client side.</li> </ul> <p>Q. When can you change the behavior of a cursor, by applying a sort, skip, or limit to it? A. This can be done at any point before the first document is called and before you''ve checked to see if it is empty.</p>"},{"location":"9-Drafts/mongodb-notes/#counting-results","title":"Counting Results","text":"<pre><code>    db.abc.count();\n    db.abc.count({age : 34}); - arguments accepted are same as find();\n</code></pre>"},{"location":"9-Drafts/mongodb-notes/#updates","title":"Updates","text":"<ul> <li>its accepts two arguments, one is analogous to where clause like we pass in find command.</li> <li>The second argument is what we want to replace in the found doc. All the key:value in second arg replaces all the existing key:value in document except the _id field.</li> <li>It basically replaces wholosole document but it is dangerous.</li> <li>it replaces completely.</li> </ul> <pre><code>    &gt; db.people.update({name:\"Rahul\"},{name:\"Rahul\",age:32});\n    WriteResult({ \"nMatched\" : 1, \"nUpserted\" : 0, \"nModified\" : 1 })\n</code></pre>"},{"location":"9-Drafts/mongodb-notes/#set-unset-inc","title":"Set, Unset, Inc","text":"<ul> <li>We can use update with $set to change only a particular key:value.</li> <li>e.g. db.people.update({name:Vaibhav},{$set : {  key:value, age:30 }})</li> </ul> <ul> <li>if the field does not exist then it will be added.</li> </ul> <ul> <li>db.people.update({ name:\"Vaibhav\"}, { $inc : { \"age\" : 1 }}); - this increases age by 1, if it age does not exist then it is crated as age:1.</li> </ul> <ul> <li>e.g.     &gt; db.people.update({name:\"vaibhav\"},{$set : { \"name\":\"Vaibhav\" } });     WriteResult({ \"nMatched\" : 1, \"nUpserted\" : 0, \"nModified\" : 1 })</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#unset-command","title":"$unset Command","text":"<ul> <li>it is used to remove a particular key value from a document.</li> </ul> <p>```js db.people.update({key:value},{$unset,{key:1}});     the second key value is removed from document. <pre><code>```js\n    &gt; db.people.update({name:\"Rahul\"},{$unset:{age:1}})\n    WriteResult({ \"nMatched\" : 1, \"nUpserted\" : 0, \"nModified\" : 1 })\n    &gt; db.people.find();\n    { \"_id\" : ObjectId(\"57befca2daf90e8c76d1910e\"), \"name\" : \"Vaibhav\", \"age\" : 30 }\n    { \"_id\" : ObjectId(\"57befdbddaf90e8c76d1910f\"), \"name\" : \"neeraj\", \"age\" : 31 }\n    { \"_id\" : ObjectId(\"57bfa58cdaf90e8c76d19cc8\"), \"name\" : \"Rahul\" }\n\n    &gt; db.people.update({name:\"Rahul\"},{$unset:{age:-1}})\n    this also removes the age from the document.\n</code></pre></p>"},{"location":"9-Drafts/mongodb-notes/#using-push-pop-pull-pullall-addtoset","title":"Using $push, $pop, $pull, $pullAll, $addToSet","text":"<ul> <li>a:[1,2,3,4,5] - is and array key value</li> <li>.update({find},{$set:{\"a.2\":5}}) - updates third value in array to 5.</li> <li>$push:{a:10} - add 10 in array.</li> <li>$pop:{a:1} - removes right most element.</li> <li>$pop:{a:-1} - removes left most element.</li> <li>$pushAll:{a:[3,54,23,5]} - pushes 4 more element to the array. It duplicates element even if it exists.</li> <li>same way we have pull and pullAll operators.</li> <li>$addToSet - add element to array if it does not exist. It does not dupicate a value.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#upserts","title":"Upserts","text":"<ul> <li>Updates if record exists else inserts. So if matching values are found then it updates. else it inserts.</li> <li>e.g. db.people.update({name:\"Rahul\"}, {$set: {age:28}} , { upsert : true } );</li> <li>The last argument tells shell to insert if name:Rahul is not in the collection.</li> <li>If the matching condition is not enought to find a particular result then also mongodb will insert the new doc.</li> </ul> <pre><code>    &gt; db.people.update({age:{$gt:50}},{name:\"Sunny\"},{upsert:true})\n    WriteResult({\n        \"nMatched\" : 0,\n        \"nUpserted\" : 1,\n        \"nModified\" : 0,\n        \"_id\" : ObjectId(\"57c169edc1fc9c52ee7d6103\")\n    })\n\n    This insers a new document to the collection:\n    { \"_id\" : ObjectId(\"57c169edc1fc9c52ee7d6103\"), \"name\" : \"Sunny\" }\n</code></pre> <ul> <li>it can be used when we need to mix data from someother collection and we are not sure if the doc exists or not.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#multi-update","title":"Multi-Update","text":"<ul> <li>by default update command only update a single record even if first argument provides more than one result.</li> <li>to update multiple rows provide 3rd argument as { multi : true }</li> <li>e.g. db.people.update( {}, {$set : { title: \"Dr\" }} , { multi : true } ) # this matches every doc because of find {}.</li> </ul> <ul> <li>this happens because MongoDB provides single thread to write operation.</li> <li>if multiple operations are performed on single document then it is mutually shared and one process updates and waits for other to</li> <li>update and so on.</li> </ul> <ul> <li>In multi-update there is pause and yield mechanism that follow. The multi update updates say 4 doc and then  pause to allow other write operation to occur on the document. then again picks the doc and updates other 4 doc.</li> <li>it allows other readers and writers to operator.</li> <li>mongodb does not allow isolated transaction while these multi updates are occurring.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#remove","title":"Remove","text":"<ul> <li>first argument is same as find, the document should be passed. Must pass a doc, else all doc will be removed.</li> <li>{name: {$gt : \"M\"}} - all after m are removed.</li> </ul> <ul> <li>e.g.</li> <li> <p>db.people.remove({name:\"Anuj\"});</p> </li> <li>WriteResult({ \"nRemoved\" : 1 })</li> </ul> <ul> <li>blank removes all the records one by one.</li> <li>contrary to drop.</li> <li>db.coll.drop();</li> <li>which removes all doc at</li> </ul> <ul> <li>drop is faster and also removes other data associated with the collection.</li> </ul> <ul> <li>by remove the indexes are not removed, while drop removes the indexes as well.</li> <li>also in remove a write or read operation may see a collection with half removed records. this does not happen in case of drop.</li> <li>also in here one doc is not half removed. it is isolated and atomic to a particular read write operation.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#java-driver","title":"Java driver","text":"<ul> <li>Add dependecy in pom.xml, it adds necessary jars</li> <li>create db connection client,</li> <li>db connection handle,</li> <li>provide db name.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#java-driver-representing-documents","title":"Java Driver: Representing Documents","text":"<ul> <li>We can represent docs using BSON Document class.</li> <li>we can append to the object as many doc as we want.</li> <li>it accepts different data types as well.</li> <li>Document class has helper functions like getString(), getInteger() that convert the object to particular data tyoe and then return the value.</li> </ul> <ul> <li>$or takes in an array of documents and combines them with or conditions.</li> </ul> <ul> <li>when the value is array in document and we specify it to match in find, then only the outer array is looked in.</li> <li>no recursion occurs or inner depth arrays are matched.</li> </ul> <ul> <li>projection is used to include and exclude coloumns.</li> </ul> <pre><code>    col.find(doc)\n    .projection(doc)\n    .sort(doc)\n    .skip(20)\n    .limit(10)\n    .into(doc);\n</code></pre> <ul> <li>each of above doc can be replaced by builders.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#update-and-replace","title":"Update and replace","text":"<ul> <li>replaceOne, update is used to update the information.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#delete","title":"delete","text":"<ul> <li>deleteOne()</li> <li>deleteMany()</li> <li>find takes filter as document to provide where clause functionality.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#blog-internals","title":"Blog, internals","text":"<ul> <li>DAO is Data Access Object</li> <li>it is java class to access data of various objects. like user, session, blog etc.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#session-management","title":"session management","text":"<ul> <li>get signup</li> <li>result is singup page</li> <li>post details</li> <li>valid then writes to users, sessions table table, store cookie, redirects to welcome page</li> <li>sessions table holds new session.</li> </ul> <ul> <li>the id value in cookie in browser is same what we store in session collection.</li> </ul> <p>Week 2 ends</p> <p>Week 3 begins: Schema Design</p>"},{"location":"9-Drafts/mongodb-notes/#schema-design","title":"Schema Design","text":"<ul> <li>can keep in 3rd normal form.</li> <li>but in mongo keep in application read form.</li> <li>organise to suit application data access pattern.</li> <li>imp fatc</li> <li>pre joins/embed</li> <li>no join</li> <li>no constraints</li> <li>atomic operations but no transaction</li> <li>no declared schema, but has a similar struct in a collection,</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#transaction-in-mongodb","title":"Transaction in MongoDB","text":"<ul> <li>we have atomicity possible in MongoDB</li> <li>we can achieve Transaction by making the whole Transaction in one doc only, instead of in multiple docs using join.</li> <li>MongoDB will make sure that doc is locked and is seen only after an update.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#one-to-one","title":"One to One","text":"<ul> <li>eny one can embed in other,</li> <li>should avoid embedding if doc size grows more than 16mb</li> <li>or to avoid bulk sizing.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#one-to-many","title":"One to many","text":"<ul> <li>there should be two collections with link in id.</li> <li>in case of, \"One to Few\", the few ones should be embedded into the one.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#many-to-many","title":"Many to Many","text":"<ul> <li>have to embedd into each other as array of IDs,</li> <li>for some reasons can be embedded as well.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#benefits-of-embedding","title":"Benefits of embedding","text":"<ul> <li>High latency and high bandwidth of disk.</li> <li>so if disk spins once then we can quickly get data.</li> <li>but for one spin it takes time.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#trees","title":"Trees","text":"<ul> <li>To represent trees, add ancestors, complete list with hierarchies.</li> </ul> <p>week 3 ends, week 4</p>"},{"location":"9-Drafts/mongodb-notes/#performace","title":"Performace","text":"<p>Performace can be increased by storage engine. from 3.0 we have pluggable storage engines.</p>"},{"location":"9-Drafts/mongodb-notes/#storage-engines-introduction","title":"Storage Engines: Introduction","text":"<p>Engine allows mongodb to talk to disks:</p> <ul> <li>Mongo has pluggabel storage engines form 3.0;</li> <li>it decides what ot keep in mem and wht in disk. Since disks are slow.</li> <li>it doesnt effect communication bw servers.</li> </ul> <p>MMAP:</p> <ul> <li>Asks to read data form memory in a page. If not found then data is brought from disk.</li> <li>multile reader single writer lock</li> <li>collection le=vl locking is done.</li> <li>only one write in 1 collection</li> <li>multile writers can happen in diff coll</li> <li>in place update occurs but if size excceeds then collection is moved to a differnt place.</li> <li>so power of two sizes is used in whcih a collection is kept in mem with more size as required to grow doc.</li> <li>os makes decision for managing in mem and in disk, db does not deciede.</li> <li>the minimum record space in MongoDB 3.0 is 32 bytes.</li> <li>MMAPv1 automatically allocates power-of-two-sized documents when new documents are inserted</li> <li>MMAPv1 is built on top of the mmap system call that maps files into memory</li> </ul> <p>WiredTiger:</p> <ul> <li>It is faster</li> <li>document lvl concurrency</li> <li>lock free, optimistic concurrency</li> <li>two writes cannot happen 2gthr.</li> <li>100gb data file is brought in mem in pages.</li> <li>WT manages what can be in mem and wht can be in disk,</li> <li>in mem not compressed, in disk it is compressed.</li> <li>compresses data of doc and indexes.</li> <li>no inplace update.</li> <li>writes the data to new place. and  frees old one.</li> <li>marks old as unused and creates new.</li> <li>this allows doc lvl concurrency.</li> <li>overall faster. <code>mongod -dbpath WT -storageEngine WiredTiger</code></li> </ul> <ul> <li>new dir is required to change engine bcz it cannot read other eni=gine memory. <code>mongod -dbpath new_dir -storageEngine WiredTiger</code></li> </ul>"},{"location":"9-Drafts/mongodb-notes/#indexes","title":"indexes","text":"<ul> <li>when a collection is stored in a disk it is stored in random order.</li> <li>to find all like name = some. the scan all doc. can be million.</li> <li>this determines speed and performace.</li> </ul> <ul> <li>index is ordered set of thing,</li> </ul> <ul> <li>may be alphabetically sorted. it has pointer to physical location.</li> </ul> <ul> <li>it uses binaray search and it will take log(2) to provide the doc.</li> </ul> <ul> <li>index can be for combinarion. like naem, hair color.</li> <li>then index entry is combinarion of cols.</li> </ul> <ul> <li>it can be used for just.</li> <li>name</li> <li>name, hairColor,</li> <li>name, hairColor, DOB</li> </ul> <ul> <li>but not</li> <li>hairColor</li> <li>or DOB.</li> </ul> <ul> <li>if we change anything on doc. then bTree is updated. so writes will be slower.</li> <li>reads will be much faster.</li> </ul> <ul> <li>insert all data and then create index.</li> </ul> <ul> <li>index uses disc space too. so cant be crated for all keys,</li> </ul> <ul> <li>so 10 million reocrds can be indexed for faster reads and redice disc IO.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#creating-indexes","title":"Creating Indexes","text":"<ul> <li>10 million students with score of 4 exams.</li> </ul> <ul> <li>explain commands tells what db is doing when we query.</li> </ul> <ul> <li>db.students.explain.find({student_id:5});</li> </ul> <ul> <li>in winning plan:<ul> <li>doing a collection scan/</li> <li>findOne is faster,</li> </ul> </li> </ul> <ul> <li>db.students.createIndex({student_id:1}); - student id ascending.</li> <li>it takes a while to create an index.</li> </ul> <ul> <li>now the queries are nice and fast.</li> </ul> <ul> <li>now on explain the winning plan shows that</li> <li>it uses indexName student_id.</li> </ul> <ul> <li>db.students.explain(true).find({student_id:5});</li> <li>this executes and tells docs scanned as well.</li> <li>it gives execution stages.</li> </ul> <ul> <li>compound index:<ul> <li>db.students.explain.find({student_id:1, class_id:-1});</li> <li>this sorts the index as stated.</li> </ul> </li> </ul>"},{"location":"9-Drafts/mongodb-notes/#discovering-and-deleting-indexes","title":"Discovering and deleting indexes","text":"<ul> <li>db.students.getIndexes(); - lists indexes</li> <li>db.students.dropIndex({student_id:1}); - deletes index.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#multikey-indexes","title":"multiKey Indexes","text":"<ul> <li>these are created on arrays.</li> <li>if we have arrays liek tags.</li> <li>then index can be created for all elements in array.</li> <li>we cant combine two arrays index.</li> <li>arrays cna be combined with a single value column.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#dot-notaton-and-multi-key-index","title":"Dot Notaton and multi key index","text":"<ul> <li>to reach indide an array or embedded doc.</li> </ul> <ul> <li>to create index on array elemnt.</li> </ul> <ul> <li>db.students.createIndex({'scores.score':1});</li> <li>this takes a long time and created an index.</li> <li>it is multi key index.</li> </ul> <ul> <li>find scores.score $gt 99 with explain</li> </ul> <ul> <li>show s winningplan with index being used.</li> </ul> <ul> <li>find ppl with exam score above 99.</li> </ul> <ul> <li>db.students.find({'scores': {\\(elemMatch: {type:'exam', score:{\\)gt:99.9}}}});</li> </ul> <ul> <li>it matches for element with match at least one.</li> </ul> <ul> <li>but if we run it with explain the we see that in winnig plan:</li> </ul> <ul> <li>it finds score 99.8 to inf</li> <li>then finds docs with type exam, so it examined all document.</li> </ul> <ul> <li>so all docs were examined.</li> </ul> <ul> <li>we can make a mistake by using AND operator because it doesnot gurantee the correct result.</li> <li>explain output tell more.</li> <li>next stage in winning plan better explains.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#index-creation-options-unique","title":"Index Creation Options, Unique","text":"<p>- - no two docs can have same key if it is indexed.</p> <ul> <li>we can make a unique index.</li> </ul> <ul> <li>db.stuff.createIndex({thing:1}, {unique:true});</li> </ul> <ul> <li>this makes things column unique.</li> <li>and if the data has duplicate values then we cannot create the unique index. we get an error.</li> </ul> <ul> <li>we get duplicate key error if we inset dulplicate value.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#sparse-indexes","title":"Sparse Indexes","text":"<ul> <li>ti can be used when index key is missing in doc.</li> <li>so id we have c in few docs.</li> <li>then  unique index will be created on thise that have c value and rest will be left inseted of having null causing unique to break.</li> </ul> <ul> <li>so to create a unique index on phone numbers we can make sparse index.</li> </ul> <ul> <li>db.employees.createIndex({cell:1},{unique:true, sparse:true});</li> <li>this will make an index on and will neglect nulls.</li> </ul> <ul> <li>check using getIndexes.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#background-index-creation","title":"Background Index Creation","text":"<ul> <li>foreground is fast</li> <li>blocks all readers and writes.</li> <li>so cannot do on prod server.</li> </ul> <ul> <li>in Background</li> <li>it is slower</li> <li>dont block r/w.</li> </ul> <ul> <li>to work on replica set working set, take one out and create index on one and then rotate around to create on all. without performace bounty.</li> </ul> <ul> <li>db.students.createIndex({'scores.score':1}, {background:true});</li> <li>this will not create a r/w lock in collection.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#using-explain","title":"Using Explain","text":"<ul> <li>it does not bring data to clientt].</li> <li>it is used to see whats gonna happen.</li> </ul> <ul> <li>from 3.0 explain changed</li> <li>explain returns and explainable object.</li> <li>we can see find, updatem remove, aggregate but not</li> <li>insert.</li> </ul> <ul> <li>db.foo.explain().find()</li> <li>.update() - etc..</li> </ul> <ul> <li>we can see things like docs scanned ,  n returned</li> <li>indexes used etc.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#explain-verbosity","title":"Explain verbosity","text":"<p>- - query Planner is default. - we have execution stats and - allPlans Execution.</p> <ul> <li>these are level of output.</li> </ul> <ul> <li>var exp =  db.example.explain(\"executionStats\");</li> <li>exp.find({a: 17, b:57});</li> </ul> <ul> <li>it gives winning plan, query planner.</li> <li>in executionstats we get</li> <li>nReturned,</li> <li>executionTimeMillis.</li> <li>in each stage we ger doc returned,</li> </ul> <ul> <li>index is bound to min and max.</li> </ul> <ul> <li>index showuld be created to be used.</li> <li>all query should use atleast one index.</li> </ul> <ul> <li>so the index which is never used is waste</li> <li>and query which is not using atleast one index should be optimized.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#covered-queries","title":"Covered queries","text":"<ul> <li>query which can be looked from index only without examining the documents is called covered query.</li> </ul> <ul> <li>so if we have i,j,k as keys and we have index on them as well.</li> <li>and if we find (  { i:25, j:87} )</li> <li>then it will use index adn also use the docs.</li> <li>docs are used because we need _id filed as well/</li> <li>which is not in index and is picked from the docs.</li> <li>so avoid fetching _id and get faster results.</li> </ul> <ul> <li>also, we need to project exactly what we need and what we don;t</li> <li>if we say _id:0</li> <li>then too the docs will be scanned.</li> </ul> <ul> <li>mongodb examines docs</li> <li>we should specify which column we need and also make _id:0 to not show ID.</li> <li>then mongodb looks lesser docs. it may only look indexes</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#when-is-an-index-used-choosing-an-index","title":"When is an index used, choosing an index","text":"<ul> <li>cooses index.</li> <li>created query plans for selected.</li> <li>then the fastest plan is picked.</li> <li>winning is :</li> <li>returned all, or</li> <li>returned a threshhold sorted.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#index-sizes","title":"Index Sizes","text":"<ul> <li>we should pay attension on size of indexes</li> <li>index should fit into memory (ram) for better performace</li> <li>index size depends on storage engines</li> <li>In fact, the index size can be considerably smaller (at the cost of some CPU space) in WiredTiger with --wiredTigerIndexPrefixCompression enabled.</li> <li>size can be seen using stat command.</li> <li>e.g. db.students.stats();</li> <li>db.students.totalIndexSize();</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#number-of-index-entries-cardinality","title":"Number of Index Entries, Cardinality","text":"<ul> <li>It depends on the type of values on which index is being created.</li> <li>regular index is 1:1</li> <li>sparse index, having nulls or other values may have values less than docs</li> <li>multikey index, is one that can be on array or orther collection. it has values more than index. significantly larger.</li> <li>so on update the entire tag collection need to be built again on the disk.</li> </ul> <p>--following is not revised.</p>"},{"location":"9-Drafts/mongodb-notes/#geospatial-index","title":"Geospatial Index","text":"<ul> <li>These index 2d and 3d location indexes.</li> <li>e.g.</li> <li>'location':[x,y]</li> <li>ensureIndex({\"location\":'2d',type:1});</li> <li>find({location:{$near:[x,y]}});</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#geospatial-spherical-index","title":"Geospatial Spherical Index","text":"<ul> <li>this index is called 2dsphere index.</li> <li>it is used to represent the earth lat and longitudes.</li> </ul> <ul> <li>e.g.</li> </ul> <pre><code>db.places.find({\n    location:{\n        $near: {\n            $geometry: {\n                type: \"Point\",\n                coordiantes :[-125.256, 37.1521]\n            },\n            $maxDistance:2000\n        }\n    }\n}).pretty();\n</code></pre>"},{"location":"9-Drafts/mongodb-notes/#text-index","title":"Text Index","text":"<ul> <li>these are used to create index on text.</li> <li>if we normally pass a word in find then it will match that word completely in the document. if whole set of words match a value.</li> <li>but if we make a text index on that key. then we can find using the index find.</li> </ul> <pre><code>db.coll.createIndex({\"words\":'text'});\ndb.coll.find({$text:{$search:'dog'}});\n</code></pre> <ul> <li>it searches in logical or operator.</li> <li>that is it matches single words in the index.</li> </ul> <ul> <li>we can rank them as well using score</li> </ul> <ul> <li>e.g.</li> </ul> <pre><code>db.coll.find({$text:{$search:\"some text here\"}}, {score:{$meta: 'textScore'}}).sort({score:{$meta:'textScore'}});\n</code></pre> <p>-- following is revised.</p>"},{"location":"9-Drafts/mongodb-notes/#efficiency-of-indexes-used","title":"Efficiency of indexes used","text":"<ul> <li>Goal is to make r/w faster.</li> <li>selectivity - minimize records c=scanned.</li> <li>sorts - how sorts are handeled.</li> <li>Examinig and making indexes fast:</li> </ul> <pre><code>db.students.find({student_id:{$gt:500000}, class_id:54}).sort({student_id:1}).hint({class_id:1}).explain(\"executionstats\");\n</code></pre> <ul> <li>so in the above case 20k docs were returned but 80k docs were examined.</li> <li>to inc the efficiency we can porvide hint() to mongodb.</li> </ul> <ul> <li>we can pass index name to it so that it can use the index.</li> <li>passing the hint() after find() reduced the nScanned and made result faster.</li> </ul> <pre><code>db.grades.find({\n    \"score\":{$gte:65}\n}).sort({\"score\":1});\n</code></pre> <ul> <li>we should try to eliminate as much part of collection as possible and then further fetch the docs.</li> <li>so index should also be created in a way that first it helps in eliminating the max part and then it should sort or filter other values.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#logging-slow-queries","title":"Logging slow queries","text":"<ul> <li>to debug we can profile whats slow in our app.</li> <li>mongo logs slow queries abive 100 ms</li> <li>it comes on the mongoD log screen.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#profiling","title":"Profiling","text":"<ul> <li>writes to system profile.<ul> <li>0 off.</li> <li>1 log slow queries.</li> <li>2 log all queries.</li> </ul> </li> <li>logs to system.profile.</li> </ul> <ul> <li>2 for debugging and see traffic.</li> </ul> <ul> <li> <p>mongod --profile 1 --slowms 2</p> </li> <li>it will log slow queries taking more than 2 ms.</li> </ul> <p><code>db.system.profile.find({ns:/school.students}).sort({ts:1}).pretty();</code></p> <ul> <li>this gives all quries logged sorted by timestamp.</li> </ul> <p><code>db.system.profile.find({millis:{$gt:1}}).sort({ts:1}).pretty();</code></p> <ul> <li>this gives all queries taking more than a ms.</li> </ul> <pre><code>db.getProfilingLevel()\ndb.setProfilingLevel(1,4) // level and ms.\n</code></pre>"},{"location":"9-Drafts/mongodb-notes/#mongostat-command","title":"MongoStat command","text":"<ul> <li>like ioStat command in unix.</li> <li>gives 1sec info about ins, upd etc.</li> </ul> <ul> <li>:~ mongostat</li> <li>this is run on cmd. this gives all stats.<p>e.g.</p> </li> </ul> <pre><code>&gt;mongostat\ninsert query update delete getmore command % dirty % used flushes vsize   res qr|qw ar|aw netIn netOut conn                      time\n    *0    *0     *0     *0       0     1|0     0.0    0.0       0  2.5G 24.0M   0|0   0|0   79b    17k    1 2016-08-29T16:13:25+05:30\n    *0    *0     *0     *0       0     1|0     0.0    0.0       0  2.5G 24.0M   0|0   0|0   79b    17k    1 2016-08-29T16:13:26+05:30\n    *0    *0     *0     *0       0     1|0     0.0    0.0       0  2.5G 24.0M   0|0   0|0   79b    17k    1 2016-08-29T16:13:27+05:30\n    *0    *0     *0     *0       0     1|0     0.0    0.0       0  2.5G 24.0M   0|0   0|0   79b    17k    1 2016-08-29T16:13:28+05:30\n</code></pre> <p>these are pretty obvi.</p>"},{"location":"9-Drafts/mongodb-notes/#mongotop","title":"mongotop","text":"<p>- - it is same as unix top command. - it gives a high level view of where mongo is spending its time.</p> <ul> <li>it tell db on which time is spent most. it tells read time and write time.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#sharding","title":"Sharding","text":"<ul> <li>Splitting large collection into multiple servers.</li> <li>when we cannot get performace from a single server then we can shard.</li> <li>mongos is mongo shard that connects to shards.</li> <li>mongod has replicas.</li> <li>app talks to router, mongos.</li> <li>mongos talks to mongod on each server.</li> </ul> <ul> <li>when u can;t get performace from one server.</li> </ul> <ul> <li>insert must include shard key. should be aware fo shard key for collection</li> </ul> <ul> <li>for update if there is no shard key then the request will be broadcasted.</li> <li>multi update is broadcasted.</li> </ul> <ul> <li>update, remove and find are broadcasted to all shard.</li> </ul> <p>Week 5</p>"},{"location":"9-Drafts/mongodb-notes/#aggregation-pipeline","title":"Aggregation pipeline","text":"<p>Same as we pass in unix thru pipe.</p> <pre><code>    $project    - reshape   - 1:1\n    $match      - filter    - n:1\n    $group      - aggregate - n:1\n    $sort       - sort      - 1:1\n    $skip       - skips     - n:1\n    $limit      - limits    - n:1\n    $unwind     - denormalize 1:n\n    $out        - output    - 1:1\n</code></pre> <p>we will using all these.</p>"},{"location":"9-Drafts/mongodb-notes/#simple-agg-example-exlained","title":"Simple agg example exlained","text":"<pre><code>    db.products.aggregate([\n        {$group:\n            {\n                _id : \"$manufacturer\",\n                nProd: { $sum : 1 }\n\n            }\n        }\n\n    ]);\n</code></pre> <p>This will group by manufacturer and find number of products.</p>"},{"location":"9-Drafts/mongodb-notes/#compound-grouping","title":"Compound Grouping","text":"<p>group by more than a key.</p> <pre><code>    db.products.aggregate([\n        {$group:\n            {\n                _id : {\n                    \"manufacturer\" : \"$manufacturer\",\n                    \"category\" : \"$category\" } ,\n                nProd: { $sum : 1 }\n\n            }\n        }\n\n    ]);\n</code></pre> <p>Note: the _id in mongoDB can be a document as well.</p>"},{"location":"9-Drafts/mongodb-notes/#aggregation-expressions-overview","title":"Aggregation expressions overview","text":"<ul> <li>$sum - sum or count</li> <li>$avg</li> <li>$min</li> <li>$max</li> <li>$push - builds arrays.</li> <li>$addToSet - does not duplicates.</li> <li>$first - sort the doc and then find first from group</li> <li>$last - same but last.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#sum","title":"sum","text":"<pre><code>    db.products.aggregate([\n        {$group:\n            {\n                _id : { \"maker\" : \"$manufacturer\" },\n                Sum_Prices : { $sum : \"$price\" }\n\n            }\n        }\n\n    ]);\n</code></pre>"},{"location":"9-Drafts/mongodb-notes/#avg","title":"avg","text":"<pre><code>    db.zips.aggregate([\n        { $group : \n            {\n                \"_id\" : \"$state\",\n                \"avg_pop\" : { \"$avg\" : \"$pop\" }\n            }\n        }\n    ]);\n</code></pre> <p>this groups by state and finds avg of population.</p>"},{"location":"9-Drafts/mongodb-notes/#addtoset","title":"addToSet","text":"<pre><code>    db.zips.aggregate([{\n        $group:{\n            \"_id\":\"$city\",\n            \"postal_codes\":{\"$addToSet\":\"$_id\"}\n        }\n    }]);\n</code></pre> <ul> <li>ids are postalcodes, what it does is, it adds all postalcodes to new</li> <li>key postal_codes and groups by city.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#max","title":"max","text":"<pre><code>    db.zips.aggregate([{\n        $group:{\n            \"_id\":\"$state\",\n            \"max_pop\":{\"$max\":\"$pop\"}\n        }\n    }]);\n</code></pre> <ul> <li>this groups by state and finds maximum population.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#group-stages","title":"Group stages","text":"<p>Pass result through pipe line.</p> <pre><code>    db.fun.aggregate([\n        {$group:{\n                _id:{a:\"$a\", b:\"$b\"},\n                c:{$max:\"$c\"}\n            }\n            },\n                {$group:{\n                    _id:\"$_id.a\",\n                    c:{$min:\"$c\"}\n            }\n        }\n    ])\n</code></pre>"},{"location":"9-Drafts/mongodb-notes/#projections","title":"Projections","text":"<ul> <li>we can:<ul> <li>remove key</li> <li>add new keys</li> <li>reshape keys</li> <li>use func:<ul> <li>$toUpper</li> <li>$toLower</li> <li>$add</li> <li>$multiply</li> </ul> </li> </ul> </li> </ul> <p>-we can show results as we want to by some operations on the values of keys.</p> <pre><code>    db.products.aggregate([\n        {$project:\n            {\n                _id:0,\n                'maker':{ $toLower: \"$manufacturer\" },\n                'details' : {\n                                'category' : \"$category\" ,\n                                'price' : { \"$multiply\" : [\"$price\",10] }\n                            },\n                'item' : '$name'\n            }\n        }\n    ]);\n\n    // below in wrong query:\n    db.zips.aggregate([{\n        $project:{\n            _id:0,\n            city:{$toLower:$city}, //wrong! $city has to be in \"\" . for _id as well\n            pop:1,\n            state:1,\n            zip:$_id\n        }\n    }]);\n\n    // correct:\n\n    db.zips.aggregate([{\n        $project:{\n            _id:0,\n            city:{$toLower:\"$city\"},\n            pop:1,\n            state:1,\n            zip:\"$_id\"\n        }\n    }]);\n</code></pre>"},{"location":"9-Drafts/mongodb-notes/#match","title":"Match","text":"<p>filter, if matches doc is pushed to pipeline. pre aggregate the filter.</p> <p>e.g. filter state CA, and then do the stuff.</p> <pre><code>    db.zips.aggregate([\n        {$match:{\n            state:\"CA\"\n            }\n        },\n        { $group : { \n            _id:\"$city\",\n            population:{$sum:\"$pop\"},\n            zip_codes: { $addToSet : \"$_id\"}\n            }\n        },\n        { $project : {\n                _id:0,\n                city: \"$_id\" ,\n                population:1,\n                zip_codes:1,\n            }\n        }\n    ]);\n</code></pre> <p>Note: One thing to note about $match (and $sort) is that they can use indexes, but only if done at the beginning of the aggregation pipeline.</p>"},{"location":"9-Drafts/mongodb-notes/#sort-skip-limit","title":"Sort, Skip, Limit","text":"<ul> <li>it is disk and memory based.</li> <li>it can be done before and after grouping.</li> </ul> <pre><code>    db.zips.aggregate([\n        {$match:{\n                state:\"NY\"\n            }\n        },\n        { $group : { \n                _id:\"$city\",\n                population:{$sum:\"$pop\"}\n            }\n        },\n        { $project : {\n                _id:0,\n                city: \"$_id\" ,\n                population:1\n            }\n        },\n        { $sort: {\n                population: -1\n            }\n        },\n        { $skip: 4 },\n        { $limit: 5}\n\n    ]);\n</code></pre>"},{"location":"9-Drafts/mongodb-notes/#first-and-last","title":"First and Last","text":"<pre><code>    db.zips.aggregate([\n        { $group : { \n                _id:{state:\"$state\", city:\"$city\"},\n                population:{$sum:\"$pop\"}\n            }\n        },\n        { $sort: {\n                \"_id.state\":1,\n                \"population\":-1\n            }\n        },\n        /## group by state, get first city */\n        { $group:{\n                _id: \"$_id.state\",\n                city : { $first : \"$_id.city\" },\n                population : {$first:\"population\"}\n            }\n        },\n        {$sort:{\n                _id:1\n            }\n        };\n</code></pre>"},{"location":"9-Drafts/mongodb-notes/#limitations-of-the-aggregation-framework","title":"Limitations of the Aggregation Framework","text":"<p>100 mb limit for pipeline. 16 mb limit for python.</p> <p>in sharded system when we group by then aggregation query goes to each shard. then when we require all data, then all data goes to primary shard. so same level of scalability is not found which can be found in map reduce jobs in Hadoop.</p> <p>Week 6 :</p>"},{"location":"9-Drafts/mongodb-notes/#application-engineering","title":"APPLICATION ENGINEERING","text":"<p>durability, that data is on disk fault taulerance, what happens on crash. sharding, distribution across servers.</p>"},{"location":"9-Drafts/mongodb-notes/#write-concern","title":"Write Concern","text":"<p>DB writes pages to memory. thse are written to disk, depending on mem pressure,</p> <p>journal is log of every single thing happening on db. writes to journal as well. when journal is written then data is actually written on disk.</p> <p>When a data is persistent. we have two values that govern this, one is W and other is J.</p> <p>w=1 denotes that data is written, it can be written to memnory or to disk but no surity of it,</p> <p>j means weather or not we wait for journal to be written to disk before we continue.</p> <p>by default, j= false and w=1. j=true is that data in journal is written to disk. this gives surity that the data is persistent now.</p> <p>the operation performed in MongoDB is on memory and not on disk. (fast) journal is written periodically.</p> <p>if server crashed we may loose data. (w came back but journal is not written) disk is 100 to 1000 times slower.</p> <p>by default w=1, j=false. this means we;ll wait for write to be acknowledged but not journal to be written this is fast. lil vulnerable.</p> <p>w=1.j=true can be done by driver at db or coll level. slow. vulnerability is removed.</p> <p>w=0 is not recommended. and the write is not acknowledged.</p> <p>in replicated env we have other values of w that have significance.</p>"},{"location":"9-Drafts/mongodb-notes/#network-errors","title":"Network Errors","text":"<p>we might not get response from server about write when write happened but n/w error occured.</p> <p>in case of insert we can try again with same _id and can at max get duplicate key error.</p> <p>in update problem occurs. so for $inc we cannot determine that update occured or not.</p> <p>to avoid update we can convert update to insert. delete and insert.</p>"},{"location":"9-Drafts/mongodb-notes/#introduction-to-replication","title":"Introduction to Replication","text":"<p>availability and fault toleracne. (in case of fire.) all are mongod. replicates asynchronously to sec. sec elect, strict majority. data written to p will be asynchronously will be written to s. when p goes down then election occurs. by majority the s becomes p. then s becomes p and later when p comes up it comes as s.</p> <p>by default we have 3 replications.</p>"},{"location":"9-Drafts/mongodb-notes/#replica-set-elections","title":"Replica Set Elections","text":"<p>type of nodes: regular - arbiter (voting) - to vote in case of even nodes. delayed/regular - disaster recovery, can be an hr behind, can;t become primary node. priority 0 hidden - can;t be primary. used for analytics. p=0.</p>"},{"location":"9-Drafts/mongodb-notes/#write-consistency","title":"Write Consistency","text":"<p>write will goto p reads can goto s as well. but may be stale data. lag is not determined as sync is asynchronous.</p>"},{"location":"9-Drafts/mongodb-notes/#creating-a-replica-set","title":"Creating a Replica Set","text":"<p>in real we keep on diff phy servers. in our case we make on one server with diff dir and diff ports. 3 mondod instances are started.</p> <p>replSet rs1 : this tells that they belong to one replica set.</p> <p>mkdir -p /data/rs1 /data/rs2 /data/rs3 mongod --replSet m101 --logpath \"1.log\" --dbpath /data/rs1 --port 27017 --oplogSize 64 --fork --smallfiles mongod --replSet m101 --logpath \"2.log\" --dbpath /data/rs2 --port 27018 --oplogSize 64 --smallfiles --fork mongod --replSet m101 --logpath \"3.log\" --dbpath /data/rs3 --port 27019 --oplogSize 64 --smallfiles --fork</p> <p>To run a mongod process as a daemon (i.e. fork), and write its output to a log file, use the --fork and --logpath options.</p> <p>we also need to tie them together so that they can work in sync,</p> <p>we need to config and tell that all are associated with each other.</p> <p>config = { _id: \"m101\", members:[ {_id : 0, host : \"localhost:27017\"}, { _id : 1, host : \"localhost:27018\"}, {_id : 2, host : \"localhost:27019\"} ] };</p> <p>rs.initiate(config); rs.status();</p> <p>After starting mongod s and then tying them together we start a client using: mongo --port 27018 we don;t get a normal port. rs1.SECONDARY&gt; rs.status();</p> <p>we get above. and the result is: all nodes status. a big doc. sec, pri, sec. all nodes info comes.</p> <p>we cannot write on secondary.</p> <p>we then move to primary and insert a doc. then goto sec and find the same collection. we can't query sec'. we set</p> <p>rs.slaveOk(); then we can read form s.</p>"},{"location":"9-Drafts/mongodb-notes/#replica-set-internals","title":"Replica Set Internals","text":"<p>the replica sets have oplog. this is a log of change. the primary writes all to oplog. the secondary reads the oplog from primary and makes changes to primary.</p> <p>to see oplog on primary</p> <p>use local show collections oplog.rs is one we need. it has detail of insert just performed.</p> <p>now do: $ ps -ef | grep mongod the find process id of mongod primary/ $ kill 60494 this will bring down the primary.</p> <p>then secondary becomes primary. rs.status() shows the down server as not reachable.</p> <ul> <li>Failover and Rollback</li> <li>When p fails and secondary takes its place.</li> <li>now s may be behind and does not have some writes.</li> <li>the p gets back as s in some time.</li> <li>then p syncs with secondary to take new writes and realises that it has extra writes.</li> <li>it then rollsback those writes and saves to a file.</li> <li>this is failover and Rollback.</li> </ul> <ul> <li>Connecting to a Replica Set from the Java Driver</li> </ul> <ul> <li>When Bad Things Happen to Good Nodes</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#write-concern-revisited","title":"Write Concern Revisited","text":"<ul> <li>concerns which arise when we write to hard disk.</li> <li>w and j are the properties that govern how write will work.</li> <li>setting w=1 will wait for primary node to respond to acknowledgement of write.</li> <li>w=2 will wait for primary as well as secondary.</li> <li>w=3 will wait for 3 to respond.</li> <li>j=1 will wait for primary to write the journal to the disk.</li> <li>how long we wait is, is called wtimeout. it can be set in drivers.</li> <li>these 3, w,j and wtimeout define write concern.</li> <li>these can be set in connection, collection driver or when defining replica set.</li> <li>w:majority is used to wait until majority acknowledges the write.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#read-preferences","title":"Read Preferences","text":"<ul> <li>we usually read and write to the primary but we can set it to read from the secondary as well.</li> <li>it can be set to:</li> <li>primary - read only from primary</li> <li>primary preferred - if not available then read from secondary</li> <li>secondary - only rotate among secondaries</li> <li>secondary preferred - if not available then primary</li> <li>Nearest - sends to nearest one.</li> <li>tags - sends to tagged node.</li> <li>we can configure program to connect to secondary.</li> <li>then we can fail primary by</li> <li>rs.stepDown()</li> <li>then also the read continues with on secondary without faliure.</li> </ul> <pre><code>import pymongo\nimport time\n\nread_pref = pymongo.read_preferences.ReadPreference.SECONDARY\n\nc = pymongo.MongoClient(host=[\"mongodb://localhost:27017\",\n\"mongodb://localhost:27018\",\n\"mongodb://localhost:27019\"],\nread_preference=read_pref)\n\ndb = c.m101\nthings = db.things\n\nfor i in range(1000):\ndoc = things.find_one({'_id':i})\nprint \"Found doc \", doc\ntime.sleep(.1)\n\n\nduring execution if we stepDown primary. the read continues.\n</code></pre>"},{"location":"9-Drafts/mongodb-notes/#sharding-and-replication","title":"Sharding and Replication","text":""},{"location":"9-Drafts/mongodb-notes/#review-of-implications-of-replication","title":"Review of Implications of Replication","text":"<ul> <li>seed lists - this is info and responsibility of driver to elect primary, keep all nodes data and keep track of them all.</li> <li>write concern - concern that w,j and wtimeout determine.</li> <li>read Preferences - how we set the reads.</li> <li>errors can happen - event after replications, errors can happen and will continue to happen because of n/w failure, h/w failure etc. for this knowledge of data and where it goes in application is necessary.</li> </ul> <p>One thing to remember is that the driver will check, upon attempting to write, whether or not its write concern is valid. It will error if, for example, w=4 but there are 3 data-bearing replica set members. This will happen quickly in both the Java and pymongo drivers. Reading with an invalid readPreference will take longer, but will also result in an error. Be aware, though, that this behavior can vary a little between drivers and between versions.</p>"},{"location":"9-Drafts/mongodb-notes/#introduction-to-sharding","title":"Introduction to Sharding","text":"<ul> <li>horizontal scalabiling.</li> <li>Shard are dbs distributed.</li> <li>each shards can have replicas. these are different hosts.</li> <li>so shard s1 can have 3 replicas. R0. so s1-s5 will have 15 hosts. (5*3).</li> <li>router is calles mongos.</li> <li>it does sometimes range based sharding.</li> <li>so on Querying mongos knows where that particaular order_id will fall.</li> <li>it conncects quesries to diff hosts.</li> <li>we use range based distribution.</li> <li>done on the basis of shard key, may be order_id.</li> <li>mongos for certain order no. will send to particaular chuck.</li> <li>these chunks lives on particular shards.</li> <li>all replicas in shards are mondgod.</li> <li>If shard key is not in knowledge of mongos then the request is sent to all shards.</li> <li>As of MongoDB 2.4, we also offer hash-based sharding, which offers a more even distribution of data as a function of shard key, at the expense of worse - performance for range-based queries.</li> <li>Sharding is at db level.</li> <li>MongoS are stateless and can easily be replicated.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#building-a-sharded-environment","title":"Building a sharded environment","text":"<ul> <li>this more of a DBA task.</li> <li>we can setup 3 shards having 3 replicas in each.</li> <li>we ll  have a mongos server connected to app. it listens to port number 27017 which is default.</li> <li>the other relicas use non standard ports as these are all on same pc (hosts) and act as different hosts.</li> <li>then we have config server (3). these have information about the shards.</li> <li>so data is broken into chunks.</li> <li>sharding can be done on</li> <li>range based - it uses a range on say some id</li> <li>hash based - is uses hash algorithm to shard the data.</li> </ul> <ul> <li>below is the script to start a sharded system on local computer.</li> <li>our mongo will connect to mongos and not mongo d.</li> <li>the sh.status() gives data of sharded system.</li> </ul>"},{"location":"9-Drafts/mongodb-notes/#implications-of-sharding","title":"Implications of sharding","text":"<p>every doc shud hav shard key and it is immutable (it can;t be changed). index is req for shard key index should start with shard key if multiKey. in update, either shard key should be there or mulli update should be true. for update shard key has to be specified. else it is sent to all nodes.</p> <p>no unique index can be set unless it is part of shard key. reason for no unique is that it doesn;t know about other shards. we should choose shard key as one that we are going to use in most of our query as a key,</p>"},{"location":"9-Drafts/mongodb-notes/#sharding-and-replication_1","title":"Sharding and replication","text":"<p>they both are usually done togther. mongos connects to pimary of replica mainly for failover within shard, mongos reconnects. write concers are still there. j true or w majority are still there. they apply to each node. availability and concerns still apply.</p>"},{"location":"9-Drafts/mongodb-notes/#choosing-a-shard-key","title":"Choosing a shard key","text":"<p>it shud have sufficient cardinality (variety of values.) so that it can be put in all shards hotspoting (all requests going to one single place) shud be avoided. so inserts should be such that the inserts goto different shards e.g so username can be used as shard key. it gives nice parallelism.</p> <p>hotspoting in writes should be avoided. anything that is monotonously increasing should be avoided in shards there are $minKey and $maxKey and values within it goes into the shard. when any value is greater than highest value of $maxKey then it always goes to the highest chunk, so all inserts will goto one shard only. sharding on (vendor,order_date) is pretty well as we get lot of cardinallity.</p>"},{"location":"9-Drafts/mongodb-notes/#snippets","title":"Snippets","text":"<pre><code>### clean everything up\n\necho \"killing mongod and mongos\"\nkillall mongod\nkillall mongos\necho \"removing data files\"\nrm -rf /data/config\nrm -rf /data/shard*\n\n\n### start a replica set and tell it that it will be shard0\n\necho \"starting servers for shard 0\"\nmkdir -p /data/shard0/rs0 /data/shard0/rs1 /data/shard0/rs2\nmongod --replSet s0 --logpath \"s0-r0.log\" --dbpath /data/shard0/rs0 --port 37017 --fork --shardsvr --smallfiles\nmongod --replSet s0 --logpath \"s0-r1.log\" --dbpath /data/shard0/rs1 --port 37018 --fork --shardsvr --smallfiles\nmongod --replSet s0 --logpath \"s0-r2.log\" --dbpath /data/shard0/rs2 --port 37019 --fork --shardsvr --smallfiles\n\nsleep 5\n\n### connect to one server and initiate the set\n\necho \"Configuring s0 replica set\"\nmongo --port 37017 &lt;&lt; 'EOF'\nconfig = { _id: \"s0\", members:[\n{_id : 0, host : \"localhost:37017\" },\n{ _id : 1, host : \"localhost:37018\" },\n{_id : 2, host : \"localhost:37019\" }]};\nrs.initiate(config)\nEOF\n\n### start a replicate set and tell it that it will be a shard1\n\necho \"starting servers for shard 1\"\nmkdir -p /data/shard1/rs0 /data/shard1/rs1 /data/shard1/rs2\nmongod --replSet s1 --logpath \"s1-r0.log\" --dbpath /data/shard1/rs0 --port 47017 --fork --shardsvr --smallfiles\nmongod --replSet s1 --logpath \"s1-r1.log\" --dbpath /data/shard1/rs1 --port 47018 --fork --shardsvr --smallfiles\nmongod --replSet s1 --logpath \"s1-r2.log\" --dbpath /data/shard1/rs2 --port 47019 --fork --shardsvr --smallfiles\n\nsleep 5\n\necho \"Configuring s1 replica set\"\nmongo --port 47017 &lt;&lt; 'EOF'\nconfig = { _id: \"s1\", members:[\n{_id : 0, host : \"localhost:47017\" },\n{ _id : 1, host : \"localhost:47018\" },\n{_id : 2, host : \"localhost:47019\" }]};\nrs.initiate(config)\nEOF\n\n### start a replicate set and tell it that it will be a shard2\n\necho \"starting servers for shard 2\"\nmkdir -p /data/shard2/rs0 /data/shard2/rs1 /data/shard2/rs2\nmongod --replSet s2 --logpath \"s2-r0.log\" --dbpath /data/shard2/rs0 --port 57017 --fork --shardsvr --smallfiles\nmongod --replSet s2 --logpath \"s2-r1.log\" --dbpath /data/shard2/rs1 --port 57018 --fork --shardsvr --smallfiles\nmongod --replSet s2 --logpath \"s2-r2.log\" --dbpath /data/shard2/rs2 --port 57019 --fork --shardsvr --smallfiles\n\nsleep 5\n\necho \"Configuring s2 replica set\"\nmongo --port 57017 &lt;&lt; 'EOF'\nconfig = { _id: \"s2\", members:[\n{_id : 0, host : \"localhost:57017\" },\n{ _id : 1, host : \"localhost:57018\" },\n{_id : 2, host : \"localhost:57019\" }]};\nrs.initiate(config)\nEOF\n\n\n### now start 3 config servers\n\necho \"Starting config servers\"\nmkdir -p /data/config/config-a /data/config/config-b /data/config/config-c\nmongod --logpath \"cfg-a.log\" --dbpath /data/config/config-a --port 57040 --fork --configsvr --smallfiles\nmongod --logpath \"cfg-b.log\" --dbpath /data/config/config-b --port 57041 --fork --configsvr --smallfiles\nmongod --logpath \"cfg-c.log\" --dbpath /data/config/config-c --port 57042 --fork --configsvr --smallfiles\n\n\n### now start the mongos on a standard port\n\nmongos --logpath \"mongos-1.log\" --configdb localhost:57040,localhost:57041,localhost:57042 --fork\necho \"Waiting 60 seconds for the replica sets to fully come online\"\nsleep 60\necho \"Connnecting to mongos and enabling sharding\"\n\n### add shards and enable sharding on the test db\n\nmongo &lt;&lt;'EOF'\ndb.adminCommand( { addshard : \"s0/\"+\"localhost:37017\" } );\ndb.adminCommand( { addshard : \"s1/\"+\"localhost:47017\" } );\ndb.adminCommand( { addshard : \"s2/\"+\"localhost:57017\" } );\ndb.adminCommand({enableSharding: \"school\"})\ndb.adminCommand({shardCollection: \"school.students\", key: {student_id:1}});\nEOF\n</code></pre>"},{"location":"9-Drafts/notepad/","title":"Notepad","text":"<p>Staging area - Start with H1, later move to <code>term-notes.md</code></p>"},{"location":"9-Drafts/notepad/#android-notes","title":"Android Notes","text":"<p>ADB is utility to interact with android phone. It can install/uninstall apks. change connections etc.  All commands here, adb shell.</p> <p>Enable Developer Options &gt; USB Debugging</p> <p>adb must be installed on your mac/pc.</p> <p>Uninstall blotwares</p> <ul> <li><code>adb devices</code> see your device</li> <li><code>adb shell</code> enter phone shell</li> <li><code>pm uninstall -k --user 0 com.mipay.wallet.in</code> to use pm is pkg mgr, and uninstall an app.</li> </ul> <p>References:</p> <ul> <li>https://forum.xda-developers.com/t/uninstall-system-apps-without-root-vivo-bloatware.3817230/</li> <li>https://technastic.com/vivo-bloatware-preinstalled-apps-list/</li> </ul>"},{"location":"9-Drafts/notepad/#easy-soft-sys","title":"Easy Soft Sys","text":"<p>Color Pallet:</p> <ul> <li>Blue - #00a1e7, rgb(0,161,231) https://www.colorhexa.com/00a1e7</li> <li>Grey - #3f3f3f, rgb()</li> <li>Orange - #e74600, rgb(231,70,0)</li> </ul> <p>Font: Gill Sans Nova Extra Condensed Bold</p>"},{"location":"9-Drafts/notepad/#bookdown","title":"Bookdown","text":"<p>Quick getting started.</p> <p>Steps:</p> <ul> <li><code>mkdir bookdown</code></li> <li><code>cd bookdown/</code></li> <li><code>git clone https://github.com/seankross/bookdown-start</code></li> <li><code>cd bookdown-start/</code></li> <li><code>r</code></li> <li><code>bookdown::render_book(\"index.Rmd\")</code></li> </ul> <ul> <li>all <code># heading 1</code> are chapters.</li> <li>Add <code>Part I</code> before a chapter to make it part in a book, <code># (PART) Data Science {-}</code></li> <li><code>&gt; options(bookdown.render.file_scope = FALSE);</code> to use parts in diff directories.</li> </ul> <p>To support GitHub flavoured MarkDowm, you need to add the following line to <code>_output.yml</code> file:</p> <p><code>md_extensions: +lists_without_preceding_blankline+pipe_tables+raw_html+emoji</code></p> <p>Working on a book:</p> <ul> <li>All mds are in <code>./data_science</code> folder.</li> <li>All images are in <code>./images</code> folder.</li> <li>Add new md file to <code>./_bookdown.yml</code> file. It also has index order.</li> <li>To build and run:<ul> <li><code>r</code></li> <li><code>bookdown::render_book(\"index.Rmd\")</code></li> <li>new site availabe at <code>./docs/index.html</code></li> <li><code>quit()</code> to exit R shell</li> </ul> </li> </ul> <p>References:</p> <ul> <li>Bookdown cookbook</li> <li>Bookdown</li> <li>Rafalab dsbook</li> <li>Rafalab book source github.</li> <li>Bookdown data science notes book</li> <li>Python Visualizations in bookdown</li> <li>Using Python Environments</li> <li>Show plotly html js in Rmarkdown stackoverflow</li> <li>code options cheat sheet</li> <li>publishing on github</li> <li>pandoc markdown formats.</li> </ul>"},{"location":"9-Drafts/notepad/#digital-marketing-notes","title":"Digital Marketing Notes","text":"<p>Instagram page earning:</p> <ul> <li>original images</li> <li>regular posting</li> </ul> <p>Instagram Bot:</p> <ul> <li>Scrapper - https://towardsdatascience.com/increase-your-instagram-followers-with-a-simple-python-bot-fde048dce20d#:~:text=Open%20a%20browser%20and%20login,users%20you%20followed%20using%20the</li> <li>Post - https://www.youtube.com/watch?v=vnfhv1E1dU4</li> </ul>"},{"location":"9-Drafts/notepad/#tableau","title":"Tableau","text":""},{"location":"9-Drafts/notepad/#writeback-in-tableau","title":"Writeback in Tableau","text":"<pre><code>## Mega String\n\n\"( '\"\n+[CC interaction_ID]\n+\"', '\"\n+[CC Status]\n+\"', '\"+[CC Note]+\"', '\"+USERNAME()+\"' )\"\n\n## HideInsert\n\n[CC W InsertRun] = 0\n\n## HideReset\n\n[CC W InsertRun] = 4\n\n## IncrementAdd\n\n[CC W Incrementer]+1\n\n## zero\n\n0\n\n## One\n\n1\n\n## Blank\n\n\"\"\n\n## sheet reset\n\nSaved successfully!\nGo To Flow View \u2b9e\n\n## Seet Sumbit\n\nSubmit \u2b9f\n\n## CC Submitted\n\nWriteback proc source\n\n## Actions on form\n\nselect - reset - go to - next\n\nselect - reset - set - insertrun to 0\n\nselect - submit - set - cc mega string\n\nselect - submit - set - increment to +1\n\nselect - submit - set - insetRun 1\n\n## actions on table sheet\n\nselect - table - set - insertRun 1\n\nselect - table - set - string blank\n\nselect - table - set - id to row selected\n</code></pre>"},{"location":"9-Drafts/notepad/#d3","title":"D3","text":"<p>Add D3 library. Then specific module.</p> <ul> <li>it is collection of module that work together</li> <li>data is bounded to the selections, it join-by-index</li> <li>By default, the data join happens by index: the first element is bound to the first datum, and so on. Thus, either the enter or exit selection will be empty, or both. If there are more data than elements, the extra data are in the enter selection. And if there are fewer data than elements, the extra elements are in the exit selection.</li> <li>selectAll() data() enter() append() - to add elements, SDEA. https://observablehq.com/@d3/d3-hierarchy?collection=@d3/d3-hierarchy</li> </ul> <p>ObservableHQ:</p> <ul> <li>Live, web edit, d3 notebooks.</li> <li>markdown and JS blocks</li> <li>lots of d3 features. like counts, action buttons etc</li> <li>can make dasboard as well.</li> </ul>"},{"location":"9-Drafts/notepad/#youtube-channel-notes","title":"YouTube Channel Notes","text":"<p>Start creating a web of terms , make understand each thing, chamkao cheezo ko. makeit understnad to 6yr old guy start from docs, make reading a habit, start taking notes. math teacher lessongs, i see, i do, i ... small age learn, big understand, then decision.</p> <p>Follow:</p> <ul> <li>miguel grinberg - https://twitter.com/miguelgrinberg</li> <li>Claudio Bernasconi - https://twitter.com/CHBernasconiC</li> </ul>"},{"location":"9-Drafts/notepad/#dwbi","title":"DWBI","text":"<ul> <li>DW is creating a dimentional model of data that lets users easily ask questions.</li> <li>Data is identified as DIm and Facts then stored as star schema</li> <li>Facts are aggregatabe, or can be factless</li> </ul> <ul> <li>Dimensions have primary-key, natural-key, surrogate-key. PK is simple numerical increment.  <ul> <li>The degenerate dimension is a dimension key without a corresponding dimension table. So a order-number in fact table is a dimension key without a dimention table.</li> <li>Natural Key has a meaning, like Emp-ID, while surrogate keys are numneric that start from 1 and increment by 1. These meaning less surrogate keys should be used for join between facts and dimensions.</li> </ul> </li> </ul> <ul> <li> <p>Four Steps to do dimentional modelling</p> <ul> <li>1 - Identify the business process<ul> <li>what do you want to understand</li> <li>Eg:<ul> <li>as a retail owner, i want to customer purchases at POS, so that I can analyze products selling, stores and promotions.</li> </ul> </li> </ul> </li> </ul> <ul> <li>2 - Identify the grain<ul> <li>lowest atomic grain is best because it its highly dimensional hence gives more information</li> <li>Eg:<ul> <li>Retail - individual product on POS transaction</li> </ul> </li> </ul> </li> </ul> <ul> <li>3 - Identify the dimensions<ul> <li>they are determined automatically once we have the grain identified, if dimension breaks the grain futher then discard it or revisit grain statement.</li> </ul> </li> </ul> <ul> <li>4 - Identify the facts, anything not in same grain goes to another fact table.</li> </ul> </li> </ul> <ul> <li>Date Dimension<ul> <li>it has date attributes like <code>Date Key (PK)</code>, <code>Date</code>, <code>Day of Week</code>, <code>Holiday Indicator</code>, <code>Weekday Indicator</code>, <code>Day in Month</code>, <code>Day in Year</code>, <code>Last Day in Month Indicator</code>, <code>Week Ending Date</code>, <code>Week in Year</code>, <code>Month Name</code>, <code>Month in Year</code>, <code>Year-Month (YYYY-MM)</code>, <code>Quarter</code>, <code>Year-Quarter</code>, <code>Year</code></li> <li>date is stored separately as dimension because it can help keep date-calculations in advance. Eg, a date can have, different formats, is-weekday?, is-holiday?, week-number, day-in-year?, day-of-week and many more. it helps to keep calendar-logic in dimension rather tahn application. Can have holiday indicator. Roughtly 20years of date can be listed in 7,300 rows.</li> <li>time-of-day - it should be date-time fact in fact table to avoid explosion of date dimention, if required to keep it as dimension, it can be separate dimension as time-of-day.</li> </ul> </li> </ul> <ul> <li>Product Dimension<ul> <li>it can have attributes relate to procust like <code>Product Key (PK)</code>, <code>SKU Number (NK)</code>, <code>Product</code>, <code>Brand</code>, <code>Subcategory</code>, <code>Category</code>, <code>Department Number</code>, <code>Department</code>, <code>Package Type</code>, <code>Package Size</code>, <code>Fat Content</code>.</li> <li>it has merchandise hierarchy flattened out. Typically, individual SKUs roll up to brands, brands roll up to categories, and categories roll up to departments.</li> <li>List-Price is numeric but can be in dim as it is not additive and doesn't change on event, or is not event driven. It can be added once we know qunitity or weight at event. Or for some case it can be stored both in fact and dimension.</li> <li>a typical product dimension can have 50+ attributes and 300,000+ SKUs.</li> <li>Master File - In large grocery business, there can be a product master file where you can manage all products for all stores and then individual stores can pull a subset from it.</li> </ul> </li> </ul> <ul> <li>Store Dimension<ul> <li>It can store attributes like <code>Store Key (PK)</code>, <code>Number (NK)</code>, <code>Name</code>, <code>Street Address</code>, <code>City</code>, <code>County</code>, <code>City-State</code>, <code>State</code>, <code>Zip Code</code>, <code>Manager</code>, <code>District</code>, <code>Region</code>. You can see there is a hierarchy here.</li> </ul> </li> </ul> <ul> <li>Promotion Dimension</li> </ul>"},{"location":"9-Drafts/notepad/#home-lab","title":"Home Lab","text":"<ul> <li>Proxmox<ul> <li>Proxmox is virtualization without OS. Proxmox is itself like a OS similar to Virtual Box</li> <li>Once installed it can be accessed from web or shell.</li> <li>From Web, you can create vm, upload iso for vm, define vm hardware requirements, network configs etc.</li> <li>VM can be accessed using shell or vnc via web browser.</li> <li>Proxmox provides easy container creation with pre-built templates.</li> </ul> </li> </ul> <ul> <li>QEMU and KVM<ul> <li>This can be installed on linux, like On ubunut server (host).</li> <li>using cmd line we can craete new vms, provide configs.</li> <li>using ip:port we can open these vms on web using vnc.</li> </ul> </li> </ul> <ul> <li>Docker<ul> <li>container starts runs commands and exits, to make it run continuously, config has to be made, like container as web server.</li> <li>you can start multiple container with same image, they are all isolated and can be identical. it helps scaling.</li> </ul> </li> </ul> <ul> <li>Go to r/homelab for more.</li> </ul>"},{"location":"9-Drafts/notepad/#flask-ll","title":"Flask LL","text":"<ul> <li>has <code>.flaskenv</code></li> <li>data passsed to templates is json.</li> <li>templates, has includes having footer and nav. manin is layout,htmk. other extend it, and add <code>block content</code>.</li> <li>Req and res are all JSON API format.</li> </ul>"},{"location":"9-Drafts/notepad/#home-ubuntu-server","title":"Home Ubuntu Server","text":"<ul> <li>System disk SSD, Storage disk HDD, is good.</li> <li>Services you offer should be reliable which requires to give thoughts deeply.</li> <li>Static IP Adress can be manual or via DHCP if you have a router.</li> <li>SSH shell is real shell and if it gets terminated in between of task, the task is lost and may make system currupt. To come over this, use <code>tmux</code>.</li> <li>tmux is a virtual shell that shows real shell that keeps running until a system is running or you terminate it. A shell open in tmux and be disconnected and reconnected to begin from same place as you disconnected at.</li> <li>Never interrupt a apt upgrade process. It can damage the system.</li> <li>It is good to set important security upgrades to run automatically.</li> </ul> <ul> <li>Snap - is app package with dependencies bundles and works for any distribution. It is platform independent.<ul> <li>easy to install, updates automatically.</li> </ul> </li> </ul> <ul> <li>User Accounts<ul> <li>human accounts</li> <li>many services need account to run, they create during installation or you can create manually.</li> </ul> </li> </ul> <ul> <li>Services<ul> <li>run in a process, they have parent process to.</li> </ul> </li> </ul> <ul> <li>Power Management<ul> <li>UPS should be used. UPS comes with USB attachement, that lets ubnutu server know that it is running on battery and if battery if below a certain level you can configure server to gracefully shutdown.</li> </ul> </li> </ul> <ul> <li>Cockpit - systme management via web</li> <li>Samba - File sharing locally</li> <li>KVM/QEMU - to run VMs</li> <li>Container - LCX or Docker - Docker preferred.</li> <li>Media - jellyfin/plex on docker. Jellyfin config and settings can be on server, so that whenever new image is available on docker hub, we can pull it, while config and cache can be picker from the server. Build two folders in <code>/srv/jellyfin/{config/cache}</code>, map both of these as volume to docker. Add another volumen to container which will have media, thsi can be on server or any path on network. Make this docker container run on reboot automatically by option reboot always or via adding systemd.</li> <li>File Sharing - NextCloud</li> </ul>"}]}