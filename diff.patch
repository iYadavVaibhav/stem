diff --git a/diff.patch b/diff.patch
new file mode 100644
index 0000000..98cfbc6
--- /dev/null
+++ b/diff.patch
@@ -0,0 +1,664 @@
+diff --git a/.gitignore b/.gitignore
+index 6ca1032..e37b95c 100644
+--- a/.gitignore
++++ b/.gitignore
+@@ -1,5 +1,4 @@
+ venv
+ site
+ history.txt
+-.vscode
+-*.log
+\ No newline at end of file
++.vscode
+\ No newline at end of file
+diff --git a/docs/0-Information-Technology/python-notes.md b/docs/0-Information-Technology/python-notes.md
+index 325e720..d174288 100644
+--- a/docs/0-Information-Technology/python-notes.md
++++ b/docs/0-Information-Technology/python-notes.md
+@@ -219,9 +219,6 @@ file = open('note.txt','a')
+ file.write("quick brown")
+ file.write("munde, not fox")
+ file.close()
+-
+-# if excists
+-os.makedirs(pdf_dir, exist_ok=True)
+ ```
+ 
+ - non recursive replace`[os.rename(f, f.replace('_', '-')) for f in os.listdir('.') if not f.startswith('.')]`
+@@ -238,25 +235,6 @@ for subdir, dirs, files in os.walk(directory):
+     print(file_path, new_file_path) #rename your file
+ ```
+ 
+-- zip a folder
+-
+-  ```python
+-  import zipfile, os
+-  dir_to_zip = 'path to dir'
+-      
+-  def zipdir(path, ziph):
+-      # ziph is zipfile handle
+-      for root, dirs, files in os.walk(path):
+-          for file in files:
+-              ziph.write(os.path.join(root, file), 
+-                        os.path.relpath(os.path.join(root, file), 
+-                                        os.path.join(path, '..')))
+-
+-  with zipfile.ZipFile(dir_to_zip+'.zip', 'w', zipfile.ZIP_DEFLATED) as zipf:
+-      zipdir(dir_to_zip, zipf)
+-  ```
+-  
+-
+ 
+ 
+ ## Logging in Python
+@@ -385,14 +363,6 @@ dt.now().ctime() # Thu Oct 20 08:16:51 2022
+ end = dt.now()
+ end - start # datetime.timedelta(seconds=11, microseconds=129035)
+ delta = (end - start).seconds # 11
+-
+-# get last week start and end
+-today = datetime.date.today()
+-# my_date = datetime.date(2023,4,6)
+-my_date = today
+-start = my_date - datetime.timedelta(days=my_date.weekday(), weeks=1)
+-end = start + datetime.timedelta(days=6)
+-print(start, end)
+ ```
+ 
+ ## Testing in Python
+@@ -498,81 +468,8 @@ Install web driver
+ 
+ - visit `https://chromedriver.chromium.org/downloads` and download version same as your browser version.
+ - unzip and move `chromedriver` to `/usr/local/bin/chromedriver`
+-- selenium, beautiful-soup and pandas works best
+-
+-
+-
+-```python
+-import os, time, datetime, json
+-
+-from selenium import webdriver
+-from selenium.webdriver.common.by import By # search elem by
+-
+-from bs4 import BeautifulSoup   # to parse DOM
+-import pandas as pd             # to store data structure
+-
+-# Download drive that is compatible to your chrome installation
+-driver_path = "C:\code\chromedriver_win32\chromedriver.exe"
+-pdf_dir = r"C:\code\path-to-pdfs"
+-
+-chrome_options = webdriver.ChromeOptions()
+-
+-# print PDF to file
+-settings = {"recentDestinations": [{"id": "Save as PDF", "origin": "local", "account": ""}], "selectedDestinationId": "Save as PDF", "version": 2}
+-prefs = {'printing.print_preview_sticky_settings.appState': json.dumps(settings), "savefile.default_directory": pdf_dir}
+-#change destination to save as pdf and save in required folder
+-chrome_options.add_experimental_option('prefs', prefs)
+-chrome_options.add_argument('--kiosk-printing')
+ 
+-driver = webdriver.Chrome(driver_path, options=chrome_options)
+-
+-url_to_scrape = "https://www.something.com"
+-driver.get(url_to_scrape)
+-driver.implicitly_wait(5)
+-
+-# find by name and send keys
+-username_box=driver.find_element(by=By.NAME, value="username")
+-username_box.send_keys("some text")
+-
+-# find by x-path and click
+-driver.find_element(By.XPATH,'//*[@id="submit-button"]').click()
+-
+-# scroll to click, Can not click on a Element: ElementClickInterceptedException
+-checkbox_xpath = f"/html/body/div[4]/.../div"
+-checkbox_elem = driver.find_element(By.XPATH,checkbox_xpath)
+-driver.execute_script("arguments[0].scrollIntoView(true);", checkbox_elem)
+-
+-driver.back()
+-driver.quit() 
+-
+-# parse HTML
+-soup = BeautifulSoup(driver.page_source)
+-
+-# find all where
+-items = soup.find_all('li', attrs={'class': 'the-items'})
+-
+-# building lists of data
+-rows = []
+-for i,item in enumerate(items):
+-    row = []
+-    row.append(i) # index of elem, can be used later for traversing
+-    row.append(item.p.text)
+-    row.append(item.find_all('li')[0].text)
+-    row.append(item.find_all('li')[1].text)
+-    row.append(item.find_all('li')[2].text)
+-    row.append(item.h3.text)
+-    rows.append(row)
+-
+-# build DataFrame
+-df = pd.DataFrame(columns=['id','date_','region','strength','source','title'], data=rows)
+-```
+-
+-
+-- Links
+-  - <https://realpython.com/modern-web-automation-with-python-and-selenium/>
+-  - [Kiwidamien Github - Webscraping Beyond Beautifulsoup And Selenium](https://kiwidamien.github.io/webscraping-beyond-beautifulsoup-and-selenium.html)
+-  - [Beautiful Soup 4 Readthedocs - En Latest Index](https://beautiful-soup-4.readthedocs.io/en/latest/index.html)
+-  - [Stackoverflow - PDF printing from Selenium with chromedriver](https://stackoverflow.com/q/59893671/1055028)
++More - <https://realpython.com/modern-web-automation-with-python-and-selenium/>
+ 
+ 
+ 
+diff --git a/docs/1-Software-Engineering/cs-se-basics.md b/docs/1-Software-Engineering/cs-se-basics.md
+index c8782f3..57b49dc 100644
+--- a/docs/1-Software-Engineering/cs-se-basics.md
++++ b/docs/1-Software-Engineering/cs-se-basics.md
+@@ -53,7 +53,7 @@ active threads, sometimes sharing resources such as memory or file
+ handles. Multithreaded web servers start a pool of threads and
+ select a thread from the pool to handle each incoming request.
+ 
+-## OOPS
++### OOPS
+ 
+ - Object is a Class and has
+   - `attributes` - variables
+diff --git a/docs/1-Software-Engineering/devops.md b/docs/1-Software-Engineering/devops.md
+deleted file mode 100644
+index a0b1a0d..0000000
+--- a/docs/1-Software-Engineering/devops.md
++++ /dev/null
+@@ -1,105 +0,0 @@
+----
+-date: 2023-05-03
+----
+-
+-# DevOps
+-
+-It is a methodology that brings, development, QA and IT operations close together, automated and test driven, container based to have isolated similar environment so that the changes are minimal and are tested and hence don’t break.
+-
+-## CI/CD Pipeline
+-
+-- CI/CD stands for Continuous Integrationa and Continuous deployment.
+-- it is DevOps methodology to being IT and Ops together.
+-- It is a series of steps to deliver new version of software. Improves delivery throughout SDLC (software development life cycle) which is development, testing, production and monitoring. These steps can be automated to make it error free and fast.
+-- Steps may include - compiling code, unit tests, code analysis, security and binaries creations. And/or packaging code into container image.
+-- benefits, includes early integrating and testing, enhancing developer productivity, accelerating delivery, and finding/fixing bugs faster.
+-- commit to production is continuous and automated.
+-- CI/CD pipelines are completely tailor-made based on the needs and requirements and could have multiple stages and jobs, and could be complex and comprehensive.
+-
+-```mermaid
+-graph LR
+-
+-Build --> Test --> Merge --> Release[Automatically Release to Repository] --> Deploy[Automatically Deploy to Production]
+-
+-subgraph a[Continuous Integration]
+-  Build
+-  Test
+-  Merge
+-end
+-
+-subgraph b[Continuous Delivery]
+-  Release
+-end
+-
+-subgraph c[Continuous Deployment]
+-  Deploy
+-end
+-```
+-
+-- CI-CD has increased over years because on cloud-native development it is much more efficient way and is more required way. Compared to traditional Virtual-Machine deployment where it could had been left to be done manually
+-
+-- CI-CD Implementation
+-  - Typically building a CI/CD pipeline consists of the following phases/stages.
+-    - Code: Checked into the repository.
+-    - Build: Build is triggered and deployed in a test environment.
+-    - Test: Automated tests are executed.
+-    - Deploy: Code is deployed to stage, and production environments.
+-
+-## Containers
+-
+-- Containers are **packages of code** together with **necessary elements** (like runtimes, libraries) required to run a software on any environment.
+-- application is **abstracted** from environmant on which they run. It makes software run anywhere, be it on-prem, cloud or personal-laptop. container is packaged in a way that it can run on any OS and it makes **shared use** of resources like CPU, Memory, Storage and Network at OS level.
+-- **Separation** of Responsibilities - if you use containers, developers only code and containerize without worrying about deploment env, IT-Ops only deploy container without worrying about version, dependencies, OS-requirements.
+-- Compared to Virtual-Machines, Containers are **lightweight**, use less resource and virtualize at the **OS level** while VMs virtualize at the hardware level, use more resouse and are heavy.
+-
+-
+-- [Cloud Google - Learn What Are Containers](https://cloud.google.com/learn/what-are-containers)
+-
+-## Kubernetes
+-
+-- lets you **manage containers**
+-- automated container orchestration project
+-- manages containers, machine and services
+-- improves reliability and reduces time on devops
+-- `Google Kubernetes Engine` (GKE) - Is Google cloud kubernetes **service**
+-- `Kubernetes cluster` is a **set of nodes** that run containerized applications.
+-- `Edge computing` is a distributed computing paradigm that brings **computation and storage closer** to the sources of data. Often called 'The Edge' or 'at the edge'. Good for time-sensitive data.
+-
+-## Docker
+-
+-- lets you **build container**
+-- it  is a set of platform as a service products that use OS-level virtualization
+-- Docker lets you put everything you need to run your application into a box that can be stored and opened when and where it is required
+-- `Docker Image` is read-only immutable **template** that defines the layout of container.
+-- `Docker Container` is runtime instance of Docker Image. Created using `docker run` command. It runs on Docker Engine.
+-- `Docker Engine` is the software that hosts (runs) the containers. it is container runtime.
+-
+-## Jenkins
+-
+-- open source **automation server** that facilitates **automating CI** (Continuous Integration) and DevOps by automating build, test, deploy.
+-- it is orchestration tool, that manages 'chain of actions' to acieve CI.
+-- it is used to implement CI/CD workflows as pipelines.
+-- it is written in Java
+-- automation reduces time, minizes error, makes release frequent.
+-
+-- CI - on commit, code is build, then tested. if test is passed, build is tested for deployment. if deployment is successful on UAT, code is pushed to PROD.
+-- Jobs (collection of steps) is called stages.
+-- Alternatives - `Github Actions`
+-
+-## Github Actions
+-
+-- GitHub Actions is a continuous integration and continuous delivery (CI/CD) platform that allows you to automate your build, test, and deployment pipeline.
+-- You can create workflows that has jobs, and its trigger.
+-- on an event, job(s) gets triggered, that has steps, which can be actions or script to execute.
+-
+-- [Docs Github - Actions Learn Github Actions Understanding Github Actions](https://docs.github.com/en/actions/learn-github-actions/understanding-github-actions)
+-
+-## Chef DevOps
+-
+-- for Automating Infrastructure Management
+-- Chef is an automation tool that provides a way to define infrastructure as code. Infrastructure as code (IC) simply means that managing infrastructure by writing code.
+-
+-## AWS CloudFormation
+-
+-- it is a service provided by Amazon Web Services that enables users to model and manage infrastructure resources in an automated and secure manner. Using CloudFormation, developers can define and provision AWS infrastructure resources using a JSON or YAML formatted Infrastructure as Code template.
+-
+diff --git a/docs/1-Software-Engineering/linux-terminal.md b/docs/1-Software-Engineering/linux-terminal.md
+index d2e1ba2..63ae536 100644
+--- a/docs/1-Software-Engineering/linux-terminal.md
++++ b/docs/1-Software-Engineering/linux-terminal.md
+@@ -219,7 +219,24 @@ Virtual box add on:
+ - `sudo apt update`
+ - `sudo apt install virtualbox-guest-dkms virtualbox-guest-x11 virtualbox-guest-utils`
+ 
++## Enable SSH and access from remote
+ 
++- On ubuntu server or desktop
++
++    ```sh
++    sudo apt install openssh-server # install ssh
++    sudo systemctl status ssh  # view status
++    sudo ufw allow ssh    # Ubuntu ships with a firewall configuration tool called UFW
++    ip a  # get IP address, something like 10.0.2.15
++    ```
++
++- on virtual box [enable network port forwarding on virtual box](https://www.makeuseof.com/how-to-ssh-into-virtualbox-ubuntu/#:~:text=Step%202%3A%20Configuring%20the%20VirtualBox%20Network)
++
++- on remote `ssh -p 2222 username@10.0.2.15`
++
++- Links
++  - <https://www.makeuseof.com/how-to-ssh-into-virtualbox-ubuntu/>
++  - <https://linuxize.com/post/how-to-enable-ssh-on-ubuntu-20-04/?utm_content=cmp-true>
+ 
+ ## Linux Ways
+ 
+diff --git a/docs/2-Data-Engineering/data-architecture.md b/docs/2-Data-Engineering/data-architecture.md
+index ce82cae..facb5ec 100644
+--- a/docs/2-Data-Engineering/data-architecture.md
++++ b/docs/2-Data-Engineering/data-architecture.md
+@@ -87,7 +87,3 @@ In Data Modelling, Logical Model is conceptual (pen & paper), focus on business
+ 
+ It implemets logical model, with variations based on system parameters like memory, disk, network and software type.
+ 
+-
+-## Multi Dimentional Modelling
+-
+-BI developers create cubes to support fast response times, and to provide a single data source for business reporting.
+diff --git a/docs/0-Information-Technology/data-frameworks-tools.md b/docs/2-Data-Engineering/data-frameworks-tools.md
+similarity index 100%
+rename from docs/0-Information-Technology/data-frameworks-tools.md
+rename to docs/2-Data-Engineering/data-frameworks-tools.md
+diff --git a/docs/2-Data-Engineering/data-solutions.md b/docs/2-Data-Engineering/data-solutions.md
+index 312b5af..2a4b7f5 100644
+--- a/docs/2-Data-Engineering/data-solutions.md
++++ b/docs/2-Data-Engineering/data-solutions.md
+@@ -1,11 +1,6 @@
+----
+-description: Data Solutions
+-date: 2022-09-05
+----
+-
+ # Data Solutions
+ 
+-_Here are all the "conceptual" notes related to data soulutions, archirecture and engineering. It can have links to practical notes._
++*Here are all the "conceptual" notes related to data soulutions, archirecture and engineering. It can have links to practical notes.*
+ 
+ ## Data Strategy
+ 
+@@ -31,32 +26,10 @@ C --> D{Data Strategy}
+ D -->|Roadmap| E[(Target\nData State)] --> F(Data-driven\ndecision making)
+ ```
+ 
+-## Data Lifecycle
+-
+-- Generation
+-- Collection
+-- Storage
+-- Processing - integration, cleaning, reduction, transformation
+-- Management
+-- Analysis - Clustering, Regression, Forecasting, Prediction
+-- Visualization - Interpretation
+-- Decision Making
+-- Destruction
+-
+-```mermaid
+-graph LR;
+-
+-a[data collection \n or generation] --> b[data storage] --> c[data processing] --> d1[data analysis \n or visualization] --> e[decision making]
+-
+-subgraph governance
+-b
+-c
+-end
+-```
+-
+ ## Data Architecture
+ 
+ 
++
+ *Now that you have a strategy with known challenges and a roadmap to target state, it is time to build the architecture and do the engineering work aligned to roadmap to rach the target state.*
+ 
+ Data Architecture defines the **blueprint** for managing data from **collection** to **storage** and **transformation** to **consumption**. It is base foundation to support business objectives. It is essential to determine the sharp and quick tools that solve the purpose.
+diff --git a/docs/2-Data-Engineering/data-testing.md b/docs/2-Data-Engineering/data-testing.md
+deleted file mode 100644
+index ba20cd3..0000000
+--- a/docs/2-Data-Engineering/data-testing.md
++++ /dev/null
+@@ -1,12 +0,0 @@
+----
+-date: 2023-05-03
+----
+-
+-# Data Testing
+-
+-With incresed data focused apps, it is improtant to have data fully tested
+-
+-- great expectations
+-  - is python library to test data
+-
+-
+diff --git a/docs/2-Data-Engineering/pandas.md b/docs/2-Data-Engineering/pandas.md
+deleted file mode 100644
+index 5171ddb..0000000
+--- a/docs/2-Data-Engineering/pandas.md
++++ /dev/null
+@@ -1,12 +0,0 @@
+-# Pandas
+-
+-
+-
+-```python
+-df['date'] = pd.to_datetime(df['date_'], format='%d %b %Y')
+-```
+-
+-- Links:
+-  - <https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf>
+-  - [Towardsdatascience - 15 Ways To Create A Pandas Dataframe](https://towardsdatascience.com/15-ways-to-create-a-pandas-dataframe)
+-  
+diff --git a/docs/3-Management-&-Strategy/ajile-sprint-scrum.md b/docs/3-Management-&-Strategy/ajile-sprint-scrum.md
+index d97eab3..6d7b2ca 100644
+--- a/docs/3-Management-&-Strategy/ajile-sprint-scrum.md
++++ b/docs/3-Management-&-Strategy/ajile-sprint-scrum.md
+@@ -2,76 +2,25 @@
+ 
+ _all about agile, scrum, sprints_
+ 
+-Agile is **methodology** that helps you move away from waterfall methodology of software development. Agility is a **mindset**. It helps **adapt** to changes which are inevital in software development.
+ 
+-## Agile Approach
+-
+-It is **interactive** and **incremental**, it not that you need to develop like manufacturing unit. It lets you develop interactivily with business users and lets you do incremental developments with entire development cycle involved in each increment and using **best practices** like TDD, XP, CI CD. Making flat structure and having a **collective ownership**. Having **customer satisfaction** as top priority. Keeping **people over process**, and always **welcome change** as it is inevitable. **Frequent feedback** from business people and developer is required and they **work daily together**. The measure is working software and **value** it brings. All this can be facilitated by Scrum.
+-
+-Following shows small batch of development lifecycle
+-
+-```mermaid
+-graph LR
+-subgraph s1[Sprint 1]
+-r1[Requirements] --> a1[Analysis-Design] --> d1[Development] --> t1[Test] --> m1[Deployment-Maintenance]
+-end
+-
+-subgraph s2[Sprint 2]
+-r2[Requirements] --> a2[Analysis-Design] --> d2[Development] --> t2[Test] --> m2[Deployment-Maintenance]
+-end
+-
+-subgraph s3[Sprint 3]
+-r3[Requirements] --> a3[Analysis-Design] --> d3[Development] --> t3[Test] --> m3[Deployment-Maintenance]
+-end
+-
+-s1 --Release 1--> s2 --Release 2--> s3 --Release 3--> fr[Final Product]
+-
+-```
+-
+-## Scrum
+-
+-Scrum is a framework that facilitates development in an agile way, it is not a methodology. Scrum has three pillars on which Scrum values are defined. Empiricism – don't predict but keep adjusting based on situations. Scrum makes it possible. Feature over document.
+-
+-The three pillars are:
+-
+-- Transparency - be transaparent with work and team members. share progress and artifacts
+-- Inspection - have boards, dashboards ready to inspect
+-- Adaptation - adapt to change based on inspection, what needs to be corrected.
+-
+-```mermaid
+-graph TD;
+-
+-subgraph s1[Scrum - The three Pillars TIA]
+-  Transparency
+-  Inspection
+-  Adaptation
+-end
+-
+-s1 --> Focus
+-s1 --> Commitment
+-s1 --> Courage
+-s1 --> Openness
+-s1 --> Respect
+-
+-```
+-
+-## Scrum Roles
+-
+-Three team roles – product owner, scrum master, developers
+-
+-- **Product Owner**
+-  - A person who created backlog and shows vision of stakeholder
+-  - Build backlog and prioritise
+-
+-- **Development Team** – 3-9 members, flat structure, all own everything, collective ownership
+-
+-- **Scrum Master** – make product owner and developers align to scrum practise, solves conflicts, coaches individuals to implement scrum
+-
+-## Scrum Events
++- Retrospective
++  - What did we do well?
++  - What should we have done better?
++  - actions to take based on "What should we have done better"
++  - actions taken from last retro actions? else carry them
++  - Learnings
++    - don't under estimate tasks
++    - keep buffer capacity for meetings/PR-requests
+ 
+-- **Sprint** – is time bound container event
++- Backlog Grooming / Refinement
++  - Break stories into smaller **tasks**
++  - Tasks have "Definition of Ready" DoR - covers requirements coming into the sprint
++  - Tasks are **prioritized**, estimated
++  - Tasks may get assigned
++  - 1-2 hour productive meeting
++  - [link](https://www.productplan.com/glossary/backlog-grooming/)
+ 
+-- **Sprint Planning** – what to work on and how
++- Sprint Planning
+   - Ahead of our sprint planning:
+     - Please update your capacity for the next sprint. link
+     - Please create, estimate and assign tasks with "definition of done" DoD
+@@ -89,155 +38,11 @@ Three team roles – product owner, scrum master, developers
+     - Estimated and assigned
+     - Reestimate tasks carried forward
+ 
++Backlog Grooming vs Sprint Planning
+ 
+-- **Daily Scrum** – keep on track
+-
+-- **Sprint Review** – reviews work
+-
+-- **Sprint Retrospective** – discuss to be more effective
+-  - What did we do well?
+-  - What should we have done better?
+-  - actions to take based on "What should we have done better"
+-  - actions taken from last retro actions? else carry them
+-  - Learnings
+-    - don't under estimate tasks
+-    - keep buffer capacity for meetings/PR-requests
+-
+-- **Backlog Grooming / Refinement** (unofficial event) – break large items into small implementable items
+-  - Break stories into smaller **tasks**
+-  - Tasks have "Definition of Ready" DoR - covers requirements coming into the sprint
+-  - Tasks are **prioritized**, estimated
+-  - Tasks may get assigned
+-  - 1-2 hour productive meeting
+-  - [link](https://www.productplan.com/glossary/backlog-grooming/)
+-
+-- **Backlog Grooming vs Sprint Planning**
+-  - Scope - BG looks at entire project for months, SP looks at near future for weeks
+-  - Grain - BG breaks into tasks, SP breaks in to sub-tasks
+-  - Detail - BG adds DoR, SP adds DoD
+-
+-## Scrum Artifacts
+-
+-They are either done or not done
+-
+-- **Product backlog** – has large and small tasks, small can be picked, large are broken in grooming
+-  - Requirements
+-  - Enhancement requests
+-  - Defects
+-  - User stories
+-  - New feature
+-
+-- **Sprint backlog**
+-
+-- **Product increment** – done version, product itself, has value and is usable, not necessarily a release but is polished enough to be shippable.
+-
+-## Excellence in Development
+-
+-Excellence in development help align with scrum principles like flat structure, collaborative ownership, welcoming change and frequent releases. One such methodology is **Extreme Programming** (**XP**) . It is standards that lets program effectively. Agile team combine XP programming with scrum to be highly productive. It has following ways of working:
+-
+-- **Execution** – small tasks in scrum, whole team is accountable, readily changeable
+-
+-- **Incremental Design** – not all at once, by one by one, refactoring code. Code 10 mins and run test – it gives quick feedback on quality.
+-
+-- **Pair Programming** – two individual on one machine,  one types another reviews and suggested and they swap
+-  - Instant peer review, improved code quality
+-  - Knowledge sharing – helps become T-shape knowledge than I-shaped
+-  - Inclusiveness, more interactions, less distractions.  
+-
+-- **Test Driven Development** - TDD
+-  - do not code until you fail a test
+-  - First write a test to fail, then code to pass the test, then improve the test to fail and complete the code to pass. Repeat it.
+-  - Think of test before implementation
+-  - There are tools to mock dependencies to function, it makes tests easy.
+-  - Coverage is good to be 100%.
+-  - Removes bugs in monolithic code, in early stage
+-
+-
+-```mermaid
+-graph LR;
+-
+-a[write a failing test] --> b[make the test pass] --> c[refactor] --> a
+-
+-```
+-
+-
+-- **DevOps**
+-  - Problem – release bringing changes that can break prod. Agile brings frequent releases, hence frequent break in prod.
+-  - Solution – a methodology that brings, development, QA and IT operations close together, automated and test driven, container based to have isolated similar environment so that the changes are minimal and are tested and hence don’t break.
+-  - Continuous integration – commit frequently, trigger build and test automatically to identify risks
+-  - Continuous delivery and deployment – code is delivered and deployed continuously in prod
+-  - Devops and Agile are mindset shift that remove hand-off in teams and bring them together with collective responsibility using automation in tests and builds.
+-
+-
+-```mermaid
+-graph LR;
+-
+-code --> build --> test --> release --> deploy --> operate --> monitor --> plan --DevOps--> code
+-
+-```
+-
+-## User Story / Tasks
+-
+-- **User Story** - it is user requirement in sticky note – having requirement in form "As a … I want… so that I .. . Done when..", where
+-  - `As a..` - user role who will be benefited
+-  - `I want...` - what feature or outcome
+-  - `So that I...` – reason why this needs to be done
+-  - `Done when...` - acceptance criteria
+-  - User story is a promise, not a contract. It can be small or large task or an epic.
+-
+-- **Epic** – large user story, a process to large to be estimated
+-
+-- **Themes** – groups epic with tags
+-
+-- **Estimations**
+-  - absolute it days, hours
+-  - relative is estimating based on story relative to other stories. Use fibrinocci – 1,2,3,5,8,13. or exponential scale. 1,2,4,8,16. Or t-shirt size. Xs,s,m,l,xl.
+-  - Planning poker – lets users share a score. Have discussion based on score, why 3 or 8. Play again to get closer score. Discuss again, until you get same score from team members
+-
+-## Agile reports
+-
+-- **Burndown** – work left and time to do
+-
+-- **Burnup** – work done and time to do
+-
+-- **Cumulative  Flow Diagram** – CFD – shows work done by state – to-do, in progress, done. Mostly used in Kanban, show bottleneck like acceptance is taking more time or developing, or delivering.
+-
+-## Kanban
+-
+-- Kanban – lean manufacturing principles like cars. Work process management methodology
+-  - Little’s law = work in progress L = completion rate ($\lambda$) X cycle time (W)
+-  - $L = \lambda \times W$
+-  - Visual mode to track.
+-  - WIP is limited as we are not good multi-taskers. Every stage has a WIP limit. Like you can have on 2 tasks in progress, or 4 in review, the other tasks can come in only if the previous ones are done.
+-
+-- [ ] How to use Kanban or Agile for personal management.
+-
+-## Jira - Atlassian
+-
+-Jira is a work management tool. We can create `Project`, which can have `issues`. Issues can be in `backlog` or can be part of `sprint`.
+-
+-**Issues** mostly have:
+-
+-- Summary - one liner
+-- Type - task / story / subtask / bug / epic
+-- Description - As a.. I want to.. so that.. - has definition of done
+-- Reporter - Person who creted this
+-- Assignee - Person who will do this
+-- Status - Backlog / ToDo / In Progress / In Review / Done
+-- Epic Link - Broader work
+-- Story points - Estimate of duration
+-- Linked Issue - Dependencies / blocker
+-- Priority - Trivial / Critical / High / Medium / Low
+-- Version/Release
+-
+-Issues can be arranged and managed by versions / epics / sprints. **Hierarchy** is by portfolio outcome, business outcome, epic, task, sub-task.
+-
+-**Boards** are used to displays issues and to track progress of project(s). **Kanban** is simple board, shows tasks on board with swimlanes and state. Also
+-
+-- Scrum is Ajile board concentrated on backlog and Sprints.
+-- Dashboards can display activity, filters, boards etc
+-- Filters can be created and shared. They have search criteria, can add JQL (Jira Query Language) to it.
+-- Project - can have confluence page to have documentation of project
++- Scope - BG looks at entire project for months, SP looks at near future for weeks
++- Grain - BG breaks into tasks, SP breaks in to sub-tasks
++- Detail - BG adds DoR, SP adds DoD
+ 
+ ## Links
+ 
diff --git a/docs/0-Information-Technology/flask.md b/docs/0-Information-Technology/flask.md
index 75bb6c0..c58a282 100644
--- a/docs/0-Information-Technology/flask.md
+++ b/docs/0-Information-Technology/flask.md
@@ -41,16 +41,6 @@ Flask is a microframework in Python. It is used to create a webapp. It can start
     or
   - `export FLASK_APP=main.py` will make an variable that tells python which app to run.
   - `flask run` executes the app or if flask is not in path then do `python -m flask run`
-  - flask --app has app command
-  - flask run has --host or -h, --port or -p and --no-debug
-
-```sh
-set FLASK_ENV=production
-set FLASK_DEBUG=0
-cd repo\prj1
-venv\Scripts\activate
-flask --app app:create_app('uat') run --no-debug -h 0.0.0.0 -p 5002
-```
 
 
 
@@ -68,7 +58,7 @@ _flask basics, request-response handling, contexts_
   - `request` Object has methods and attributes having info on method, form, args, cookies, files, remote_addr, get_data().
 
 - **Contexts**
-  - Code is logic with data. Data is variables or constants or objects. This data can be configurations, input data or data from file/database. In flask, "Context" is used to keep track of this data.
+  - Code needs data to be processed, that data can be configurations, input data or data from file/database. Context is used to keep track of this data.
   - It let certain objects to be globally accessible, but are not global variable. They are globally accessible to only one thread. There can be multiple threads serving multiple requests from multiple client.
   - Context is simply data that is specific to something. Eg
     - **App-context** is specific to app, like its mail server, its database location, or other configurations. Keeps track of application-level data. Objects: `current_app`, `g`.
@@ -78,7 +68,7 @@ _flask basics, request-response handling, contexts_
   - context is automatically made available once app is initialized.
   - context can be made explicitly available by calling `with app.app_context():`
   
-- **Request Handling** - How flask handles a request?
+- **Request Handling**
   - when there is request, web server activates a thread that initializes app and this app context is pushed with data that is available globally, similarly request context is also pushed.
 
     ```mermaid
@@ -88,7 +78,6 @@ _flask basics, request-response handling, contexts_
 
 - **Flask variables for Request Handling**
   - `current_app` variable in Application context, has info of active application.
-  - **Imp**: `current_app` is app-context, but is **only available when serving a request**, that is, in a route function only. It can be used in any module but the function should be called when serving a request.
   - `g` variable in Application context, it is object that is unique for each request, temp access **during** handling of a request. It resets once request is served. Holds app info hence app context. Can be used to load user on each request. show logged in user on templates.
   - `request`, in request context, obj having client req data.
   - `session`, in request context, stores data across requests, i.e., a dictionary to store values that can be accessed in different requests from same session.
@@ -377,9 +366,6 @@ DB_package or ORM - Python has packages for most database engines like MySQL, Po
     # where clause
     user = User.query.filter_by(username=data['username']).first() # or .all()
 
-    # order, after select or where, add. Its, Model.field.
-    .order_by(Response.responded_at.desc())
-
     # select * from users
     users = User.query.all()
 
@@ -503,20 +489,12 @@ DB_package or ORM - Python has packages for most database engines like MySQL, Po
   export MAIL_PASSWORD="password"
   ```
 
-- Email is only sent when FLASK_ENV = production
-
 - **Sending errors via Email**
   - Errors can be sent via email using Logs.
 
-- **Dev - send emails to console**
-  - `MAIL_SERVER = 'localhost'`
-  - `MAIL_PORT = 8025`
-  - `python -m smtpd -n -c DebuggingServer localhost:8025`
-
   - Links
     - [Flask docs - Email errors to admin](https://flask.palletsprojects.com/en/2.2.x/logging/#email-errors-to-admins)
     - [MG's microblog - email errors](https://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-vii-unit-testing-legacy#:~:text=of%20the%20application.-,Sending%20errors%20via%20email,-To%20address%20our)
-    - [Pythonbasics - Flask Mail](https://pythonbasics.org/flask-mail/)
 
 
 ## Blueprint - Large App Structure in Flask
@@ -1095,30 +1073,8 @@ _needs improvements after hands-on_
 
 ## Error Handling
 
-- **Try.. Except..**
-  - You can use `try except finally` block to handle errors that you think might occur. With requests, it is **best** to handle errors at **last step** , that is before making the response, because, if at any previous step an error has occured it will bubble up. In another scenario, use `try.. except` at the step where you have another option to do in case of error. Eg, handle error in view when you make a db call that is last function before returning response. Do not handle it in model or db connections unless you have another database to fall over to or another table to ping.
-
-- **Email / Log error**
-  - Error can be emailed to admin automatically.
-
-- **Error Templates**
-  - You can have templates for exceptions and errors so they don't go out to end users.
-  - These templates only work, when `FLASK_ENV=production` and `FLASK_DEBUG=0` in your environment.
-
-    ```py
-    # blueprint handler
-    @bp.app_errorhandler(404)
-    def internal_error(error):
-        return render_template('errors/404.html'), 404
-    
-    # app handler
-    @app.errorhandler(500)
-    def internal_error(error):
-        return render_template('errors/500.html'), 500
-    ```
-
-- Links
-  - [RealPython Flask App Part III](https://realpython.com/python-web-applications-with-flask-part-iii/#error-handling)
+- We can have templates for exceptions and errors so they don't go out to end users.
+- Link [RealPython Flask App Part III](https://realpython.com/python-web-applications-with-flask-part-iii/#error-handling)
 
 
 
diff --git a/docs/0-Information-Technology/material-mkdocs.md b/docs/0-Information-Technology/material-mkdocs.md
index a962966..e2787de 100644
--- a/docs/0-Information-Technology/material-mkdocs.md
+++ b/docs/0-Information-Technology/material-mkdocs.md
@@ -239,4 +239,3 @@ Based on a year of work, following structure has emerged and has worked in arran
 
 - [Sphinx - Furo - theme](https://github.com/pradyunsg/furo)
 - [Sphinx - Pydata-sphinx-theme](https://github.com/pydata/pydata-sphinx-theme)
-- [Github - Using YAML + Markdown format in documentation comments #878](https://github.com/dotnet/csharplang/discussions/878)
diff --git a/docs/0-Information-Technology/python-notes.md b/docs/0-Information-Technology/python-notes.md
index f85081f..5d2cd7f 100644
--- a/docs/0-Information-Technology/python-notes.md
+++ b/docs/0-Information-Technology/python-notes.md
@@ -60,7 +60,7 @@ date: 2021-05-05
 - `venv\Scripts\activate` activate environment
 - `pip install -r requirements.txt --find-links=pip-packages --no-index` install requirements
   - `--no-index` tells to not use any repository like pypi
-  - `--find-links` tells a path to find all packages, or `-f`
+  - `--find-links` tells a path to find all packages
 - `flask --app app:create_app('testing') run`
 
 - Links
@@ -124,16 +124,6 @@ Undo `conda deactivate && conda remove --name prj1env --all` and remove files if
 
 ## Python Programming
 
-- **with**
-  - it is keyword, used to handle unmanaged resources like database and filestreams. It gurantees closing of resources once used.
-  - eg, `with open(filepath) as f:` - do something, and as the block ends, the resource is closed.
-
-- **next(iterable, default)**
-  - returns - next item in iterable
-  - params
-    - `default` - optional, value to return if end of list is reached.
-  - eg, `next(mylist, "orange")`
-
 - **Dictionary** - data type
 
     ```python
@@ -207,45 +197,6 @@ Except:  Here you can handle the error
 Else: If there is no exception then this block will be executed
 Finally: Finally block always gets executed either exception is generated or not
 
-```py
-
-# minimum
-try:
-  2/0
-except:
-  print('err')
-
-# nesting, finally and multiple exception example
-try:
-  connection = pyodbc.connect(con_uri)
-  cursor = connection.cursor()
-
-  # nesting of try
-  try:
-    cursor.execute(query)
-    n_rows = cursor.rowcount
-    cursor.commit()
-    cursor.close()
-  
-  # in case of any exception
-  except Exception as e:
-    cursor.rollback()
-    logger.error(f'sql_execute - Query failed!. Error "{str(e)}".')
-  
-  # this executes irrestive of exception occuring
-  finally:
-    connection.close()
-
-# Excepting a custom error
-except pyodbc.OperationalError as e:
-  logger.error(f'sql_execute - No connection to "{service}". Message: "{str(e)}"')
-
-# Excepting all other errors
-except Exception as e:
-  logger.error(f'sql_execute - No connection to "{service}". Message: "{str(e)}"')
-
-```
-
 ## Files Handling in python
 
 `f  = open(filename, mode)` Where the following mode is supported:
@@ -258,21 +209,10 @@ except Exception as e:
 - a+: To append and read data from the file. It won’t override existing data.
 
 ```python
-
 ## read
-f = open('notes/abc.txt', 'r') # returns handle to file
-f.readline() # returns one line, keep using for next lines
-
-content = f.read() # returns whole file, arg is size. Not efficient.
-line_list = f.readlines() # read all lines as list, each list item is a line. Not efficient.
-
-f.close()
-
-# peak large file
-with open(filepath) as f:
-    head = [next(f) for _ in range(10)]
-print(head)
-
+file_handle = open('notes/abc.txt', 'r')
+content = file_handle.read()
+file_handle.close()
 
 ## write
 file = open('note.txt','a')
@@ -280,7 +220,7 @@ file.write("quick brown")
 file.write("munde, not fox")
 file.close()
 
-# if exists
+# if excists
 os.makedirs(pdf_dir, exist_ok=True)
 ```
 
@@ -298,7 +238,20 @@ for subdir, dirs, files in os.walk(directory):
     print(file_path, new_file_path) #rename your file
 ```
 
-- zip a folder
+- **Handling CSV and JSON**
+
+  - use python std `csv` n `json` library for data handling with file.
+
+    ```py
+    # pandas iter rows
+    def CSVToJson():
+      df=pd.read_CSV('/home/paulcrickard/data.CSV')
+      for i,r in df.iterrows():
+      print(r['name'])
+      df.to_JSON('fromAirflow.JSON',orient='records')
+    ```
+
+- **zip a folder**
 
   ```python
   import zipfile, os
@@ -370,6 +323,7 @@ logging.debug('Something is happenning')
 # 02/16/2023 01:50:17 PM - root - DEBUG - Something is happenning
 
 
+
 ## New filename for each run
 import os, time
 from time import localtime
@@ -391,36 +345,26 @@ logging.basicConfig(level=logging.DEBUG, \
 
 logging.info("New Working directory is: " + str(os.getcwd()))
 
-# or use RotatingFileHandler
 
-"""Using Components. Using Both File and Stream Handlers"""
 
+## Using Components
+import logging
 logger = logging.getLogger()
-
-formatter = logging.Formatter('[%(asctime)s] %(levelname)s in %(name)s.%(module)s: %(message)s')
-
-# Setup file handler
-fhandler  = logging.FileHandler('my.log')
-fhandler.setLevel(logging.DEBUG)
+fhandler = logging.FileHandler(filename='mylog.log', mode='a')
+formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
 fhandler.setFormatter(formatter)
-
-# Configure stream handler for the cells
-chandler = logging.StreamHandler()
-chandler.setLevel(logging.DEBUG)
-chandler.setFormatter(formatter)
-
-# Add both handlers
 logger.addHandler(fhandler)
-logger.addHandler(chandler)
 logger.setLevel(logging.DEBUG)
 
-# Show the handlers
-logger.handlers
+logging.error('hello!')
+logging.debug('This is a debug message')
+logging.info('this is an info message')
+logging.warning('tbllalfhldfhd, warning.')
 
-# Log Something
-logger.info("Test info")
-logger.debug("Test debug")
-logger.error("Test error")
+## 2015-01-28 09:49:25,026 - root - ERROR - hello!
+## 2015-01-28 09:49:25,028 - root - DEBUG - This is a debug message
+## 2015-01-28 09:49:25,029 - root - INFO - this is an info message
+## 2015-01-28 09:49:25,032 - root - WARNING - tbllalfhldfhd, warning.
 ```
 
 Links
@@ -438,16 +382,13 @@ Links
     - `dt.now()` => datetime.datetime(2022, 10, 20, 8, 14, 44, 277307)
   - `datetime.date` date specific methods
   - `datetime.time` time specific methods
-  - String to date - `datetime.strptime('some string', "%Y-%d-%m %H:%M:%S")`
 - `time` module has `sleep()` function to pause program execution
-  - DateTime as String - `time.strftime('%Y-%m-%d %H:%M:%S')`
 
 ```python
 import time
 
 time.time() # timestamp
 time.sleep(2) # sleeps for two seconds
-time.strftime('%Y-%m-%d %H:%M:%S') # '2023-06-12 11:18:06'
 ```
 
 ```python
@@ -497,8 +438,8 @@ def test_sum():
 
 ## Documenting Code in Python
 
-- **Why** - when you revisit after months, it _saves time_ to pick back
-  - when it is public or team work, it helps _others contribute_
+- **Why** - when you revisit after months, it *saves time* to pick back
+  - when it is public or team work, it helps *others contribute*
 
 - Documenting is making it understandable to users, like react-docs
 - Commenting is for developers, to understand why code is written. It can be to understand, reasons, description or
@@ -509,7 +450,7 @@ def test_sum():
   - you can set this as `my_func.__doc__ = "Some string"`
   - or the next line after function in `"""Some string"""` automatically sets the docstring for the function.
   - docstring structures are of three types
-    - Google - google's way (_mostly used_)
+    - Google - google's way (*mostly used*)
     - reStructured - python style
     - em - same as done in java
 
@@ -559,20 +500,39 @@ def test_sum():
   - [Tstdriven.io - Concurrency  Parallelism AsyncIO](https://testdriven.io/blog/concurrency-parallelism-asyncio/)
 
 
-## Snippets Python
+## Snippets & Ways Python
 
-Taking input - `msg = str(input("Message? : "))`
+- Taking input - `msg = str(input("Message? : "))`
+- use `//` to divide and return only integer part of division or Quotient
+- you can pass function as an argument to another function.
+- you can define a function in an function. the inner function has access to arguments in outer function.
 
 
 ## Web Scraping - Selenium
 
-Install web driver
+- Selenium is browser automation tool
+- BeautifulSoup is DOM parser
+- Pandas for data handling
+- Both can work together, bs4 is best for extraction while selenium is best for performing actions or interactions.
+
+- **Selenium Setup**  
+  - *browser* - You need browser installed (Firefox or Chrome)
+  
+  - *driver* - you need driver for browser installed and added to bath. Its a binary or exe.
+    - visit `https://chromedriver.chromium.org/downloads` and download version same as your browser version.
+    - unzip and move `chromedriver` to `/usr/local/bin/chromedriver`
+  
+  - *python package* - you need selenium installed in python
+    - `python -m pip install selenium`
 
-- visit `https://chromedriver.chromium.org/downloads` and download version same as your browser version.
-- unzip and move `chromedriver` to `/usr/local/bin/chromedriver`
-- selenium, beautiful-soup and pandas works best
+  - [Install Chromium, ChromeDriver and Selenium on Ubuntu .sh](https://gist.github.com/DerekChia/d8b30e035def0ce875ff45ae6b2002f5)
 
+- **XPATH in Chrome**
+  - it is easy to find DOM elems in browser using console. It highlights the element and lets you hit and try.
+  - to inspect a xpath in chrome, write `$x('//div[@role="button"]')`, it finds all elements and returns a list, expand the list the hover to see which element is where on page. then use it in Python.
+  - to get any attribute value in python use `elem.get_attribute('innerText')`
 
+- **Python Code Snippets for Web Scraping**
 
 ```python
 import os, time, datetime, json
@@ -583,39 +543,89 @@ from selenium.webdriver.common.by import By # search elem by
 from bs4 import BeautifulSoup   # to parse DOM
 import pandas as pd             # to store data structure
 
-import getpass                  # to take hidden password inputs
+import pickle
+import requests
+
+chrome_options = webdriver.ChromeOptions()
 
 # Download drive that is compatible to your chrome installation
 driver_path = "C:\code\chromedriver_win32\chromedriver.exe"
-pdf_dir = r"C:\code\path-to-pdfs"
+driver_path = "/usr/local/bin/chromedriver"
 
-chrome_options = webdriver.ChromeOptions()
+# Optional Options and preferences
 
 # print PDF to file
-settings = {"recentDestinations": [{"id": "Save as PDF", "origin": "local", "account": ""}], "selectedDestinationId": "Save as PDF", "version": 2}
-prefs = {'printing.print_preview_sticky_settings.appState': json.dumps(settings), "savefile.default_directory": pdf_dir}
+pdf_dir = r"C:\code\path-to-pdfs"
+
 #change destination to save as pdf and save in required folder
-chrome_options.add_experimental_option('prefs', prefs)
+pdf_settings = {
+  "recentDestinations": [{"id": "Save as PDF", "origin": "local", "account": ""}],
+  "selectedDestinationId": "Save as PDF",
+  "version": 2
+  }
+
+prefs = {
+  "credentials_enable_service": False,
+  "profile.password_manager_enabled": False,
+  "printing.print_preview_sticky_settings.appState": json.dumps(pdf_settings),
+  "savefile.default_directory": pdf_dir
+  }
+
+chrome_options.add_experimental_option("prefs", prefs)
+chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
+chrome_options.add_experimental_option("useAutomationExtension", False)
+
 chrome_options.add_argument('--kiosk-printing')
 
 driver = webdriver.Chrome(driver_path, options=chrome_options)
 
 url_to_scrape = "https://www.something.com"
-driver.get(url_to_scrape)
+driver.get(url_to_scrape) # web page loads
 driver.implicitly_wait(5)
 
-# find by name and send keys
-username_box=driver.find_element(by=By.NAME, value="username")
-username_box.send_keys("some text")
 
-password = getpass.getpass('Enter your password:')
+""" Cookie Handling """
+cookies_path = username+"_cookies.pkl"
+
+# read pickle
+if(os.path.exists(cookies_path)):
+    cookies = pickle.load(open(cookies_path, "rb"))
+    print('Cookie exists')
+
+    # Check logged in by finding insta-home icon
+    driver.get(url_to_scrape)
+    sleep(randint(2,5))
+    # set cookie from pickle
+    try:
+        for cookie in cookies:
+            driver.add_cookie(cookie)
+        print('Cookies added')
+    except e:
+        print('Error adding existing cookie.'+e)
+
+# Save cookie to pickle
+pickle.dump( driver.get_cookies() , open(username+"_cookies.pkl","wb"))
+
+""" Doing actions """
+
+# Check if a field exists
+xlogin = '//*[@id="loginForm"]/div/div[3]/button'
+xlogin_exists = len(driver.find_elements(By.XPATH, value=xlogin)) != 0
+
+
+# find by name and send keys
+driver.find_element(by=By.NAME, value="username").send_keys("some text")
 
 # find by x-path and click
 driver.find_element(By.XPATH,'//*[@id="submit-button"]').click()
 
+# Get attribute
+xpath = '//span[@id="ctdat"]'
+elem = browser.find_element(By.XPATH, value=xpath)
+date_text = elem.get_attribute('innerText')       # use any JS attribute
+
 # scroll to click, Can not click on a Element: ElementClickInterceptedException
-checkbox_xpath = f"/html/body/div[4]/.../div"
-checkbox_elem = driver.find_element(By.XPATH,checkbox_xpath)
+checkbox_elem = driver.find_element(By.XPATH, "/html/body/div[4]/.../div")
 driver.execute_script("arguments[0].scrollIntoView(true);", checkbox_elem)
 
 driver.back()
@@ -627,6 +637,14 @@ soup = BeautifulSoup(driver.page_source)
 # find all where
 items = soup.find_all('li', attrs={'class': 'the-items'})
 
+# Request and Beautiful - Scrapping HashTags
+term = "nature"
+hastag_url = 'https://best-hashtags.com/hashtag/'+term
+page = requests.get(hastag_url)
+soup = BeautifulSoup(page.content, 'html.parser')
+hashtags = soup.select("body > div.wrapper > div.job-description > div > div > div.col-md-8 > div > div > div:nth-child(1) > div:nth-child(4) > p1")[0].text.strip()
+
+
 # building lists of data
 rows = []
 for i,item in enumerate(items):
@@ -645,10 +663,12 @@ df = pd.DataFrame(columns=['id','date_','region','strength','source','title'], d
 
 
 - Links
-  - <https://realpython.com/modern-web-automation-with-python-and-selenium/>
+  - [Real Python](https://realpython.com/modern-web-automation-with-python-and-selenium/)
   - [Kiwidamien Github - Webscraping Beyond Beautifulsoup And Selenium](https://kiwidamien.github.io/webscraping-beyond-beautifulsoup-and-selenium.html)
   - [Beautiful Soup 4 Readthedocs - En Latest Index](https://beautiful-soup-4.readthedocs.io/en/latest/index.html)
   - [Stackoverflow - PDF printing from Selenium with chromedriver](https://stackoverflow.com/q/59893671/1055028)
+  - [XPATH - Guide](https://www.lambdatest.com/blog/complete-guide-for-using-xpath-in-selenium-with-examples/)
+  - [XPATH - text contains](https://stackoverflow.com/questions/12323403/how-do-i-find-an-element-that-contains-specific-text-in-selenium-webdriver-pyth)
 
 
 
diff --git a/docs/1-Software-Engineering/cs-se-basics.md b/docs/1-Software-Engineering/cs-se-basics.md
index d814f40..c8782f3 100644
--- a/docs/1-Software-Engineering/cs-se-basics.md
+++ b/docs/1-Software-Engineering/cs-se-basics.md
@@ -47,11 +47,11 @@ title: CSE Basics
 
 ## Computer Science
 
-- **Thread** is the smallest sequence of instructions that can be managed independently. It is common for a process to have multiple active threads, sometimes sharing resources such as memory or file handles. Multithreaded web servers start a pool of threads and select a thread from the pool to handle each incoming request.
-
-- **Daemon** - In multitasking computer operating systems, a daemon is a computer program that *runs as a background process*, rather than being under the direct control of an interactive user.
-
-- **Unix Domain Socket** or Unix-Socket is endpoint for data communication between processes. Eg, Docket client (process-1) and Docker daemon (process-2) communicate using a REST API over Unix Sockets.
+A thread is the smallest sequence of instructions that can be man‐
+aged independently. It is common for a process to have multiple
+active threads, sometimes sharing resources such as memory or file
+handles. Multithreaded web servers start a pool of threads and
+select a thread from the pool to handle each incoming request.
 
 ## OOPS
 
diff --git a/docs/1-Software-Engineering/data-structures-algorithms.md b/docs/1-Software-Engineering/data-structures-algorithms.md
index 2f2f122..672781e 100644
--- a/docs/1-Software-Engineering/data-structures-algorithms.md
+++ b/docs/1-Software-Engineering/data-structures-algorithms.md
@@ -1,3 +1,68 @@
 # Data Structures & Algorithms
 
-<https://www.quora.com/How-do-I-start-learning-or-strengthen-my-knowledge-of-data-structures-and-algorithms/answer/Robin-Thomas-16>
+## DSA Overview
+
+- **Why is DSA important?**
+  - In every problem there is a budget, which defines time and money. Hence, it is important to understand the **cost of memory** and **time of execution**.
+
+- **Edge Cases**
+  - extreme cases of input or situation
+  - like: empty array, bad format
+
+- **Steps to solve a problem using DSA**
+  1. state prob clearly
+  2. define inputs and outputs and their formats
+  3. state test cases and edge cases in english
+  4. state correct solution in english
+  5. code and test
+  6. repeat 2-5
+  7. analyze algo's complexity or inefficiency
+  8. refactor code, test. repeat 2-8
+
+- **Best Practices in Coding using DSA**
+  - create a function
+  - give meaningful name and follow standards
+  - do test driven development
+  - 10 mins code built test repeat, small steps
+  - abstract code into functions and make them generic. eg, repetitive logic can be moved to a separate function and used in changing condition function. as in binary-search logic is same but matching condition may vary, so make a generic binary-search function and use it in other functions.
+  - problem solving approach and coding both are important, **approach & right technique** is more important that coding
+  - don't start coding directly, rather write clearly the problem statement and then solution in **plain english**, then code.
+- linear search algorithm or brute force algorithm
+  - simple, start from 0 and traverse whole array
+
+- **Time calculation of code execution**
+  - `1Ghz` machine can execute `10^9` **instructions** in 1 second (giga = 9). Instructions are low level binary code. so a high level **python statement** can execute tens or thousands of instructions, but thats how you calculate execution time of once statement, and the **set of statements** in function and then number **iterations** of that function in worst case gives its complexity.
+  - `iterations --of--> function of statements --having--> statement --build_of--> instructions --processed_on--> CPU`
+
+- **Analyze complexity and find inefficiency**
+  - can you **reduce the number of iterations** in the loop? eg, use binary search instead of linear search on a sorted array.
+  - complexity of algo is the **max time** it will take to finish, that is total time in **worst case**. it is usually based on the size of array `n`
+  - **O(n)** or **Big O Notation** or 'Order of n' means that a code-block is executed `n` number of times at max.
+  - **O(log n)** means that code will iterate `log n` times.
+
+## Binary Search Algorithm
+
+- **Binary Search Algorithm**
+  - if an array is sorted
+  - find the number in middle
+  - then look left or right
+  - complexity is `log(n)` - that means that in worst case the number of iterations will be `log(n)` for an array of length `n`
+  
+  - how?
+    - lets say we do `k` iterations (at max) to search a number in array of length `n`, i.e. loop will run no more than k times and is hence worst case scenario, so
+    - iter: 1, length: n/2
+    - iter: 2, length: n/4 = n/2^2
+    - iter: 3, length: n/4 = n/2^3
+    - ...
+    - iter: k, length: n/2^k, this is last iteration and we know in worst case the length will be reduced to 1.
+    - `n/2^k = 1` or solve it to get
+    - `k = log(n)` which means that the code will have to run maximum `log(n)` number of times.
+
+
+- Links
+  - [Binary Search, Linked Lists and Complexity | Data Structures and Algorithms in Python](https://www.youtube.com/watch?v=clTW4lydwOU&list=PLyMom0n-MBrpakdIZvnhd6PFUCKNAyKo1&index=1) - direct code, no plain algo dry run
+
+
+## Links
+
+- [DSA Guide - Quora](https://www.quora.com/How-do-I-start-learning-or-strengthen-my-knowledge-of-data-structures-and-algorithms/answer/Robin-Thomas-16)
diff --git a/docs/1-Software-Engineering/devops.md b/docs/1-Software-Engineering/devops.md
index 19f4848..a0b1a0d 100644
--- a/docs/1-Software-Engineering/devops.md
+++ b/docs/1-Software-Engineering/devops.md
@@ -67,47 +67,12 @@ end
 
 ## Docker
 
-- **What is Docker**
-  - Docker is a platform that helps separate application from infrastructure by using isolated environments called _containers_.
-  - it  is a set of platform as a service products that use OS-level virtualization
-  - It lets you put everything you need to run your application into a box that can be stored and opened when and where it is required.
-  - Docker-Image is template that defines layout of container, container is runtime instance of image, and runs on docker-engine, which is software that runs containers.
-  - Docker containers are defined by docker-image (template) and are run on docker-engine (runtime on host).
-
-- **How Docker works** - It is built in Go language and uses Linux kernel features for isolation, functionality like `namespaces` lets it possible.
-
-- **Docker Architecture & Overview**
-  
-  - **Docker Objects** are items you create and use in docker. They are images, containers, networks, and volumes.
-  
-  - **Docker Image** is read-only immutable _template_ that defines the layout of container. They are based on other images (kinda inheritance), like your `app-image` can be build on `ubuntu-image` with added details as installation of python, mssql and configurations to run the application. To define image you create _docker-file_. Each instruction in file is like a layer in image. Each change in statement, update only that layer not the whole image. This makes it fast and lightweight.
-  
-  - **Docker Container** is runtime instance of Docker Image. Created using `docker run` command. It runs on Docker Engine. You can create, start, stop, move, or delete a container using the Docker API or CLI. Container can connect to network, storage and can be saved as new image in its current state. Containers are mostly isolate from each other but you can control isolation of network/storage/subsystem on host machine. Eg, to run a container with image name _ubuntu_ and then run command _/bin/bash_ use: `$ docker run -i -t ubuntu /bin/bash`. It pulls image from if not available locally, crates container, allocates storage resources, adds network interface, starts container and executes the command _/bin/bash_. `-i` is interactively and `-t` attached to terminal, this lets you interact and see output of container in your terminal.
-
-  - **Docker Engine** is the software that hosts (runs) the containers. it is container runtime.
-
-  - **Docker daemon** it is background process, `dockerd` that listens to Docker-API requests and manages Docker Objects (image, container, network, volume).
-
-  - **Docker client** a process, `docker` that lets users interactions. It sends users commands to daemon. So `docker run` is send from client to daemon `dockerd` which does the job.
-  - > Docker Desktop is GUI that is easy and has client and daemon, and other helpers.
-
-  - **Docker registries** a registry that stores docker-images. It can be yours or a public register _Docker Hub_ that anyonce can use (like GitHub). Commands like `docker pull` or `docker run` read, and `docker push` write docker-image to configured registry.
-
-    ```mermaid
-    graph LR;
-
-    User <--> Client <--> Docker_API <--> Daemon
-
-    subgraph Registry
-      Image-1
-      Image-2
-      
-    end
-
-    Daemon <--push / pull--> Image-2
-    Daemon <--run--> Image-1
-
-    ```
+- lets you **build container**
+- it  is a set of platform as a service products that use OS-level virtualization
+- Docker lets you put everything you need to run your application into a box that can be stored and opened when and where it is required
+- `Docker Image` is read-only immutable **template** that defines the layout of container.
+- `Docker Container` is runtime instance of Docker Image. Created using `docker run` command. It runs on Docker Engine.
+- `Docker Engine` is the software that hosts (runs) the containers. it is container runtime.
 
 ## Jenkins
 
diff --git a/docs/1-Software-Engineering/git.md b/docs/1-Software-Engineering/git.md
index 2b63203..2920dec 100644
--- a/docs/1-Software-Engineering/git.md
+++ b/docs/1-Software-Engineering/git.md
@@ -36,10 +36,10 @@ Git is version control software to track changes in source code. GitHub is cloud
 
 ## Configuration
 
-- You can get/set/edit/unset variables that configure git. Values can be set at **global** or **system** or **local** (repo) level.
-- Global configs are stored in `.gitconfig` file in `home` dir usually. It holds YAML data. You can set values using commands and see the values set.
+- You can get/set/edit/unset options with this command
+- values can be set at **global** or **system** or **local** (repo) level
   - `~/.gitconfig` global
-  - `./.git/config` local in repo
+  - `./.gitconfig` local in repo
 - username and email are required to be correctly set locally as they are written in each commit (who did it?). This also helps to associate correct account when pushed to remote.
 - format is `git config --flag name.key value`
 
@@ -55,7 +55,6 @@ Git is version control software to track changes in source code. GitHub is cloud
   - `git config user.name "Your Name"` sets user name in local repo file
   - `git config --global credential.helper 'cache --timeout=72000'` caches. so enter credentials once every 20 hours
   - `git config --global credential.helper store` - stores the username and password in store utility
-  - `git config --global --unset http.proxy` to unset a variable
 
 ## Operations
 
@@ -191,20 +190,22 @@ do so (now or later) by using -c with the switch command. Example:
 - Merge process
 
 ```sh
-# ON Secure   ---------------------------
-git checkout dev
+# in local on branch ofc, commited
+git checkout -b ofc_masked # create branch
+# delete internal files, commit
 git add .
-git commit -m "ready_to_diff"
+git commit -m 'ofc_masked for diff'
 
 git checkout -b zip
-# delete all except `.git`, download zip, extract
+rm -rf . # delete all 
+# then extract zip downloaded from remote
 git add .
-git commit -m "downloaded_for_diff"
+git commit -m "remote for diff"
 
-git diff dev zip > diff.patch
+git diff ofc_masked zip > diff.patch
 
 
-# ON Public   ----------------------------------
+# ON Remote   ----------------------------------
 
 git checkout master
 git add .
@@ -216,7 +217,6 @@ git checkout -b master_patched
 # download diff.patch, and save
 
 "C:\Program Files\Git\usr\bin\patch.exe" -p1 < diff.patch
-# do Y for reverse patch
 
 # check manually for `*.orig` files and verify changes
 
@@ -230,18 +230,7 @@ git add .
 git commit -m "patched_merged"
 git push
 
-# ON Secure   ---------------------------
-
-git checkout master
-git branch -D zip
-git branch -D dev
-# delete all except `.git`, download zip, extract
-git add .
-git commit -m "downloaded"
-git checkout -b dev
-# Done! Ready to work
-
-
+# download and extract in master on local
 
 # git diff --no-prefix ofc_masked zip > diff.patch # for this
 # "C:\Program Files\Git\usr\bin\patch.exe" -p0 < diff.patch # use this
@@ -299,21 +288,45 @@ Now once you have written your code, you can add and commit new code to local gi
 - finally make `master` as default on remote and delete `main`.
 - or simply rename on github.com.
 
-## SSH Authentication to push to remote
+## Different username push to GitHub
 
-You can connect to GitHub using the Secure Shell Protocol (SSH), which provides a secure channel over an unsecured network. It can help connect one machine to another using keys and thus avoiding to provide username and password/token on each request.
+- you can get error if you have tow different github user account or you want to push to someone else's account. In that case you can do following to push to GitHub as a different user.
 
-- **SSH Basics**
-  - `~/.ssh` is a folder that has your keys.
-  - `ssh-keygen` is command to generate keys. It has switched -t -b -C [ ]
-  - file `id_rsa.pub` has your public key. This is secret, but can be givent to bitbucket, so that they have your public key and can authenticate you without your username password.
-  
-- **Add SSH** to another service
-  - if `~/.ssh/id_rsa.pub` exists do `cat ~/.ssh/id_rsa.pub` else generate SSH Key `ssh-keygen`, passphrase is optional.
-  - copy the content, Open GitHub, click your profile icon, settings, SSH and GPC Keys, Click on the new ssh key button.
-  - enter any title and key that you copied.
+```sh
+# Configre new username
+git config user.name username
+git config user.email user.name@gmail.com
+
+# Amend last commit if done wrong
+git commit --amend --author="User Name <user.name@gmail.com>"
+
+# Check if last commit is updated
+git log
+
+# Generate SSH Key for new user
+ssh-keygen -t rsa -C "user.name@gmail.com"
 
-- **Links** - <https://docs.github.com/en/authentication/connecting-to-github-with-ssh>
+# Change SSH Key to be used in Git Repo
+git config --local core.sshcommand 'ssh -i /home/username1/.ssh/id_rsa_username -F /dev/null'
+
+# Pull GitHub content to rebase
+git config pull.rebase true 
+
+# Pull to avoid conflict
+git pull origin main
+
+# Push to GitHub with new username
+git push --set-upstream origin main
+```
+
+## SSH Authentication to push to remote
+
+You can connect to GitHub using the Secure Shell Protocol (SSH), which provides a secure channel over an unsecured network. It can help connect one machine to another using keys and thus avoiding to provide username and password/token on each request. `id_rsa.pub` is default public key.
+
+- if `~/.ssh/id_rsa.pub` exists do `cat ~/.ssh/id_rsa.pub` else generate SSH Key `ssh-keygen`, passphrase is optional.
+- copy the content, Open GitHub, click your profile icon, settings, SSH and GPC Keys, Click on the new ssh key button.
+- enter any title and key that you copied
+- more - <https://docs.github.com/en/authentication/connecting-to-github-with-ssh>
 
 Checking
 
@@ -351,10 +364,6 @@ If you push to git from two different repositories then there may be conflict. e
 - You can decide if you want keep your branch changes or not. If you want to keep the changes what you did, delete the conflict marker they are, `<<<<<<<, =======, >>>>>>>` and then do a merge.
 - Once done, `add commit push` :)
 
-## Modifying Commits
-
-- `git log`
-
 ## Version controlling in GIT
 
 You can see previous versions of file in your git repository.
diff --git a/docs/1-Software-Engineering/linux-terminal.md b/docs/1-Software-Engineering/linux-terminal.md
index 5cc0ab2..825c3d4 100644
--- a/docs/1-Software-Engineering/linux-terminal.md
+++ b/docs/1-Software-Engineering/linux-terminal.md
@@ -260,22 +260,7 @@ Other
 - `rm -rf` removes non/empty dir and files forcefully
 - `rm` removes files not directories.
 
-- **ls** - shows file in dir
-  - `ls -l` as a list
-  - `ls -h` human readable sizes
-  - `ls -r` reverse order
-  - `ls -t` sorts by last modified date and time, default is latest at top
-  - `ls -a` shows all, includes hidden files
-  - Examples
-    - `ls -lt | head` shows last 10 modified files.
-
-- **head/tail** - show first or last lines
-  - `head -n 2` - `--lines` - shows first 2 lines.
-  - Examples
-    - `ls -lt | head -n 5` shows only 5 files from prev command.
-    - `head -5 notes.txt` - shows top 5 lines from file.
-
-## youtube-dl
+### youtube-dl
 
 - `youtube-dl --extract-audio --audio-format mp3 -o "%(title)s.%(ext)s" http://www.youtube.com/watch?v=fdf4542t5g` -o is --output of filename.
 
@@ -283,7 +268,7 @@ Other
 
 - `pkg` is **similar** to `apt`, it is another package manager in termux, it is even higher wrapper on apt. You can use both.
 
-- Installing termux
+- **Installing termux**
   - Termux is no more updated on play store, so you can use old or download latest from [F-droid.org](https://f-droid.org/en/packages/com.termux/)
   - Once installed do following on phone
 
@@ -294,7 +279,7 @@ Other
     exit
     ```
 
-- Enabling SSH
+- **Enabling SSH**
   - SSH lets you use phone linux from another laptop. Do this on phone:
 
     ```shell
@@ -307,24 +292,46 @@ Other
 
   - Now open cmd or terminal on a machine connected to same wifi and do `ssh 192.168.1.17 -p 8022`, where IP is address of phone.
 
-- Termux - working with packages
+- **Termux - working with packages**
   - Search:  `pkg search <query>`
   - Install: `pkg install <package>`
   - Upgrade: `pkg upgrade`
 
-- Termux - subscribing to additional repositories
+- **Termux - subscribing to additional repositories**
   - Root:    `pkg install root-repo`
   - X11:     `pkg install x11-repo`
 
-- Setting Python and Git
+- **Setting Python and Git**
 
   ```shell
   pkg install python -y
   pkg install git -y
   ```
 
+- **Crontab on Termux**
+  - it is used to schedule jobs in linux and make them run at certain intervals.
 
-- Links
+    ```sh
+    pkg install cronie termux-services
+    sv-enable crond
+
+    # `service-daemon start` or restart terminal 
+
+    ~/code$ crontab -l
+    no crontab for u0_a234
+
+    crontab -e 
+
+    0 * * * * cd /data/data/com.termux/files/home/code && /data/data/com.termux/files/usr/bin/python tbd_logger.py
+
+    ```
+
+  - Example run logs
+    - `time 2023-04-22 19:39 - battery 85% - cron at 1m`
+    - `time 2023-04-23 12:22 - battery 68% - cron switched to 1hr`
+
+
+- **Links**
   - [Termux - Docs](https://termux.dev/docs)
   - [Termux - Installing](https://github.com/termux/termux-app#installation)
   - [Termux - Backing up](https://wiki.termux.com/wiki/Backing_up_Termux)
@@ -369,5 +376,6 @@ pip install youtube-dl
 
 ## Links
 
-- cheat book - <https://github.com/0nn0/terminal-mac-cheatsheet#english-version>
-
+- [terminal cheat book - mac](https://github.com/0nn0/terminal-mac-cheatsheet#english-version)
+- [Setting cronjob on mac](https://www.jcchouinard.com/python-automation-with-cron-on-mac/)
+- [vim Getting Started](https://opensource.com/article/19/3/getting-started-vim)
diff --git a/docs/1-Software-Engineering/oops-python.md b/docs/1-Software-Engineering/oops-python.md
index ce126f8..5de849f 100644
--- a/docs/1-Software-Engineering/oops-python.md
+++ b/docs/1-Software-Engineering/oops-python.md
@@ -14,11 +14,9 @@
 class Book:
 
     # Class attribute, common for all instances
-    # executes when class is imported
     kind = "Novels"
     
     def __init__(self, row):
-        # executes when object is created
         # row is db record
 
         # instance attribute
@@ -34,7 +32,7 @@ class Book:
     @classmethod
     def fetch_book(cls, id):
         # fetch the row from db or api
-        row = fetch_service(id) # some other database function
+        row = fetch_service(id)
         obj = cls(row)
         return obj
 
diff --git a/docs/2-Data-Engineering/data-architecture.md b/docs/2-Data-Engineering/data-architecture.md
index 0403075..ce82cae 100644
--- a/docs/2-Data-Engineering/data-architecture.md
+++ b/docs/2-Data-Engineering/data-architecture.md
@@ -1,82 +1,44 @@
 # Data Architecture
 
-_how to architect, where to architect, stages of storage, storage solutions_
-
-Aim of architecturing database is to collect and store data in a way that it is optimised for reading to enable analytics and BI.
-
-## Overview / Concepts
-
-- **Database Modeling**
-  - _Logical Model_ is modeling on paper/ppt
-  - _Physical Model_ is modeling on database with the data.
-
-- **Data Storage Stages** can follow this journey
-  - Staging area where it is dump from feeds, can be OLTP dumps
-  - Data Warehouse - where it is stored in star schema, is clean, easy to understand and update and ready to use.
-  - Data Mart - then denormalised simple query for consumers, focusing on small business areas / purpose.
-
-- **Database Schema** is collection of _database objects_ (tables, views and indexes).
-
-- **3NF Schema** minimizes redundancy by splitting data in multiple tables and linking them with relationships. Adding new entity is easy without effecting current applicaiton. But, this makes reading data slow as the query joins multiple tables.
-
-- Data Warehousing, data mart build, database modeling, dimentional modeling, data modeling,  - they all have a common goal to **improve data retrieval** (select query optimized).
+Aim is to collect and store data in a way that it is optimised for reading to enable analytics and BI.
+
+## Concepts
+
+- Modeling - Logical is modeling on paper/ppt. Physical is modelling on database with the data.
+- Data storage can follow this journey
+  - staging area where it is dump from feeds
+  - then 3nf schema - where it is clean and ready for joining, it is data warehouse
+  - then denormalised simple query for consumers, it is data mart
+- schema is collection of database objects (tables, views and indexes).
+- 3NF Schema minimizes redundancy by splitting data in multiple tables and linking them with relationships. Adding new entity is easy without effecting current applicaiton. But, this makes reading data slow as the query joins multiple tables.
+- Data Warehouse can have multiple star-schema, each based on a business-process such as sales tracking or shipments. Each star-schema represents a data-mart, this can serve the BI needs. Star-schema have fraction of table compared to 3NF. 15-20 star-schema can cover all LOBs of enterprise. BI users can easily query and join multiple star-schemas as they few tables.
+- Star schemas can have denormalized dimensions for easy understanding and faster data retrieval and less complex queries.
+  - Most important is to consider the level of detail, grain of data.
+- Both 3NF and Star-schema don't contradict but can work in layers with 3NF as foundation and star-schema as access and opttimized layer.
 
 
 ## Data Storage Solutions
 
-- **Data Lake** - is dumped data with no purpose.
-
+- **Data Lake** - is dumped data with no purpose
 - **Staging Area** is a dump from feeds. It simplfies cleaning and consolidation.
-
-- **Data Warehouse** central combined database optimized for analysis of historical data.
-
+- **Data Warehouse** - data from different sources into central store to provide **single source of truth** on which BI and Analysts can rely.
+  - **OLAP vs OLTP** - Compared to OLTP (transaction processing), warehousing is read oriented, for analytics workload OLAP.
+    - read oriented, vs insert/update/delete
+    - denormalized for reads, fully normalized for consistency
+    - ETL batch updates, always up to date.
+  - Big data warehousing handling petabytes in an distributed environment. Handle 3Vs, real time, no sql, petabytes? It is ETL but at industry level,
+- **Data Mart** - usually build for single purpose, for particualr LOBs, can be physically designed or implemented logically by creating views, materialized view or summary data in warehouse (they have an overlap). It mainly focuses on a subset of data instead of complete enterprise data. They can exist as
+  - Island is right from source, can be inconsistent.
+  - Dependent is fed from warehouse, mostly consistent.
+- **Operation Data Store** - ODS gives data warehouses a place to get access to the most current data, which has not yet been loaded into the data warehouse. Usually current day data.
 - Usually - Data Lake > Data Warehouse > Data Mart
+- Data Warehousing, data mart build, database modeling, dimentional modeling, data modeling,  - they all have a common goal to **improve data retrieval** (select query optimized).
 
-
-## Data Warehousing Concepts
-
-- **What is Data Warehouse**
-  - simply it is a database.
-  - designed in a way to facilitate easy reads and accomodates change in model like adding a new dimension.
-  - lets slice and dice data from different dimensions and by time.
-  - lets view highly-aggregated data and same time lets drill-down to lowest granularity.
-  - the data is non-volatile (does not change) and lets analyze what occured over time.
-  - it includes **ETL process**, multi-dimensional modeling, backups, availability
-  - Big data warehousing handling petabytes in an distributed environment. Handle 3Vs, real time, no sql, petabytes? It is ETL but at industry level.
-
-- **Why is Data Warehouse required**
-  - to combine data from different sources intoe **single source of truth** on which BI and Analysts can rely.
-  - to enhance organization's performance by analyzing data.
-  - to maintian historical records to look over years.  
-
-- **How Data Warehouse works**
-  - _Read Optimized_ - they are designed to query and analyze rather than transaction processing.
-
-- **Characteristics of a Data Warehouse**
-  - simplicity of access and high-speed query performance.
-
-- **OLAP vs OLTP** - OLTP (Online Transaction Processing), OLAP (Online Analytical Processing)
-  - OLAP is optimized for quick reads and analysis, OLTP is optimized for insert/update/delete
-  - OLAP is denormalized for reads, OLTP is fully normalized for consistency
-  - OLAP is populated with ETL batch updates, OLTP is always up to date with transactional writes.
-
-- **Data Mart** - similar to warehouse but is usually build for single purpose, for particualr LOBs.
-  - It can be physically designed or implemented logically by creating views, materialized view or summary data in warehouse (they have an overlap).
-  - It mainly focuses on a subset of data instead of complete enterprise data.
   
-  - They can exist as
-    - _Island Data Marts_ - it is right from source (OLTP), can be inconsistent. Quick workaround if there is no data warehouse.
-    - _Dependent Data Marts_ - it is fed from warehouse, mostly consistent. Lengthy as it needs data warehouse to be built.
-
-- **Operation Data Store** - ODS gives data warehouses a place to get access to the _most current data_, which has not yet been loaded into the data warehouse. Usually _current day_ data. It **not** historic.
-
-- **Data Warehouse Architectures**
-  - Basic - Source-data to warehouse to users, no data-marts, no staging-area.
-  - Staging and warehouse - from source data is landed to staging area then to warehouse.
-  - Staging, warehouse and datamarts - data lands from source to staging area, then to warehouse, then individual LOBs can have data-marts for more refined usecases. Also called EDW (Enterprise Data Warehousing)
-
-> Figure: **Architecture** of a Data Model (with optional "Staging Area" and "Data Marts")
+Figure: **Architecture** of a Data Model (with optional "Staging Area" and "Data Marts")
   
+  ![Data Warehouse](https://docs.oracle.com/en/database/oracle/oracle-database/21/dwhsg/img/dwhsg064.gif)
+
   ```mermaid
   flowchart LR
   ds1[(Ops Sys 1)] --> sa[(Staging\nArea)]
@@ -97,60 +59,35 @@ Aim of architecturing database is to collect and store data in a way that it is
   dm3 --> u3
   ```
 
+## Build a Data Warehouse
 
-## Logical Design in Data Warehousing
+*This defines process to turn architecture to system deliverable*
 
-- **What is Logical Modeling**
-  - Logical Model is conceptual (pen & paper), focus on business needs and build subject-oriented `schema`. It more to understand use case, end user and the information you need.
+### Logical Model
 
-- **How to build Logical Model**
-  - Identify the things of importance, _entity_ (data item, like user, book) and its properties _attributes_ (columns; like name, dob).
-  - Determine data _granularity_, week, day, month.
-  - Determine how entities are related to each other, _relationships_. Also called _entity relationship modeling_.
-  - Determine the _unique identifier_ for each entity record, which is `primary_key` in physical model. It applies to OLAP, OLTP, 3NF EDW, star and snowflake.
-  - Next, divide data into _facts_ and _dimensions_. Write down all dimension and facts required. Several distinct dimensions, combined with facts, enable you to answer business questions.
-  - _Identify the source data_ that will feed the data mart model, that is, populate the facts and dimensions.
-  - _Design your schema_ for data mart, star-schema, snow-flake schema or other.
-  - Lastly you need a _routine/pipeline_ to _move data_ from sources to mart as facts and dimensions. Determine the _frequency_ at which the data is refreshed.
-  
-- **Facts**
-  - It is numeric, transactional data, fast changing. Mostly tall table with numeric data, datetime and contains forign keys  of dimaension table which combined make composite key as its primary key.
-  - Fact table with aggregated facts is called _summary table_.
-  - A fact table has a _composite key_ made up of the primary keys of the dimension tables of the schema.
-  
-  - **Adding rows to fact table**, there are three ways
-    - Transaction-based: row shows a lowest grain transaction for a combination of dimension.
-    - Periodic Snapshot: each row is related to a period, like daily or weekly.
-    - Accumulating Snapshot: each row shows occurance of process, that is, multiple rows for one process but each row tracks a movement.
-
-- **Dimensions**
-  - It is descriptive, slow changing, known as lookup tables or reference tables. Mostly wide. It may contain hierarchies. Eg, product, customer, time.
-  - Data is kept at lowest level of detail, it can be rolled up higher level of hierarchy.
-
-- **Star Schema**
-  - It is simple, having fact in centre and dimensions around it, just like a star, where only one join establishes the relationship between the fact table and any one of the dimension tables.
-  - Star-schema have fraction of table compared to 3NF. 15-20 star-schema can cover all LOBs of enterprise. BI users can **easily query** and join multiple star-schemas as they have few tables.
-  - Star schemas can have **denormalized dimensions** for easy understanding and **faster data retrieval** and less complex queries.
-  - Most important is to consider the level of detail, grain of data.
-  - Both 3NF and Star-schema don't contradict but can work in layers with 3NF as foundation (OLTP) and star-schema as access and optimized layer.
-  - Data Warehouse can have multiple star-schema, each based on a business-process such as sales / tracking / shipments. Each star-schema represents a data-mart, this can serve the BI needs.
+In Data Modelling, Logical Model is conceptual (pen & paper), focus on business needs and build subject-oriented `schema`
 
+- **Data Gathering** - Identify the things of importance, `entity` (data item, like user, book) and its properties `attributes` (columns; like name, dob).
+- **Entity-Relationship Modeling** - Determine how entities are related to each other, `relationships`. Determine the unique identifier for each entity record, `primary key`. It applies to OLAP, OLTP, 3NF EDW, star and snowflake.
+- Determine data `granularity`, week, day, month.
+- Divide data into
+  - **facts** - numeric, transactional data, fast changing. Mostly tall table with numeric data, datetime and contains forign keys  of dimaension table which combined make composite key as its primary key.
+    - Summary fact tables contain aggregated facts.
+  - **dimensions** - descriptive, slow changing, known as lookup tables or reference tables. Mostly wide. It may contain hierarchies. Eg, product, customer, time. Data is kept at lowest level of detail, it can be rolled up higher level of hierarchy.
+- Write down all dimension and facts required. Several distinct dimensions, combined with facts, enable you to answer business questions.
+- **Identify the Source Data** - that will feed the data mart model, that is populate the facts and dimensions.
+- Design your `schema` for data mart
+  - **Star Schema**
+    - it is simple, having fact in centre and dimensions around it, just like a star, where only one join establishes the relationship between the fact table and any one of the dimension tables.
+- Your design should result in
+  - set of entitis and attributes corresponding to fact and dimentions tables.
+  - a model/pipeline to move data from sources to mart as facts and dimensions.
 
-## Physical Design in Data Warehousing
+### Physical Model
 
-*This defines process to turn architecture to system deliverable. From - Oracle® Database - Data Warehousing Guide 21c*
 It implemets logical model, with variations based on system parameters like memory, disk, network and software type.
 
 
-## Multi Dimentional Modeling
+## Multi Dimentional Modelling
 
 BI developers create cubes to support fast response times, and to provide a single data source for business reporting.
-
-## Other Tasks in Data Warehousing
-
-- Configuring database to be used as a warehouse.
-- Performing upgrades of new release.
-- Managing users, security and objects.
-- Backing up and performing recoveries.
-- Monitoring performance and taking preventive actions.
-
diff --git a/docs/2-Data-Engineering/data-frameworks-tools.md b/docs/2-Data-Engineering/data-frameworks-tools.md
index 8368e75..81df5fe 100644
--- a/docs/2-Data-Engineering/data-frameworks-tools.md
+++ b/docs/2-Data-Engineering/data-frameworks-tools.md
@@ -2,7 +2,35 @@
 
 _data processing tools, libraries and frameworks_
 
-- **Apache Spark** - large-scale data processing as pandas.
+- **Processing Engines** - lets do the transformations. Eg, Spark
+- **Workflow Managers** - Airflow and Nifi
+- **Administrative Tools** - `pgAdmin` for PostgreSQL and `Kibana` for Elasticsearch.
+- **Cluster** is a group of server, where each server is called node.
+
+- **Connection pooling** is a technique of creating and managing a `pool of connections` that are ready for use by any thread that needs them. It is a cache of database connections maintained so that the connections can be reused when future requests to the database are required. Connection pools are used to enhance the performance of executing commands on a database. Pool means a particular thing collected together for shared use by several people. `JDBC` does this. **JDBC** (Java Database Connectivity) is a standard interface that enables communication between database management systems and applications written in Oracle Java. Most database build this as driver eg, `postgresql-42.2.10.jar`
+
+- **Data Pipelines**
+  - Combine database, a programming language, a processing engine and a data warehouse to build a pipeline. Here, database can be source, programming language can be used to control the processing engine to transform the data and load into data warehouse.
+  - pipeline can be scheduled using crontab, a better workflow manager is `Apache Airflow` or NiFi.
+  - for dev you can install all above tools on same machine, but in prod they are network of machines.
+
+  - **In Prod**
+    - production data pipeline – idempotence and atomicity
+    - you need to stage the data as file or in database
+    - you need to validate the data, use `great-expectations`
+    - idempotence - if you accidentally click run on your pipeline three times in a row by mistake, there are not duplicate records – even if you accidentally click run multiple times in a row. use ID for this or date.
+    - Atomicity means that if a single operation in a transaction fails, then all of the operations fail. If you are inserting 1,000 records into the database, as you did in Chapter 3, Reading and Writing Files, if one record fails, then all 1,000 fail.
+    - version controlling
+    - logging and monitoring
+
+- **Apache Beam** - unified programming model to define and execute data processing pipelines, including ETL, batch and _stream-processing_.
+
+## Apache Spark
+
+- **Apache Spark**
+  - allows distributed parallel execution.
+  - external data -> loaded to -> data frame -> which is -> RDD -> runs on -> different nodes within the cluster.
+  - large-scale data processing as pandas.
   - InMemory to avoid diskwrites slowness  of mapreduce
   - Data Structure is RDDs
   - Interactive analytics are faster, just like we do in Jupyter where next step is based on prev.
@@ -10,31 +38,140 @@ _data processing tools, libraries and frameworks_
   - Actions - count first collect reduce - give single result
   - PySpark - Python API for spark, RDD as DataFrame so makes similar to Pandas.
 
-- **Apache Beam** - unified programming model to define and execute data processing pipelines, including ETL, batch and _stream-processing_.
 
-- **Apache Kafka** - distributed event store and _stream-processing_ platform
+## Apache Kafka
+
+- distributed event store and _stream-processing_ platform
+- stream processing compared to batch processing
+- architecture it supports is scalable and makes data distributed, replicated and fault-tolerant, hence, allowing stream processing in real-time.
+- it is based on pub-sub (publishing and subscribing) messaging system.
+- kafka sends data in real-time to `topics`. data may be infinite and incomplete at time of query.
+- consumers who process data can read topics.
+- works on 3-nodes cluster.
+  - use IP of three servers in configuration.
+
+- **how it works**
+  - kafka uses logs to store data and calls it topics. it is saved to disk as log file. they are horizontally scaled and partitioned.
+  - producer can write to partitions as fire-and-forget, or synchronous, or asynchronous
+  - consumers can read and be part of a consumer group, so that they consume from different partitions at fast rate.
+
+- **configuration**
+  - configuration connects zookeeper and kafka together, it is where you define the server and port to connect and data and log directories.
+  - `zookeeper.properties` file has info on configs for zookeeper `dataDir`, `servers`, `clientPort`
+  - kafka configs are in `server.properties` file, like, `log.dirs=`, `zookeeper.connect=`.
+
+- **hello test**
+  - create a topic, a producer, some messages, a consumer to read them.
+
+  - create topic called 'dataengineering'
+    - `bin/kafka-topics.sh --create --zookeeper localhost:2181,localhost:2182,localhost:2183 --replicationfactor 2 --partitions 1 --topic dataengineering`
+
+  - list all topics
+    - `bin/kafka-topics.sh –list --zookeeper localhost:2181,localhost:2182,localhost:2183`
+
+  - write messages to topic
+    - you can use console to add messages to a topic
+    - `bin/kafka-console-producer.sh --broker-list localhost:9092,localhost:9093,localhost:9094 -topic dataengineering`
+
+  - read messages from topic
+    - you can read from beginning or define an offset if already read.
+    - `bin/kafka-console-consumer.sh --zookeeper localhost:2181,localhost:2182,localhost:2183 --topic dataengineering –from-beginning`
+
+  - whatever you write in producer appears on consumer after a lag. this shows the connectivity between two no you can use Python, Airflow/NiFi to build a pipeline.
+
+- **Kafka data pipeline using NiFi**
+  - use NiFi to build processors that act as producer and consumer.
+  - Consumer can have multiple consumers in consumer-group.
+  - later you can add it to prod pipeline as normal that is, read kafka -> staging, transformation, validation, loading, etc.
+
+- **Batch vs Streaming**
+  - if streaming data is unbounded (infinite), then you need to rethink of validating it for completeness, recalculate min, max and avg.
+  - you can use `time-window` to make unbounded data bounded, that is, if 2022 records are fetched then avg for that year is calculated and will not change, however, new data for 2023 can still be unbounded and keep coming.
+    - `fixed` - like 1 min, no overlapping
+    - `sliding` - of 1 min, slides 10s, has overlapping
+    - `session` - no time bound but event based, like log in to log out activity.
+    - also the time can be `event-time`, `ingest-time` or `processing-time`
+
+- **Producing and consuming with Python**
+  - use library
+  - import producer and consumer
+  - add servers and topics, collect recept as callback.
+  - `from confluent_kafka import Producer`
+
+    ```python
+    from confluent_kafka import Producer
+    def receipt(err,msg):
+        ...
+
+    p=Producer(..)
+    p.produce('users',m.encode('utf-8'),callback=receipt)
+
+    from confluent_kafka import Consumer
+    c=Consumer({... : ...})
+    c.list_topics().topics
+    t.topics['users'].partitions
+    c.subscribe(['users'])
+    while True:
+      msg=c.poll(1.0)
+      ...
+    
+    c.close()
+
+    ```
+
+
+
+
+## Apache Airflow
+
+- _workflow management platform_ for data engineering pipelines
+- workflow manage, can be distributed. used DAGs.
+- create your data flows using pure Python.
+- The default database for Airflow is SQLite. This is acceptable for testing and running on a single machine, but to run in production and in clusters, you will need to change the database to something else, such as PostgreSQL.
+
+
+## Apache NiFi
+
+- a framework for building data pipelines, used DAGs.
+- looks like informatica on the web.
+- NiFi allows you to build data pipelines using prebuilt `processors` that you can configure.
+
+- processors are -
+  - `GenerateFlowFile` - generates file
+  - `PutFile` - saves file
+  - `ExecuteSQL` - executes sql connecting to JDBC.
+  - you can configure properties of processor
+  - create a connection
+  - specify a relationship
 
-- **Apache Flink** - _stream-processing_ and batch-processing framework
+- clustering and the remote execution of pipelines?
 
-- **Apache Storm** - distributed _stream-processing_
 
-- **Apache Spark Streaming** - distributed _stream-processing_. Extension of core framework.
+## Apache Zookeeper
 
-- **Apache Airflow** - workflow management platform for data engineering pipelines
+- manages information about the clusters of kafka.
+- elects leaders
+- zookeeper is also installed in clusters.
 
-- Links
-  - [Medium - Stream Processing Frameworks and differences](https://medium.com/@chandanbaranwal/spark-streaming-vs-flink-vs-storm-vs-kafka-streams-vs-samza-choose-your-stream-processing-91ea3f04675b)
 
+## Apache Flink
 
+_stream-processing_ and batch-processing framework
 
 
+## Apache Storm
 
+distributed _stream-processing_
 
 
+## Apache Spark Streaming
 
+distributed _stream-processing_. Extension of core framework.
 
 
+## Links
 
+- [Medium - Stream Processing Framworks and differences](https://medium.com/@chandanbaranwal/spark-streaming-vs-flink-vs-storm-vs-kafka-streams-vs-samza-choose-your-stream-processing-91ea3f04675b)
 
 
 
diff --git a/docs/2-Data-Engineering/data-python.md b/docs/2-Data-Engineering/data-python.md
index 9bef347..c54691f 100644
--- a/docs/2-Data-Engineering/data-python.md
+++ b/docs/2-Data-Engineering/data-python.md
@@ -9,7 +9,7 @@
 
 ### PyODBC
 
-Works with most databases but **not** well with MSSQL+Pandas. For MSSQL+Pandas use sqlalchemy MSSQL engine.
+Works with most databases but not well with MSSQL+Pandas
 
 ```python
 
@@ -50,53 +50,32 @@ fx_df = pd.read_sql(query, connection)
 
 Works as connection engine as well as ORM
 
-```python
-
-import sqlalchemy
-
-connection_url = "mssql+pyodbc://server_name\schema_name/database_name?driver=SQL+Server"
-
-## MS SQL
-connection_url = "mssql+pyodbc:///?odbc_connect="+urllib.parse.quote('driver={%s};server=%s;database=%s;Trusted_Connection=yes')
-
-## Postgres
-connection_url = "postgresql://user:pass@server:port/database"
-
-engine = sqlalchemy.create_engine(connection_url, echo=False)
-connection = engine.connect()
-sql = "select top 10 * from [db].[schema].[table]"
-cursor = connection.execute(sql)
-res = cursor.fetchall()    # list of rows 
-connection.close()
-
-# OR -- 
+  ```python
 
-with engine.connect() as connection:
-connection.execute("UPDATE emp set flag=1")
+  import sqlalchemy
 
-df.to_sql('table_name', con=engine, schema='dbo', if_exists='append', index=False)
+  connection_url = "mssql+pyodbc://server_name\schema_name/database_name?driver=SQL+Server"
+  
+  ## MS SQL
+  connection_url = "mssql+pyodbc:///?odbc_connect="+urllib.parse.quote('driver={%s};server=%s;database=%s;Trusted_Connection=yes')
+  
+  ## Postgres
+  connection_url = "postgresql://user:pass@server:port/database"
 
+  engine = sqlalchemy.create_engine(connection_url, echo=False)
+  connection = engine.connect()
+  sql = "select top 10 * from [db].[schema].[table]"
+  cursor = connection.execute(sql)
+  res = cursor.fetchall()    # list of rows 
+  connection.close()
 
-## Transactions in v1.45
+  # OR -- 
 
-engine = sqlalchemy.create_engine(con_mssql)
-connection = engine.connect()
-trans = connection.begin()
+  with engine.connect() as connection:
+    connection.execute("UPDATE emp set flag=1")
 
-try:
-    connection.execute(sql_mark_unpivoted)
-    df.to_sql(name='dv_flows_big', con=connection, if_exists='append', index=False)
-    # connection.execute('err') # <-- triggers error
-    trans.commit()
-    logger.info(f"Transaction of update and load completed successfully.")
-    logger.info(f"Data loaded to dv_bigflows, shape: {df.shape}")
-except Exception as e:
-    logger.error(f"Update and load failed! Rolling back. Error: {e}")
-    trans.rollback()
-    logger.error(f"Rolled back!")
-finally:
-    trans.close()
-```
+  df.to_sql('table_name', con=engine, schema='dbo', if_exists='append', index=False)
+  ```
 
 ### Flask_sqlalchemy
 
@@ -130,7 +109,7 @@ connection_url = 'sqlite:///' + os.path.join(basedir, 'data.sqlite')
 ### Pandas
 
 - needs a connector to database like sqlalchemy or pyodbc
-- `df_txns = pd.read_sql(sql=sql, con=conn_dsn)`
+- `df_txns = pd.read_sql(sql=sql, con=conn_dvs)`
 - `df.to_sql('table_name', con=engine)` - sqlalchemy
 
 - pd write has issues
@@ -149,30 +128,6 @@ rows = cursor.fetchall()
 connection.commit() # for non read tasks
 connection.close()
 
-
-"""
-    Transactions in SQLite
-    Works file for sql statements, 
-    but not for Pandas df.to_sql() as it commits as another transaction.
-"""
-connection = sqlite3.connect(sqlite_path)
-connection.isolation_level = None
-try:
-    c = connection.cursor()
-    c.execute("begin")
-    c.execute("some update")
-    c.execute("some insert")
-    #c.execute("fnord") # <-- trigger error, above update and insert will rollback
-    df.to_sql(name='orders', con=connection, if_exists='append', index=False)
-    #c.execute("fnord") # <-- trigger error, now, it raises exception, but pandas did the commit.
-    connection.commit()
-    logger.info(f"Transaction of update and load completed successfully.")
-except connection.Error as e:
-    logger.error(f"Update and load failed! Rolling back. Error: {e}")
-    connection.rollback()
-finally:
-    connection.close()
-
 ```
 
 ### mysql-connector-python
@@ -247,13 +202,6 @@ class Author(Base):
   - define the **class model**, `Author` for `author` database table
   - This uses SQLAlchemy ORM features, including `Table`, `ForeignKey`, `relationship`, and `backref`.
 
--- **Querying**
-
-```py
-query.filter(people.student.is_not(None)) 
-query.filter(people.student.is_(None)) 
-```
-
 - **Links**
   - <https://realpython.com/python-sqlite-sqlalchemy/#working-with-sqlalchemy-and-python-objects>
   - Flask SQLAlchemy in Flask Notes
@@ -318,34 +266,6 @@ class Post:
 
 
 
-```
-
-## SQLite ETL
-
-- when **reading CSV** you can read data in correct data-type by specifying `dtype` and `thousands` and `parse_dates`.
-- when **adding** a column use proper data-type to new column has required format. eg, use `pd.to_datetime()` to add date column.
-- when **saving to SQL**, pandas creates tabes with data-type similar to pandas columns type.
-- when **reading a SQL**, pandas _might not_ read in proper date format. Again use `parse_dates` to fix it.
-
-```py
-# Data Types
-dtype_ = {
-    'Quantity' : 'int64',
-    'Amount' : 'float64',
-}
-dv_date_parser = lambda x: pd.to_datetime(x, format="%d/%m/%Y", errors='coerce')
-
-df = pd.read_csv(file, low_memory=False, dtype=dtype_, thousands=',',
-                 parse_dates=['Date'], date_parser=dv_date_parser)
-
-# Add datetime type column
-df['load_datetime'] = pd.to_datetime(arg='now', utc=True)
-
-# To SQL
-df.to_sql(name='orders_staging', con=conn_target, if_exists='append', index=False)
-
-# Read SQL
-df = pd.read_sql(sql=sql_read, con=conn_target, parse_dates=['Sale Date', 'load_datetime'])
 ```
 
 ## PyODBC ETL
diff --git a/docs/2-Data-Engineering/data-solutions.md b/docs/2-Data-Engineering/data-solutions.md
index 43eb5b4..0606a24 100644
--- a/docs/2-Data-Engineering/data-solutions.md
+++ b/docs/2-Data-Engineering/data-solutions.md
@@ -5,7 +5,11 @@ date: 2022-09-05
 
 # Data Solutions
 
-_Here are all the "conceptual" notes related to data soulutions, archirecture and engineering. It can have links to technical notes._
+_Here are all the "conceptual" notes related to data soulutions, archirecture and engineering. It can have links to practical notes._
+
+**Data engineering** is the development, operation, and maintenance of data infrastructure, either on-premises or in the cloud (or hybrid or multi-cloud), comprising databases and pipelines to extract, transform, and load data.
+
+**Data engineers** need to be knowledgeable in many areas – programming, operations, data modeling, databases, and operating systems. The **breadth** of the field is part of what makes it fun, exciting, and challenging.
 
 ## Data Strategy
 
@@ -13,15 +17,15 @@ To meet medium or long term business objectives, many aspect of organisation nee
 
 **Steps to build a Data Strategy**
 
-- understand the business objectives. Eg, how often are email responded. why the money is going?
+- understand the **business objectives**. Eg, how often are email responded. why the money is going?
 - assess how data is stored and consumed in org.
-- understand current data challenges. Eg, not being captured. isolated availability with no link up or down. non traceble data. stale data. not connected to pipeline or lake.
+- understand current **data challenges**. Eg, not being captured. isolated availability with no link up or down. non traceble data. stale data. not connected to pipeline or lake.
   - how can you collect data, apply data
 - work with business to define optimum target state to meet business objectives, incorporating
   - data architecture and engineering
   - data management and operating model
   - data analytics, reporting and visualization - or business intelligence
-- build a road map for data journey, define actionable data strategy.
+- **build a road map** for data journey, define actionable **data strategy**.
 
 ```mermaid
 flowchart LR
@@ -56,8 +60,7 @@ end
 
 ## Data Architecture
 
-
-*Now that you have a strategy with known challenges and a roadmap to target state, it is time to build the architecture and do the engineering work aligned to roadmap to rach the target state.*
+_Now that you have a strategy with known challenges and a roadmap to target state, it is time to build the architecture and do the engineering work aligned to roadmap to rach the target state._
 
 Data Architecture defines the **blueprint** for managing data from **collection** to **storage** and **transformation** to **consumption**. It is base foundation to support business objectives. It is essential to determine the sharp and quick tools that solve the purpose.
 
diff --git a/docs/2-Data-Engineering/databases.md b/docs/2-Data-Engineering/databases.md
index dd9e4fa..a8e63ed 100644
--- a/docs/2-Data-Engineering/databases.md
+++ b/docs/2-Data-Engineering/databases.md
@@ -7,6 +7,30 @@ date: 2019-05-06
 
 *all about databases, SQL only*
 
+**Database** lets store data ususally in tabular format.
+
+## Type of Databases
+
+- **Relational Databases**
+  - store in rows. Eg, MySQL, `PostgreSQl`
+  - good to store transactions and build relationships.
+  - Eg, **PostgreSQL** is relational db.
+
+- **Columnar Databases**
+  - used commonly in warehousing. eg, Amazon Redshift, Google BigQuery, Apache Cassandra.
+
+- **NoSQL Databases**
+  - They do not have defined structure and are document based.
+  - **Elasticsearch** - a search engine based on `Apache Lucene` and can be used as nosql db.
+  - **Apache Kibana** - adds GUI to ElasticSearch.
+
+## PostgreSQL
+
+- **PostgreSQL** is relational db.
+  - `pgadmin4` is GUI to it.
+  - login with linux user on the server as `username@server`
+  - then add server, expand it, see database, schemas, public.
+
 ## SQLite
 
 - It is a micro database that can work in memory or a saved in file, eg, `store.db` .
@@ -108,3 +132,17 @@ WHERE EmailPromotion = 2
 
 - `redis-server` to start the server.
 
+## Elasticsearch
+
+- **Elastic Search** is a search engine based on `Apache Lucene` and can be used as nosql db.
+- you can set cluster and node in elastic search.
+- once installed it doesn't has GUI but is rather an API. at `localhost:9200` you can see JSON out from API
+- uses JSON Query called Elastic Query DSL (Domain Specific Language)
+
+- **Apache Kibana**
+  - adds GUI to ElasticSearch.
+  - you can use it to build visualizations and dashboards of your data held in Elasticsearch.
+  - after installation it can be accessed on `http://localhost:5601`
+  - you can create viz from index in elasticsearch. viz is widget that can be added to dashboard. dashboard filter filter all widget if field is present.
+  - `developer tools` is scratchpad area where you can crate and query data/indices.
+
diff --git a/docs/2-Data-Engineering/pandas.md b/docs/2-Data-Engineering/pandas.md
index b8c2311..5171ddb 100644
--- a/docs/2-Data-Engineering/pandas.md
+++ b/docs/2-Data-Engineering/pandas.md
@@ -1,227 +1,9 @@
 # Pandas
 
-- `df.T` - Transposes dataframe
 
-## Read CSV
-
-```py
-
-# Data Types
-dtype_ = {
-    'Quantity' : 'int64',
-    'Amount' : 'float64',
-}
-
-# Date Parser
-my_date_parser = lambda x: pd.to_datetime(x, format="%d/%m/%Y", errors='coerce')
-
-# CSV with proper data and currency read
-df = pd.read_csv(file, low_memory=False, dtype=dtype_, thousands=',',
-                 parse_dates=['load_date', 'transaction_date'], date_parser=my_date_parser)
-
-# find date range in huge CSV
-df = pd.read_csv(filepath, usecols=['Date'], parse_dates=['Date'], date_parser=my_date_parser)
-df.Date.max(), df.Date.min()
-
-```
-
-## Select data
-
-- [Indexing and Slicing Data - pandas.pydata.org](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html)
-
-- You can select data by _index_ or _label_. Not only select by also **assign**, that is **write** data.
-- `loc` take real labels, that is column-names, and row-names. Does not take index.
-- `iloc` takes indexes, and also in different sq brackets can take labels.
-- `at` and `iat` are for accessing `only single value` in dataframe.
-- no function just slicing, `df[2:4]` works only on row indexes.
-
-```py
-df.iloc[1:3,2:4] # pass row,column, takes index in same way as list.
-df.iloc[1:3][2:4] # same as above
-df.iloc[:,[2,4,6,22]] # all rows and specific index cols
-df.iloc[1:3]['product','category'] # works
-
-df.loc[1:3,'category']
-df.loc[1:3,'product':'value'] # shows all columns between product and value
-df.loc[1:3,['product','value']] # shows only prod and value, not in betweens.
-df.loc[1,'product'] # returns cell, as the format of column, str, int.
-
-df[2:20:2] - # row 3 to 20 with freq2, that is every alt row
-```
-
-## Filter Conditions - Where clause
-
-```py
-df = df[df.column_name != value]
-df[df.column_name.isin(values_list)]
-df[~df.column_name.isin(values_list)] # reverse of condition
-df = df[(df.height != 11) & (df.age != 31)] # multiple conditions
-```
-
-## Delete Data
-
-- Ops1: You can use where-clause, then index slicing and copy result to df_new variable
-- Ops2: Use `df.drop` and pass indexes to drop. To find indexes to drop, use `query` or df-slicing.
-
-```py
-# Ops1: find True-False list, slice dataframe, store results
-df_new = df[df.col_name != value]
-
-# Ops2: where value is 0, slice dataframe, get index, pass to drop as row indexes to drop, do in place.
-df.drop( df[df['value']==0].index, inplace=True)
-```
-
-## Modify Data - Updates / Inserts
-
-- the way you select cell/column, there you can assign value to make updates or inserts
-
-```py
-df.loc[4,'emp_name'] = 'John' # updates cell at row_index 4 and column "emp_name" with value 'John'
-
-
-
-```
-
-## Summarize / Aggregate Data
-
-- `df.product.nunique()` - unique products in dataframe. [pandas.Series.nunique](https://pandas.pydata.org/docs/reference/api/pandas.Series.nunique.html)
-- `df.product.value_counts()` - unique products and count of records for each. [pandas.Series.value_counts](https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html)
-
-## Group Data
-
-- aggregation functions - mean, sum, count, size, max, min, first, last. alos, `agg`
-- sorting - `.sort_values(by='count',ascending=False).head(20)`
-
-```py
-# Groupby multiple columns & multiple aggregations
-result = df.groupby('Courses').aggregate({'Duration':'count','Fee':['min','max']})
-
-#Create a groupby object
-df_group = df.groupby("Product_Category")
-#Select only required columns
-df_columns = df_group[["UnitPrice(USD)","Quantity"]]
-#Apply aggregate function
-df_columns.mean()
-
-```
-
-## Create New Columns or Calculated Fields
-
-```py
-# applying col wise, so x is a row and function will apply column wise on each row
-
-def build_some(x):
-    fname = x['first_name'].strip().title()
-
-    if pd.notnull(x['surname']) and len(x['surname']) > 0:
-        name = fname + ' ' + x['surname']
-
-    return (name, fname)
-
-df[ ['name', 'fname'] ] = df.apply(lambda x : build_some(x) , axis = 1, result_type='expand')
-
-
-## ID col
-sql_max_id = """
-select max(id) as max_id from [dbo].[employee]
-"""
-
-connection = sqlalchemy.create_engine(connection_string, echo=False)
-df_max_id = pd.read_sql(sql=sql_max_id, con=connection)
-
-# hold max id in table
-max_id = df_max_id.iat[0,0] or 0
-
-# adds id from max to new lenght
-df_users_dvs['id'] = range(max_id+1, max_id+1+len(df_users_dvs))
-
-# Ideally use, auto-increment / Identity
-```
-
-
-
-## Date Operations
-
-```py
-df_wm['date_wm'].dt.year # year from datetime
-```
-
-## Renaming Column Names
-
-```py
-# cleans column names and handles renaming same column name
-# replaces space and - with _, adds index to same name.
-
-col_list = []     # holds names to check duplicates
-renamer = dict()  # holds col name and its number of duplicates
-for col in df.columns:
-    new_col_name = col.lower().replace(' ','_').replace('-','_')
-    if new_col_name in col_list:
-        #rename 
-        index = int(renamer.get(new_col_name,0) )
-        renamer[new_col_name] = index + 1
-        new_col_name += '_'+ str(index + 1)
-        pass
-    else:
-        col_list.append(new_col_name)
-    print(f'col: {col}, new: {new_col_name}')
-    #df.rename(columns = {col: new_col_name}, inplace=True)
-```
-
-## Plot in Pandas
-
-- Params
-  - `kind`:str - The kind of plot to produce:
-    - 'line' : line plot (default)
-    - 'bar' : vertical bar plot
-    - 'barh' : horizontal bar plot
-    - 'hist' : histogram
-    - 'box' : boxplot
-    - 'kde' : Kernel Density Estimation plot
-    - 'density' : same as 'kde'
-    - 'area' : area plot
-    - 'pie' : pie plot
-    - 'scatter' : scatter plot (DataFrame only)
-    - 'hexbin' : hexbin plot (DataFrame only)
-
-```py
-import pandas as pd
-import matplotlib.pyplot as plt
-# ...
-df.plot(kind = 'scatter', x = 'Duration', y = 'Calories')
-plt.show()
-```
-
-## pandas.DataFrame.query
-
-- [pandas.DataFrame.query](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html)
-- pass where condition
-
-```py
-df_new = df.query('sales != 0')
-```
-
-## pandas.DataFrame.drop
-
-- [pandas.DataFrame.drop](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html)
-- is used for dropping whole columns or rows
-- Params
-  - `labels` - list of index or column label, that is, index of row or column name.
-  - `axis` - 0 for row, 1 for columns. default=0 or row.
-- Returns
-  - DataFrame
-
-```python
-df.drop(labels=['Employee_Name2'], axis=1) # deletes column
-df.drop([0, 1]) # deletes row at index 0 and 1, that is, first and second row.
-```
-
-## pandas.to_datetime
 
 ```python
 df['date'] = pd.to_datetime(df['date_'], format='%d %b %Y')
-df['load_datetime'] = pd.to_datetime(arg='now', utc=True)
-
 ```
 
 - Links:
diff --git a/docs/9-Drafts/notepad.md b/docs/9-Drafts/notepad.md
index 755e4fe..9bf59e9 100644
--- a/docs/9-Drafts/notepad.md
+++ b/docs/9-Drafts/notepad.md
@@ -256,48 +256,3 @@ Follow:
 - miguel grinberg - <https://twitter.com/miguelgrinberg>
 - Claudio Bernasconi - <https://twitter.com/CHBernasconiC>
 
-## DWBI
-
-- DW is creating a dimentional model of data that lets users easily ask questions.
-- Data is identified as DIm and Facts then stored as star schema
-- Facts are aggregatabe, or can be factless
-
-- Dimensions have primary-key, natural-key, surrogate-key. PK is simple numerical increment.  
-  - The degenerate dimension is a dimension key without a corresponding dimension table. So a order-number in fact table is a dimension key without a dimention table.
-  - Natural Key has a meaning, like Emp-ID, while surrogate keys are numneric that start from 1 and increment by 1. These meaning less surrogate keys should be used for join between facts and dimensions.
-
-
-- Four Steps to do dimentional modelling
-  
-  - 1 - Identify the business process
-    - what do you want to understand
-    - Eg:
-      - as a retail owner, i want to customer purchases at POS, so that I can analyze products selling, stores and promotions.
-  
-  - 2 - Identify the grain
-    - lowest atomic grain is best because it its highly dimensional hence gives more information
-    - Eg:
-      - Retail - individual product on POS transaction
-  
-  - 3 - Identify the dimensions
-    - they are determined automatically once we have the grain identified, if dimension breaks the grain futher then discard it or revisit grain statement.
-  
-  - 4 - Identify the facts, anything not in same grain goes to another fact table.
-
-
-- Date Dimension
-  - it has date attributes like `Date Key (PK)`, `Date`, `Day of Week`, `Holiday Indicator`, `Weekday Indicator`, `Day in Month`, `Day in Year`, `Last Day in Month Indicator`, `Week Ending Date`, `Week in Year`, `Month Name`, `Month in Year`, `Year-Month (YYYY-MM)`, `Quarter`, `Year-Quarter`, `Year`
-  - date is stored separately as dimension because it can help keep date-calculations in advance. Eg, a date can have, different formats, is-weekday?, is-holiday?, week-number, day-in-year?, day-of-week and many more. it helps to keep calendar-logic in dimension rather tahn application. Can have holiday indicator. Roughtly 20years of date can be listed in 7,300 rows.
-  - time-of-day - it should be date-time fact in fact table to avoid explosion of date dimention, if required to keep it as dimension, it can be separate dimension as time-of-day.
-
-- Product Dimension
-  - it can have attributes relate to procust like `Product Key (PK)`, `SKU Number (NK)`, `Product`, `Brand`, `Subcategory`, `Category`, `Department Number`, `Department`, `Package Type`, `Package Size`, `Fat Content`.
-  - it has merchandise hierarchy flattened out. Typically, individual SKUs roll up to brands, brands roll up to categories, and categories roll up to departments.
-  - List-Price is numeric but can be in dim as it is not additive and doesn't change on event, or is not event driven. It can be added once we know qunitity or weight at event. Or for some case it can be stored both in fact and dimension.
-  - a typical product dimension can have 50+ attributes and 300,000+ SKUs.
-  - Master File - In large grocery business, there can be a product master file where you can manage all products for all stores and then individual stores can pull a subset from it.
-
-- Store Dimension
-  - It can store attributes like `Store Key (PK)`, `Number (NK)`, `Name`, `Street Address`, `City`, `County`, `City-State`, `State`, `Zip Code`, `Manager`, `District`, `Region`. You can see there is a hierarchy here.
-
-- Promotion Dimension
